#+title: Intro Outline

\section{In Defense of Inconsistency}

If you ever want to be right, it is important to acknowledge that you might now be wrong.  And not only might you be wrong in the margins---wrong about the details of a calcultion or underinformed---you could be fundementally misguided about how the world works. You could be using meaningless concepts, confusing cause and effect, and misplacing your trust in people and institutions that seek to manipulate you. Worst of all, your own internal calculations may not even be self-consistent.

If there is one thing on which we can all agree, it is a contempt for inconsistency.
The foundations of microeconomics have taught us that we are entitled to our own tastes (and beliefs, to some extent), but only if they at least adhere to an internal logic.
Otherwise we can be easily cheated \cite{RAT}, manipulated \cite{explosion}, and humiliated \cite{logical falacy, empty set thesis}.

That is why theoretical computer scientists are careful to prove things about clearly defined models,
and it is why applied computer scientists build artificial agents that are are consistent by construction:
  it grants an invulnerabilty to our one common enemy: inconsistency.

Yet this invunlerability comes with shackles.
Would \emph{you} trade your freedom to think in weird and backwards ways, to puzzle over paradoxes, to experience deep surprise and be challenged by internal conflict---for a guarantee of self-consistency?

In order to always present a consistent front, it is necessary to fully process new information before moving on. Mulling is prohibited, as
is waiting for clarification.
Half-baked ideas are a nonstarter.
This regimented consistency-first lifestyle works very well when your observations only tell you're mistaken in the margins and only need to update a few parameters here and there.
But what if you're wrong in a more fundemental way?
Can you imagine how hard it would be to reconceptualize things without ever venturing into an inconsistent state?
Imagine having to seriously refactor a large codebase, with an interface that only allows you to see code that compiles.
So if (by no fault of your own) you do happen to be wrong about how the world works---that is, really, fundementally wrong---then taking on inconsistency may be worthwhile.

By way analogy, we all agree debt is bad, but would you willingly give up your freedom to take on debt?
Many of the most powerful people and institutions in this world would not.
Running a budget surplus works well, until you fall ill or need to buy a house.

For analogous reasons, modern AI systems have been slipping away from the consistency that they aspire to.
For instance, it is expensive and even undesirable to exactly model your training data; instead we try to do the opposite, by adding priors (also known as regularizers; see \cref{sec:regularizers}) to prevent this.
The priors are typically inconsistent with the data, which is arguably the point.
The result is a far more robust learning process, both theoretically \cite{ftrl} and practically \cite{regularizers...}.
#
#
These techniques are billed as more efficient approximations to the consistent thing we really mean.
But I argue that we should just call them what they are: inconsistent belief states.

# Furthemore, that's how they aspire to be. In practice, inference can be difficult, and we cut corners, so our systems are not always fully consistent.

It is said that a man with two watches never knows the time.  But a man with only one watch

The  of inconsistency is redundancy.


But how can we quash our shared if we cannot see them?

Inconsistency may be our one shared enemy, and doing battle with it may be a universal good, but the battle is only worth fighting with a certain code of contact.
It's very easy to be consistent: simply discard your beliefs.

Inconsistency is a reasource that must be managed responsibly.

Being able to tolerate

(( Perhaps even wrong that your mental state is as coherent as it semes to you. ))


Since we are finite beings with limited, we must not dismiss the possibility of being wrong in a way that will fundementally challenge us---at least, not if we aim to grow.






Internally conflicted.
Internal conflict should not de debilitating.
Taking that possibility seriously is
Once you accept the posibility of being wrong,

Much of computer science is about reducing one problem to another,

 - Computer Science is about communicating with computers.
   - Algorithms is the field that studies which useful tasks can (or cannot) be translated into computer code.
   - (( Complexity theory is about the relationships between computational problems, and how much we can expect from an algorithm ))
   - Security is about making sure we don't communicate too much to computers.
   - And machine learning is about communicating to


 - Part of that communication is in writing down a model.
 - Probabilities have won. It is worth trying to understand why.
   - (Hint: it's because they can represent everything)


Understanding The Relationships Between Things.
Understanding relationships between concepts is important. If you are workng on X and your neighbor is working on X from the opposite perspective, it is often useful to share notes. It also guards against the possibility of reinventing the wheel many times under different names, a problem that becomes inceasingly visible as computer science continues to grow. It is already almost impossible to read all of the good papers in a
    (( but why does this have anything to do with PDGs or my PhD work?  Because it connects things. But it's just another paper.  The reason it's nice to connect things is that this tells us where stuff is coming from.))


Declarative Modeling.



What makes one modeling language better than another?

 - Generality. If everything B can do A can do also, then A is at least as expressive as B.
   But by this metric alone, there is nothing better than set theory or assembly language.

 - Easy to compute with. There should be a reasonably fast way to use the langauge to "do stuff".

 - Naturality and Simplicity in standard use cases.  If languages A and B are essentially the same, except that it take a lot more time to say everything with language A, then language B is simpler and more efficient

 - A Good Prior.

* INTRO DRAFT

One way
