
\subsection{Additional Structure Preserved by PDGs}
\subsubsection{Monoidal Structure}

If we allow unions of variables at the qualitative level, this means we are working in a different category $\Ar^{**}$ whose objects $\ob \Ar^{**}$ are all subsets of $\N$, and equipped with down-projection morphisms, and joining hyperarcs.
Every PDG $\dg M : \Ar^* \to \Meas_{\underline\Delta}$ can be naturally lifted to a PDG 
$\dg M^{+\X}: \Ar^{**} \to \Meas_{\underline\Delta}$
in the obvious way: taking the additional joint variables to the appropriate product of measurable spaces, and treating their downprojections ($\mat X \tto \mat Y$, for $\mat X \supseteq \mat Y$) appropriately. 
We already saw one consequence of this change: the limits of such a PDG must be internally coherent in a certain way; the represent not the local marginal polytope, but the global marginal polytope.
% .  In particular, a cone over $1$ must contain a marginal $\mu_{\X}$ over all variables that must agree with all other marginals.  So, in this context, it suffices to summarize the limit with this one distribution. 

\paragraph{Variable Union as Monoidal Structure, for $\Ar^{*,\mathit{hold}}$}
Usually people call the a monoidal operation ``tensor'', but we will now define a monoidal operation that does not line up with the usual tensor product. 
On objects, which are sets of variables, $\odot$ behaves like union, and on morphisms, it simply multiplies densities.
% $\ovee$
\begin{align*}
    A \odot C &= A \cup C; \\
    \odot( p(B|A), q(D|C) )
    &:= p(B|A) \cdot q(D|C)\\
    &= (a_0, y, c_0) \mapsto 
        \Big(
        (b_0, z, d_0) \mapsto p(b_0, z|a_0,y) q(d_0,z|c_0,y)
        \Big).
\end{align*}
where $z$ gives the values of the common variables $A \cap C$, so that $a = (a_0, z)$ and $c = (c_0,z)$.

If $A$ and $C$ are disjoint, as are $B$ and $D$, then $\odot$ coincides with the tensor product $\otimes$. 
When they share variables, the resulting operation does something different: instead of having two distinct copies of eacn input and output, $p \odot q$ takes in only one copy of each shared sources, and produces a density over only one copy of their shared targets. 
For example $p(X) \odot p(X)$ is the subprobability $p(X)^2$. 
It is worth noting that a morphism $p(Y|X)$ is idempotent, in the sense that $p(Y|X) \odot p(Y|X)$ iff it is a deterministic function. 

Although $\odot$ is well-defined, it is not functorial in all cases,
\unskip\footnote{For example, $p(Y|X) \odot p(Y|X) = p^2(Y|X)$ which is a strict subprobability measure, while $(q \circ p)(Z|X)$ and $(r \circ p)(W|X)$ are both cpds, and $(q \circ p) \odot (r \circ p)$ will also be a cpd on $W \cup Z$, supposing that  $W$ and $Z$ are disjoint. So it cannot be the case that $(q \circ p) \odot (q \circ)$
}
and hence inadmissible as the basis of a monoidal structure for the full category of stochastic morphisms
.
However, if we restrict to the subcategory of purely generative morphisms---%
that is, arcs $a$ satisfying $\Tgt a \supseteq \Src a$---then $\odot$ becomes functorial. 

{\color{red} CAREFUL! This messes up the types of composition! If $f : X \to Y$ is converted to $f' : X \to XY$ and $g : Y \to Z$ is converted to $g': Y \to YZ$, then $g' \circ f'$ is not defined (because $XY \ne Y$) and so composition cannot proceed, without first including a forget/downprojection map!}

There are also many other properties that we must verify in order to get a symmetric monoidal category; we now verify them.

\begin{itemize}[wide,]
\item \textbf{Functoriality.}
% The left and right are symmetric.
% Suppose $p(Z|X) = f(Z|Y) \circ g(Y|X)$, and take any $h(B|A)$. 
% We need to show that $p \odot h = (f \odot h)$ 
We need to show that
\[
\left(
\begin{tikzcd}
    X_1 \ar[d,"f_1"'] \ar[dd,bend left,"p"] \\ 
    X_2 \ar[d,"f_2"'] \\
    X_3
\end{tikzcd}
\right) 
\odot
\left(
\begin{tikzcd}
    Y_1 \ar[d,"g_1"'] \ar[dd,bend left, "q"] \\ 
    Y_2 \ar[d,"g_2"'] \\
    Y_3
\end{tikzcd}
\right) 
= 
\begin{tikzcd}[row sep=5ex]
    X_1 \cup Y_1 \ar[d,"f_1 \odot g_1"description]  \\ 
    X_2 \cup Y_2 \ar[d,"f_2 \odot g_2"description] \\
    X_3 \cup Y_3
\end{tikzcd}.
\]
As mentioned above, this is not true in general. But we have assumed that $X_1 \subseteq X_2 \subseteq X_3$ and $Y_1 \subseteq Y_2 \subseteq X_3$.
To simplify notation, let's redefine $X_3 \gets X_3 \setminus X_2$ and $X_2 \gets X_2 \setminus X_1$. 
In this new notation, our goal becomes proving the commutativity of the following diagram:
\[
\left(
\begin{tikzcd}
    X_1 \ar[d,"f_1"']  \\ 
    X_2 \cup X_1\ar[d,"f_2"'] \\
    X_3 \cup X_2 \cup X_1
\end{tikzcd}
\right) 
\odot
\left(
\begin{tikzcd}
    Y_1 \ar[d,"g_1"'] \\ 
    Y_2 \cup Y_1 \ar[d,"g_2"'] \\
    Y_3 \cup Y_2 \cup Y_1
\end{tikzcd}
\right) 
= 
\begin{tikzcd}[row sep=5ex]
    X_1 \cup Y_1 \ar[d,"f_1 \odot g_1"description]  \\ 
    X_2 \cup X_1 \cup Y_2 \cup Y_1 \ar[d,"f_2 \odot g_2"description] \\
    X_3 \cup X_2 \cup X_1 \cup Y_3 \cup Y_2 \cup Y_1
\end{tikzcd}.
\]
% Let $X_2^- := X_2 \setminus X_1$, $X_3^- := X_3 \setminus X_2$, and define $Y_2^-, Y_3^-$ analogously.
% We can then define
We now compute
\begin{align*}
    (f_2 \circ f_1) (x_1'')(x_1,x_2,x_3) 
         &= \iint_{X_2,X_1} f_1(x_2',x_1'|x_1'') f_2(x_3,x_2,x_1|x_2',x_1') \mathrm d x_1' \mathrm d x_2' \\
         &= \iint_{X_2,X_1} f_1(x_2'|x_1'') 
            f_2(x_3|x_2) \delta(x_1) \mathrm d x_1' \mathrm d x_2' \\
\end{align*}

\item 


\end{itemize}


\subsubsection{(Pre)additivity}

\subsection{The Category of PDGs}

        
\section{Dependency Graphs for Other Monads}

The Big Questions:
\begin{enumerate}
    \item How does the monadic view of composition (bind / multiply), which describes composition in the underlying Kleisli category, interact with the ``scoring function semantics''? 
    \item Is there an important shared feature among the monads $T$ for which analogues of PDGs work out?  To set up the scoring semantics, it seems we need
    \begin{enumerate}
        \item a way to quantify ``degree of funtional dependence'' along an arc $S \to T$, in the limit object $\lim \mathcal M$, and
        \item a way to quantify degree of between $T(X,Y,Z)$ and $X \to T(Y)$.
        
        If there is an analogue of marginalization, then there is a map $T(X,Y,Z) \to T(X,Y)$, and there is a 
    \end{enumerate}
    
    \item Since most monads do not construct continuous geometry as nicely as the probability monad, relations are not continuous, there is no obvious analogue of parallel, symmetric, ``mixture composition''. Even when we give a loss function semantics (which we do below) this does not correspond to an obvious computational picture in the same way. 
\end{enumerate}

\subsection{Relational Dependency Graphs}

We will use $\mathcal P$ do denote the relational monad. 


\paragraph{Scoring Function Semantics.}
Suppose that instead of mapping $\ed aXY$ to a conditional probability
$\p_a(Y|X)$, we instead map it to a relation $R_a(X,Y)$. 

The analogue of a scoring function might operate
on a universal reation $U \subseteq \V\!\X$, and look something like:
\begin{align*}
    \Inc( U ) &= \sum_{\ed aXY \mathrlap{\in \Ar}} \beta_a 
        % \thickD(U(A,B) || R_a(A,B))
        \Big\Vert U(X,Y) - R_a(X,Y) \Big\Vert_1
        \\
    &\quad\text{where}\quad
    \Big\Vert U(X,Y) - R_a(X,Y) \Big\Vert_1 = 
        % \sum_{x,y} \mathbbm1[R(x,y) \exists \mat z.~ U(x,y,\mat z)] |
        \#\Big\{(x,y) \in \V(X,Y) \,\Big|\, R(x,y) \Leftrightarrow \exists\mat z.~ U(x,y,\mat z) \Big\},
\end{align*}
with each $\beta_a \in \mathbb N$. 

The analogue of a qualitative arrow $\ed aXY$, indicating that one attribute determines another, also requires a scoring function.
The fact that the argument to $\IDef{\Ar,\balpha}$ 
    is a joint distribution may not be critical,
    so long as we can find a suitable replacement notion of uncertainty along an arc.
%     not critical to the calculation. 
% What \emph{is} important is that $\mu$ is associated with a
% notion of uncertainty $\H_\mu(T|S)$ along each $\ed aST$.
% For example, $\mu$ could be a database, 
% n place of a joint distribution, we could have a 
% universal $D \subseteq \V\!\X$, with 
One possible analogue of conditional entropy might then be
\begin{align*}
    \H_U(Y|X) &= \log 
    % \#\{ 
    % \mat x \in D.~~  
        % S(\mat x) 
    % \}
    (\text{maximum \# of possible values of $Y$ given $X$}) \\
        &= \max_{x \in \V X}
            \log \# \{ y \in \V Y ~|~
                \exists 
                \omega \in U.~~
                \Src \omega = s,\; \Tgt \omega = t
            \}.
        \\
        &= \max_{x \in \V\! X} \log \#\{ y \in \V Y \mid \exists \mat z.~U(x,y,\mat z) \}.\numberthis\label{eq:rel-condl-ent-analogue}
\end{align*}
This shares an important property with conditional entropy: it is zero iff the value of $Y$ is determined by $X$ in the relation.
%
% If $U$ is empty, then this quantity will be is non-negative
% If there are 
It is undefined when $U$ is empty, and otherwise non-negative. 
The other important property of conditional entropy is monotonicity with respect to weakening.
\unskip\footnote{%
    Clearly an extra target to a hyperarc (i.e., extending $Y$ to $Y' = (Y, Z)$) can only make \eqref{eq:rel-condl-ent-analogue} larger. 
Perhaps less obviously: it also has the property that adding an extra source (i.e., extending $X$ to $X'= (X,Z)$) can only reduce the value of \eqref{eq:rel-condl-ent-analogue}.
This is because if the maximum is over joint pairs $(x,z)$, then only the maximum number of $\{ y : (x,y,z) \in U\}$ contribute, while all such $z$ are amalgamated and make the number larger in the case of an existential quantifier.
}

Altogether, the analogue of $\IDef{}$ is 
\begin{align*}
\IDef{\Ar}^*(U) :=
    \sum_{a \in \Ar} \max_{x \in \V\! X} \log \#\{ y \in \V Y \mid \exists \mat z.~U(x,y,\mat z) \}
    - \log \# U
\end{align*}

Here, as in the probabilistic case, there is an interpretation in terms of storage costs. 
Suppose $U$ is fixed and known. The first term is the number of bits needed to specify separately each target given the source (knowing that the result is in $U$), while the second is the number of bits needed to specify an element of $U$ directly.
The value is undefined iff $|U| = 0$ because, intuitively, it is impossible to specify a joint setting $\omega \in U$ if $U$ is empty. 


Now, for some examples.
\begin{example}
    Suppose $\Ar = \{ {\to} X, \; Y{\gets} \}$. Then 
    $\IDef{\Ar}^*(U(X,Y)) \ge 0$ with equality iff $U(X,Y) = U_X(X) \bowtie U_Y(Y)$, for some unary relations $U_X(X)$ and $U_Y(Y)$. 
\end{example}

More generally, it can be shown that, for target-partitinal hypergraphs $\Ar$ without sources, the quantity $\IDef{\Ar}^*(U)$  measures how far a joint relation $U$ is from decomposing independently along the specified arcs. 

\begin{prop}
    Let $\{\X_1, \ldots, \X_n\}$ be a partition of $\X$, and $\Ar = \{\to \X_i\}_{i =1}^n$ be the hypergraph consisting of a single hyperarc pointing to each partition, each without any sources. 
    In this case, $\IDef{\Ar}^*(U) \ge 0$, with equality iff
    $U = U_1 \bowtie \cdots\bowtie U_n$ is the natural join of $n$ subrelations $U_i \subseteq \V(\X_i)$. 
    (In addition, in that case it must be that each $U_i = \prod_{\X_i}(U)$ is the projection of $U$ onto the variables $\X_i$.)
)
\end{prop}

Let's turn to some more complicated examples. First, let's start with overlapping targets, and keep everything unconditional. 
It is easy to see that
\[
\IDef{[{\to}X{\gets}]}^*(U(X)) = \log |U|,
\]
which is defined when $U \ne \emptyset$, non-negative on this domain, and equal to zero if and only if $U(X)$ is a singleton. Thus, it captures determinism (almost) exactly the same way as in the probabilistic case. 
Now, for a more complicated example. 

\begin{example}
    In the probabilistic setting, all $\mu(X,Y)$ are compatible with the hypergraph $[{\to} X {\to}Y]$, intuitively because optimal codes for specifying $(x,y) \sim \mu$ directly take the same amount of information as optimal codes to first specify $x$, and then use a code dependent on $x$ to specify $y$. 
    
    In this relational setting, the two are not always the same.
    % \begin{enumerate}
    %     \item
         If $U = \{(x,y) : x \in S, y = f(x) \}$, then
        $\H_U(X) = \log |S|$ and
        $\H_U(Y|X) = 0$, since there is always precisely 1 $y$ for which $(x,y) \in U$. Thus $\IDef{[{\to}X{\to}Y]}^*(U) = 0$. 
        Under the cost-of-storage interpretation: this means when $U$ can be generated by selecting a subset of $S$ and applying a (known) function, then specifying a joint sample $(x,y)$ requires the same number of bits as specifying $x$.
        
    A generalization of this holds for what might be called ``$k$-multi-functional'' relationships, where $k \ge 1$ is some natural number. Suppose $f: X \to Y^k$ produces $k$ distinct values of $Y$ for each $x \in X$. 
    If $U = \{ (x,y) : x \in S, y \in f(x) \}$, then $\H_U(Y|X) = \log k$, but also $|U| = k |S|$. 
    So again $\IDef{[{\to}X{\to}Y]}^*(U) = 0$.
    This is becaus, again, it takes the same number of bits to first specify a value of $X$, and then use that value to specicfy a value of $Y$, as it does to specify $(x,y)$ together. 
    It can be shown that, as a function of non-empty relations $U$, the value of $\IDef{[{\to}X{\to}Y]}^*(U)$ is non-negative and zero precisely if $U$ is of the form described above. 
    % \end{enumerate}
    
    This is beause, when $f$ may produce a variable number of points depending on $x$, $\IDef{[{\to}X{\to}Y]}^*(U)$ will be positive overall. So, when $x$ is such that $|f(x)|$ is maximal, then specifying fist $x$ and then the appropriate $y$, is less efficient than specifying $(x,y)$ together. 
\end{example}

More generally, for directed graphs, we have an analogue of a conditional independence.

\begin{defn}
    Suppose $A,B,C \subseteq \X$. In a relation $R(\X)$, $A$ and $B$ are said to be conditionally independent given $C$ 
    (symbolically, $R \models A \CI B \mid C$)
    iff 
    % for all $c \in \V C$, 
    \[
    \forall (a,b,c) \in \V(A,B,C).\qquad
    R(a,b,c) ~\iff~ \Big(~\exists a' \in \V (A).~ R(a',b,c) \land \exists b' \in \V (B).~R(a,b',c) \Big)
    .
    \]
    We write $A \CI B$ to abbreviate $A \CI B \mid \emptyset$, i.e., the special case where there are no given variables.
    % $\forall c \in \V C.~R(a,b,c) \$
\end{defn}

\begin{prop}
    $R(A) \models A \CI A$
\end{prop}


\begin{prop}
    Suppose $A,B,C$ are sets of attributes. 
    $R \models A \CI B \mid C$ 
    iff 
    $R \models (A \setminus C) \CI (B \setminus C) \mid C$.
\end{prop}
\begin{proof}
    First, we claim that, for all $a,b,c \in \V(A,B,C)$, we have that $R(a,b,c) \iff R(a[A\setminus C], b[B\setminus C],c)$.
    If $\{a,b,c\}$ agree on shared values, then $R(a[A\setminus C], b[B\setminus C],c)$ must equal $R(a,b,c)$, by definition.
    On the other hand, if $\{a,b,c\}$ do not agree on shared values, then $R(a,b,c) = 0$, and there are three possibilities for conflict.
    % and by definition there is some variable $X \in A \cup B \cup C$ such that. 
    If this is because $a$ and $c$ conflict, then it is possible that $R(a[A\setminus C],b,c) = 1$. {\color{red} contradicing our claim!}
    
    Let $A' := A \setminus C$ and $B' := B \setminus C$. 
    
        
    Suppose $R \models A \CI B \mid C$, meaning that for all $a,b,c$, 
    \begin{align*}
        R(a,b,c)
         &\iff \exists a'' \in \V (A).~ R(a'',b,c) \land \exists b'' \in \V (B).~R(a,b'',c) \\
        &\iff \exists a'' \in \V (A).~ R(a''[A\setminus C],b[B\setminus C],c) \land a''[A\cap C] = c[A \cap C]
            \\&\qquad\quad\land \exists b'' \in \V (B).~R(a[A\setminus C],b''[B\setminus C],c) \land b''[B\cap C] = c[B \cap C] 
            \\
        &\iff \exists a' \in \V (A').~ R(a', b[B'], c) 
            \land \exists b' \in \V (B').~R(a[A'],b',c).
    \end{align*}
    % $R(a,b,c)$ if and only if $\exists a' \in \V (A).~ R(a',b,c) \land \exists b' \in \V (B).~R(a,b',c)$.
    {\color{red} incomplete; possibly untrue}
\end{proof}

\begin{prop}
    % Suppose $\X_1, \X_2, \X_3$ are disjoint.
    $R(\X_1) \bowtie S(\X_2) \models \X_1 \CI \X_2 \mid \X_1 \cap \X_2$.
\end{prop}
\begin{proof}
    
\end{proof}

\begin{prop}
    If $G$ is a directed acyclic graph, then 
    $\IDef{\Ar_G}^*(U) \ge 0$, with 
\end{prop}

\begin{example}
    Now, consider the 2-cycle
\end{example}

}% end temphide
