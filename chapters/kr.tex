% REPRESENTING THINGS WITH PDGs
    \label{chap:repr-tools}

PDGs are extremely expressive.
We saw in \cref{sec:bn-pdg,sec:factor-graphs,sec:expfam} that PDGs
can capture graphical models such as Bayesian Networks and Factor graphs---but this is only the beginning. 
%
% This chapter, based on a collection of unpublished notes, 
In this chapter, we will see how a wide variety of other kinds of epistemic information can be captured with PDGs.


Probability is the dominant way that computer computer scientists,
statisticians, and economists
% micro\-economists
think about epistemic state.
This is due to standard betting arguments suggesting that any sufficiently rational agent (e.g., one resistant to dutch books)
must act as if it had probabilistic beliefs \citep{sep-dutch-book,savage}.
This leads us to make a simple but conceptually important observation: a joint probability distribution is itself a special case of a PDG (\cref{sec:prob-as-pdg}). 
% Nevertheless, not everything is probabilistic---at least, not 
% Many logics are based on binary truth/falsehood
%
\commentout{%
Some are built on binary truth and falsehood, and epistemic logics dealing with possibility and necessity are built on top of them.
Possibility is one important one; others include belief/plausibility measures, which have been touted as generalizations of probability. 
Perhaps surprisingly, many of these objects can be represented with PDGs as well.
}%

    % Thus, the standard epistemic state of a 
% This means that a standard
% So, because standard 
In \cref{sec:widgets}, we develop a library of more intricate tools, which we call \emph{widgets}, that can capture various modeling tools and fragments of epistemic information.
    % constraints, costs, couplings, 
% This will allow us to 
%
Doing so will enable us to deliver on the promises made in \cref{sec:alt-pdgs}, of being able to use the current PDG formalism to express seeming generalizations of PDGs. 

Probability, despite its dominance, does have shortcomings, and is not the only tool we have for representing knowledge and uncertainty \citep[\S2]{halpern-RAU}.
% Possibility is one important one; others include belief/plausibility measures, which have been touted as generalizations of probability. 
% Perhaps surprisingly, many of these objects can be represented with PDGs as well.
In \cref{sec:other-uncertainty-repr}, we will use the tools developed in \cref{sec:widgets} to show that PDGs capture some of these other well-established representations of uncertainty, most notably sets of probability measures, known as credal sets \citep{Walley1991-SRIP} (\cref{ssec:cvx-prob-sets}) and Dempster-Shafer Belief functions \citep{shafer1976mathematical} (\cref{ssec:belplaus-as-pdg}).
% In the remainder of the section, we 
We also give a quick overview of other representations captured by PDGs that will play larger roles later on: cluster (pseudo)marginals, causal models, and neural networks (\cref{ssec:capture-causal-models,ssec:implicit-neural-repr,ssec:pseudomarginal-cluster-graph}). 
% The key . We will explore this in 

\section{Probabilities and Random Variables}
    \label{sec:prob-as-pdg}


We start with an obvious construction
% that is nonetheless important to keep in mind
\unskip: a joint distribution can be viewed as a very special case of a PDG. 
Let $\X$ be a set of variables, and recall that $\V\!\X$ is the set of all joint settings of the variables $\X$. 
A joint distribution $\mu \in \Delta \V\!\X$ can be implicitly regarded as a PDG that has a single hyperarc $\emptyset \to \X$.
We attach $\mu$ as the (c)pd, and give it weights $\alpha=0$
%
(at least, if the distribution is intended to represent only purely observational information) and 
$\beta = 1$ (default confidence). 
% We will see more about these parameters later on. 

With this simple construction in mind,  
    let's revisit the a point made \cref{sec:var-randomvar}
    now that we can talk about PDGs. 
To a probability theorist, these joint distributions $\mu$ may seem to be of a very special form, because they are over product spaces. 
In probability theory, the setup is typically instead that one has a (measurable) set $\Omega$ of outcomes, and then random variables are in fact \emph{(measurable) functions} $X : \Omega \to \V\!X$. 
Observe that this too is an immediate special case of a PDG
in which $W$ is a variable with possible values $\V W := \Omega$:
%
\begin{center}
    \begin{tikzpicture}[xscale=1.6]
        % \coordinate (mergeorigin) at (3,2);
        \node[dpadded] (omega) at (3,2) {$W$};
        \foreach \i in {1,...,5} {
            \node[dpadded] (X\i) at (\i,0) {$X_\i$};
            \draw[arr2, shorten <=0pt, ->>] (omega) to (X\i);
        }
        % \draw[arr2,-,shorten >=0pt,shorten <=0pt] (mergeorigin) to +(0,0.5);
        % \node[above left=0pt and 4pt of mergeorigin]{$\mu$};
        \draw[arr2,<-] (omega) to node[above]{$\mu$} +(-1.3,0);
    \end{tikzpicture}
% \]
\end{center}
%
(In fact, this PDG happens to also be a BN, if one isn't worried about calling $W$, itself typically a product of variables, a variable.)
% No matter what confidence parameters we use, so long as $\beta_\mu > 0$, 
It is easy to see that these PDGs are consistent, and represent precisely the distribution $\mu$ and the set of random variables, no matter what $\bbeta > 0$ we select.

Of course, the semantics of PDGs developed in \cref{sec:semantics}
make heavy use of the usual definition of a joint distribution $\mu \in \Delta \V\!\X$, and so it would be circular to implicitly convert joint distributions to PDGs before developing the results of the previous chapter---%
but now that PDG semantics are on solid ground, we may freely regard a joint distribution as a PDG. 
% This will be useful to keep in mind later, 
%     but it is neither surprising,
%     nor is this property unique to PDGs. 
% Indeed, $\mu$ can also be viewed as a factor graph with only one factor. 
% What makes PDGs special is their ability to do other things at the same time, simultaneously keeping many balls in the air, so to speak. 
What makes PDGs special is their capability to do this while simultaneously representing other things. 

% \temphide{
% \paragraph{Updating by combining PDGs.}
% \TODO
% }

\paragraph{Updating with PDGs combination.}
We can regard a probability distribution $\mu \in \Delta\Omega$ as a PDG 
\begin{tikzpicture}[baseline=-3pt]
    \node[dpad0] (W) {$W$};
    \draw[arr2, <-] (W) to node[above,pos=0.65]{$\mu$} +(-1.1,0);
\end{tikzpicture} with $\V W = \Omega$.
At the same time, we can view
an observed event $U : \Omega \to \{0,1\}$ as a PDG
\begin{tikzpicture}[baseline=-3pt]
    \node[dpad0] (W) at (0,0) {$W$};
    \node[dpad0] (U) at (1.5,0) {$U?$};
    \draw[arr2] (W) to node[above]{$U$} (U);
    \draw[arr2, <<-] (U) to node[above,pos=0.65]{$\delta_1$} +(1.2,0);
\end{tikzpicture} with $\V(U?) = \{ 0,1\}$.
% , we can combine them.  
What happens when we combine them, to form a new PDG $\dg M := \mu + U$? 
Because the conditional distribution $\mu | U$ is the distribution that minimizes relative entropy from $\mu$, it follows that
\begin{itemize}[nosep]
    \item The unique element of $\bbr{\dg M}^*_0$ is $\mu | U$
    \item The observational inconsistency $\aar{\dg M}_0 = \I_\mu[U] = - \log \mu(U)$
        is the log probability of $\mu$ (\cref{prop:pdg-Ix})
\end{itemize}

More generally, if our observation had been not an event (i.e., a deterministic distribution over a binary variable), but some other high-confidence distribution over a variable $p(X)$, we would have found that the resulting optimal distribution was the result of applying Jeffrey's rule \citep{	  Jeffrey68} to update $\mu$ with $p$. 
This an immediate restatement of a standard result
    \citep[pg. 109]{halpern-RAU}.



\section{Widgets}
    \label{sec:widgets}

PDGs may be expressive, but they are structured objects with clear and specific specific syntax.
After some brief reflection, one might even find the syntax unnecessarily restrictive. 
Recall: in specifying the data for an arc $X \to Y$, one must specify a complete probability distribution $p(Y | x)$ over the values of $Y$ for \emph{every} value $x \in \V\!X$. 
% This has some serious implications---for instance
% Consequently, 
This appears to be a serious limitation.
For instance, if $X$ and $Y$ are binary variables, one might want to 
annotate an arc $X \to Y$ with data indicating that $X \implies Y$
    (i.e., if $X=1$ then $Y=1$).
% For $X{=}1$, everything works out, 
There are no problems in supplying a probability over $Y$ when $X{=}1$,
but unfortunately we also seem to be on the hook to provide
    a distribution over $Y$ when $X{=}0$. 
To take another example, what if we do not want to supply the full probabilistic information, but only whether or not it is possible that $Y{=}y$ given $X{=}x$? 
% Or perhaps we only know the marginals of a dist
% For instance, one might want to specify that $Z$ must be even when $X{=}0$, 
Farther afield, what if we want to model soft constraints, or distances, or couplings between marginal distributions?

% None of this information fits the syntax of a PDG---at least, so not at in any obvious way. 
At first glance, it appears that modeling any of these concepts might require introducing generalizations of the PDG formalism. 
Yet it turns out that each of these concepts can be compiled to a small PDG that exactly captures it, which we call a \emph{widget}. 
% When adding an arc $X \to Y$, we 
% As expressive as they are, 


\subsection{Relations and Constraints}
    \label{sec:relation-widget}
    \label{sec:constraint-widget}

A \emph{constraint} on a set $\Omega$ of possible worlds is a subset $C \subseteq \Omega$
of satisfying values, or equivalently, 
a function $C : \Omega \to \{\mathtt{true}, \mathtt{false}\}$ assigning to each $\omega \in \Omega$ a Boolean indicating whether or not it is allowed. 
%
% Although PDGs may seem as though they can only describe 
A probability distribution $\mu \in \Delta \Omega$ over 
    a finite set $\Omega$ encodes in particular a constraint
    on the possible values of $\Omega$ that can be observed---namely, that
they must be among $\Supp \mu = \{ \omega \in \Omega : \mu(\omega) > 0 \} \subseteq \Omega$. 
% \unskip\footnote{An analogous statement may be made when $\Omega$ is infinite, 
%     but we defer this discussion until \cref{sec:continuous-pdgs}.}
%
But is it possible to encode a constraint (in a PDG) without adding any additional probabilistic information?
%
% A relation $R(X_1, \ldots, )$
%
In a factor graph, the answer is yes: simply multiply by a factor that encodes the constraint $C$---a factor happens to be equivalent to the uniform distribution over $C$. 
% But this approach only works in a factor graph because factors lose their probabilistic interpretations in context. 
Unfortunately, this construction only works because factors lose their probabilistic interpretations in context.
The same approach does not have the effect we are looking for in a PDG, because beyond articulating the constraint, it also involves specifying a belief that all elements of $C$ were equally likely---information we do not have and would like to avoid assuming. 
In this section, we will develop a widget that will enable PDGs to represent precisely this information and nothing more. 

    
Recall that, in a PDG, the sample space $\Omega$ consists of joint settings of variables;
% variables $\V\!\X$,
a constraint on joint settings of variables is known more familiarly as a
\emph{relation} (see \cref{primer:relations}).
We have already seen how PDGs can encode certain relations, such as
in compiling a directed hypergraph to a graph (\cref{constr:hyperedge-reducton}),
where we managed to enforce a constraint on the triple of variables $(\var A, \var B, \var A\times \var B)$ ensuring that the value of $\var A \times \var B$ is always the pair consisting of the value of $\var A$ and the value of $\var B$.
But this is a very special kind or relation---an equality relationship between variables that already appear to have a clear structural relationship.
Can we encode arbitrary relational constraints with a PDG? 
It turns out we can.

% To a relation 
Consider a relation $R = R(\var{A}_1, \ldots, \var{A}_n)$ between the
    (values of) variables $\var A_1, \ldots, \var A_n$.
We associate $R$  with a PDG
% $\PDGof{R}$
\[
\PDGof{R} := ~~
    \begin{tikzpicture}[center base, scale=1.2]
        \node[dpadded] (A1) at (-1,0) {$\var A_1$};
        \node at (0,0) {$\cdots$};
        \node[dpadded] (An) at (1,0) {$\var A_n$};
        \node[dpadded] (T) at (0, -1.4) {$\Tru$};
        \mergearr[arr1,->>] {A1}{An}{T}
        \node [%
            below left=0pt and 3pt 
            % above= 1pt
            of center-A1AnT]{R};
		\draw[arr0, <<-] (T) to
			node[above, inner sep=2pt, pos=0.6]
				{\trut}
			+(-1.2,0);
    \end{tikzpicture},
\]
where $\Tru$ is a variable that intuitively represents ``truth'';
    it technically can take on two possible values:
     $\V\Tru := \{\trut, \truf\}$,
    but it always takes on $\trut$. 
      % the event $\Tru \!\!=\! \trut$,
Note that the arc labeled ``$R$'' is attached to the obvious way of regarding $R$ as a conditional probability distribution, i.e., the cpd $\delta R(\Tru \mid \var A_1, \ldots, \var A_n)$. 


% Just as we have simplified our notation by dropping the special variable 
We simplify notation by implicitly converting $R$ to the PDG $\PDGof{R}$ 
whenever it would be helpful to regard it as one, thereby identifying $R$ and $\PDGof{R}$. 
Furthermore, we also adopt the graphical notation 
that leaves $\Tru$ implicit, so that the binary relation $R(\var X,\var Y)$ and the
    constraint $C \subseteq \V \var W$ can be depicted graphically as
\begin{center}
    \begin{tikzpicture}[center base, scale=1]
        \node[dpad1] (X) at (-1,0) {$\var X$};
        \node[dpad1] (Y) at (1,0) {$\var Y$};
            
        \coordinate (crash1-pool) at (0,-0.35);
        \coordinate (crash1) at (0,-0.5);
        \draw[arr2,shorten >=0pt,-] (X) to[out=0,in=90] (crash1-pool);
        \draw[arr2,shorten >=0pt,-] (Y) to[out=180,in=90] (crash1-pool);
        \draw[arr2,shorten >=0pt,shorten <=0pt,->>] (crash1-pool) to (crash1);
        \draw[arr2,<<-,shorten <=0pt] (crash1) to +(0,-0.2);
            % \coordinate (crash2) at (3, 0);
            % \draw[arr2,->>,shorten >=0pt] (W) to (crash2);
            % \draw[arr2,<<-,shorten <=0pt] (crash2) to +(0.2,0);
    %
        \node[above=2ex of crash1] {$R$};
    \end{tikzpicture}
    \hspace{2em}\text{ and }\hspace{2em}
    \begin{tikzpicture}
        \node[dpad1] (W) at (0,0) {$\var W$};
        \coordinate (crash2) at (1, 0);
        \draw[arr2,->>,shorten >=0pt] (W) to (crash2);
        \draw[arr2,<<-,shorten <=0pt] (crash2) to +(0.2,0);
        \node[above=2pt of crash2]{$C$};
    \end{tikzpicture}~,
\end{center}
respectively, in PDG notation.
\newmaterial{%
Where does this notation come from?
}%
Intuitively, the variable $\Tru$ is uninteresting because it always takes the value $\trut$, so we may as well surpress it. 
So, visually, all we have done is to shrink the variable $\Tru$ to a single point, and suppress the label $\trut$ on the event $\Tru{=}\trut$.
\newmaterial{%
What remains are a pair of double-headed arcs joined head-to-head,
}
    
Just as we have left $\Tru$ implicit in the graphical notation,
% Correspondingly, 
we will want to identify a distribution $\mu(\X)$ with 
the ``extended'' distribution $\mu(\X) \delta(\Tru{=}\trut)$---except
in the following theorem statement, where we treat it carefully.
% Observe that, as far as finite PDGs are concerned, 
% relations, events, and constraints are essentially the same.


While it is not surprising that this construction encodes a relation
    as far as PDG semantics are concerned, it is still worth verifying.
\begin{linked}{prop}{constraints-work}
    If $R = R(\var A_1, \ldots, \var A_n) = R(\mat X)$ is a relational constraint
    on the variables $\mat X = \{\var A_1, \ldots, \var A_n\}$, then,
    writing $R$ for both the PDG $\dg M_R$ and the event $R \subseteq \V\mat X$, we have:
    \begin{enumerate}[topsep=0pt]
        \item $\SD{R\,} 
        % (= \SD{\dg M_R})
            = \{ \mu(\mat X, \Tru) : \Supp \mu \subseteq R \times \{\trut\} \}
            = \{ \mu(\mat X, \Tru) : \mu(R) = \mu(\Tru{=}\trut) = 1 \}
            $.
        \item  
        Moreover, for all PDGs $\dg M$ 
            % with variables $\X$,
            % containing the variables $\mat X$:
            with variables $\X \supseteq \mat X$, 
        \begin{enumerate}
            \item 
            % $\SD{\dg M + R} 
            % \cong
            %     % =
            %     % \SD{\dg M} \cap \{ \mu(\X) : \Supp \mu \subseteq R\}
            %     % \SD{\dg M} \cap \{ \mu(\X) : \mu(R) = 1\}
            %     \big\{ \mu \in \SD{\dg M} ~\big|~ \mu(R) = 1 \big\}
            %     $
            $\mu \in \SD{\dg M + R}$ 
            iff $\mu(\X) \in \SD{\dg M}$ and $\mu(R) = 1$ 
                ~(and also $\mu(\Tru{=}\trut)=1$).
                \onlyfirsttime{\footnote{\forjoe{%
                    This extra condition is unimportant because 
                    we are interested only in distributions that place full support on $\Tru{=}\trut$; above the theorem, we identify $\mu(\X)$ with $\mu(\X) \delta(\Tru=\trut)$
                }}}
            % where $\cong$ identifies 
            
            
            \item provided $\aar{\dg M}_\gamma < \infty$, $\bbr{\dg M + R}^*_\gamma$ consists entirely of $\mu$ satisfying $\mu(R) = 1$. 
            
            \item for all $\gamma \ge 0$, $\aar{\dg M + R}_\gamma \ge \aar{\dg M}_\gamma$ with equality if and only if
            there exists some $\mu \in \bbr{\dg M}^*_\gamma$ for which $\mu(R) = 1$. 
        
        \end{enumerate}
    \end{enumerate}
\end{linked}

% Now that we can describe relational constraints, one might
% naturally wonder about databases.


\subsection{``Soft'' Constraints and Barriers}
    \label{sec:soft-constraint-widget}

We have seen how ``hard constraints''  $\V\!X \to \{0, 1\}$ can be added to a PDG.
But what about ``soft'' constraints, i.e., the gray area between zero and one? 
%
This is very different from specifying a probability over $X$; we could assign every $X$ a value of $0.99$, or $0.01$; there is no reason that the assignments should sum to $1$.
%
In light of the construction in the previous section, we now have an obvious candidate for how to do this: simply do precisely what we did for a hard constraint, except use a cpd $c(\Tru \mid X)$ that is not deterministic in place of $\delta R(\Tru\mid X)$. 


Given a set $\mat X$ of variables, and a function
$s : \V\mat X \to [0,1]$, let $\dg M_s$ denote the PDG with variables 
    $\mat X \cup \{ \Tru\}$ 
        % (with $\V\Tru = \{\trut, \truf\}$ as before),
    and two cpds:
    the event $\Tru{=}\trut$ (as before),
    and the cpd 
    $s(\Tru \mid X)$ with $s(\Tru{=}\trut|X{=}x) = s(x)$.
Semantically, the PDG $\dg M_s$ has almost all of the properties we have seen in the case of hard constraints; the following is a strengthening of \cref{prop:constraints-work}. 
% As before, $\Tru$  takes values ,
%      and the event $\Tru \!\!=\! \trut$.
% Now, given a
% cost function $c: \V\!X) \to \mathbb R_{\ge 0}$,
% define the cpd $\hat c(\Tru |X)$ by
% $
%     \hat c(\trut | x) := e^{-c(x)}.
% $
% By threatening to generate the falsehood {\truf} with probability dependent on the cost of $X$, $\hat c$ ties the value of $X$ to inconsistency.
% Now, given such a cpd $p(\Tru \mid X)$, 
% for any PDG we have 

\begin{linked}{prop}{soft-constr}
    % Given a PDG $\dg M$ and a 
    Given a 
    function $s : \V\!X \to [0,1]$. 
    %
    we can regard $s$ as a cpd $s(\Tru \mid X)$ with $s(\Tru{=}\trut|X{=}x) = s(x)$. 
    We then have the following analogues of the results for hard constraints:
    %
    % Just as with a hard constraint, we have
    % (a)  $\mu \in \SD{ \dg M + s }$ iff $\mu(\X) \in \SD{\dg M}$ and $\mu(s(X)=1) = 1$
    % (b) provided
    % (c)  For all $\gamma \ge 0$,
    %     $\aar{\dg M + s}_\gamma \ge \aar{\dg M}_\gamma$
    %     with equality iff $\exits \mu \in \bbr{\dg M}^*_\gamma$ such that $\mu(s(X) = 1)$. 
    %
    % However, the analogue of (b) is much nicer. 
    \begin{enumerate}[topsep=0pt]
        \item $\SD{ \dg M_s } 
        % (= \SD{\dg M_R})
            % = \{ \mu(X, \Tru) : \Supp \mu \subseteq R \times \{\trut\} \}
            % \cong \{ \mu(X) : \Supp \mu \subseteq \{ x \in \V\!X : s(X) = 1\} \}
            = \{ \mu(\mat X, \Tru) : \mu(s(X) = 1) = 1 = \mu(\Tru{=}\trut) \}
            $.
        \item  
        Moreover, for all PDGs $\dg M$ 
            % with variables $\X$,
            % containing the variables $\mat X$:
            with variables $\X \supseteq \{ X \}$, 
        \begin{enumerate}
            \item 
            % $\SD{\dg M + R} 
            % \cong
            %     % =
            %     % \SD{\dg M} \cap \{ \mu(\X) : \Supp \mu \subseteq R\}
            %     % \SD{\dg M} \cap \{ \mu(\X) : \mu(R) = 1\}
            %     \big\{ \mu \in \SD{\dg M} ~\big|~ \mu(R) = 1 \big\}
            %     $
            $\mu \in \SD{\dg M + s(X)}$ 
            iff $\mu(\X) \in \SD{\dg M}$ and $\mu(s(X)) = 1$ 
                {\color{gray}~(and $\mu(\Tru{=}\trut)=1$)}.
            % where $\cong$ identifies 
            
            
            \item provided $\aar{\dg M}_\gamma < \infty$, 
                the set $\bbr{\dg M + \dg M_s}^*_\gamma$ consists entirely of $\mu$ satisfying $\mu(s(X) > 0) = 1$. 
            
            \item for all $\gamma \ge 0$, $\aar{\dg M + \dg M_s}_\gamma \ge \aar{\dg M}_\gamma$ with equality if and only if
            there exists some $\mu \in \bbr{\dg M}^*_\gamma$ for which $\mu(s(X) = 1) = 1$. 
        
        \end{enumerate}
    \end{enumerate}
\end{linked}

% \begin{linked}{prop}{log-barrier}
% Moreover, 
Despite the similarity so far, there is a big difference between hard and soft constraints:
    values of $x$ that have intermediate satisfaction scores $s(x) \in (0,1)$,
    while not fully consistent, are still possible.
Indeed, such values of $x$ may even be probable in the optimal distribution, 
    % when the context is inconsistent enough. 
    if that happens to be the best way to resolve an inconsistency.
% To understand more deeply, 
To get a deeper understanding, it is helpful to look at the scoring function of 
$\dg M_s$ in the context of another (arbitrary) PDG:
\begin{equation}
    \bbr{\dg M_s + \dg M}_\gamma(\mu) 
        = \bbr{\dg M}_\gamma(\mu) - \Ex_\mu [ \log s(X) ].
        \label{eq:soft-constr-score}
\end{equation}
The key is the final term;
    % it is not just any cost,
    adding $\dg M_s$ to $\dg M$ has had the effect of
    augmenting the scoring function $\bbr{\dg M}_\gamma$ with 
    something called a \emph{log barrier function}.
% A barrier function is a smooth approximation to a hard constraint
%     added to an minimization objective, 
%     with the property that values outside the satisfying area (i.e., with $s(x) = 0$ )
% Logarithms are popular because they grow relatively slowly and seem to stabilize gradients. 
%
Log barrier functions play an important role in solving constrained optimization problems with interior point methods. In fact, a similar function will ultimately be the key to inference in PDGs themselves, in \cref{chap:infer}.

    % $ \displaystyle
    %     \aar*{\!\begin{tikzpicture}[center base]
    %         \node[dpad0] (X) at (0,0) {$X$};
    %         \node[dpad0] (2) at (1.1,0) {$\Tru$};

    %         \draw[arr2] (X) to
    %             node[above, pos=0.4,inner sep=2pt]{$\hat c$}
    %             (2);
    %         \draw[arr2, <-] (X) to
    %             node[above, pos=0.6, inner sep=2pt]{$p$}
    %             node[below, pos=0.6, inner sep=2pt]
    %                 {${\color{gray}\scriptscriptstyle(\mskip-2mu\infty\mskip-2mu)}$}
    %             +(-1, 0);
    %         \draw[arr2, <<-] (2) to
    %             node[above, inner sep=2pt, pos=0.6]
    %                 {\trut}
    %             +(0.9,0);
    %     \end{tikzpicture}\!}
    %         = \!
    %     \Ex_{x\sim p}\!    [\,c(x)\,].
    % $
% \end{linked}


% Setting confidence $\beta_p := \infty$ may not be realistic since
% we're still training the model $p$,
% but doing so is necessary to recover $\Ex_p c$.%


\subsection{Couplings and Wasserstein Metrics}
    \label{sec:coupling-widget}
% The set of couplings $\
% Suppose that $p(X)$ and $q(Y)$ are probability distribuitons.
A \emph{coupling} between distributions $p(X)$ and $q(Y)$ is a joint distribution over $X$ and $Y$ whose marginal on $X$ is $p$ and whose marginal on $Y$ is $q$. 
Couplings are an important tool for probabilistic reasoning, and are the basis of optimal transport theory \citep{santambrogio2015optimal}, 
a wide range of programming logics \citep{Kaminski_Katoen_Matheja_2020}.
%
% Let $\Pi(p,q)$ be the set of couplings of $p$ and $q$, i.e.,
% the set of joint distributions that have respective marginals $p$ and $q$.
Formaly, the set of couplings between $p$ and $q$ is defined as
\[
    \Pi(p(X), q(Y)) := \Big\{ \mu \in \Delta\V(X,Y) :\quad
        \mu(X) = p, ~\mu(Y) = q \Big\};
\]
Observe that this
% Observe that this 
is exactly the set of distributions
consistent with a PDG containing $p$ and $q$, i.e., 
\[
    \Pi(p, q) = 
    \SD*{\begin{tikzpicture}[center base]
        \node[dpad0] (X) at (-0,0) {$X$};
        \node[dpad0] (Y) at (1,0) {$Y$};

        \draw[arr2, <-] (X) to
            node[left]{$p$}
            +(0,1);
        \draw[arr2, <-] (Y) to
            node[left]{$q$}
            +(0,1);
    \end{tikzpicture}~~}.
\]

The couplings themselves are not all that a PDG can capture.
Suppose we have a distance metric $d$ on a space $X$.
% For $k \in [1,\infty)$, 
% the $k$-Wasserstein distance between $p,q \in \Delta X$ is given by
% \[  
%     W_k(p,q) := \inf_{\mu \in \Pi(p,q)} \Ex_{\mu} \Big[d(X,Y)^k\Big]^{\frac1k}.
% \]
The \emph{Wasserstein distance} (also called earth-mover's distance or Kantorovich metric) between $p,q \in \Delta X$, given by
\[  
    W_1(p,q) := \inf_{\mu \in \Pi(p,q)} \Ex_{\mu} \Big[d(X,Y)\Big],
\]
is a foundational quantity in optimal transport \citep{}.
%
This definition effectively takes $p(X)$ and $q(X)$ with
    high confidence, by constraining to $\mu \in \Pi(p,q)$.
But, in order to represent this as a PDG, we need to represent 
the $d$ in probabilistic terms.
% A distance is not a probability. 
% But we can encode the belief that the values of $X$ and $Y$ are close, according to $d$.
% Let $\Tru$ be a variable that in principle can be
% either $\trut$ or $\truf$, but happens to always be $\trut$.
% % to convert it  to a Gibbs distribution. 
% To place more probability in $\truf$ as $d(X,Y)$ increases.
%
A distance is not a probability, but we can convert it to a soft constraint and write it in probabilistic terms, as in \cref{sec:soft-constraint-widget}.
Accordingly, 
% Let $\Tru$ be a variable that in principle can be
% either $\trut$ or $\truf$, but happens to always be $\trut$.
% % to convert it  to a Gibbs distribution. 
% To place more probability in $\truf$ as $d(X,Y)$ increases.
% \[
%     \hat d(\Tru=\trut | X,Y) \propto \exp(- d(X,Y)).
% \]
we have:
\begin{prop}
    Suppose that $p(X)$ and $q(Y)$ are probability distribuitons. 
    Given a distance measure $d : \V(X,Y) \to [0,\infty]$, 
    \newmaterial{%
    define a soft constraint according to 
    $(x,y)\mapsto e^{-d(x,y)}$, 
    which results in a cpd
    $\hat d(\Tru{=}\trut\mid x,y) := \exp(- d(x,y))$.
    The (observational) inconsistency of these of these pieces of information is
    the Wasserstein distance $W_1(p,q)$. That is,
    % be a similarity measure.
    }%
    % Interpreting $\hat d$ as a soft constraint, we have:
    \[
        \aar[\Big]{\dg M_{\hat d} + p! + q!}_0 =
        \aar**{\begin{tikzpicture}[center base]
            \node[dpad0] (X) at (-1,0) {$X$};
            \node[dpad0] (Y) at (1,0) {$Y$};
            \node[dpad0] (T) at (0,-1) {$\Tru$};
            \draw[arr2, <<-] (T) to node[above]{$\trut$} +(-1,0);

            \draw[arr2, <-] (X) to
                node[left]{$p!$}
                +(0,1);
            \draw[arr2, <-] (Y) to
                node[left]{$q!$}
                +(0,1);

            \mergearr XYT
            \node[above=0pt of center-XYT] {$\hat d$};
        \end{tikzpicture}}\vphantom{\Bigg|}_{\!\!0}
        = \inf_{\mu \in \Pi(p,q)} \Ex_{\mu} \Big[d(X,Y)\Big] = W_1(p,q),
    \]
    where the exclamation points indicate high confidence $(\beta = \infty)$. 
\end{prop}
\begin{proof}
    The high confidence specification of $p$ and $q$ constrains the optimal distributions to $\Pi(p,q)$.
    \newmaterial{%
    Whatever the optimal distribution $\mu^*$,
    the score of that distribution, by \eqref{eq:soft-constr-score},
    is 
    \[
    \Ex\nolimits_{\mu^*}[ - \log \hat d(\Tru{=}\truf\mid X,Y) ]
    = \Ex\nolimits_{\mu^*}[ - \log \exp( - d(X,Y))] 
    = \Ex\nolimits_{\mu^*}[d(X,Y)].
    \qedhere
    \]
    % and any distribution generates its expected distance, as a consequence of the results of \cref{sec:soft-constraint-widget}. 
    }
\end{proof}

% \subsection{Graph-Hypergraph Equivalence}
\subsection{Incomplete CPDs and Individual (Conditional) Probabilities}
    \label{sec:prob-widget}
    
A cpd between discrete variables can be represented by
    a stochastic matrix (i.e., a matrix whose rows sum to one).
It turns out that it is possible to use the machinery of PDGs
    to, effectively, give only some of the values in that matrix.
In PDG notation, we abbreviate the widget that implements $p(Y|X)$ with some missing values by adding an empty circle at the tail, i.e., drawing 
\begin{tikzpicture}[center base]
    \node[dpad1] (X) at (0,0) {$X$};
    \node[dpad1] (Y) at (1.5,0) {$Y$};
    \draw[arr2,o->] (X) to (Y);
\end{tikzpicture}.    

We now present an important extreme case of that construction:
how, for any $p \in [0,1]$, we can construct a PDG
that represents the belief that $\Pr(Y{=}y|X={x}) = p$, but says nothing about
how the probability splits between other values of $y$, and also says nothing
about the probability of $Y$ if $X \ne x$.


\def\XxYy{{X{=}x\Vert Y{=}y}}
\def\XxYyshort{{X_x Y_y}}
\def\Yy{{Y{=}y}}
\def\Yyshort{{Y_y}}

% We now describe the construction.
First, we introduce two new auxiliary variables.
The first variable, which we might like to call ``$\Yy$'', but
    mostly refer to as $\Yyshort$ to prevent confusion with the synonymous
    event, is a binary variable, with $\V(\Yyshort) = \{y, \lnot y\}$,
    and takes the value $y$ if $Y=y$, and $\lnot y$ if $Y \ne y$.
The second variable, which we would like to call ``$\XxYy$'',
    but instead mostly refer to as $\XxYyshort$ to prevent notational confusion,
    can take three values: $\V(\XxYyshort):= \{ x, y, \lnot y \}$.
The value $x$ is meant to correspond exactly to the event $X{=}x$,
    much like before, so that $\XxYyshort = x$ if and only if $X = x$.
The values $y$ and $\lnot y$ also correspond to their respective
    events, but more loosely; the variable $\XxYyshort$ only takes one of these
    values when $X \ne x$.
Note that both variables can be determined from $X$ and $Y$
(although we will need to enforce this with additional arcs), and
therefore there is a unique way to extend a
distribution over $X$ and $Y$ to also include the variables $\Yyshort$ and $\XxYyshort$.

% Now, supposing that $\XxY$ takes its special value $X{=}x$ if and only if $X=x$, and otherwise takes the value of $Y$, then there's a clear choice of distribution on $B$ given $X$: if $X$ takes its special value, we $\Pr(B) = p$. Otherwise, $\Pr(B)$ is a point mass on the value $b$. And this value of $X$ an be calculated from joint values of $B$ and $A$.
With these definitions in place, there is
    now an obvious way to add an arc from the variable $(\XxYyshort)$
    to the variable $\Yyshort$, together with a cpd asserting that $\Pr(Y{=}y|X{=}x)=p$.
This cpd is written as a stochastic matrix $\hat p$
    defined on the right of \cref{fig:pyx-widget}.
The PDG we have just constructed is illustrated on the left of \cref{fig:pyx-widget}.
In addition to $\hat p$ and the new variables, this PDG
    includes the structural constraints $s_1$ and $s_2$ needed to define the variables
    $\XxYyshort$ and $\Yyshort$ in terms of $X$ and $Y$; they are deterministic functions,
    drawn in double-headed gray arrows.
% are illustrated below.

% \begin{center}
\begin{figure}
    \centering
% $\dg M^{y|x=p}:=$
    \colorlet{proofmatt}{white}
    \begin{tikzpicture}[center base]
        \node[dpadded] (X) at (0,-0.5){$X$};
        \node[dpadded, right=1.6 of X](Y){$Y$};

        \node[tpt={astar|$x$}] at (0.0,1.5) {};
        \node[tpt={b1|$y$},right=0.55 of astar]{};
        \node[tpt={b2|$\lnot y$},right=0.4 of b1]{};
        % \node[right=0.2 of b2] (bdots){$\cdots$};
        % \node[tpt={bn|$y_n$},right=0.2 of bdots]{};

        \node[Dom={$\XxYyshort$[label distance=-1.5em, xshift=1.2em] (XxYy)
            around {\lab{astar}\lab{b1}\lab{b2}(1.6,1.85)}} ] {};

        \node[tpt={y1|$y$}] at (3.7, 1.5){};
        \node[tpt={y2|$\lnot y$},right=0.5 of y1]{};
        \node[Dom={%
            %$Y{=}y\,$%
            $\Yyshort$%
            [label distance=-1.4em, xshift=1.3em] (Yy)
            around {\lab{y1}\lab{y2}(3.7,1.75)}} ] {};

        %% structural arrows
        \mergearr[black!35!proofmatt,arr2,->>]{X}{Y}{XxYy}
        \node[black!35!proofmatt,below=2pt of center-XYXxYy]{$s_1$};

        %%
        \draw[black!35!proofmatt, arr2, ->>] (Y) to node[below right]{$s_2$} (Yy);

        \draw[arr2] (XxYy) to node[above]{$\hat p$} (Yy);
    \end{tikzpicture}
    % \hspace{0.6cm}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \begin{align*}
            \hat p(\Yyshort|\XxYyshort) &= 
            % \begin{matrix}
            %     &  \begin{matrix} y & \lnot y \end{matrix} \\
            %     \begin{matrix} x \!\! \\ y \!\! \\ \lnot y \!\! \end{matrix} &
            %     \begin{bmatrix}
            %         \;p & 1-p \; \\
            %         \;1 & 0  \;\\
            %         \;0 & 1 \;
            %     \end{bmatrix}
            % \end{matrix}
            \begin{idxmat}{$x$,$y$,$\lnot y$}{$y$,$\lnot y$}
                p & 1-p  \\
                1 & 0  \\
                0 & 1 
            \end{idxmat}
            \\[2ex]
            s_1(\XxYyshort|X,Y) &:= \delta{\singlespacing\begin{cases}
                x & \text{if $X = x$} \\
                y & \text{if $X \ne x$ and $Y=y$} \\
                \lnot y & \text{if $X \ne x$ and $Y\ne y$} \\
            \end{cases}}\\
            s_2(\Yyshort|Y) &:= \delta{\singlespacing\begin{cases}
                y & \text{if $Y=y$} \\
                \lnot y & \text{if $Y\ne y$} \\
            \end{cases}}
            % p \mathrm{~or~} b~(X) &:= \begin{cases}
            % 	p(B) & \text{if $X = a^*$} \\
            % 	\delta_{B=b} & \text{if $X = b$}
            % \end{cases}
        \end{align*}
    \end{minipage}
% \end{center}
    % \medskip
    \caption{A widget PDG for capturing a single conditional probability: a statement of the form $\Pr(Y{=}y\mid X{=}x) = p$, for $p \in [0,1]$.}
    \label{fig:pyx-widget}
\end{figure}

So, when we add $\Pr(Y=y|X=x) = p$ to a PDG $\dg M$, what we really mean is:
first convert construct a widget as above, and add that structure (i.e., the new variables
$X_xY_y$ and $Y_y$, their definitions $s_1$ and $s_2$, and the cpd $\hat p$) to $\dg M$.

In what sense does this ``work''?
The first order of business
is to prove that it behaves as we should expect,
semantically.
This means showing that the widget contains precisely the information
    that $\Pr(Y{=}y|X{=}x) = p$, and nothing more. 
We now explore two ways of making this precise with the semanics of PDGs.
% , in the case we're interested in.

\begin{prop}
        \label{prop:individual-prob-1}
    If $\dg M$ is a PDG, then 
    % \[
    $
    \mu \in \SD[\big]{\dg M} 
    % \text{~~and~~}
    $ with $
    \mu(Y{=}y|X{=}x) = p
    $
    if and only if
    % \quad\iff\quad
    $
    \mu 
    % \text { extends to }
    $ extends (via $s_1,s_2$) to $
    % \SD[\Big]{\dg M + ~\Pr(Y{=}y|X{=}x) = p}
         % = \Big\{ \mu \in \SD[\Big]{\dg M}  ~\Big|~
         %        \mu(y|x) = p
         %    \Big\}
         % =  \SD[\Big]{\dg M} \cap  \Big\{ \mu : 
         %        \mu(y|x) = p
         %    \Big\}
    \bar\mu \in \SD[\big]{\dg M + ~\Pr(Y{=}y|X{=}x) = p}.
    % \]
    $
\end{prop}
\begin{proof}
    ($\implies$.)
    Suppose $\mu \in \SD{\dg M}$ has $\mu(y|x) = p$, and let $\bar\mu$ be its extenson via the functions $s_1$ and $s_2$ to the variables $\XxYyshort$ and $\Yyshort$. 
    % Since $\mu(y|x) = p$, 
    $s_1$ ensures that $\XxYyshort = x$ precisely when $X = x$. 
    By assumption, the probability (according to $\mu$) that $Y{=}y$
    when $X{=}x$ is $p$. But $s_2$ ensures that $Y{=}y$ precisely when $\Yyshort{=}y$. 
    Thus,
    $\bar\mu(\Yyshort {=} y \mid \XxYyshort {=} x) = \mu(Y{=}y \mid X{=}x) = p$.
    So $\bar \mu$ satisfies the constraint given by the first row of $\hat p$
        in \cref{fig:pyx-widget}.
    When $\XxYyshort \ne x$, on the other hand, then $s_1$ and $s_2$ ensure
    that $\XxYyshort = \Yyshort = Y$, thereby satisfying the other rows 
    of the cpd $\hat p$. 
    Thus, $\bar \mu$ satisfies all cpds of $\dg M$, in addition to $s_1, s_2$, and $\hat p$, and therefore $\bar\mu \in \SD[\big]{\dg M + ~\Pr(Y{=}y|X{=}x) = p}$.
    
    ($\impliedby$.)
    Choose some $\bar\mu \in \SD[\big]{\dg M + ~\Pr(Y{=}y|X{=}x) = p}$,
    and let $\mu$ be its marginal on the variables of $\dg M$. 
    By the logic above, $\bar\mu(\Yyshort {=} y \mid \XxYyshort {=} x) = \mu(Y{=}y \mid X{=}x)$.
    Since $\bar\mu$ satisfies the cpd $\hat p$, it follows that $\bar\mu(\Yyshort {=} y \mid \XxYyshort {=} x) = \mu(Y{=}y \mid X{=}x) = p$ as desired.
    Since $\bar\mu$ also satisfies all the cpds of $\dg M$, we also have $\mu \in \SD{\dg M}$.
\end{proof}

We have shown that the effect of our widget on a PDG's set-of-distributions semantics is precisely to restrict to distributions $\mu$ in which $\mu(y|x) = p$. 
But this result is vacuous for inconsistent PDGs, whose set-of-distributions semantics is empty. 
We now give a result of similar character for the inconsistency semantics of a PDG, 
which shows our construction behaves appropriately for all PDGs. 

\begin{prop}%\label{lem:inc-inc-eq}
    % For all PDGs $\dg M$ with $\bbeta \ge \mat 0$ and all $\gamma > 0$, we have that
    Suppose $\dg M$ is a PDG with variables $\X$ and $\bbeta \ge \mat 0$.
    Then, for all $X,Y \subseteq \X$, $x \in \V X$, $y \in \V Y$, $p \in [0,1]$ and $\gamma \ge 0$,
    we have that:
    \[
        \aar[\Big]{\dg M + ~\Pr(Y{=}y|X{=}x) = p}_\gamma \ge \aar{\dg M}_\gamma,
    \]
    with equality if and only if there exists $\mu \in \bbr{\dg M}^*_\gamma$
    such that $\mu(Y{=}y|X{=}x) = p$.
    (Note that this condition is trivially satisfied when $\mu(X{=}x) =0$.)
\end{prop}
\begin{lproof}
    The inequality is immediate; it is an instance of monotonicity of inconsistency
    % \cite[Lemma 1]{one-true-loss},
    \cref{lemma!},
    which we will discuss in depth in \cref{sec:monotone}. Intuitively: believing more cannot make you any less
    inconsistent.  We now prove that equality holds iff there is a minimizer with the appropriate conditional probability.

    $(\impliedby)$. Suppose there is some $\mu \in \bbr{\dg M}^*_\gamma$ with $\mu(Y{=}y|X{=}x) = p$.
    Because $\mu \in \bbr{\dg M}^*_\gamma$, we know that
    $\bbr{\dg M}_\gamma(\mu) = \aar{\dg M}$.
    Let $\hat \mu$ be the extension of $\mu$ to the new variables ``$\XxYy$'' and $``\Yy$'',
        whose values are functions of $X$ and $Y$ according to $s_1$ and $s_2$. Then,
    {\allowdisplaybreaks
    \begin{align*}
        &\aar[\Big]{\dg M + ~\Pr(Y{=}y|X{=}x) = p}_{\!\gamma}
            \\&\le \bbr[\Big]{\dg M + ~\Pr(Y{=}y|X{=}x) = p}_\gamma(\hat \mu) 
            \\&= \bbr{\dg M}_\gamma(\mu) + \Ex_{\mu}\left[
                \log \frac{\hat\mu(\Yyshort | \XxYyshort)}{ \hat p(\Yyshort | \XxYyshort)} \right]
            \\&= \bbr{\dg M}_\gamma(\mu) +
                \mu(X{=}x,Y{=}y) \log \frac{\mu(Y{=}y|X{=}x)}{p} \\
                &\hphantom{=\bbr{\dg M}_\gamma(\mu)}
                + \mu(X{=}x,Y{\ne} y) \log \frac{\mu(Y{\ne} y|X{=}x)}{1-p} 
            \\&= \bbr{\dg M}_\gamma(\mu) +
                \mu(X{=}x,Y{=}y) \log(1)
                + \mu(X{=}x,Y{\ne} y) \log(1) 
            \\&= \bbr{\dg M}_\gamma(\mu)
            % \\&
            = \aar{\dg M}_\gamma.
    \end{align*}}
    The equality between the third and fourth lines
    is perhaps the trickiest to see, but follows
    because for joint settings in which $X{\ne}x$,
    one can easily see that $\hat\mu(\Yyshort|\XxYyshort)$
    equals 1 with probability 1, as does $\hat p(\Yyshort|\XxYyshort)$.
    So, after dividing one by the other and taking a logarithm,
        these cases contribute nothing to the expectation.
    What remains are the two possibilities where $X{=}x$, which are shown in the second line.
    %
    To complete this direction of the proof, it suffices to observe
    that we already knew the inequality held in the opposite direction
    (by monotonicity), so the two terms are equal.

    $(\implies)$.  Suppose the two inconsistencies are equal, i.e.,
    \[
    \aar[\Big]{\dg M + ~\Pr(Y{=}y|X{=}x) = p}_\gamma = \aar{\dg M}_\gamma.
    \]
    This time, choose $\hat\mu \in \bbr{\dg M+ ~\Pr(Y{=}y|X{=}x) = p}^*_\gamma$,
        and define $\mu$ to be its marginal on the variables of $\dg M$
        (which contains the same information as $\hat \mu$ itself).
    %
    Let $q := \mu(Y{=}y|X{=}x)$. Then,
    {\allowdisplaybreaks
    \begin{align*}
        \aar{\dg M}_\gamma &= \aar[\Big]{\dg M + ~\Pr(Y{=}y|X{=}x) = p}_{\!\gamma} \\
         &= \bbr[\Big]{\dg M + ~\Pr(Y{=}y|X{=}x) = p}_\gamma(\hat \mu) \\
         &= \bbr{\dg M}_\gamma(\mu) +
             \mu(X{=}x,Y{=}y) \log \frac{\mu(Y{=}y|X{=}x)}{p} 
             \\&\hphantom{\bbr{\dg M}_\gamma(\mu)}
             + \mu(X{=}x,Y{\ne} y) \log \frac{\mu(Y{\ne} y|X{=}x)}{1-p} \\
        &= \bbr{\dg M}_\gamma(\mu) +
            \mu(X {=} x) \left[ q \log \frac qp + (1-q) \log \frac{1-q}{1-p} \right] \\
        &= \bbr{\dg M}_\gamma(\mu) + \mu(X{=}x) \kldiv qp \\
        &\ge \aar{\dg M}_\gamma + \mu(X{=}x) \kldiv qp
    \end{align*}}
    Therefore $0 \ge \mu(X{=}x) \kldiv qp$. But relative entropy is non-negative,
    by Gibbs inequality. This shows $\mu(X{=}x) \kldiv qp = 0$.
    So either $\mu(X{=}x)$, or $p = \mu(Y{=}y|X{=}x)$, and the first case is just
    a special case of the second one.
    In addition, the algebra above shows that $\mu \in \bbr{\dg M}^*_\gamma$, as its
        score is $\aar{\dg M}_\gamma$.
    Thus, we have found $\mu \in \bbr{\dg M}^*_\gamma$ such that $\mu(Y{=}y|X{=}x) = p$, completing the proof.
\end{lproof}

\paragraph{``Conditional Conditional'' Probabilities.}
We now turn to another, closely related modeling task. 
What if we wanted to specify a full cpd $p(B|A)$, but with the stipulation that it was only meaningful when a third ``guard variable'' $G$ takes a certain value (say, $G{=}1$)?
% Having an independent guard variable 
This can be a useful thing to model. For instance, it exactly describes
    a common practice for reducing errors in machine learning, by identifying domain shift: train a classifier (whose output is $G$) to detect if the original predictor $p(B|A)$ is relevant to the current context, and only apply $p$ if it is (i.e., $G{=}1$).
A second, more abstract use for this primitive is to enrich PDGs with another notion of confidence. 
By adding a guard variable $G_a$ for each $a \in \Ar$, and asserting that each $G_a = 1$ occurs with probability $1 - \epsilon_a$ (for some $\epsilon_a \in [0,1]$), it becomes possible to articulate a different and more standard kind of probability-based confidence in the reliability of each cpd.
(Observe that the special case in which each $\epsilon_a= 0$ is the definition of a PDG we already have.)
We discuss the difference between approaches like this and the meaning of our primitive $\beta$ in \cref{chap:conf,chap:relent-soup}. 

Technically speaking, the specification of $p(B|A)$ under the condition that $G = 1$ is really no different from a cpd $p(B|A, G{=}1)$.
Gven that we have already proved that PDGs an represent arbitrary conditional probabilities, it is entirely expected that PDGs can represent this too. 
A widget for it is illustrated in \cref{fig:guard-variable}.
% Now that we have verified that it is possible to specify arbitrary conditional probabilities with a PDG, it should come as no surprise that its can specify 

\begin{figure}
    \centering
	% \begin{tikzpicture}[center base]
	% 	\node[dpadded] (A) at (0,-0.5){$A$};
	% 	\node[dpadded, right=1.5 of A](B){$B$};
    % 
	% 	\node[tpt={astar|$a$}] at (0.2,1.5) {};
	% 	\node[tpt={b1|$b_1$},right=0.35 of astar]{};
	% 	\node[tpt={b2|$b_2$},right=0.2 of b1]{};
	% 	\node[right=0.2 of b2] (bdots){$\cdots$};
	% 	\node[tpt={bn|$b_n$},right=0.2 of bdots]{};
    % 
	% 	\node[Dom={$X$[label distance=-2em, xshift=-1.0em] (X)
	% 		around {\lab{astar}\lab{b1}\lab{bn}(1,1.65)}} ] {};
    % 
    % 
	% 	\mergearr[->>]{A}{B}{X}
	% 	\node[below=2pt of center-ABX]{$\varphi$};
    % 
	% 	\draw[arr2] (X) to node[above right,yshift=-3pt]{$p$ or $b$} (B);
	% \end{tikzpicture}
	% % \hspace{1cm}
    % \hfill
	% \begin{minipage}{0.3\textwidth}
	% 	\begin{align*}
	% 		\varphi(A,B) &:= \begin{cases}
	% 			\delta_{X=a} & \text{if $A = a$} \\
	% 			\delta_{X=B} & \text{otherwise}
	% 		\end{cases}\\[1em]
	% 		p \mathrm{~or~} b~(X) &:= \begin{cases}
	% 			p(B) & \text{if $X = a$} \\
	% 			\delta_{B=b} & \text{if $X = b$}
	% 		\end{cases}
	% 	\end{align*}
	% \end{minipage}
    \begin{tikzpicture}[center base]
		\node[dpadded] (A) at (0,-0.5){$A$};
		\node[dpadded, right=1.5 of A](B){$B$};
		\node[dpadded, above=1 of A](G){$G$};

%			\node[dpadded, inner sep=0.8em, label={[label distance=-1.5em,shading = axis, top color=white, bottom color=black!04, xshift=2.2em]145:$G\,{?}A\,{:}\,B$}, text=gray] (GAB) at (3,1.5) {$\cong \V(A) \sqcup \V(B)$};
		\node[dpadded, inner sep=0.8em, label={[label distance=-1.5em,shading = axis, top color=white, bottom color=black!04, xshift=-1.0em]145:$X$}, text=gray] (GAB) at (3.5,1.5) {$\cong \V(A) \sqcup \V(B)$};
%

%			\mergearr[->>]{A}{B}{GAB}
		\coordinate (cent) at (1.2, 0.5);
		\draw[arr,-] (A) -- (cent) to[] (B);
		\draw[arr,->>] (G) to[bend right] (cent) -- (GAB.south west);
%			\node[below=2pt of cent]{$G\,{?}A\,{:}\,B$};
		\node[below=2pt of cent]{$\varphi$};

		\draw[arr2] (GAB) to[bend left] node[below right,yshift=-3pt]{$p$ or $b$} (B);
	\end{tikzpicture}
	% \hfill
    \hspace{1cm}
	\begin{minipage}{0.3\textwidth}
		\begin{align*}
			\varphi(G,A,B) &:= \text{ if } G \text{ then } A \text{ else } B. \\[1em]
			p \mathrm{~or~} b~(X) &:= \begin{cases}
				p(B|A\!=\!a) & \text{if } X = a \\
				\delta_{B=b} & \text{if } X = b 
			\end{cases}
		\end{align*}
	\end{minipage}
    \caption{A widget implementing a CPD whose presence is conditioned on a guard variable, i.e., $p(B|A,G{=}1)$}
        \label{fig:guard-variable}
\end{figure}

\subsection{Probability Ranges}


\begin{figure}
    \centering
% $\dg M^{y|x=p}:=$
    \colorlet{proofmatt}{white}
    \begin{tikzpicture}[center base]
        \node[dpadded] (X) at (2,5.5){$X$};
        \node[dpadded] (Y) at (5.6, 4.2){$Y$};

        \node[tpt={uastar|$x$}] at (-1.0,3.5) {};
        \node[tpt={ub1|$y$},right=0.55 of uastar]{};
        \node[tpt={ub2|$\lnot y$},right=0.4 of ub1]{};
        \node[Dom={$X_x\overline Y_y$[label distance=-1.5em, xshift=1.2em] (XxuYy)
            around {\lab{uastar}\lab{ub1}\lab{ub2}(0.6,3.85)}} ] {};

        \node[tpt={lastar|$x$}] at (-0.5,1.6) {};
        \node[tpt={lb1|$y$},right=0.55 of lastar]{};
        \node[tpt={lb2|$\lnot y$},right=0.4 of lb1]{};
        \node[Dom={$X_x\underline Y_y$[label distance=-1.5em, xshift=1.2em] (XxlYy)
            around {\lab{lastar}\lab{lb1}\lab{lb2}(1.1,1.95)}} ] {};

        \node[tpt={ly1|$y$}] at (3.9, 1.6){};
        \node[tpt={ly2|$\lnot y$},right=0.5 of ly1]{};
        \node[Dom={$\underline Y_y$%
            [label distance=-1.45em, xshift=1.0em] (lYy)
            around {\lab{ly1}\lab{ly2}(3.9,1.85)}} ] {};

        \node[tpt={uy1|$y$}] at (2.8, 3.5){};
        \node[tpt={uy2|$\lnot y$},right=0.5 of uy1]{};
        \node[Dom={$\overline Y_y$%
            [label distance=-1.4em, xshift=1.3em] (uYy)
            around {\lab{uy1}\lab{uy2}(2.8,3.75)}} ] {};


        %% structural arrows
        \mergearr[black!35!proofmatt,arr2,->>]{X}{uYy}{XxuYy} % {0.5,3}
        % \node[black!35!proofmatt] at (0.2,3) {$s_1$};
        \node[black!35!proofmatt, above left=2pt of center-XuYyXxuYy] {$s_1$};
        
        \cmergearr[black!35!proofmatt,arr2,->>]{X}{lYy}{XxlYy}{1.6,2.5}
        \node[black!35!proofmatt] at (1.85,2.6){$s_3$};

        %%
        \draw[black!35!proofmatt, arr2, o->>] (Y) to node[below right]{$s_2$} (lYy);
        \draw[black!35!proofmatt, arr2, o->>] (Y) to node[above]{$s_4$} (uYy);

        \draw[line width=7pt,white] (XxlYy) to (lYy);
        \draw[arr2] (XxlYy) to node[above]{$\hat a$} (lYy);
        \draw[line width=7pt,white] (XxuYy) to (uYy);
        \draw[arr2] (XxuYy) to node[above]{$\hat b$} (uYy);
    \end{tikzpicture}
    % \hspace{0.6cm}
    % \hfill
    % \begin{minipage}{0.3\textwidth}
        \begin{align*}
            \hat b(\overline Y_y|X_x\overline Y_y)
            = \hat a(\underline Y_y|X_x\underline Y_y) = 
                \begin{idxmat}{$x$,$y$,$\lnot y$}{$y$,$\lnot y$}
                        a & 1-a  \\
                        1 & 0  \\
                        0 & 1 
                \end{idxmat},
                \\
            % \hat b &\text { is defined analogously,}
                % \text{ where $p =$}
                % \\[2ex]
            s_2(\underline Y_y | Y) := {\singlespacing\begin{cases}
                \delta_y & \text{if $Y=y$} \\
                \texttt{(null)} & \text{if $Y\ne y$} 
            \end{cases}} \quad \text{ and } \quad
            s_4(\overline Y_y | Y) := {\singlespacing\begin{cases}
                \texttt{(null)} & \text{if $Y=y$} \\
                \delta_{\lnot y} & \text{if $Y\ne y$} 
            \end{cases}}\\
            % \parbox{5.5cm}{
            \intertext{
            where \texttt{(null)} indicates no information
                (see \cref{sec:prob-widget}),
                }
            \mathllap{\text{and}~~} 
                s_1(X_x\overline Y_y|X,\overline Y_y) = 
                s_3(X_x\underline Y_y|X,\underline Y_y) = 
                \delta{\singlespacing\begin{cases}
                x & \text{if $X = x$} \\
                y & \text{if $X \ne x$ and $Y_y=y$} \\
                \lnot y & \text{if $X \ne x$ and $Y\ne y$} \\
            \end{cases}}\\
            \text{as in \cref{fig:pyx-widget}.}
            % p \mathrm{~or~} b~(X) &:= \begin{cases}
            % 	p(B) & \text{if $X = a^*$} \\
            % 	\delta_{B=b} & \text{if $X = b$}
            % \end{cases}
        \end{align*}
    % \end{minipage}
% \end{center}
    % \medskip
    \caption{A widget PDG for capturing a conditional probability range, a statement of the form $\Pr(Y{=}y\mid X{=}x) \in [a,b]$.}
    \label{fig:pyx-range-widget}
\end{figure}

Probability requires the modeler to assign a specific number to every event $U \subseteq \Omega$. This can be restrictive; what if you simply don't know, and so are not prepared to assign a probability to $U$? 
One heavily studied line of work aimed at addressing this issue allows the modeler to instead specify a range of values rather than a single one; thus one can say ``the probability of $U$ lies between $\frac13$ and $\frac12$'', for example.
It is easy to see that specifying an actual probability distribution is the special case where the upper and lower bounds are equal, but in general, this approach yields a far richer theory \citep{Walley1991-SRIP}.

At first, it appears that allowing a modeler to specify beliefs this way would require us to define an generalized PDG in which the modeler specifies not a cpd $p(Y|X)$, but rather upper and lower bounds on it. But yet again, it turns out to be possible with the definition we already have.

There are multiple ways to do this. One idea to specify $\Pr(Y{=}y\mid X{=}x) \in [a,b]$ with a PDG is to add an ``overapproximation'' variable $\overline Y_y$ that is guaranteed to take on the value $y$ whenever $Y$ does (but may also take on the value $y$ other times), as well as an ``underapproximation'' variable $\underline Y_y$ that can only take on the value $y$ when $Y$ does (but even then, may not). 
Once these are defined (e.g., with constraints), it is not too difficult to encode the upper and lower probabilities as cpds that target these new variables. 
A (perhaps clunky) way of fully formalizing this is fleshed out in \cref{fig:pyx-range-widget}.

\section{Other Representations of Knowledge and Uncertainty}
    \label{sec:other-uncertainty-repr}

Probability is the dominant way of talking about epistemic uncertainty
in economics and computer science, but it is far from the only one \citep{halpern-RAU}.
Probability---at least, in the form of a joint distribution $\mu$---has some shortcomings. 
One of the biggest is its inability to represent ignorance well
    (a point we will return to in \cref{chap:conf}).
This has lead many to develop generalizations of probability better suited to representing ignorance \citep{shafer1990probability,Walley1991-SRIP}.
It turns out that several of the most celebrated generalizations of probabilities can be viewed as special cases of PDGs. 


\subsection{Convex Sets of Probabilities}
    \label{ssec:cvx-prob-sets}
We have seen that $\SD{\dg M}$ represents a convex set of probabilities. 
But does this work the other way around? What if we already have a convex set $\mathcal P$ of distributions over $\Omega$ that we would like to model with a PDG? 
There is actually a very straightforward way of doing this.
To simplify matters, suppose that $\mathcal P$ is a \emph{polytope}, meaning that it has finitely many extreme points, which we call vertices. 
Let $V$ be a random variable whose possible values $\V V$ are vertices of $\mathcal P$, and define a conditional probability distribution 
% \begin{align*}
$
    p(\Omega \mid V{=}v) := v,
$
% \end{align*}
which typechecks because a vertex $v$ is a distribution over $\Omega$. 
\[
\dg M_{\cal P} :=~
\begin{tikzpicture}[center base]
    \node[dpadded] (V) at (-2,0) {$V$};
    \node[dpadded] (W) at (0,0) {$W$};
    
    \draw[arr2] (V) to node[above]{$p$} (W);
\end{tikzpicture}
\]

\begin{prop}
    $\mu \in \mathcal P$ iff it can be extended to some $\bar\mu \in \SD{\dg M_{\cal P}}$.
    That is, 
    $\mathcal P = \{ \mu(W) : \mu \in \SD{\dg M_{\cal P}} \}$. 
\end{prop}

Interestingly, this approach also resolves some confusion about maximum entropy. 
Let's look at an example.

\begin{example}
    \label{ex:coin}
Suppose we are about to flip a coin; but we know that it is either fair or double-headed, but do not know which.
One way of modling this is might be to take $\Omega = \{ H, T\}$ to be the possible outcomes of the coin flip, and represent our belief with the set of distributions
$\mathcal P = \{ \mu_{.5}, \delta_H \}$, where $\mu_{.5}$ assigns probability 1/2 to both $H$ and $T$, and $\delta_H$ assigns probability 1 to $H$. 
% According to the principle of maximum entropy, we should 
% The principle of maximum entropy tells us to select as a representative distribution 
From a set of distributions consistent with observations, the principle of maximum entropy tells us to select the one with the highest entropy---which is a fair coin. 
This seems quite extreme; the possibility of the double headed coin has made no difference to the maximum-entropy distribution!

Part of the problem, arguably, is that we haven't actually modeled the whole situation---we've left out an important part of the picture: our knowledge about the bias of the coin, and how that affects the outcome.  This information is very easy to add to a PDG: 

\begin{center}
\begin{tikzpicture}[center base]
    \node[bpt={h | $H$}] at (-0.3,0) {};
    \node[bpt={t | $T$}] at (0.3,0) {};
    %			\draw[arr,->, thin, red, dashed] (x1) to[bend right=40](x2);
    \node[bDom={Coin (C) around \lab{h}\lab{t}}] {};
    
    \node[dpadded, left=1 of C] (B) {Bias};
    
    \draw[arr] (B) to node[above]{$p$} (C);
\end{tikzpicture}
$\qquad\qquad\displaystyle
p = 
\begin{idxmat}{$F$,$H^2$}{$H$,$T$}
    .5 & .5 \\ 1& 0
\end{idxmat}
$
\end{center}

Now, in the maximum entropy distribution, the coin is twice as likely to be fair as it is to be one-sided, and overall it has a 2/3 chance of landing heads. 
This is a more reasonable summary of our belief state---we do not truly believe that the coin is fair.
Furthermore, if we select $\alpha = 1$ for this edge, 
then the distribution with minimal structural deficiency (the analogue of maximum-entropy when there is causal information present) prescribes the distribution that says the coin has a 3/4 chance of landing heads, which is in the very middle of our range. 
\end{example}

It is well-known that different representations can change the result of maximizing entropy \citep{seidenfeld1986entropy,critique-maxent,friedman1971jaynes}. 
But some representations better capture a scenario more naturally than others, and PDGs 
% make it easy to select representations that interact appropriately with entropy. 
facilitate the choice of representations that interact well with entropy. 
% As discussed in \cref{sec:bn-pdg}, \cref{theorem:bns-are-pdgs} is a powerful example of how 
This is because, generaly speaking, PDGs force you to make your modeling choices in an explicit, uniform way (a point we will emphasize repeatedly in \cref{chap:one-true-loss}).
In \cref{ex:coin}, the PDG representation lead us to a space which, upon applying the principle of maximum entropy (or better yet, of minimum information deficiency) gives an answer that appears eminantly more reasonable than the simplest model we found without the PDG. 
% It is worth noting that the naive space is not even 
% It is difficult to imagine how one could even obtain the naive 
Indeed, it appears impossible to obtain a representation that leads 50/50 maximum entropy presumption when using a PDG.
\newmaterial{%
% \Cref{ex:coin} is far from a proof that using PDGs leads to representations that are more appropriate in which to apply the principle of maximum entropy, 
%     but it is a
% is just one (more) piece of supporting evidence, adding it to a pile that includes \cref{theorem:bns-are-pdgs}. 
}%

The construction in this section gives a generic way of 
    % leaving out information in a probabilistic model. 
    % leaving out information in a probabilistic model. 
    encoding ignorance in a probabilistic model. 
At a technical level, it amounts to exploiting the fact that a cpd can be viewed as imposing a barycentric coordinate system for a convex set of distributions (as laid out at the end of \cref{ssec:fin-prob}).
In the next section, we investigate another representation of uncertainty with more structure, which can be captured with PDGs in a rather different way.


\subsection{Belief and Plausibility Functions}
    \label{ssec:belplaus-as-pdg}
    
We now move on to another representation of uncertainty, which generalizes the notion of a probability distribution over a (for simplicity, finite) set $\Omega$, called a \emph{belief function} \citep{shafer1976mathematical}. 
Like a probability measure,
a belief function $\Bel$ assigns a degree of belief in $[0,1]$ to subsets $U \subseteq \Omega$.  
Belief functions must satisfy certain axioms ensuring that $\Bel(U) + \Bel(\bar U) \le 1$, and thus $\Plaus(U) := 1 - \Bel(\bar U) \ge \Bel(U)$. 
It can be shown that a probability distribution is the special case when these two relationships hold with equality, so that $\Bel = \Plaus$.  

Belief functions admit an alternate representation in terms of a \emph{mass function} $m : 2^\Omega \to [0,1]$, which yields belief and plausibility functions according to
\[
    \Bel_m(U) := \sum_{V \subseteq U} m(V)
    \qquad\text{and}\qquad
    \Plaus_m(U) := \sum_{\substack{V \subseteq \Omega \\ V \cap U \ne \emptyset}} m(V).
\]
Moreover, the correspondince is unique.  That is, there are 1-1 correspondence between belief functions $\Bel$, plausibility functions $\Plaus$, and mass functions $m$ \cite[Thm 2.6.3]{halpern-RAU}. 
The only requirements on $m$ are that:
\[
m(\emptyset) = 0
\qquad\text{and}\qquad
\sum_{A \subseteq \Omega} m(V) = 1.
\]
So, in other words, $m$ is a probability over non-empty subsets $V \subseteq \Omega$. 
There is also a natural relation between values of $V$ (i.e., subsets of $\Omega$) and values of $W$ (i.e., elements of $\Omega$): containment ($\ni$). 
% As we have seen, both of these 
Both $m$ and $\ni$ can be modeled with a PDG. What happens if we put them together?
Intuitively, this PDG describes a situation in which a subset $V \subseteq \Omega$ is drawn according to $m$, and then $\omega \in V$ is picked non-deterministically.

\begin{defn}
    If $m$ is the mass function representing the belief function $\Bel$ and the plausibility function $\Plaus$, then we associate 
    all of these objects with the same PDG,
    \[
    \dg M_{m}, \dg M_{\Bel}, \dg M_{\Plaus} := \quad
    \begin{tikzpicture}[center base]
        \node[dpad1] (V) at (0,0) {$V$};
        \node[dpad1] (W) at (2,0) {$W$};
        \draw[arr2,<-] (V) to node[above]{$m$} +(-1.2,0);
        \coordinate (crash1-pool) at (1,-0.35);
        \coordinate (crash1) at (1,-0.5);
        \draw[arr2,shorten >=0pt,-] (V) to[out=0,in=90] (crash1-pool);
        \draw[arr2,shorten >=0pt,-] (W) to[out=180,in=90] (crash1-pool);
        \draw[arr2,shorten >=0pt,shorten <=0pt,->>] (crash1-pool) to (crash1);
        \draw[arr2,<<-,shorten <=0pt] (crash1) to +(0,-0.2);
        % \coordinate (crash2) at (3, 0);
        % \draw[arr2,->>,shorten >=0pt] (W) to (crash2);
        % \draw[arr2,<<-,shorten <=0pt] (crash2) to +(0.2,0);
%
        \node[above=2ex of crash1] {$\ni$};
        % \node[above=1pt of crash2] {$ {\in} U$};
    \end{tikzpicture}~,
    \]
    where $W$ is a variable taking values in $\V W := \Omega$,
    and $V$ is a variable whose possible values $\V(V) := 2^\Omega$
        are subsets of $\Omega$. 
\end{defn}

For those who are not yet fluent with the constraint notation, $\dg M_m$ is just the PDG containing the mass function $m$ in the form of a distribution over subsets of $\Omega$, as well as a constraint that the actual world $W$ is a member of whatever subset is chosen by $m$. 

\begin{linked}{theorem}{belplaus-pdg}
    If $m$ is the mass function the belief function $\Bel$ and the plausibility
    function $\Plaus$, and $\dg M := \dg M_{\Bel} = \dg M_{\Plaus}$, then:
    % \begin{enumerate}[wide,label={(\alph*)},topsep=0pt]
    \begin{enumerate}[label={(\alph*)},topsep=0pt]
    \item 
        A distribution $\mu \in \Delta \Omega$ is the marginal
        of an extended distribution
        $\bar\mu \in \SD[\big]{\dg M} \subseteq \Delta(2^\Omega \times \Omega)$ 
        if and only if
        $\Bel(U) \le \mu(U) \le \Plaus(U)$ for all
        $U \subseteq \Omega$. 
    
    \item 
        For all $U \subseteq \Omega$, 
        $\displaystyle
        \aar*{\begin{tikzpicture}[center base]
            \node[dpad1] (V) at (0,0) {$V$};
            \node[dpad1] (W) at (2,0) {$W$};
            \draw[arr2,<-] (V) to node[above]{$m$} +(-1.2,0);
            \coordinate (crash1-pool) at (1,-0.35);
            \coordinate (crash1) at (1,-0.5);
            \draw[arr2,shorten >=0pt,-] (V) to[out=0,in=90] (crash1-pool);
            \draw[arr2,shorten >=0pt,-] (W) to[out=180,in=90] (crash1-pool);
            \draw[arr2,shorten >=0pt,shorten <=0pt,->>] (crash1-pool) to (crash1);
            \draw[arr2,<<-,shorten <=0pt] (crash1) to +(0,-0.2)
                %extend lower margin
                node[below]{}
                ;
            \node[above=2ex of crash1] {$\ni$};
            %
            \coordinate (crash2) at (3, 0);
            \draw[arr2,->>,shorten >=0pt] (W) to (crash2);
            \draw[arr2,<<-,shorten <=0pt] (crash2) to +(0.2,0);
            \node[above=1pt of crash2] {$ {\in} U$};
        \end{tikzpicture}}
         = - \log \Plaus ( U ).
        $
        That is, the inconsistency of simultaneously
        having belief function $\dg M_{\Bel}$ and also that the event $U$ occurs, is the analogue of the information content but for plausibility.
    % \item 
    \end{enumerate}
\end{linked}

% A few remarks before we prove the theorem. 
% This theorem, which we shall prove momentarily, 
This theorem shows that PDGs of this form coincide semantically with Dempster-Shafer belief functions. 
Part (a) states that the set-of-distribution semantics is precisely the set-of-distribution semantics for $\Bel$.
Indeed, if we follow the conventions of \citet[Theorem 2.6.1]{halpern-RAU} and define $\mathcal P_\Bel := \{ \mu \in \Delta \Omega : \Bel(U) \le \mu(U) \le \Plaus(U)$ for all $U \subseteq \Omega \}$, 
\unskip\footnote{For observant readers who followed the reference: it is not hard to see that the upper and lower bounds are equivalent; \citet{halpern-RAU} uses only the lower bound, but we give both for symmetry.}
then we have shown that $\mu$ is compatible with $\dg M_{\Bel}$ if and only if is an element of $\mathcal P_{\Bel}$. 

Meanwhile, part (b) states that something quite intuitive
%
% : after adding a constraint
% that event $U$ occurs, the resulting degree of inconsistency is a direct function of the (im)plausibility of $U$ to begin with---if $U$ was completely plausible, there is no inconsistency in learning $U$. 
\unskip, but might benefit from being unpacked somwhat.  The PDG on the left-hand side is just $\dg M_{\Bel}$ plus a constraint representing the event $U$. 
How inconsistent is it to have belief function $\Bel$ and also believe $U$ occurs with certainty? 
It's the analogue of the information content $\I_\mu[U] = - \log \mu(U)$, but with plausbility instead of probability. 
Indeed, if $\Plaus(U) = 1$, then there is no inconsistency to also believing $U$ occurs, and if $\Plaus(U) = 0$, then it's so implausible so as to be infinitely inconsistent. 

% Now for the proof of the theorem.
Now for the proof of the theorem.
% The proof of \cref{theorem:belplaus-pdg} involves some interesting
%     combinatorics, so we present it here. 

\begin{lproof}
        \label{proof:belplaus-pdg}
    \textbf{(a)}~
    The forwards direction is easy. 
    Fix some $\bar\mu \in \SD{\dg M}$, let $\mu \in \Delta \Omega$ be its marginal on $W$. 
    Select an arbitrary $U \subseteq \Omega$. 
    % In an outcome $()$
    Keep in mind that $\bar\mu$ is a joint distribution over pairs $(A, \omega)$ that satisfy $\omega \in A \subseteq \Omega$, whose marginal on $A$ is distributed according to $m$. 
    For every such pair $(A, \omega) \in \Supp \bar\mu$, 
    if $A \subseteq U$ then clearly $\omega \in U$. 
    Thus, $\Bel(U) = \sum_{A \subseteq U} m(A) \le \sum_{\omega \in U} \mu(W{=}\omega) = \mu(U)$.
    Similarly, if $\omega \in U$, then it must be that $A \cap U \ne \emptyset$, 
    so $\mu(U) \le  \sum_{A : A \cap U \ne \emptyset} m(A) = \Plaus(U)$.

    % For the reverse direction, 
    The reverse direction is far more subtle than it appears;
        here we present a relatively compact proof of it based on Hall's
        Mariage Theorem \citep{Hall1935}. 
    Suppose that $\mu \in \Delta \Omega$ satisfies
        $\Bel(U) \le \mu(U) \le \Plaus(U)$ for all $U \subseteq \Omega$.
    Assume for simplicity that the possible outputs of $m$ and $\mu$ are rational numbers, and let $N$ be their common denominator. 
    We now construct a bipartite graph $G = (L, R, \mathcal E)$ with $|L| = |R| = N$ vertices in each part.
    The left part $L = \sqcup_{\omega \in \Omega} L_\omega$ 
    is partitioned to be in bijection with the elements of $\Omega$, with
        $|L_\omega| = \mu(\omega)\cdot N$. 
    % (This is made possible by our assumption that $\mu$ is )
    Symmetrically, $R = \sqcup_{A \in \Omega} R_A$, where each $R_A$
        consists of $|R_A| = m(A) \cdot N$ vertices.
    For notational convenience, for $u \in L$, let $\omega_u$ be the element of $\Omega$ corresponding to the partition containing $u$; symmetrically,  let $A_v$ be the subset associated with the partition of $v \in R$. 
    Finally, let $\mathcal E := \{ (u,v) \in L \times R : \omega_u \in A_v\}.$
    Observe that a perfect matching $E \subseteq \mathcal E$ in the graph $G$ amounts to a coupling between the marginal distributions $\mu \in \Delta\Omega$ and $m \in \Delta(2^\Omega)$, supported only on $(\omega, A)$ such that $\omega \in A$.
    To be fully precise, that coupling would be a joint distribution $\bar\mu_E(\omega, A) := \frac1N |\{ (u,v) \in E : \omega_u \in A_v \}|$ that is both an element of $\SD{\dg M_\Bel}$ and an extension of $\mu$. 
    What remains is only to show that there exists such a perfect matching, for which we turn to Hall's Mariage criterion.     
    
    Hall's Mariage Theorem states that there exists a perfect matching
    for $G$ if and only if, $|T| \le  |\partial_G(T)|$ for all $T \subseteq L$,
    where $\partial_G(T) = \{ v \in R : \exists u \in T.~\omega_u \in A_v\}$ is the set of vertices connected to $T$ by an edge. 
    We now prove that this is the case.
    Choose any subset $T \subseteq L$ of vertices, and define $U_T := \{ \omega \in \Omega : T \cap L_\omega \ne \emptyset \}$ to be the elements of $\Omega$ represented by some vertex in $T$. 
    On one hand,
    \[ 
        |T| \le | \bigcup_{u \in T} L_{\omega_u} | 
            =  \sum_{\substack{\omega \in \Omega \\ L_\omega \cap T \ne\emptyset }}
            = \mu(U_T).
    \]
    On the other hand, 
    \begin{align*}
        |\partial_G(T)] &=  |\{ v \in R : \exists u \in T.~\omega_u \in A_v \}| \\
            &= \sum_{\substack{A \subseteq \Omega \\ \exists u \in T.~ \omega_u \in A}} m(A)
            = \sum_{\substack{A \subseteq \Omega \\ U_T \cap A = \emptyset}} m(A)
            = \Plaus(A).  
    \end{align*}
    But recall that, by assumption, $\mu(U_T) \le \Plaus(U_T)$. 
    This allows us to 
    chain together the previous seven (in)equalities, thereby proving that $|T| \le |\partial_G(T)|$ as desired.
    By Hall's criterion, this means there must exist a perfect matching, and we have already seen this means $\mu$ can be extended to some $\bar\mu \in \SD{\dg M_{\Bel}}$, as desired.
    % \begin{align*}
    %     \mu(U_T) &\le \Plaus(U_T)
    %         &= \sum_{\substack{A \subseteq \Omega \\ A \cap U_T \ne \emptyset}} m(A)
    %         &= |\{ v \in R : \exists u \in T :  \}
    % \end{align*}
    % $\mathcal A_T := \{ A \subseteq \Omega : \exists u \in T : \omega_u \in A\}$.
    % \begin{align*}
    %     \sum_{\substack{A \subseteq \Omega \\ A \cap U \ne \emptyset}}
    % \end{align*}
    % For the reverse direction, we first need to build up a few concepts first.  
    % According to that theorem, for every $U \subseteq \Omega$, we know that $\Bel(U) = \inf_{\mu' \in \mathcal P_\Bel} \mu'(U)$, and $\Plaus(U) = \sup_{\mu' \in \mathcal P_\Bel} \mu'(U)$.  Since $\mathcal P_\Bel$ is closed and bounded, the infimum and supremum are achieved. For a given $U$, let $\mu_*^{U}$ and $\mu^*_{U}$ denote the minimizing and maximizing distributions, respectively, so that $\Bel(U) = \mu_*^{U}(U)$ and $\Plaus(U) = \mu^*_{U}(U)$.  
    %
    % We can now address the reverse direction directly. Suppose that $\mu$ satisfies
    % $\Bel(U) \le \mu(U) \le \Plaus(U)$ for all $U \subseteq \Omega$---that is, $\mu \in \mathcal P$. We now construct a cpd $q(W \mid V)$ according to 
    % \[
    %     q(\omega \mid A) := \mu_*^{A}(\omega) 
    %         + \frac{\mu(A) - \Bel(A)}{\Plaus(A) - \Bel(A)} (\mu^*_{A}(\omega) - \mu_*^{A}(\omega) ).
    % \]
    % $q(W|A)$ is a probability distribution for every fixed $A$, since it is a convex combination of two probability measures.
    % \begin{align*}
    %     % \Big(\text{ e.g., })
    %     \sum_{\omega \in \Omega} q(\omega \mid A)
    %     &= \sum_{\omega \in \Omega} \mu_*^{(A)}(\omega) 
    %     + \frac{\mu(U) - \Bel(U)}{\Plaus(U) - \Bel(U)} \sum_{\omega \in \Omega} (\mu^*_{(A)}(\omega) - \mu_*^{(A)}(\omega) ) 
    %     = 1 + 0. 
    % \end{align*}
    % We claim that the distribution $\bar\mu(V, W) := m(V) q(W | V)$ is an extension of $\mu$ and an element of $\SD{\dg M_{\Bel}}$. 
    % \begin{itemize}
    %     \item We start by showing that $\bar\mu(W) = \mu(W)$. 
    %     \begin{align*}
    %         &\sum_{A \subseteq \Omega} \bar\mu(A, \omega)  \\
    %         &= \sum_{A \subseteq \Omega} m(A)  
    %         \mu_*^{(A)}(\omega) 
    %         + 
    %         \sum_{A \subseteq \Omega} m(A) \frac{\mu(A) - \Bel(A)}{\Plaus(A) - \Bel(A)} (\mu^*_{A}(\omega) - \mu_*^{A}(\omega) )\\
    %         &=
    %     \end{align*}
    %     \item 
    % Clearly its marginal on $V$ is $m$, as required. 
    % \end{itemize}
    


    
    \textbf{(b)}~
    Let $\mathcal P := \{ \bar\mu \in \Delta(2^\Omega \times \Omega) : \bar\mu(W \in U) = \bar\mu(W \in V) = 1\}$ be the set of all distributions satisfying the hard constraints
    of the PDG in question. 
    Let $\mathcal A_U := \{ A \subseteq \Omega : A \cap U \ne \emptyset\}$
    be the collection of subsets of $\Omega$ with non-empty intersection with $\Omega$.
    It is not not too difficult to see that the marginal projection of $\mathcal P$
        onto the variable $V$ 
        is the same as the set
    $\Delta \mathcal A_U$ of distributions supported on sets with non-empty intersection with $U$. 
    With these facts in mind, we calculate
    \begin{align*}
    \aar*{\begin{tikzpicture}[center base]
        \node[dpad1] (V) at (0,0) {$V$};
        \node[dpad1] (W) at (2,0) {$W$};
        \draw[arr2,<-] (V) to node[above]{$m$} +(-1.2,0);
        \coordinate (crash1-pool) at (1,-0.35);
        \coordinate (crash1) at (1,-0.5);
        \draw[arr2,shorten >=0pt,-] (V) to[out=0,in=90] (crash1-pool);
        \draw[arr2,shorten >=0pt,-] (W) to[out=180,in=90] (crash1-pool);
        \draw[arr2,shorten >=0pt,shorten <=0pt,->>] (crash1-pool) to (crash1);
        \draw[arr2,<<-,shorten <=0pt] (crash1) to +(0,-0.2);
        \node[above=2ex of crash1] {$\ni$};
        %
        \coordinate (crash2) at (3, 0);
        \draw[arr2,->>,shorten >=0pt] (W) to (crash2);
        \draw[arr2,<<-,shorten <=0pt] (crash2) to +(0.2,0);
        \node[above=1pt of crash2] {$ {\in} U$};
    \end{tikzpicture}}
    &= \inf_{\bar \mu \in \mathcal P} \kldiv{ \mu(V) }{ m }\\
    &= \inf_{\nu \in \Delta\{ A \subseteq \Omega : A \cap U \ne \emptyset\}}
            \Ex_{A \sim \nu} \Big[ \log \frac{\nu(A)}{m(A)} \Big],
    % \intertext{(since the objective on first line does not depend on the marginal on $W$, except through the constraint)}
    \end{align*}
    since the objective on first line does not depend on the marginal on $W$, except through the constraint.
    For those who are not yet used to the notation, recall that the PDG in question is just $\dg M_m$ together with a constraint that the value of the variable $W$ lies in the given set $U$.   
    In turn, this inconsistency is equal to
    \begin{align*}
    % &=
     \inf_{\nu \in \Delta \mathcal A_U}
            \sum_{A \in \mathcal A_U}
                \nu(A) \log \frac{\nu(A)}{m(A)} 
    \ge \Big({\textstyle\sum_{A \in \mathcal A_U} \nu(A)} \Big)
        \log \frac{\textstyle\sum_{A \in \mathcal A_U} \nu(A)}{{\textstyle\sum_{A \in \mathcal A_U} m(A)}},
    \end{align*}
    by the log-sum inequality \citep{CoverThomas}.
    Moreover, the log-sum inequality states that this holds 
    with equality if and only if there is some constant $k$ so that
        $\nu(A) = k m(A)$ for all $A \in \mathcal A_U$. 
    By definition, $\Supp \nu \subseteq \mathcal A_U$, and thus
    $\sum_{A \in \mathcal A_U} \nu(A) = 1$. Therefore the inconsistency 
    we have been calculating is equal to
    \[
        - \log \sum_{A \in \mathcal A_U} m(A) = - \log \Plaus(U).
        \qedhere
    \]
\end{lproof}




\subsection{Pseudomarginals on Cluster Graphs}
    \label{ssec:pseudomarginal-cluster-graph}

Many inference algorithms for graphical models make use of a data-structure that tracks various marginal distributions, which may not be consistent \citep{wainwright2003tree,wainwright2008graphical,KF09},
sometimes called a \emph{pseudomarginal} \citep{wainwright2008graphical} or a \emph{cluster graph (with associated data)} \citep{KF09}; we will call them \emph{cluster marginals}.

Concretely, given a set of variables $\X$, a cluster graph is 
graph, whose node set is a collection $\C$ of clusters, each $C \in \C$ of which is a subset of $\X$. 
% When adjacent clusters agree on the marginal distr
A cluster marginal $\bmu = \{ \mu_C(C) \}_{C \in \C}$ consists of a distribution $\mu_C \in \Delta \V C$ over the variables in each cluster $C \in \C$.
Because $\bmu$ is just a collection of joint distributions, 
it is exactly what is needed to supply observational data for a hypergraph
$
    \Ar = \{ \varnothing \to C \}_{C \in \C}.
$
Let $\dg M_{\bmu}$ denote this PDG.
 
A cluster marginal is said to be \emph{calibrated} if, for every edge $C{-}D$ in the cluster graph, the distributions $\mu_C$ and $\mu_D$ agree the marginals of their common variables, which is a necessary condition for $\dg M_{\bmu}$ to be consistent.
Belief propogation is a way of calibrating these objects, and effectively is a way of reducing local inconsistencies between pairs of clusters. We will return to this in \cref{chap:LIR}. 

When the cluster graph has a special property that makes it a \emph{tree decomposition} 
    (see \cref{sec:tw} or ```clique tree'' in \citep{KF09}),
then local consistency is equivalent to global consistency \citep{wainwright2008graphical},
and so a cluster marginal $\bmu$ of this kind is calibrated if and only if $\SD{\dg M_{\bmu}} \ne \emptyset$.
We will cover these concepts in detail in \cref{chap:infer} when we
    use them to develop an inference algorithm for PDGs.
Here, we merely point out that these data structures are themselves PDGs. 

\subsection{Causal Models}
    \label{ssec:capture-causal-models}
    
A \emph{structural equations model} (SEM) \citep{pearl2009causality} is a collection of equations that explain how each ``endogenous variable'' gets its value, using a mechanism that can depend on other variables in the model.
In other words, a SEM is a collection of (deterministic) functions, each taking as input joint settings of variables, and outputting a single value.
Since deterministic functions are special cases of cpds, there is a natural way to regard a causal model as a PDG, and moreover, the confidence weights do not impact the semantics when the cpd is deterministic.  
Furthermore, the usual way of adding probability to a causal model is no different from adding a probability distribution to the corresponding PDG. 
The semantics of the resulting PDG exactly capture the behavior of causal model in the absence of intervention.

We have not yet given an analogue of intervention for a general PDG, although it's straightforward to encode interventions by altering the functions in the usual way (i.e., replacing the causal equations with constants).
We will make all of this precise in \cref{sec:causal}.
More broadly, \cref{chap:QIM} develops a framework that allows us to something much deeper about the relationship between causal models and a concept called \emph{QIM-compatibility} (the subject of \cref{chap:QIM}), a notion closely related to (and captured by) the qualitative information in a PDG. 

\subsection{Implicit Neural Representations}
    \label{ssec:implicit-neural-repr}

A more modern way to represent knowledge is implicitly, by means of the parameters of a neural network. 
But neural networks are often viewed as conditional probability distributions, and hence can be directly included as the quantitative data for an arc of a PDG.
So too can the other components of machine learning systems such as data, priors, and observations. 
Indeed, as we will see in \cref{chap:one-true-loss,chap:LIR}, the PDG formalism has a great deal to say about modern neural representations as well.

Visually, it is uncommon to use the notation of graphical models to describe the architecture of neural networks; instead, the standard is to represent operations and layers of a neural network as nodes, and their connections as edges---like a circuit diagram.
In many cases, the duals of these diagrams, appropriately annotated with the network parameters, are literally PDGs.

\TODO[TODO: diagram here would be nice touch!]




\begin{subappendices}
\section{Proofs}

\recall{prop:constraints-work}
\begin{lproof} \label{proof:constraints-work}
    1.
    If $\mu \in \SD{R\,}$, then $\mu(\Tru{=}\trut) = 1$, so if $\mu(\mat X{=}\mat x) > 0$, then $\mu(\Tru{=}\trut \mid \mat X{=}\mat x) = 1$ for all joint settings $\mat x \in \V(\mat X)$. But $\mu(\Tru{=}\trut \mid \mat X{=}\mat x) = R(\mat x)$, and thus $R(\mat x) = 1$.
    Conversely, if $\mu(\mat X, \Tru) = \mu(\mat X)\delta(\Tru{=}\trut)$ and $\mu(R) = 1$, then 
    \[
        \mu(\Tru \mid \mat X)
        = (\mu(\mat X) \delta(\Tru {=} \trut)) / \mu(\mat X)
        = \delta R(\Tru \mid \mat X).
    \]
    Thus $\mu$ is consistent with all conditional probabilities in the widget $\PDGof R$,
    and so $\mu \in \SD{\dg M_R} = \SD{R\,}$.
    
    2. 
    (a)
    If $\mu \in \SD{\dg M + R}$, then by definition $\mu(\Tru{=}\trut) = 1$. By the same reasoning as in part 1, we also find that $\mu(R) = 1$. We also know that $\mu$ satisfies the cpds of $\dg M$, so $\mu(\X) \in \SD{\dg M}$. 
    Conversely, if $\mu(\X) \in \SD{\dg M}$, and $\mu(\Tru{=}\trut) = \mu(R) = 1$, 
    then, again by the logic in part 1, $\mu(\Tru \mid \mat X) = \delta R$.
    Therefore $\mu$ satisfies all cpds of $\dg M$, and all cpds of the widget $\PDGof{R}$, and thus $\mu \in \SD{\dg M + R}$.
    
    (b) 
    % In search of a contradiction, suppose that there is some 
    For all 
    $\mu \in \bbr{\dg M}^*_\gamma$, we have
     % with $\mu(R) < 1$. 
    % Then there is some setting $\mat x$
    % If $\mu(\Tru{=}\trut) < 1$, then $$
    \begin{align*}
        \aar{\dg M}_\gamma 
        &= \bbr{\dg M}_\gamma^*(\mu) 
        \\&\ge \Ex_{\mu} \Big[
            \log \frac{\mu(\Tru)}{\mathbbm1[\Tru=\trut]}
            + \log \frac{\mu(\Tru \mid \mat X)}{\delta R(\Tru \mid \mat x)}
        \Big]
        \\&= \infty \cdot \mathbbm1[\mu(\Tru{=}\trut)=1] + 
            \infty \cdot \Ex_{\mat x \sim \mu}\big[ \mat x \notin R \Rightarrow \mu(\Tru{=}\trut\mid\mat x)=0 \big]
        .
    \end{align*} 
    Now, suppose that $\mu(R) < 1$, meaning there is some mass on joint settings $\mat x \notin R$.
    If $\mu(\Tru{=}\trut \mid \mat x) \ne 0$, then the score is infinite, by the second term. 
    Yet if $\mu(\Tru{=}\trut \mid \mat x) = 0$ and there is some mass on $\mat x$, then 
    $\mu(\Tru{=}\trut) < 1$, and so again the score is infinite, by the first term.
    Thus, if the score is finite, then $\mu(R) = 1$. 
    
    (c) The inequality is an instance of monotonicity (which we see in \cref{lemma!} and is the focus of \cref{chap:reason}). 
    % For the remainder of this proof, identify distrib
    Suppose there is some $\mu \in \bbr{\dg M}^*_\gamma$ with $\mu(R) = 1$.
    % Define 
    % If $\Tru \notin \X$, then d
    Define $\bar\mu(\X, \Tru) := \mu(\X)\delta(\Tru{=}\trut)$.
    By the same logic as in the proof of part 1 of the proposition, 
    $\mu$ satisfies all of the cpds of the widget $\PDGof R$; and thus 
    $\bbr{\dg M + R}(\bar\mu) = \bbr{\dg M}_\gamma(\mu)$.
    Chaining together some inequalities, we find:
    \[
     \aar{\dg M + R}_\gamma \le 
     \bbr{\dg M + R}(\bar\mu) 
     = \bbr{\dg M}_\gamma(\mu)
     = \aar{\dg M}_\gamma
     \le \aar{\dg M + R}_\gamma, 
     \]
    and thus all of these quantities are equal.
    % The same logic works by taking $\bar\mu = \mu$, if $\Tru \in \X$ and $\mu(\Tru{=}\trut)=1$. 
    % Finally, if $\Tru \in \X$ and $\mu(\Tru{=}\trut) < 1$, then 
    % \[
    %     \aar{\dg M}_\gamma^* = \bbr{\dg M}^*_\gamma(\mu)
    % \]
    
    Conversely, suppose
    that $\aar{\dg M + R}_\gamma = \aar{\dg M}_\gamma$.
    % \[
    %     \bbr{\dg M + R}_\gamma \subseteq = \bbr{\dg M}^*_\gamma
    % \]
    If $\bbr{\dg M} = \infty$, then $\bbr{\dg M}_\gamma^*$ consists of 
        all distributions, and so the statement is vacuous. 
    On the other hand, 
    select some $\bar\mu \in \bbr{\dg M + R}_\gamma^*$.
    By applying part (b), we find that $\bar\mu(R) = 1$. 
    And, because $\aar{\dg M + R}_\gamma = \aar{\dg M}_\gamma$
    and $\bar\mu$ is an optimal distribution for $\dg M + R$, 
    we find that its marginal $\mu(\X) := \bar\mu(\X)$
    is optimal for $\dg M$:
    \[
        \bbr{\dg M}_\gamma(\mu(\X))
        % = \Ex_{\omega \sim \mu}[ \sum_{a \in \Ar} ]
        = \aar{\dg M + R}_\gamma 
        = \aar{\dg M}_\gamma
        \le \bbr{\dg M}_\gamma(\mu(\X))
    \]
    Thus, we have constructed $\mu \in \bbr{\dg M}^*_\gamma$ satisfying $\mu(R) =1$. 
\end{lproof}


\end{subappendices}
