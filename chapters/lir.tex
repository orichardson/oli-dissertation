\begin{abstract}
% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
We present a generic algorithm
    for learning and
    approximate inference
    across a broad class of statistical models,
    that unifies many approaches in the literature.
Our algorithm,
    called local inconsistency resolution (LIR),
    has an intuitive epistemic interpretation.
It is based on the theory of
    probabilistic dependency graphs (PDGs),
    an expressive class of graphical models
        rooted in information theory,
        which can capture inconsistent beliefs.
    % can be viewed as a generic recipe for
    % learning and approximate inference.
\end{abstract}

\section{Introduction}
% What causes you to update your beliefs?
% To reconsider your point of view?
% How do you show someone that they are mistaken?
% By identifying and resolving inconsistencies.
% What causes a person to change their beliefs?
% Often it is to resolve inconsistencies, amongst one's beliefs and observations.
% How do you change a person's mind?
% One way is to present evidence that contradicts their beliefs; another is to point out inconsistencies between the beleifs they already hold.
What causes a person to change their mind?
% One theory suggests
% Some say
According to some,
    % it is a response to inconsistency:
    it is a response to internal conflict:
    the result of
    discovering new information that contradicts our beliefs, or
    becoming aware of discrepancies between beleifs we already hold
    \cite{festinger1962cognitive}.
% The process is often a gradual one: inconsistencies are dealt with incrementally, and one at a time.
% However, people do not change their minds all at once, keeping in mind everything they know;
% However, it is impossible to respond to this inconsistency all at once, keeping in mind everything you know;
% Indeed, we can only respond to the ones we are aware of, and inconsistencies can be difficult to detect.
% Inconsistency
Inconsistencies
    can be difficult to detect, however
    \cite{selman1996generating}, and indeed can only
    be resolved once we are aware of them.
% Further complicating things,
% Typically
% Further complicating our lives, some parts of the picture are beyond our control;
% A further complication is that some things may be
% Some things may also be
Some things are also
 beyond our control;
    for example, we might recieve conflicting information
        from two trusted sources and be unable to resolve
        their disagreement.
    % impossible to respond to this inconsistency all at once, keeping in mind everything you know;
% So in practice, we resolve inconsistencies \emph{locallly}---little by little, and one at a time.
So in practice, we resolve inconsistencies \emph{locally}---%
    % by looking at only a small part of the picture,
    % and changing another part of it.
    % attending to and addressing
    little by little, and looking at
        only a small part of the picture at a time.
    % ---little by little, and one at a time.

This can have externalities; fixing one inconsistency can easily create
    others out of view.
Furthermore, some inconsistencies
    are not local in nature,
    and can only be seen when considering many components at once.
% Nevertheless, this process of locally resolving inconsistency can be quite useful.
% Nevertheless,
Yet despite its imperfections,
    this process of locally resolving inconsistency  can be quite useful.
    % And, as we have formalized it in the probabilistic setting,
    As we shall soon see,
    it is a powerful recipe for learning and approximate inference.
% In fact, we will show that it naturally captures a broad class of
% We provide a formalization of it, which we use to show how
%
% We provide a formalization of this process, and show
% We formalize the intuition above
% We provide a formalization of this process
We formalize the process
    in the language of probability and
    convex optimization,
    and show how that many popular techniques in the literature
    % can be viewed as local inconsistency resolution.
    arise naturally as instances of it.
    % Finally, we propse a new algorithm
% We argue that this intuition applies not only to people, but also to a
    % broad class of AI systems from the last 20 years.
% This paper describes a general process by which this occurs,

% Historically, much of the belief revision literature has focused on
% conflicts between propositions and entrenchment \cite{agm,...}.


Our approach leans heavily on the theory of
Probabilistic Dependency Graphs (PDGs), which are very flexible graphical
    models that allow for arbitrary---even inconsistent---%
    probabilistic information, weighted by confidence \cite{pdg-aaai}.
% In addition, PDG semantics
There is a natural way
% information-theoretic way
    % of measuring the inconsistency of a PDG,
    to measure how inconsistent a PDG is,
    and many standard loss functions
    can be viewed as measuring the inconsistency of a PDG that
    describes the appropriate situation \cite{one-true-loss}.
\vfull{
    Recently, techniques have been developed to calculate this inconsistency
    in polynomial time for bounded tree-width,
    although it scales exponentially with the tree-width of the graph \cite{pdg-infer}.
    As we move to variables that are continuous variables of large dimension,
        it becomes intractable to calculate this global inconsistency
        even for small graphs---the log evidence
        of a latent variable model \cite{one-true-loss} can be
        represented as a PDG inconsistency, for example.
}
% Our work
% We provide an algorithm that
%
   % implicit when we regard inconsistency as a loss function,
   % and generalizing it to training procedures other than .
   % allowing us to describe details about the training process other than
   % ``somehow minimize this global loss''.
%
% It can be intractable to calculate for .
% Calculating the inconsistency of a PDG can be intractable.
% Calculating a PDG's inconsistency can be intractable.
% Our algorithm
We introduce an algorithm
    to operationalize
    the process of adjusting parameters
    to resolve this inconsistency.

% Calculating a PDG's inconsistency can be intractable.
In general, even just calculating a PDG's degree of inconsistency is intractable.
% In general, calculating a PDG's degree of inconsistency can be intractable.
% Computing the degree of inconsistency can be intractable.
% \citet{one-true-loss} shows how variational inference can
%     be understood as the adoption of extra beliefs to
%     % overapproximate the overall inconsistency in a way that is easier to calculate.
%     % in a tractable way.
%     get an overapproximation that is easier to calculate.
% Variational inference can be understood as
Much of variational inference can be understood as
    adopting extra beliefs to
    % overapproximate the overall inconsistency in a way that is easier to calculate.
    % in a tractable way.
    % overapproximate it in a way that is easier to calculate \cite{one-true-loss}.
    minimize an over\-approximation of it that is easier to calculate \cite{one-true-loss}.
% Our algorithm allows for this kind of
% Our approach extends this by also enabling the opposite:
Our approach can capture this, but also enables the opposite:
    % also enables the opposite:
% This is one use of our algorithm,   but we also enable the opposite:
    focusing on small parts of the graph at a time to
     % get
    address tractable under\-approximations of the global inconsistency.
% We show how this technique specializes and suggests a
% Thus, it also has more potential for parallelization.
% Thus, it enjoys benefits such as higher potential for parallelization.
% Thus, it many many instances
This makes it more suitable for distributed settings,
    and more amenable to parallelization.
% Thus, it is potentially easier to parallelize.
% This makes it more natural in distributed settings.
% It is also very expressive.
% Our algorithm, called \emph{local inconsistency resolution} (LIR),
% But perhaps the most surprising aspect of our algorithm,
%     called \emph{local inconsistency resolution} (LIR),
%     is how much .
%
The algorithm, which we call \emph{local inconsistency resolution} (LIR),
    is quite expressive,
    and naturally reduces to a wide variety of learning and inference algorithms in the literature.
    % under the appropriate modeling assumptions.
This observation suggests a generic approach to learning and
    inference in models with arbitrary structure.
%
% Yet often, by placing additional confidence in some parts of the picture,
% or restricting our attention to some small part of it, the inconsistency
% can be calculated more easily.



%% CALL FOR PAPERS WANTS:
% Inference and generating methods for graphs, time series, text, video, and other structured modalities.
% >> will present this in the case study?
%
% Unsupervised representation learning of high dimensional structured data.
% >> a generic algorithm for representation learning:
% >>    describe structure.  Invent or randomize variables + memory architecture.  Choose set of views,
% >>    always including views and
%
% Scaling and accelerating inference and generative models on structured data.
% Uncertainty quantification in AI systems.
%
% Thus, our work may be viewed in several lights.
%     % - a very general probabilistic inference algorithm for structured data;
% On one hand, it is a generic inference algorithm.
%     - a broad class of natural optimizations for inference algorithms;
%     - a generic way of learning


% Many algorithms can be cast in terms of locally minimizing inconsistency.

% \section{Background and Preliminaries}
% \section{Technical Preliminaries}
\section{Mathematical Preliminaries}
% \section{Mathematical Formalism}
% Each variable $X$ can take on values from a set $\V\!X$ of possible values.
We write $\V\! X$ for the set of values that a varible $X$ can take on,
and $\Delta \V \! X $ for the set of distributions over $\V\!X$.
% Glump glump glump glump glump glump glump.
A conditional probability distribution (cpd) is a map
% (cpd)
$p(Y|X) : \V \!X \to \Delta\mskip-1mu \V \mskip-1mu Y$.
% is a distribution over the values of $Y$ for each value of $X$.
%
% \begin{defn}
    A \emph{directed hypergraph}
    % $(N, \mathcal A)$ is a set $N$ of nodes, and a set $\mathcal A$ of \emph{arcs},
    $(N, \mathcal A)$ is a set of nodes $N$ and a set of arcs $\mathcal A$,
    % $(N, \mathcal A)$ is a set of nodes $N$ and a set of arcs $\Ar$,
    % each $a \in \mathcal A$ of which is associated with
    % so that $a \in \Ar$ is associated
    % to each $a \in\! \Ar$ of which we associate
    each $a \in \mathcal A$ of which
    is associated with
    % comes with
    a set $\Src a \subseteq N$ of source nodes,
    and $\Tgt a \subseteq N$ target nodes.
    We also write $\ed {\scriptstyle a}{S}{T} \in \Ar$ to specify an
    arc $a$ together with its sources $S = \Src a$ and targets $T = \Tgt a$.
% \end{defn}

% \paragraph{Geometry.}
\textbf{Geometry.}
% We will need various parameter spaces $\Theta$; to simplify the presentation, assume that parameter spaces are a convex subsets of $\mathbb R^n$ (not necessarily of the same dimension).
We will need various parameter spaces $\Theta$.
    To simplify the presentation, assume that each $\Theta$ is a convex subset of $\mathbb R^n$ (not necessarily of the same dimension).
A \emph{vector field} over $\Theta$ is a differentiable
    map $X$ assigning to each $\theta \in \Theta$ a vector $X_\theta \in \mathbb R^n$.
The \emph{gradient} of a twice differentiable map $f : \Theta \to \mathbb R$,
    % written $\nabla_\theta f(\theta)$,
    which we write $\nabla_\Theta f(\Theta)$, is a vector field.
Given a vector field  $X$ and an initial point $\theta_0 \in \Theta$, there is a unique trajectory $y(t)$ that solves the ODE
$\{\frac{\mathrm d }{\mathrm d t}y(t) = X_{y(t)}$,  $y(0) = \theta_0\}$,
and we adopt the notation $\exp_{\theta_0}( X ) := y(1)$
    % which is standard in differntial geometry,
    for a compact description of it.
    % this trajectory.
    % compactly refer to this curve, which is standard in many contexts \cite{lee.smooth-manifolds}.
At first glance,
% Although on the surface
% Although it may seem
 $\exp$ only gives us access to $y(1)$,
    but
    % it is easy to show
     it is easily verified
     that $\exp_{\theta_0}(t X) = y(t)$.
% Although it may not be obvious, $\exp$ indeed allows us to describe the entire trajectory, because $\exp_{p_0}(tX) = p(t)$.
So altogether, the map $t\mapsto \exp_\theta(t \nabla_\Theta f(\Theta))$ is the smooth path beginning at $\theta$ that follows the gradient of $f$. It is known as
\emph{gradient flow}.

\vfull{
Given a manifold $\Theta$ and a differentiable map $P : \Theta \to \Delta \V\! X$,
     the Fisher Information Matrix
$\mathcal I(\theta)
    % = [ \Ex_{x\sim P(\theta)}[ \frac{\partial^2}{\partial \theta_i \partial \theta_j}  \log p_\theta(x) ]]_{i,j}
$
at each $\theta \in \Theta$
 % forms such a Riemannian metric;
gives rise to a Riemannian metric;
thus the mere fact that $\Theta$ parameterizes a family of
probability distributions is enough to make it a Riemannian manifold.
% by $\mat u, \mat v \mapsto \mat u^{\sf T} \mathcal I(\theta) \mat v$.
Moreover, $\mathcal I(\theta)$ is particularly natural in a probabilistic context;
    up to a multiplicative constant, it is the \emph{only} such metric on $\Theta$ that is invariant under sufficient statistics, \cite{chentsov}. \cite{amari2016information}
}

% \subsection{PDGs}
% \paragraph{Probabilistic Dependency Graphs.}
\textbf{Probabilistic Dependency Graphs.}
% \paragraph{PDG Syntax.}
A PDG is a
    directed
    graph whose arcs carry
    probabilistic and causal information, weighted by confidence \cite{pdg-aaai}.
% Equivalent variants of PDGs were defined by \citet{one-true-loss,pdg-infer};
% we give another variant whose explicit parametric nature is useful for our purposes.
% We now introduce a new, equally expressive variant,
We now introduce an equally expressive variant,
    whose explicit parametric nature will prove useful for our purposes.
    % whose explicit parametric nature will prove useful.
% ---%
    % yet it too is essentially equivalent to the others.
% We give yet another variant
% parametric variant of a PDG, which is essentially
    % equivalent (see \cref{appendix:internalization})

% \begin{defn}
%     A PDG $\dg M \!=\! (\X\mskip-2mu, \Ar, \mathbb P, \balpha, \bbeta )$
%     is a directed hypergraph
%         $(\X\mskip-2mu, \Ar)$
%     whose nodes correspond to
%     variables,
%     % together with
%     % probabilities $\mathbb P$
%     %     and
%     %     confidence vectors
%     %     $\balpha \!=\! [\alpha_a]_{a \in \Ar},\bbeta \!=\! [\beta_a]_{a \in \Ar}$,
%     %     so that
%     each arc
%     % $\ed aST \in \Ar$
%     $a \in \Ar$ of which is associated with:
%     % arc $a \in \Ar$ is associated with:
%
%     \begin{itemize}[nosep,itemsep=2pt]
%         \item (subsets of) variables $\Src a, \Tgt a \subset \X$, indicating the respective source and target variables of the edge;
%         % For example,
%         %     $$\Src L = \{A, B\} \ed L{}{} \{C\} = \Tgt L$$
%         %  intuitively represents a joint dependence of $C$ on the variables $A$ and $B$;
%         % \item A manifold $\Theta_a$ of parameters
%         \item
%         a conditional probability distribution
%         {%\subafalse
%             $\p_a(\Tgt a | \Src a)$},
%         on the target variables given
%         the values of the source variables,
%         \item a weight
%         $\beta_a \in \mathbb R \cup \{\infty\}$
%         of confidence in
%         the conditional probability distribution
%         {%\subafalse
%             $\p_a
%             %(\Tgt a | \Src a)
%             $},
%         % \discard{(as measured by the number of independent observations that support $\p_a$), }
%         and
%         \item
%         a weight $\smash{\alpha_a \in \mathbb R}$
%         indicating
%         confidence in the functional dependence of
%         {%\subafalse
%         $\Tgt a\mskip-2mu$ on $\Src a\mskip-2mu$}
%         expressed by $a$.
%         % \discard{
%         % (as measured by the expected number of independent causal mechanisms corresponding to $a$,
%         % that determine $\Tgt a$ given $\Src a$).%
%         % }
%     \end{itemize}
%     % In aggregate, $\balpha = [\alpha_a]_{a \in \Ar}$ and $\bbeta = [\beta_a]_{a \in \Ar}$
%     % are vectors over $\Ar$.
% \end{defn}

% We are particularly interested in PDGs whose cpds we can adjust smoothly.
% In the present paper, we are particularly interested in PDGs whose cpds we can adjust smoothly.
% To that end, we introduce the notion of a \emph{parametric PDG family}
% $\dg P(\Theta)$,
% which is defined the same way, except that each arc $a \in \Ar$ is also associated
% with a manifold $\Theta_a$, and $\p_a$ not itself a cpd, but rather a map from $\Theta_a$ to cpds.
% In other words, we have a cpd
%     % $\p_a^{\theta_a}(\Tgt a | \Src a)$
%     $\p_a(\Tgt a | \Src a; \theta_a)$
%     for each $\theta_a \in \Theta_a$;
%      % ison $\Tgt a$ given $\Src a$,
% thus,
% %
% In addition, we require that there be a special default paramter setting $0_a \in \Theta_a$, such that $\p_a(\Tgt a | \Src a=s; 0_a) \propto \lambda_{\Tgt a}$, for every $s \in \V\Src a$.


\begin{defn}
    % A \emph{parametric PDG family}
    % A \emph{parametric probabilistic dependency graph} (PPDG)
    A \emph{Parametric Probabilistic Dependency Graph} (PPDG)
    % $\dg P \!=\! (\X\mskip-2mu, \Ar, \mathbb P, \balpha, \bbeta )$
    $
    \dg M(\Theta) \!=\!
    (\X\mskip-2mu, \Ar, \Theta, \mathbb P, \balpha, \bbeta )$
    is a directed hypergraph
        $(\X\mskip-2mu, \Ar)$
    whose nodes correspond to
    variables,
    each arc
    $a \in \Ar$
    % $\ed aST \in \Ar$
     of which is associated with:
    % arc $a \in \Ar$ is associated with:

    \begin{itemize}[nosep,itemsep=2pt,left=0pt]
        % \item (subsets of) variables $\Src a, \Tgt a \subset \X$, indicating the respective source and target variables of the edge;
        \item a parameter space $\Theta_a \subseteq \mathbb R^n $, with a default value $\theta^{\text{init}}_a$.
        % \item
        % a conditional probability distribution
        % {%\subafalse
        %     $\p_a(\Tgt a | \Src a)$},
        % on the target variables given
        % the values of the source variables,
        \item
        a map
        $\p_a : \Theta_a \times \V\Src a \to \Delta \V \Tgt a$
        % $\p_a : \Theta_a \subafalse \times \V\Src a \to \Delta \V \Tgt a$
        % {%\subafalse
        %     $\p_a(\Tgt a | \Src a)$},
        that gives a cpd
        % $\p_{a,\theta}(\Tgt a | \Src a)$
        $\p_{a}^{\,\theta}(\Tgt a | \Src a)$
        over $a$'s targets given its sources,
        % $\p_{a}^{\,\theta}\subafalse(\Tgt a | \Src a)$
        for every $\theta \in \Theta_a$,
        % such that  $\p_{a}^{\,0}(s) \propto \lambda_{\Src a}$;
        % on the target variables given the the source variables.
    %     \item a confidence
    %     % $\beta_a \in \mathbb R \cup \{\infty\}$
    %     $\beta_a \in \mathbb R \cup \{\infty\}$
    %     in the cpd
    %     {
    %         %\subafalse
    %         $\p_a(\Tgt a | \Src a)$}%
    %     % \discard{(as measured by the number of independent observations that support $\p_a$), }
    %     % .
    %     ,
    %     and
    % \item
    %     a confidence $\smash{\alpha_a \in \mathbb R}$
    %     in the functional dependence of
    %     {%\subafalse
    %     $\Tgt a\mskip-2mu$ on $\Src a\mskip-2mu$}
    %     expressed by $a$.
    %     % \discard{
    %     % (as measured by the expected number of independent causal mechanisms corresponding to $a$,
    %     % that determine $\Tgt a$ given $\Src a$).%
    %     % }
    \item confidences
    % $\beta_a \in \mathbb R \cup \{\infty\}$
    $\smash{\alpha_a \in \mathbb R}$
    in the functional dependence of
    {%\subafalse
    $\Tgt a\mskip-2mu$ on $\Src a\mskip-2mu$}
    expressed by $a$,
    % and $\beta_a \in \mathbb R \cup \{\infty\}$
    and $\beta_a \in [0,\infty]$
    in the cpd $\p_a$\,.
    \end{itemize}
    A PDG is the object obtained by fixing the parameters; thus,
    a
    % joint setting
    choice of
        $\theta \in \Theta := \prod_{a \in \Ar} \Theta_a$ yields a PDG
    $\dg M = \dg M(\theta)$.
    %
    % We call a PDG \emph{unweighted} if $\balpha=\bbeta=\mat 1$.
    \qedhere
\end{defn}
% \vspace{-0.2ex}

%
% This
Clearly, a PDG is the
special case of a PPDG in which every $\Theta_a = \{ \theta_a^{\text{init}} \}$ is a singleton.
%  recovers the definition given by \citet{pdg-infer};
% is clearly a PDG;
Conversely, a PPDG may be viewed as a PDG by adding each $\Theta_a$ as
    a variable, as illustrated in \cref{fig:adv}.
% which is how we will draw such PDGs.
% We can take the disjoint union of two PDGs with
%
% We generally specify PDGs with graphical notation, so that a cpd
% Both PDGs and PPDGs can be specified with graphical notation, drawing \\
We often identify the label $a$ with the cpd $\p_a$,
    and specify (P)PDGs in graphical notation, drawing \\
% $f(Y|X)$ is written
% \begin{tikzpicture}[center base]
% 	\node[dpadinline] (X) at (0,0) {$X$};
% 	\node[dpadinline] (Y) at (1.1,0){\small$Y$};
%  	\draw[arr1,->] (X) -- node[fill=white, fill opacity=1, pos=0.35, inner sep=-1pt]{$f$} (Y);
% \end{tikzpicture}\,,
a cpd
$p(Y|X,Z)$ as
\begin{tikzpicture}[center base]
    \node[dpadinline] (Y) at (0.95,0) {$Y$};
    \node[dpadinline] (X) at (0,-0.25) {$X$};
    \node[dpadinline] (Z) at (0, 0.25) {$Z$};
    \coordinate (ctr) at (0.38,0);
    \cmergearr[arr2] {X.0}{Z.0}Y{ctr}
    \node[above right=-1pt and -3pt of ctr] {$p$};
\end{tikzpicture}\,
% ~~\text{and $q(Y)$ as }~~
% \begin{tikzpicture}[center base]
% 	\node[dpad0] (Y) at (1,0) {$Y$};
% 	\draw[arr2, <-] (Y) -- node[left,inner sep=2pt]{$q$} ++(0,1);
% \end{tikzpicture}~.
% ~\text{and $q(A,B)$ as }~
and $q(A,B)$ as
\begin{tikzpicture}[center base]
    \node[dpadinline] (A) at (1,0.25) {$A$};
    \node[dpadinline] (B) at (1,-0.25) {$B$};
    \coordinate (tip) at (0.15, 0);
    \coordinate (center) at (0.45,0);
    % \draw[arr2, <-] (Y) -- node[left,inner sep=2pt]{$q$} ++(0,1);
    \cunmergearr[arr1] {tip}{A.180}{B.180}{center}
    \node[above left=0pt and 0pt of center,inner sep=2pt]{$q$};
\end{tikzpicture}\,.
$\vphantom{\Bigg|}$
\\
%
% By default, take $\beta,\alpha{=}1$.
Unless otherwise specified, take $\beta,\alpha{=}1$ by default.
    % , but we will sometimes specify ``high confidence''
    % ($\beta{=}\infty$).
% Given PDGs $\dg M_1$ and \vphantom{$\big|$}$\dg M_2$,
%     we write $\dg M_1 + \dg M_2$ for the PDG
% Given PDGs $\dg M_1$ and \vphantom{$\big|$}$\dg M_2$,
We write $\dg M_1 + \dg M_2$ for the PDG
        % with their combined information.
        % with the arcs of both $\dg M_1$ and $\dg M_2$.
        that has the arcs of both $\dg M_1$ and $\dg M_2$,
        and represents their combined information.
% $\odot$ denotes pointwise multiplication.
% , or reweighting of a PDG's confidences.
% % If $\psi \in \mathbb R^{2\Ar}$,
% % If $\psi \in \mathbb R^{\Ar+ \Ar}$,
% If $\psi \in \mathbb R^{\Ar}$,
% we write $\psi \odot \dg M$ for the PDG $\dg M'$ re-weights $\bbeta$,
%     multiplying by $\psi$ pointwise.
%     %  i.e., $\beta_a' = \beta_a \psi_{0,a}$ and $\alpha_a' = \alpha_a\psi_{1,a}$.
% We the same symbol for pointwise vector multiplication generally.
% % \begin{itemize}[nosep,itemsep=2pt]
% %     \item a space $\Theta_a$ of parameters, and a
% % \end{itemize}


% \paragraph{Semantics.}
% What makes PDGs interesting are
\textbf{PDG Semantics and Inconsistency.}
The power of PDGs comes from their semantics, which sew their (possibly inconsistent)
% constituent
cpds and confidences together into joint probabilistic information.
% a measure of inconsistency, which are equivalent.
% Recall that
A PDG contains two kinds of information: structural information about
causal mechanisms, (the graph $\Ar$ and weights $\balpha$),
and observational data (the cpds $\mathbb P$ and confidences $\bbeta$).
% Corespondingly, we can score a candidate joint distribution $\mu \in \Delta \V\!\X$ by its incompatibility with each kind of information.
% This is done by scoring joint distributions $\mu \in \Delta \V\!\X$
    % by their incompatibility
With respect to a PDG $\dg M$,
the \emph{observational incompatibility}
of a joint probability measure
$\mu \in \Delta \V\!\X$ is given by
a weighted sum of relative entropies
\begin{equation}
\OInc_{\dg M}(\mu) :=
    % \sum_{L \in \Ed} \beta\ssub L\, \kldiv[\Big]{\mu(\Tgt L,\Src L)}{p\ssub L(\Tgt L | \Src L) \mu(\Src L)}.
    %oli5:
    % \sum_{a \in \Ar}
    \!
    \sum_{\smash{\ed aST \mathrlap{\,\in \Ar}}} \subafalse
    \beta_a\, \kldiv[\Big]{\mu(\Tgt a,\Src a)}{\p_a(\Tgt a | \Src a) \mu(\Src a)}
        ,
        \label{eq:oinc}
\end{equation}
and can be thought of as the excess cost of using codes
optimized for each cpd, weighted by the confidence we have in them, if $\X \sim \mu$.
If $\dg M$'s observational confidences are positive ($\bbeta > \mat 0$), then $\OInc_{\dg M}(\mu) = 0$
% iff $\mu$ satisfies the constraints imposed by every cpd of $\mathbb P$.
if and only if $\mu$ has every conditional marginal described by $\mathbb P$.

We can also score $\mu$ by its incompatibility
with the structural information $(\Ar,\balpha)$.
This \emph{structural deficiency} is given by:%
    \footnote{In \eqref{eq:sdef}, $\lambda$ is base measure,
        a property of $\X$. The precise choice is not important, but
        think: uniform or an appropriate analogue.}
    % with respect to a PDG $\dg M$ is
\begin{align*}
    % \SInc_{\dg M}(\mu) := - \H(\mu) + \sum_{L \in \Ed} \alpha\ssub L\, \H_\mu(\Tgt L | \Src L),
    \SDef_{\!\dg M}(\mu) &:=
    % \SDef_{\!(\Ar,\balpha)}(\mu) &:=
        % - \H(\mu) + \sum_{a \in \Ar} \alpha_a\, \H_\mu(\Tgt a | \Src a).
        % \pqty[\Big]{\; \sum_{\ed aST \mathrlap{\,\in \Ar}}\subafalse \alpha_a\, \H_\mu(\Tgt a | \Src a) } - \H(\mu).
        %%%%
        % \kldiv{\mu}{\lambda_{\X}} -
        % \sum_{\ed aST \mathrlap{\,\in \Ar}}\subafalse \alpha_a\,
        %     \kldiv{\mu (\Tgt a\Src a)} { \lambda_{\Tgt a} \mu(\Src a) }
        \numberthis
         \label{eq:sdef}
        %%%%
        % \\ &=
            \Ex_{\mu}  \bigg[ \log\; \frac{\mu(\X)}{\lambda(\X)}
            % \prod_{\vphantom{\big|}\ed aST \mathrlap{\,\in \Ar}}\subafalse
            \prod_{\ed aST }\subafalse
                \left(\frac{\lambda(\Tgt a|\Src a)}{\mu(\Tgt a | \Src a)}\right)^{\!\alpha_a}
            \bigg]
    ,
    % \vspace{-1ex}
\end{align*}
% can be thought of as measuring how effectively $\mu$ factors along the arcs $\Ar$.
and, roughly, measures $\mu$'s failure to arise as a result of
    independent causal mechanisms along each edge.
% There is not space to motivate it fully, but here are a few nice properties.
If $\Ar$ is a qualitative Bayesian Network, for instance,
    then $\SDef_{\!\Ar}(\mu) \ge 0$ with equality
    iff $\mu$ has the independencies of $\Ar$.
% It turns out that this information can also be expressed
% We point the reader to
%  \citet{pdg-aaai,one-true-loss,pdg-infer}
% % for further details, examples, and motivation.
% for more details, motivation, and examples.
We encourage the reader
to consult previous work for further details.

% that if $\Ar$ has the structure of $\SDef_{\!\dg M}(\mu) > 0$
% which itself has a confidence $\gamma$,


% \begin{align*}
%     \bbr{\dg M}_\gamma&(\mu)
%         := \OInc_{\dg M}(\mu) + \gamma \, \SInc_{\dg M}(\mu)
%             \numberthis\label{eqn:scoring-fn} \\[-0.2ex]
%         % =& \Ex_{\mu}\left[\, \sum_{L \in \Ed} \log \frac
%         %     {\mu(\Tgt L| \Src L)^{\beta\ssub L - \gamma \alpha \ssub L}}
%         %     {p\ssub L(\Tgt L | \Src L)^{\beta \ssub L}}
%         % \right] - \gamma \H(\mu)
%         &= \Ex\nolimits_{\mu}\bigg[
%             \,
%             % \gamma \log \mu(\X) +
%             % \sum_{a \in \Ar}
%             \sum_{\ed aST \mathrlap{\, \in \Ar}} \subafalse
%             % \sum_{a,S,T,\alpha,\beta,p \in \hat{\dg M}} \subafalse
%                 % \let\plainbeta\beta \def\beta_a{\plainbeta} \let\plainalpha\alpha\def\alpha_a{\plainalpha} \def\p_a{p}
%             \log \frac
%             {\mu(\Tgt a| \Src a)^{\beta_a - \gamma \alpha_a}}
%             {\p_a(\Tgt a | \Src a)^{\beta_a}}
%         \bigg] - \gamma \H(\mu)
% \end{align*}

With confidence $\gamma \ge 0$ in the structural information overall,
the $\gamma$-\emph{inconsistency} of $\dg M$ is the smallest possible overall incompatibility of any distribution with $\dg M$, and denoted
% \vskip-1ex
\begin{equation}\label{eq:inconsistency}
    \aar{\dg M}_\gamma :=
        % \smash
        {\inf_{\mu}\,  \Big( \OInc_{\dg M}(\mu) +  \gamma \SDef_{\dg M}(\mu) \Big).}
\end{equation}
\citet{one-true-loss} argues that this inconsistency measure
    % $\aar{- }_\gamma$
    \eqref{eq:inconsistency}
    is a ``universal'' loss function, largely showing how it
    specializes to standard loss functions in a wide variety of situations.
It follows that, at an abstract level,
    much of machine learning can be viewed as inconsistency resolution.
%
% We take this idea one step further, operationalizing the resolution process.
We take this idea a few steps further, by operationalizing the
    resolution process,
    % and making it even more expressive with an approximate, local variant
    and allowing it to be done locally.
% The latter dramatically expands what computations can be expressed.
    % significantly  expanding what can be expressed.
%
% We operationalize resolution process, and then extend this idea to allow for locality.
    % how even more can be done if we allow
%
    % can be seen as \emph{local} resolution.
% We then demonstrate that several very different historically important
    % procedures are special cases.
     % of our algorithm.
    % of an algorithm we call \emph{local inconsistency resolution}.


\vfull{
PDG inference is fixed-parameter tractable:
assuming a graph with bounded tree-width, the  can be computed in polynomial time.
and it appears that \cite{pdg-infer}.
Several important algorithms can alredy be seen as inconsistency reduction.
When viewed as a graphical model}

% A surprisingly broad class of algorithms already
% \begin{itemize}
%     [nosep]
%     \item Belief updating (Conditioning, Jeffrey's rule)
% \end{itemize}
% However, global inference

\section{Local Inconsistency Resolution (LIR)}
% Two knobs to control the model. The first one controls what you're looking at and what you get to change.
% We now describe the local inconsistency reduction algorithm.
% There are two distinct senses in which inconsistency resolution can be done locally:
%     we can either restrict how much we can see, or how much we can do.
% There are two distinct senses in which inconsistency can be resolved locally:
%     we can restrict what we can see, or what we can do.
% There are two aspects of resolving inconsistency: what
% We can either
\textbf{Attention and Control.}
There are two distinct senses in which inconsistency resolution can
    be \emph{local}: we can restrict what we can see, or what we can do about it.
Correspondingly, there are two ``focus'' knobs for our algorithm:
    one that restricts our attention to the inconsistency of a subset of arcs $\attn \subseteq \Ar$,
    % and one that restricts our control to the parameters of a subset
    and another that restricts our control to (only) the parameters of
    % a subset
    arcs
    $\ctrl
    % (\subseteq A)
     \subseteq \Ar$
     % \subseteq A$
      as we resolve that inconsistency.
% \begin{enumerate}[label=(\alph*)]
% (a)
% \item
    % restrict our attention to a subset $A \subseteq \Ar$ of arcs,
    % restrict our attention to the inconsistency of a subset of arcs $A \subseteq \Ar$,
    %     % trying to reduce inconsistency of the sub-PDG,
    %     or
    %     % taking a the inconsistency of a
    %     \label{item:attn-restrict}
    % % (b)
    % % \item
    %     restrict our control to the parameters
    %     % $\Theta_C := \prod_{a \in C}\Theta_a$ of a subset $C \subseteq \Ar$ of arcs.
    %     of arcs $C \subseteq \Ar$.
    %     \label{item:ctrl-restrict}
% \end{enumerate}
% In the first case,
% Both make the problem more tractible.
% Restricting attention \ref{item:attn-restrict}
% In other words,
The former makes for an underestimate of the inconsistency that is easier to calculate, while
% restricting control \ref{item:ctrl-restrict}
the latter
makes for an easier optimization problem.
These restrictions are not just cheap approximations, though:
    they are also appropriate modeling assumptions for
    actors that cannot see and control everything at once.

Attention and control need not be black or white.
% More general than selecting $A,C \subseteq \Ar$,
% A generalization of $A,C \subseteq \Ar$ is
A more general approach is to choose
    % an attention mask $\varphi \in \mathbb R^{\Ar}$ and
    % a control mask $\chi \in [0,\infty]^{\Ar}$.
    an \emph{attention mask} $\varphi \in \mathbb R^{\Ar}$ and
    a \emph{control mask} $\chi \in [0,\infty]^{\Ar}$.
% Large $\varphi(a)$ makes $a$ salient while $\varphi(a) \!=\! 0$ keeps it out of mind;
Large $\varphi(a)$ makes $a$ salient, while $\varphi(a) = 0$
    keeps it out of the picture.
Similarly, large $\chi(a)$ gives significant freedom to change $a$'s parameters,
small $\chi(a)$ affords only minor adjustments, and $\chi(a) \!=\! 0$
    % means we cannot change them at all.
    prevents change altogether.
Either mask can then be applied to
    % an array with an axis indexed by $\Ar$, via
    % a vector over $\Ar$ by
    % a vector indexed by $\Ar$, via
    a tensor that has an axis corresponding to $\Ar$, via
    pointwise multiplication ($\odot$).
% $\gamma$ may be viewed $\varphi$
% So, $\chi$ controls locality in the sense of ``local search''.
%
% So overall, the algorithm goes like this.
    % We recieve input in the form of a PDG $\Ctx$, and
    % a memory layout $\dg M(\Theta)$.
% We then repeatedly select an attention mask $\varphi$ and a control mask $\chi$, and update
% Once we select attention and control masks,
% % We then repeatedly select attention and control masks, and update
%     we update the parameters we can control so as to reduce the inconsistency of
%         what is in view.
% In more detail:

% At a high level, the algorithm proceeds as follows.
% The full algorithm, formalized in \cref{algo:LIR},
% The aim of LIR is to
% The LIR algorithm
\textbf{The Algorithm.}
    LIR
    modifies
    the parameters $\theta$ of a
PPDG $\MThetadense$ so as to make it more consistent with its context.
% , as follows.
% The procedure is as follows.
It proceeds as follows.
First,
    recieve context in the form of a PDG $\Ctx$%
    % recieve context as a PDG $\Ctx$%
    , and
    initialize mutable memory $\MThetadense$%
    % to $\theta^{\text{init}}$%
    .
    % a PPDG that whose parameters we will be changing to reduce its inconsistency with $\Ctx$.
In each iteration,
    choose $\gamma$ (which can be viewed as attention to structure),
    an attention mask $\varphi$ over the arcs of $\MThetadense + \Ctx$,
    and
    a control mask $\chi$ over the arcs of $\MThetadense$.
%
% Use methods of \citet{pdg-infer} to calculate
Calculate
% the inconsistency of $\varphi\odot(\dg M(\Theta) + \Ctx)$,
$\aar{\varphi\odot(\dg M(\theta) + \Ctx)}_\gamma$, the inconsistency of
    the combined context and memory, weighted by attention.
(For discrete PDGs, this can be done with the methods of \citet{pdg-infer}.)
Then mitigate this local inconsistency
    by updating
     mutable memory
     $\theta$
      via (an approximation to) gradient flow,
    changing $a$'s parameters in proporition to control $\chi(a)$.
        % context and memory $\Ctx + \dg M(\theta)$
The procedure is fully formalized in \cref{algo:LIR}.
% This is fully formalized in \cref{algo:LIR}.

% To describe the algorithm formally in full generality, we also need to des
% If $\dg M(\Theta)$ is a parametric PDG family, then we write
% $\dg M.\Ar$
% $\Ar(\dg M)$
% for the set of arcs in the underlying PDG, and
% $\dg M.\boldsymbol\theta$

\begin{algorithm}
	\caption{Local Inconsistency Resolution (LIR)}
	\label{algo:LIR}
	\begin{algorithmic}
        % \STATE \textbf{Input:}
        %     mutable memory $\dg M(\Theta) $,
        %     $\gamma \ge 0$,
        % \STATE ~~~~ procedure $\textsc{Refocus}: () \to PDG \times [0, \infty]^{\Ar(\dg M)}$
        % \STATE \textbf{Input:}
        %     context $\Ctx : $ PDG
        % \STATE ~~~~
        %     mutable memory $\Mm(\Theta) : $ PPDG
        \STATE \textbf{Input:}
            context $\Ctx$,~ mutable memory $\MThetadense$.

		% \STATE \textbf{Variables:}
            % state $\theta \in \Theta$,
            % PDG $\Ctx$.
        % \smallskip

        \STATE Initialze $\theta^{(0)} \gets \theta^{\text{init}}$;
        % \STATE Initialze $\theta \gets 0$;

        \FOR{$t = 0, 1, 2, \ldots$}
            \STATE $\Ctx \gets \textsc{Refresh}(\Ctx)$;
                \hfill{\color{gray}\small\texttt{//optional}}
            % \STATE $\varphi(\Ctx), \chi(\dg M)
            \STATE $\varphi, \chi, \gamma
                \gets \textsc{Refocus}()$;
                % \hfill{\color{gray}\small\texttt{//get loci of focus, control}}
                % Choose attention mask $\varphi: (\dg I + \dg M).\Ar \to \mathbb R$
            \STATE $\theta^{(t+1)} \!\gets \exp_{\theta^{(\mskip-1mut\mskip-1mu)}}\!
            % \STATE $\theta \gets \exp_{\theta}
                \Big\{\! {-} \chi \odot \nabla_{\!\Theta}
                \aar[\Big]{ \varphi\odot \!\big( \Ctx + \MThetadense \big) \!}_{\!\!\gamma} \Big\}$;
                % \aar[\big]{ \Ctx + \varphi\odot \dg M(\theta) }_{\!\gamma} \Big\}$;
        \ENDFOR
	\end{algorithmic}
    % \small\color{gray}
    % Issues. Choice of $\Ctx$ includes choice of $\varphi$, if $\varphi : \Ar(\dg M + \Ctx) \to \mathbb R$.
    % But if $\varphi : \Ar(\dg M) \to \mathbb R$, they're not overlapping, and $\chi,\varphi: \mathbb R^{\Ar(\dg M)}$ look more like ``duals''.
    % Of course, we can't then eliminate the refresh instruction.  I also don't like the interpretation as much: if your focus changes in a static environment, I want that to be reflected in $\varphi$, not changing context.
\end{algorithm}
% In order to fully define LIR, we must
% Before we can run \cref{algo:LIR},
% Before we can run it,
In order to
    % execute
    % \cref{algo:LIR},
    execute this procedure,
we must say something about
how the choice of $(\varphi,\chi,\gamma)$ is made.
Thus, we must supply an additional procedure \textsc{Refocus}
to select attention and control masks.
%
% we must supply two more procedures:
% and
% Different choices result in different algorithms;
    % for our purposes,
We focus mostly on the case where $\gamma$ is fixed, and
    \textsc{Refocus}
    chooses non-deterministically
    from a fixed set
    of attention/control mask pairs
    $(\varphi, \chi) \in \mathbf{F}
    % = \{ (\varphi_i,\chi_i) \}
    $,
which we call \emph{foci}.
\cref{algo:LIR} also allows us to a select a second procedure, \textsc{Refresh},
    which makes it easier to model recieving new information
    in online settings.
% The other procedure, \textsc{Refresh} is the identity by defualt, and can be removed in theory by making supplying an infinitely large initial context containing every possible result of refresh.
% The other procedure,
% \---that is, procedures that continually recieve inputs.

The ODE on the last line of \cref{algo:LIR}, which is
    an instance of gradient flow, may be approximated with an
    inner loop running an iterative gradient-based optimization algorithm.
Alternatively, if \textsc{Refocus} produces small $\chi$,
    then it is well-approximated by a single gradient descent step of size $\chi$.
At the other extreme: if $\chi$ is infinite in every component,
    then, so long as the parameterizations $\mathbb P$ are log-concave,
    the final line
% then $\chi=\infty$ finds
    % a parameter setting that globally minimizes inconsistency
 reduces to
    \[
        \theta^{(t+1)} \gets \arg\min_{\theta}
            \aar[\big]{ \varphi\odot( \Ctx + \dg M(\theta)) }_{\!\gamma}\,,
            ~~~\text{because of}
    \]
 % so long as the parameterizations $\mathbb P$ are log-concave, because of
% This is because of
\begin{theorem} \label{thm:cvx}
    If $\,\mathbb P$ is log-concave, then
    for small enough $\gamma$, the map $\theta \mapsto \aar*{\varphi \odot( \Ctx + \dg M(\theta))}_\gamma$ is convex.%
        \footnote{All proofs can be found in the appendix.}
    % and strictly so for $\gamma > 0$.
\end{theorem}
% \begin{proof}
%     It is known that $p \mapsto \aar{\dg M + p}$ is convex (see appendix),
%     and also that $\aar{\dg M + p}$
%     % Let $\varphi_C$ be components of $\varphi$ that act on the arcs of $\dg M$.
% \end{proof}
%
%
% We now consider some important speical cases.
In the remaining sections, we give a sample of
some historically important algorithms that are instances of LIR.

% We now consider some special cases of \cref{algo:LIR}.

%%%%%
% If $\Ctx$ is a PDG with variables $\X$,
% and our memory $\dg M$ contains a single joint distribution $\mu(\X)$
% then $\textsc{LIR}(\Ctx, \dg M, \gamma)$ equals $\bbr{\Ctx}^*_\gamma$, the optimal distribution
%%%%%
% If $\Ctx$ is a PDG with discrete variables $\X$,
% $\dg M(\Theta)$ consists of a single joint distribution $\mu(\X)$
%     parameterized as a vector $[0,1]^{\V\!\X}$,
% and \textsc{Refocus} always produces constant $\chi,\gamma$,
% then $\textsc{LIR}(\Ctx, \dg M)$ equals $\bbr{\chi\odot\Ctx}^*_\gamma$,
% the optimal distribution, albeit in a roundabout manner.
%%%%%
\vfull{
    If $\dg M$ is a PDG with discrete variables $\X$
    and we regard $\mu(X)$
    $\dg M(\Theta)$ consists of a single joint distribution $\mu(\X)$
        parameterized as a vector $[0,1]^{\V\!\X}$,
    and \textsc{Refocus} always produces constant $\chi,\gamma$,
    then $\textsc{LIR}(\Ctx, \dg M)$ equals $\bbr{\chi\odot\Ctx}^*_\gamma$,
    the optimal distribution, albeit in a roundabout manner.
    }
% in which (i.e., every $A_t = \Ar$) amounts to \emph{global} inconsistency reduction, i.e.,
% the universal training objective of \cite{one-true-loss}. which is \#P-hard.
% In this case, supposing $\bbeta \ge 0$,
% then the global inconsitency $\aar{\dg M}
% $ can only decrease in time
% It is worth noting that LIR does not always converge.

% \section{Warmup: Control in a Classification Setting}
\section{LIR in the Classification Setting}
Consider a parametric classifier $p_\theta(Y|X)$, perhaps
arising from a neural network whose final layer is a softmax.
Suppose $\V Y$ is a finite set of classes.
If $\V\!X$ is itself a manifold (such as the space of images), we can regard a value $x \in \V\!X$ as parameterizing a deterministic cpd, written
\tikz[center base]{\node[dpadinline](X) {$X$}; \draw[arr1, <<-](X.-165) to node[above,inner sep=1pt,pos=0.7]{$x$} +(-0.6,0);}.
% Viewing $\Theta$ as a variable, the discrimainator
% as a conditional probability $p(Y| X, \Theta)$.
Together with a labeled sample $(x,y)$,
%  and parameter setting $\theta$,
    we get a PPDG
%
% \[ %%%>>  EXPLICIT PARAMTERIZATION; THETA IS A VARIABLE  <<%%
%     \dg M := ~~
%     \begin{tikzpicture}[center base]
%         \node[dpad1] (X) at (0,0) {$X$};
%         \node[dpad1] (Y) at (2,0) {$Y$};
%         \node[dpad1] (T) at (1, 0.7){$\Theta$};
%         \cmergearr[arr1]XTY{1.2,0}
%         \draw[arr1, <<-] (T) to node[above,pos=0.65]{$\theta$} +(-1,0);
%         \draw[arr1, <<-] (X) to node[above,pos=0.65]{$x$} +(-1,0);
%         \draw[arr1, <-] (Y) to node[above,pos=0.65]{$y$} +(1,0);
%         \node[anchor=south,inner sep=2pt] at (0.8,0) {$p$};
%     \end{tikzpicture},
% \]
$
    \dg M(\theta) :=
    \begin{tikzpicture}[center base]
        % \node[dpad1] (X) at (0,0) {$X$};
        % \node[dpad1] (Y) at (1.5,0) {$Y$};
        % \draw[arr1, <<-] (X) to node[above,pos=0.65]{$x$} +(-1,0);
        % \draw[arr1, <-] (Y) to node[above,pos=0.65]{$y$} +(1,0);
        \node[dpadinline] (X) at (0,0) {$X$};
        \node[dpadinline] (Y) at (1.1,0) {$Y$};
        \draw[arr1, <<-] (X) to node[above,pos=0.65]{$x$} +(-0.85,0);
        \draw[arr1, <-] (Y) to node[above,pos=0.65,inner sep=2pt]{$y$} +(0.75,0);
        \draw[arr1] (X) to
            node[above] {$p_\theta$} (Y);
    \end{tikzpicture}
$
whose observational inconsistency is
$
    \aar{\dg M}_0 = - \log p_\theta(y|x)
$, the standard training objective for such a classifier \cite{one-true-loss}.
% This inconsistency is irreducible, since removing any of
% If $X$ and $\Theta$ are expressive enough, any of the four edges will make the inconsistency disappear
% All four components play a major role in giving rise to this inconsistency.
Each cpd plays major role in this inconsistency.
% Fixing all of $\dg M$ as our context,
% Fixing uniform global attention $\varphi = 1$,

What happens when we resolve this inconsistency
by modifying the parameters associated to different arcs?

\begin{itemize}[nosep,itemsep=3pt, left=3pt]
    \item Adjusting $\theta$
    % (i.e, $C = \{\theta\}$)
    amounts to training the network in the standard way.
        % In more detail, $\chi$
        % In this case, the value $\chi$ of the control mask corresponds to the number of optimization iterations.
        In this case, the value $\chi$ of the control mask corresponds roughly
        to the product of the learning rate and the number of optimization iterations.

    \item Adjusting $y$
    % (i.e., $C = \{y\}$)
    is like a  forward pass, in that it adjusts $y$ to match distribution $p_\theta(Y|x)$.
        % In particular, $\chi=\infty$ corresponds to selection $y(
        % If there is also a qualitative edge that states that $Y$ is deterministic

    \item Adjusting $x$
    % (i.e. $C = \{x\}$ )
    creates an adversarial example.
    That is, it makes incremental changes to the input $x$
        until the (fixed) network assigns it label $y$.
    % amounts to constructing an adversarial example---that is, fixing the network
    % If $X$ represents images, for example, this means fixing the network and making small adjustments to the image until the network assigns it the desired label.
\end{itemize}

% \textbf{classification}
% Furthermore, these .
% with memory $\dg M(\Theta) = p$ and context
%  $y_i$ for every possible
% \textbf{adversarial attack in practice.}
% Take mutable memory to also include a special arc $\ed {\mathit{adex}}{}X$,
% and that is the focus
% \textbf{A more realistic classification setting.}
\textbf{Stochastic Gradient Descent (SGD).} \label{sec:SGD}
Take the mutable state to be the classifier $p$ as before.
Define $\textsc{Refresh}$ so that it draws a batch of samples $\{(x_i,y_i)\}_{i=1}^m$,
and returns a PDG with a single arc describing their emperical distribution $d(X,Y)$;
% Define $\textsc{Refocus}$ so that it sets $\varphi = \{d \mapsto \infty, p \mapsto 1\}$
%     (corresponding to high confidence in the data, default confidence in the still-training classifier)
let $\textsc{Refocus}$ be such that $\varphi(d) = \infty$
    (reflecting high confidence in the data).
If $\eta := \chi(p) \varphi(p)$ is small, then
    LIR is SGD with batch size $m$ and learning rate $\eta$.


% \textbf{Adversarial training views.}
% \textbf{SGD: batches as context.}
\begin{figure}
    \centering
    % \tikzset{atkv/.style={red!50!white},defv/.style={green!50!gray}}
    \tikzset{atkv/.style={green!70!black},defv/.style={blue}}
    \phantom{a}\hfill
    \begin{tikzpicture}[center base]
        % \node[dpad1] (X) at (0,1) {$X$};
        % \node[dpad1] (X') at (0,0) {$X'$};
        \node[dpad1] (X) at (0,0) {$X$};
        \node[dpad1] (Y) at (1.3,0) {$Y$};

        \draw[arr1,-,shorten >=3pt,transform canvas={yshift=-1pt}, defv,dashed,ultra thick] (X) to (Y);
        \draw[arr1,-,shorten >=3pt,transform canvas={yshift=0.75pt}, atkv] (X) to (Y);
        \draw[arr1] (X) to node[above] {$p$} (Y);
        % \draw[arr1] (X) to node[right] {$\mathcal N$} (X');
        % \node[dpad1] (T) at (1, 0.7){$\Theta$};
        % \cmergearr[arr1]XTY{1.2,0}
        % \draw[arr1, <<-] (T) to node[above,pos=0.65]{$\theta$} +(-1,0);
        \coordinate (xend) at ($(X)+(-1.1,0.5)$);
        \draw[arr1,-,shorten <=5pt,transform canvas={yshift=0.75pt}, atkv] (X) to (xend);
        \draw[arr1, <-] (X) to
            %  node[left,pos=0.85]{$\mathcal N(x,1)$}
             node[above,pos=0.65]{$x$}
             (xend);
        \coordinate (x'end) at ($(X)+(-1.1,-0.5)$);
        \draw[arr1,-,ultra thick,shorten <=4pt,transform canvas={yshift= 1pt}, atkv, dashed] (X) to (x'end);
        \draw[arr1,-,shorten <=3pt,transform canvas={yshift= -0.75pt}, defv] (X) to +(x'end);
        \draw[arr1, <<-] (X) to node[above,pos=0.65]{$x'$} (x'end);
        \coordinate (yend) at ($(Y)+(1,0.5)$);
        \coordinate (y'end) at ($(Y)+(1,-0.5)$);
        \draw[arr1,-,shorten <=6pt,transform canvas={yshift=-0.75pt}, defv] (Y) to (yend);
        \draw[arr1, <<-] (Y) to node[above,pos=0.65]{$y$} (yend);
        \draw[arr1,-,shorten <=6pt,transform canvas={yshift=0.75pt}, atkv] (Y) to (y'end);
        \draw[arr1, <<-] (Y) to node[above,pos=0.65]{$y'$} (y'end);
    \end{tikzpicture}
    % $\cong$
    \hfill
    \begin{tikzpicture}[center base]
        \def\outspray{.24}
        \def\inspray{.18}
        \def\yspray{.5}
        \node[dpad1] (X) at (0,-0.6) {$X$};
        \node[dpad1] (Y) at (1.4,0) {$Y$};
        \node[dpad1] (T) at (0, 0.6){$\Theta_p$};
        % \draw[arr1,-,shorten >=3pt,transform canvas={yshift=-1pt}, defv,dashed,ultra thick] (X) to (Y);
        % \draw[arr1,-,shorten >=3pt,transform canvas={yshift=0.75pt}, atkv] (X) to (Y);
        % \draw[arr1] (X) to node[above] {$p$} (Y);
        \mergearr[arr1]XTY
        \node[above right=0pt and 0pt of center-XTY]{$p$};
        \coordinate (tend) at ($(T)+(-1.1,\outspray)$);
        \draw[arr1,-,ultra thick,shorten <=7pt,transform canvas={yshift= -1.0pt}, defv, dashed] (T.160) to (tend);
        \draw[arr1,-,shorten <=3pt,transform canvas={yshift= 0.7pt}, atkv] (T.160) to (tend);
        \draw[arr1, <<-] (T.160) to node[left,pos=0.9]{$\theta$} (tend);
        \coordinate (nend) at ($(T)+(-1.1,-\inspray)$);
        % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=-0.7pt}, defv] (T.-160) to (nend);
        \draw[arr1, <-, defv] (T.-160) to
            node[below,inner sep=2px,pos=0.85]{\scalebox{0.8}{$\mathcal N(0,1)$}} (nend);
        \coordinate (xend) at ($(X)+(-1.1,\inspray)$);
        % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=0.7pt}, atkv] (X.160) to (xend);
        \draw[arr1, <-, atkv] (X.160) to
            % node[above,pos=0.65]{$x$} (xend);
            node[above,inner sep=1px,pos=0.85]{\scalebox{0.8}{$\mathcal N(x,1)$}} (xend);
        \coordinate (x'end) at ($(X)+(-1.1,-\outspray)$);
        \draw[arr1,-,ultra thick,shorten <=6pt,transform canvas={yshift=1.0pt}, atkv, dashed] (X.-160) to (x'end);
        \draw[arr1,-,shorten <=3pt,transform canvas={yshift= -0.7pt}, defv] (X.-160) to (x'end);
        \draw[arr1, <<-] (X.-160) to node[left,pos=0.9]{$x'$} (x'end);
        \coordinate (yend) at ($(Y)+(1,\yspray)$);
        \coordinate (y'end) at ($(Y)+(1,-\yspray)$);
        % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=-0.7pt}, defv] (Y.35) to (yend);
        \draw[arr1, <<-,defv] (Y.35) to node[above,pos=0.65]{$y$} (yend);
        % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=0.7pt}, atkv] (Y.-35) to (y'end);
        \draw[arr1, <<-,atkv] (Y.-35) to node[below,pos=0.65]{$y'$} (y'end);
    \end{tikzpicture}
    \hfill
    %
    \caption[Two Illustrations for adversarial training: with a PPDG and a PDG]
        {Two illustrations of adversarial training.
         Left: the PPDG obtained by
            including a perturbed input $x'$ and target $y'$ to the
            classification setting.
        Right: the PDG obtained by making the parameters for $p$ explicit,
            together with a Gaussian prior $\Theta_p \sim \mathcal N(0,1)$ over them.
        Both are colored with two foci: the blue focus trains the network,
            and the green one creates adversarial examples. Dashes indicate control.
        }
    \label{fig:adv}
\end{figure}
\textbf{Adversarial training.}
% Now, include in the mutable state
% Let's now extend our mutible state to include
% Now consider an extended PDG
Suppose we want to slightly alter $x$ to obtain $x'$ that is classified as $y'$ instead of $y$.
% Adding $x'$ and $y'$ to $\dg M$ and relaxing $\p_x$ to be a Gaussian centered $x$ rather than a point mass,
By adding arcs corresponding to $x'$ and $y'$ to $\dg M$,
    and relaxing the cpd $\p_x$ associated with $x$ to be a Gaussian centered $x$ rather than a point mass,
    we get the PPDG on the left of \cref{fig:adv}.
% \[
% \begin{tikzpicture}[center base]
%     % \node[dpad1] (X) at (0,1) {$X$};
%     % \node[dpad1] (X') at (0,0) {$X'$};
%     \node[dpad1] (X) at (0,0) {$X$};
%     \node[dpad1] (Y) at (2,0) {$Y$};
%
%     \draw[arr1,-,shorten >=3pt,transform canvas={yshift=-1pt}, blue,dashed,ultra thick] (X) to (Y);
%     \draw[arr1,-,shorten >=3pt,transform canvas={yshift=0.75pt}, red] (X) to (Y);
%     \draw[arr1] (X) to node[above] {$p$} (Y);
%     % \draw[arr1] (X) to node[right] {$\mathcal N$} (X');
%     % \node[dpad1] (T) at (1, 0.7){$\Theta$};
%     % \cmergearr[arr1]XTY{1.2,0}
%     % \draw[arr1, <<-] (T) to node[above,pos=0.65]{$\theta$} +(-1,0);
%     \coordinate (xend) at ($(X)+(-1.1,0.5)$);
%     \draw[arr1,-,shorten <=2pt,transform canvas={yshift=0.75pt}, red] (X) to (xend);
%     \draw[arr1, <-] (X) to
%         %  node[left,pos=0.85]{$\mathcal N(x,1)$}
%          node[above,pos=0.65]{$x$}
%          (xend);
%     \coordinate (x'end) at ($(X)+(-1.1,-0.5)$);
%     \draw[arr1,-,ultra thick,shorten <=4pt,transform canvas={yshift= 1pt}, red, dashed] (X) to (x'end);
%     \draw[arr1,-,shorten <=4pt,transform canvas={yshift= -0.75pt}, blue] (X) to +(x'end);
%     \draw[arr1, <<-] (X) to node[above,pos=0.65]{$x'$} (x'end);
%     \coordinate (yend) at ($(Y)+(1,0.5)$);
%     \coordinate (y'end) at ($(Y)+(1,-0.5)$);
%     \draw[arr1,-,shorten <=2pt,transform canvas={yshift=-0.75pt}, blue] (Y) to (yend);
%     \draw[arr1, <<-] (Y) to node[above,pos=0.65]{$y$} (yend);
%     \draw[arr1,-,shorten <=2pt,transform canvas={yshift=0.75pt}, red] (Y) to (y'end);
%     \draw[arr1, <<-] (Y) to node[above,pos=0.65]{$y'$} (y'end);
% \end{tikzpicture}
% \]
% Taking a view of
An iteration of LIR
whose focus is the edges marked in green (with control over the dashed green edge)
is then an adversarial attack with Euclidean distance \cite{biggio2013advattk}.
% Taking the blue focus, on the other hand,
The blue focus, by contrast, ``patches'' the adversarial example by
    adjusting the model parameters to again classify it correctly.
Thus, LIR
 % that alternates between the two and
% with $\mat F = \{$green, blue$\}$
that alternates between the two foci,
in which $\textsc{Refresh}$ selects a fresh $(x,y,x'=x)$ from the dataset
    and target label $y'$,
% refreshes $(x,x',y,y')$
% amounts to
is
adversarial training, a standard defense to adversarial attacks \cite{goodfellow2014explaining}.
%

The ML community's focus on adversarial examples
    may appear to be a cultural phenomenon,
    % but in some sense it is .
    % but at a mathematical level, it is no accident.
    % but at a mathematical level,
    but mathematically,
     it is no accident.
At this level of abstraction, there is no difference between
    model parameters and inputs.
% We can highlight this visually by observing that, after adding L2 regularization \cite{one-true-loss}, translating the previous diagram to an ordinary PDG gives us the symmetrical:
% Making the parameterization of $p$ explicit
Indeed, if we make the parameterization of $p$ explicit
and add L2 regularization
% (i.e., a Gaussian prior $\Theta \sim \mathcal N(0,1)$),
% (in the form of a Gaussian prior $\Theta \sim \mathcal N(0,1)$),
(i.e., a Gaussian prior over $\Theta_p$),
% By making the parameterization of $p$ explicit
the symmetry
% between parameters and inputs
becomes striking (\cref{fig:adv}, right).
This may help explain why,
    even outside of adversarial contexts,
    % even in contexts,
% it is just as sensible to train the inputs, as the network \cite{FNNS}.
it can be just as sensible to train an input, as a model \cite{FNNS}.
% (S, right.)
% \[
%     \begin{tikzpicture}[center base,atkv/.style={red!50!white},defv/.style={green!50!gray}]
%         \def\spray{.24}
%         \node[dpad1] (X) at (0,0) {$X$};
%         \node[dpad1] (Y) at (2,0.5) {$Y$};
%         \node[dpad1] (T) at (0, 1){$\Theta$};
%         % \draw[arr1,-,shorten >=3pt,transform canvas={yshift=-1pt}, defv,dashed,ultra thick] (X) to (Y);
%         % \draw[arr1,-,shorten >=3pt,transform canvas={yshift=0.75pt}, atkv] (X) to (Y);
%         % \draw[arr1] (X) to node[above] {$p$} (Y);
%         \mergearr[arr1]XTY
%         \coordinate (tend) at ($(T)+(-1.1,\spray)$);
%         \draw[arr1,-,ultra thick,shorten <=3pt,transform canvas={yshift= -1.5pt}, defv, dashed] (T.160) to (tend);
%         \draw[arr1,-,shorten <=3pt,transform canvas={yshift= 1.0pt}, atkv] (T.160) to (tend);
%         \draw[arr1, <<-] (T.160) to node[above,pos=0.65]{$\theta$} (tend);
%         \coordinate (nend) at ($(T)+(-1.1,-\spray)$);
%         % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=-0.7pt}, defv] (T.-160) to (nend);
%         \draw[arr1, <-, defv] (T.-160) to node[left,pos=0.95]{$\mathcal N(0,1)$} (nend);
%         \coordinate (xend) at ($(X)+(-1.1,\spray)$);
%         % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=0.7pt}, atkv] (X.160) to (xend);
%         \draw[arr1, <-, atkv] (X.160) to
%             % node[above,pos=0.65]{$x$} (xend);
%             node[left,pos=0.95]{$\mathcal N(x,1)$} (xend);
%         \coordinate (x'end) at ($(X)+(-1.1,-\spray)$);
%         \draw[arr1,-,ultra thick,shorten <=3pt,transform canvas={yshift=2pt}, atkv, dashed] (X.-160) to (x'end);
%         \draw[arr1,-,shorten <=3pt,transform canvas={yshift= -1.0pt}, defv] (X.-160) to (x'end);
%         \draw[arr1, <<-] (X.-160) to node[below,pos=0.65]{$x'$} (x'end);
%         \coordinate (yend) at ($(Y)+(1,\spray)$);
%         \coordinate (y'end) at ($(Y)+(1,-\spray)$);
%         % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=-0.7pt}, defv] (Y.35) to (yend);
%         \draw[arr1, <<-,defv] (Y.35) to node[above,pos=0.65]{$y$} (yend);
%         % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=0.7pt}, atkv] (Y.-35) to (y'end);
%         \draw[arr1, <<-,atkv] (Y.-35) to node[below,pos=0.65]{$y'$} (y'end);
%     \end{tikzpicture}
% \]

% \section{The EM Algorithm (and VI) as LIR}
% \section{The EM Algorithm and Variational Inference as LIR}
\section{The EM Algorithm as LIR}
Suppose we have a generative model $p(Z,X | \Theta)$
describing the probability over an observable variable $X$ and a latent one $Z$.
% Let $x$ be an observation of $X$, as before.
%
Given an observation $X{=}x$,
the standard approach for trying to learn the parameters despite
the missing data is called the EM algorithm. It iteratively computes
\[
    \theta^{(t+1)}_{\text{EM}}
            = \arg\max_{\theta} \Ex\nolimits_{z\sim p(Z|x,\theta_{\text{EM}}^{(t)})}[ \,\log p(x, z | \theta) \,].
\]
% \[
%     \dg M := ~~
%     \begin{tikzpicture}[center base]
%         \node[dpad1] (Z) {$Z$};
%         \node[dpad1,above=.3 of Z] (X) {$X$};
%         \coordinate (A) at ($ (X)!.5!(Z) + (-0.7,0)$);
%         \node[dpad1,left=.5 of A] (T) {$\Theta$};
%         % \coordinate (A) at ($ (X)!.5!(Z) + (-1.1,0)$);
%         \draw[arr1] (T) -- node[above]{$p$} (A) -- (X);
%         \draw[arr1] (T) -- (A) -- (Z);
% %
%         \draw[arr2, <<-] (X) --  node[above,pos=0.8]{$ x$} ++(0.9, 0);

%         \begin{scope}[red]
%             \draw[arr2, <<-] (T) --  node[above,pos=0.8]{$\theta$} ++(-0.9, 0);
%             \draw[arr2, <-] (Z) --  node[above,pos=0.8]{$q$} ++(0.9, 0);
%         \end{scope}
% % 			\draw[arr2, <-] (Z) -- node[above,pos=0.6]{$ q^{\{\beta =\infty\}}$} ++(-0.9, 0);%
%         % \ar[r,"p"] \& Z \ar[r,"p", bend left] \& X \ar[l,"q", bend left] \& \ar[l, two heads, "x"']
%     \end{tikzpicture}
% \]
% \vspace{-2.5ex}

\begin{prop}
    % If there are no local minima,
    % If $\theta \mapsto p_\theta(x,Z)$ is log-convex,
    %  then
    % Under mild assumptions,
    % alternating between focus $\theta$ and focus $q$ with $\chi=\infty$
    % LIR on the PDG
    % \[
    LIR$\Bigg(\!
        \begin{tikzpicture}[center base]
			\node[dpad0] (X) {$X$};
			\draw[arr2, <<-] (X) --  node[above,pos=0.8]{$ x$} +(-0.9, 0);
		\end{tikzpicture}
        ~,~
        \begin{tikzpicture}[center base]
			\node[dpad0] (Z) {$Z$};
			\node[dpad0,left=.5 of Z] (X) {$X$};
			\coordinate (A) at ($ (X)!.5!(Z) + (0,0.8)$);
			\draw[arr1] (A) -- node[left, inner sep=3pt]{$p$} ++(0,-0.35) -- (Z);
			\draw[arr1] (A) -- ++(0,-0.35) -- (X);
%
			% \draw[arr2, <<-] (X) --  node[above,pos=0.8]{$ x$} ++(0.9, 0);
			\draw[arr2, <-] (Z) --
                node[above,pos=0.65, inner sep=2pt]{$q$}
                node[below,pos=0.7, inner sep=2pt]{${\color{gray}\scriptscriptstyle(\infty)}$}
                ++(0.9, 0);%
			%\scriptstyle q^{\{\beta =\infty\}}
			% \ar[r,"p"] \& Z \ar[r,"p", bend left] \& X \ar[l,"q", bend left] \& \ar[l, two heads, "x"']
		\end{tikzpicture}
    \!\Bigg)\!\!$
        % \]
    % in which \textsc{Refocus} fixes $\varphi, \gamma = \mat 1$
    in which \textsc{Refocus} fixes $\varphi = \mat 1$
    and alternates between
    % $\chi = \infty \mathbbm1_{\{\theta\}}$ and $\mathbbm1_{\{q\}}$
    full control of $p$ and $q$
    implements EM, in that
    $\theta_{\text{EM}}^{(t)} = \theta_{\text{LIR}}^{(2t)}$
    % for all $t \in \mathbb N$
    .
    % $ \theta^{(t+1)}
\end{prop}

% At a technical level, this is essentially a restatement of a theorem
% This result was first observed by
% A result like this was first observed by
This result is closely related to one due to
\citet{neal1998view},
who view it as an intuitive explanation of why the EM
algorithm works.  Indeed, it is obvious in this form that
every adjustement reduces the overall inconsistency.
The result can also be readily adapted to an entire dataset by replacing $x$ with a high confidence emperical distribution, or batched with the same technique in \cref{sec:SGD}.
It also captures fractional EM when $\chi < \infty$.
 % (\citeyear{neal1998view})
% By monotonicity, it is also clear that

This form of the EM algorithm is closely related to variational inference.
Indeed, analogous choices applied to the analysis of \citet{one-true-loss}
% to obtain a concrete training algorithm for variational autoencoders.
yields the usual training algorithm for variational autoencoders (VAEs).
\section{Generative Adversarial Training as LIR}
\def\pdata{p_{\mathrm{data}}}
\def\real{{\mathrm{real}}}
\def\fake{{\mathrm{fake}}}
LIR also subsumes more complex training procedures such as the one used to train
GANs \cite{goodfellow2020generative}.
% The general idea is that
The goal is to train a network $G$ to generate images that cannot be distinguished
    from real ones.
%
% In a GAN, we have a generator $G$ that represents a distribution over fake images,
% an empirical distribution of real images, and a discriminator that
% aims to classify images as real or fake.
% The training process involves a second network,
More precisely, define $X$ to be either an image $X_{\fake}\sim G$ or from a dataset
$X_\real \sim \pdata$, based on a fair coin $C$.
A discriminator $D$ then predicts $C$ from $X$.
%
% Consider the PDG
% Finally, the generator believes that $C$ is not determined by $X$,
%     which can be captured with a purely qualitative edge with $\alpha=-1$.
% These components are summarized by the PDG
The generator also has a belief that, even given $X$, the coin is equally likely
heads as tails (call this $e$).
This state of affairs is summarized below.
% , where ``!'' means $\beta=\infty$, i.e, full confidence.
%
% \vspace{-1ex}
\[
    % \dg M:=
    \dg M(\Theta) :=
    \begin{tikzpicture}[center base,Dcolor/.style={green!70!black},Gcolor/.style={blue}]
        % \node[dpad1](Z) at (0, 0.6){$Z$};
        \node[dpad1](Xfake) at (0,1) {$X_{\fake}$};
        \node[dpad1](Xreal) at (0,0){$X_{\real}$};
        \node[dpad1](X) at (2.1,0){$X$};
        % \node[dpad1](C) at (2,1){Real / Fake};
        % \node[dpad1](C) at (2.5,1.1){$\mathrm{Real / Fake}$};
        \node[dpad1](C) at (2.4,1.0){$C$};

        % \draw[arr, <-] (Z) to node[above,pos=0.6]{$\cal N!$} +(-1.2,0);
        %%%%% G %%%%%%
        \draw[arr1,-,line width=1.3pt,transform canvas={yshift=1pt},dashed,Gcolor, shorten <=5pt] (Xfake) to +(-1.5,0);
        \draw[arr1, <-] (Xfake) to node[above,pos=0.6]{$G$}
            % node[below,pos=0.7,inner sep=1.5pt]{\color{gray}\scriptsize$(\beta{=}1)$}
            +(-1.5,0);
        \draw[arr1, <-] (Xreal) to node[above,pos=0.6]{$\pdata!$} +(-2,0);
        % \draw[arr] (X.0) to[out=0,in=-80,looseness=1.6] node[right]{$D$} (C.-35);
        % \draw[arr] (X.10) to[out=10,in=-90,looseness=1.4] node[left]{$\frac12$} (C.-45);
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%% D %%%%%%%%%%%%%%%%%%
        \draw[arr,-,line width=1.3pt,transform canvas={xshift=1pt}, Dcolor,dashed]
                   (X.-2) to[out=0,in=-20,looseness=2.21] (C.-28);
        \draw[arr] (X.0) to[out=0,in=-20,looseness=2.2] node[right]{$D$} (C.-30);
        % \draw[arr] (X.10) to[out=10,in=-45,looseness=1.6] node[left]{$\frac12$} (C.-45);
        %%%%%%%%%%%%%%%%%%%%%%%%%%% e %%%%%%%%%%%%%%%%%%%%
        \draw[arr1,-,line width=1.1pt,transform canvas={xshift=0.5pt}, Gcolor,shorten >=3pt]
            (X.10) to[out=10,in=-55,looseness=1.6] (C.-50);
        \draw[arr1,-,line width=1.1pt,transform canvas={xshift=-0.8pt}, Dcolor,shorten >=3pt,shorten <=1pt]
            (X.10) to[out=10,in=-55,looseness=1.6] (C.-50);
        \draw[arr1]
            % (X)
            (X.10)
            to[out=10,in=-55,looseness=1.6]
            % node{\scalebox{0.8}{\contour{white}{\small$50/50$}}}
            node[left,pos=0.6,inner sep=4pt]{$e$}
            (C.-50);
        % \draw[arr] (X) to[] node[]{\small50/50} (C);
        % % \cmergearr[->>]{Xfake}{Xreal}{X}{1.2,0.1}
        \coordinate (ctr) at (1.2,0.1);
        \draw[arr1,-,shorten >=0pt] (Xfake) to[bend right=8] (ctr);
        \draw[arr1,-,shorten >=0pt] (Xreal) to[bend left=5] (ctr);
        \draw[arr1,-,shorten >=0pt] (C) to[in=150,out=180,looseness=1.5](ctr);
        \draw[arr1,->>,shorten <=0pt] (ctr) to[bend right=10](X);
        \draw[arr1, <-] (C) to node[above,pos=0.6]{\small$50/50!$} +(1.5,0);
\end{tikzpicture}
\]
The GAN objective is typically written as a 2-player minimax game:
$
    \min_{G} \min_{D}  \mathcal L^{\text{GAN}}(G,D)
$, where
\[
\mathcal L^{\text{GAN}}(G,D) = \Ex_{\vphantom{|}x \mathrlap{\sim \pdata}}
    [\log D(x)] + \Ex_{x' \sim G} [\log (1- D(x'))].
\]
% Now consider the two perspectives: the generator's perspective $G$, with
% \textbf{The Discriminator's View.}
% % According to the discriminator,
% \textbf{The Generator's View.}
% The generator controls t

\textbf{The Discriminator's Focus.}
The discriminator has full control over $D$, and attends to
everything but $e$.
That inconsistency of this PDG is what might be called
the discriminator's objective:
% the KL divergen
% $\Ex[ \kldiv{D(C|X)}]$
the expected KL divergence from $D$ to the optimal discriminator.
If $D$ also disbelieves that any image is equally likely to be fake as real
(by chosing $\varphi(e) = -1$),
% (i.e., $\begin{matrix}\varphi(e) = -1\\ \varphi(D) =+1\end{matrix}$),
then the inconsistency becomes $-\mathcal L^{\text{GAN}}$.
% $D(C|X)$ to the
% true probabililty of $C$ given $X$.
% while the generator has control over $G$.

\textbf{The Generator's Focus.}
The generator has control over $G$.
If it ignores $D$ attends only to $e$, the inconsistency
is the Jenson-Shanon Divergence between $G$ and $\pdata$.
If the generator also disbelieves the discriminator $D$
(i.e., $ \varphi(D) =-1$),
then the inconsistency becomes $+\mathcal L^{\text{GAN}}$.

Standard practice is to use small $\chi(G)$ and large $\chi(D)$,
so that the discriminator is well-adapted to the generator.

\section{Message Passing Algorithms as LIR}

Nearly every standard graphical model can be viewed
    as a factor graph, and correspondlingly admits an
    (approximate) inference procedure known
    variously as (loopy) belief propogation \cite{koller2009probabilistic},
    the generalized distributive law \cite{aji2000gendistriblaw},
    and the sum-product algorithm \cite{kschischang2001factor}.
It also turns out to be the special case of LIR specialized to factor graphs.
    % which correspond to PDGs that have equal weighting in .

% \paragraph{Belief Propogation.}
% % Let $\X$ be a set of variables,
% Consider a factor graph
A \emph{factor graph} over a set of variables $\X$ is a set of factors
% Let $\X$ be a set of variables, and consider a set of factors
 $\Phi = \{ \phi_a : \mathbf X_a \to \mathbb R_{\ge 0}\}_{a \in \Ar}$,
where each $\mathbf X_a \subseteq \mathcal X$ is called the \emph{scope} of $a$.
Conversely, for $X \in \X$, let
$\partial X
    \vfull{ := \{a \in \Ar : X \in \mat X_a\}}
$ be the set of factors with $X$ in scope.
$\Phi$ specifies a distribution
$\Pr_\Phi(\X) \propto \prod_a \phi_a(\mat X_a)$, and
% $\Pr_\Phi = \frac1Z \prod_a \phi_a(\mat X_a)$, where $Z$ is a normalization constant,and $\log Z$ is  and
corresponds to a PDG
%
% The PDG that corresponds to
% this factor graph is $\Phi$ is
\[
% $
    \dg M_\Phi = \Big\{ \begin{tikzpicture}[center base]
        \node[dpadinline] (X) {$\mathbf X_a$};
        \draw[arr,<-] (X) to node[above, pos=0.65,inner sep=3pt]{$\propto \smash{\phi_a}$}
            node[below,pos=0.7,inner sep=1.5pt]{\color{gray}\scriptsize$(\alpha,\beta{=}1)$} +(-1.4,0);
    \end{tikzpicture}~\Big\}_{a \in \Ar}
% $
\]
% ~\parbox{0.45\linewidth}{\!\!that specifies the same\\\hspace{1em}distirbution, when $\gamma=1$.}
that specifies the same joint distirbution $\Pr_{\Phi}$, when
    observation and structure are weighted equally (i.e., $\gamma=1$).
% be the corresponding PDG.
% There are several different variants of beleif propogation;
% each can be written as an instance of LIR.

% \paragraph{Sum Product Belief Propogation.}
% The state for standard sum-product belief
Sum-product belief propogation \cite{kschischang2001factor}
    aims to approximate marginals of $\Pr_{\Theta}$
    with only local computations: messages sent between factors and
        the variables they have in scope.
% passing messages between adjacent variables and factors.
 % so the state of the algorithm consists of a pair of messages
Its state consists of pairs of ``messages''
% $m_{X \sto a}$ and $m_{a \sto X}$
$\{ m_{X \sto a}, m_{a \sto X}\}$, both (unnormalized) distributions over $X$,
for each pair $(a, X)$ with $a \in \partial X$,
% for each pair $(X, a \in \partial X)$.
% $\nu_X(X)$ for each variable $X$, and a joint distribution $\nu_a(\mat X_a)$ for each
% factor $a \in \Ar$.
% $m_{i}
% Concretely, the messages passed are given by:
\def\Msg{\dg{M\mskip-1mus\mskip-2mu g}}
% Together, we can regard this collection of messages as a PDG, which we call $\Msg$.
% Just as with the original factor graph,
    % $\Msg$.
which together form a PDG $\Msg$
in the same way as
% just like
the original factor graph.
% After initializing the messages, the algorithm proceeds by repeatedly recomputing:
After initialization, belief propogation repeatedly recomputes:
% The bulk of the algorithm involves propogating messages, each of which
% is a function of other messages, as follows:
\begin{align}
    m_{X \!\sto a}(x)
        % &:=
        &:\propto
        \prod_{{b \in \partial X\setminus a}} m_{b\sto\! X} (x)
        % \qquad &
        \label{eq:X->a}
        \\
    m_{a \sto\mskip-2mu X}(x)
        % &:=
        &:\propto
        ~~\sum_{\mathclap{\mat y \in \V(\mat X_a \setminus X)}}~~ \phi_a(x, \mat y)
        ~\prod_{\mathclap{Y \in \mat X_a \setminus X}}~
                m_{Y \!\sto a} (Y(\mat y)),
        \label{eq:a->X}
\end{align}
where $Y(\mat y)$ is the value of $Y$ in the joint setting $\mat y$.
Finally, variable marginals $\{b_X\}_{X \in \X}$,
which we regard as another PDG, $\dg B$, are computed from the messages according to
$
    b_X(x) \propto \prod_{a \in \partial X} m_{a \sto X}(x)
$.
Observe that every calculation is a (marginal of) a product of factors,
    and thus amounts to inference
        % on a local factor graph of the appropriate context.
        in some ``local'' factor graph.
% On the factor graph, each node aggregates
% The process can be visualized as propogation along the edges of the factor graph.
% The standard illustration of it is not so different from the PDG $\Msg$
The traditional depiction of messages
    moving between variables and factors (see \cref{sec:bp-details}) is not so different from the PDG
% \begin{quotation}\it
%     the message sent from a vertex $v$ on an edge $e$ is the product of the local function at $v$, with all messages recieved at $v$ (other than from $e$),
%     summarized for $e$'s variable node. \cite{kschischang2001sumproduct}
% \end{quotation}
% \vskip
% The PDG $\Msg$ is not so different from this schematic:
\begin{center}
    % \vspace{-1ex}
    % $\begin{matrix}\Msg\\+
    %     {\color{blue!50!black}\dg M_\Phi}
    % \end{matrix}\!\cong\,$
    \begin{tikzpicture}[xscale=2,center base]
        % \node[factor] (a) at (0,0) {$a$};
    \begin{scope}[os1/.style={outer sep=1pt},dpad0/.append style={fill=gray!50!black,text=white,font=\mathversion{bold}}]
        % \node[] (a) at (0,0) {$a$};
        % \node[dpad0,os1] (aX) at (0.22,0){\scalebox{0.7}{$X^{(a)}$}};
        \node[dpad0,os1] (aX) at (0.22,0){\scalebox{0.7}{$X^{a}$}};
        \node[dpadded,os1] (X) at (1,0) {$X$};
        % \node[factor] (b1) at (2,0.5){$b_1$};
        % \node[factor] (bm) at (2,-0.5){$b_m$};
        \node[dpad0,os1] (b1X) at (1.8,0.5){\scalebox{0.7}{$X^{b_1}$}};
        \node[dpad0,os1] (bmX) at (1.8,-0.5){\scalebox{0.7}{$X^{b_m}$}};
        % \node[draw,circle,inner sep=2px] (Y1) at (-1,0.5){$Y_1$};
        % \node[draw,circle,inner sep=2px] (Yn) at (-1,-0.5){$Y_n$};
        \node[dpadded,os1] (Y1) at (-1.1,0.5){$Y_1$};
        \node[dpadded,os1] (Yn) at (-1.1,-0.5){$Y_n$};
        % \node[dpad0,os1] (aY1) at (-0.2,0.3){\scalebox{0.7}{$Y_{1}^{(a)}$}};
        % \node[dpad0,os1] (aYn) at (-0.2,-0.3){\scalebox{0.7}{$Y_{n}^{(a)}$}};
        \node[dpad0,os1] (aY1) at (-0.2,0.3){\scalebox{0.7}{$Y_{1}^{a}$}};
        \node[dpad0,os1] (aYn) at (-0.2,-0.3){\scalebox{0.7}{$Y_{n}^{a}$}};
    \end{scope}
        % \node[anchor=center] at (-1,0){$\scalebox{1}[1]{\vdots}$};
        % \node[anchor=center,draw] at (2,0){$\scalebox{1}[1]{\vdots}$};
        % \draw (a) -- (X);
        % \draw[double equal sign distance,shorten <=0,shorten >=0] (aX.center) to (X.center);
        % \draw[double equal sign distance,shorten <=0,shorten >=0] (aX) to (X);
    \begin{scope}[every path/.append style={gray!50,double equal sign distance}]
        \draw (aX) to (X);
        \draw (aY1) to (Y1);
        \draw (aYn) to (Yn);
        \draw (b1X) to (X);
        \draw (bmX) to (X);
    \end{scope}
        % \draw (b1) -- (X);
        % \draw (bm) -- (X);
        % \draw (Y1) -- (a);
        % \draw (Yn) -- (a);

        % red arrows X->a
    \begin{scope}[green!70!black]
        \draw[arr0,
            % arrows={->[harpoon,swap]}
            ]
            (1.5,0.8) to[in=20,out=-80] node[left, pos=0.1]{$m_{b_1 \!\stogreen \mskip-2muX}$\!} ([yshift=1px]X.24);
        \draw[arr0,
            % arrows={->[harpoon,swap]}
            ]
            (1.6,0.1) to[in=-23,out=-95] node[right, pos=0.25]{$m_{b_m \!\stogreen \mskip-2muX}$\!} ([yshift=1px]X.-15);
        \draw[arr0,
            % arrows={->[harpoon,swap]},
                dashed]
            (0.6,0.7) to[in=0,out=-90] node[left, pos=0.1]{$m_{X \!\stogreen a}$\!} ([yshift=2px]aX.east);
    \end{scope}
        % blue arrows a -> X
    \begin{scope}[blue]
        \draw[arr0,
            % arrows={->[harpoon,swap]}
            ]
            (-0.85,-0.1) to[in=160,out=110] node[right, pos=0.1]{$m_{Y_1 \!\stoblue a}$\!} ([yshift=-2px]aY1.175);
        \draw[arr0,
            % arrows={->[harpoon,swap]}
            ]
            (-0.8,-0.9) to[in=-157,out=100] node[right, pos=0.1]{\mbox{$m_{Y_n \!\stoblue a}$\!}} ([yshift=-1px]aYn.-166);

        \draw[arr0,
            % arrows={->[harpoon,swap]},
            dashed]
            (0.6,-0.7) to[in=180,out=90] node[right, pos=0.1]{\!$m_{a \stoblue \mskip-2muX}$} ([yshift=-2px]X.west);
    \end{scope}
    \begin{scope}[blue!50!black]
        \coordinate (amerge) at (-70:0.6);
        \draw[arr0,shorten <=0] (amerge) to[out=130,in=-90] (aX);
        \draw[arr0,shorten <=0] (amerge) to[out=130,in=-45] (aY1);
        \draw[arr0,shorten <=0] (amerge) to[out=130,in=-10] (aYn);
        \draw[arr0,-,shorten >=0] (0.3,-0.9) to[in=-50,out=90]
            node[left]{$\phi_a$} (amerge);
    \end{scope}
\end{tikzpicture}
% \!\!\!\raisebox{-1em}.
\raisebox{-1em}{
\!\!\!\!\!
$\subseteq\begin{matrix}\Msg\\+
    {\color{blue!50!black}\dg M_\Phi}
    \text{.\!}
\end{matrix}$}
\end{center}
%
% Computing
% Calculating
% $m_{X \sto a}$
% the message from $X$ to $a$
% Our observation here is that the message computations are
Indeed, it can be shown that (\ref{eq:X->a},\ref{eq:a->X}) minimize inconsistency of
    the dotted components in their appropriate contexts
    (shown in green and blue above, and formalized
        in \cref{sec:bp-details}).
    % shown above in orange and blue, respectively.
% Once the details are in place, we find that

% Thus, for any schedule of messages
\begin{prop} \label{prop:bp}
    If \textsc{Refocus} selects a focus non-deterministically from
    $\{ a\sto\mskip-2mu X, X\! \sto a, X\}_{X \in \X, a \in \partial X}$
    (details in \cref{sec:bp-details}), then
    % with attention and control sets as above and
    % $\gamma=1$, then
    % \[ \Big\{
    %     \begin{pmatrix}
    %         % \chi = \infty \mathbbm 1[m_{X \sto a}]\\
    %         % \phi = \infty\mathbbm1 [\{ m_{b \sto X} \}_{b \in \partial X \setminus a} \cup \{m_{X \sto a}\}]
    %         C =  \{ m_{X \sto a} \}\\
    %         A = \{ m_{b \sto X} \}_{b \in \partial X \setminus a} \cup \{m_{X \sto a}\}
    %     \end{pmatrix}
    %     ~\Big|~ a \in \Ar, X \in \mat X_a \Big\},
    % \]
    the possible runs of
    \textsc{LIR}$(
        \dg M_\Phi, \Msg
        + \dg B
         )$
    are precisely those of BP for different message schedules.
    % Further adopting a view with full control in $\beta$
\end{prop}

There are many established variants of this algorithm.
Some of them are generated different by clustering factors together---%
    in the language of \citet{koller2009probabilistic},
    that is to say choosing something other than the Bethe cluster graph
    as the basis for message passing.
Our analysis immediately applies to these other cluster graphs.
% The variational form of message passing
% With a slightly different formalism,
% There is also a well-established variational picture, in which message
% passing minimizes the Bethe approximation to the free energy.


% We suspect
\citet{minka2005divergence} offers a different perspective,
in which a broader class of message passing algorithms can be viewed as iteratively
adjusting some local context to minimize an $\alpha$-divergence.
% that are generated from the $\alpha$-divergences \cite{minka2005divergence}, because
% of the close relationship those divergences have with simple PDGs \cite{one-true-loss}.
% In particuar, he shows minimizing $\alpha$-divergences
We suspect that LIR generalizes thses procedure as well---not only
because it is similar in spirit, but also because because
these divergences can be viewed as the degree of inconsistency
of a PDG containing two distributions \cite{one-true-loss}.
% generalized belief propogation
    % \cite{generalized-bp} as well.

% \paragraph{Message Passing from Divergences}
% \citet{}
% \section{Adversarial Training as LIR}

% we also believe

%
% Here, various control comes in handy.
% \section{Diffusion as LIR}
% The diffusion objective turns out to be just the inconsistency of the appropriate PDG,
% i.e.,
% \[
%     .
% \]
% % but it has
% What makes this tractable, however, is that

% \section{Generic Generative Modeling with LIR}
% All of this suggests a general process by which
% Suppose we want to generate some structured data .
% The general idea


\section{Discussion and Future Work}

These examples are only the beginning.
Our initial investigations suggest that
opinion dynamics models,
the training process for diffusion models,
and much more, are all naturally captured by LIR.
%
The surprising generality of LIR begs some theoretical questions.
What assumptions are needed to prove that it reduces overall inconsistency,
    as is often the case?
    % Do simple choices ever lead to non-standard algorithms?
    What are the simplest choices we could make to produce an efficient
        non-standard algorithm?
    How expressive is this mode of computation?

% These results also
It also
suggests a novel approach to structured generative modeling:
haphazardly assemble a PDG with many variables, existing models, priors,
    constraints, and data of all shapes and sizes.
Then, train new models to predict variables from one another,
    using LIR (with random refocusing, say).
Is this effective?  We are excited to find out!

% \clearpage
% \newpage

% \bibliography{lir-refs}
% \bibliographystyle{icml2023}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\begin{subappendices}
% \appendix

\section{Details on Belief Propogation} \label{sec:bp-details}
The usual schematic illustration of belief propogation \cite{kschischang2001factor}
looks something like:
\begin{center}
    \begin{tikzpicture}[xscale=1.8]
        \node[factor] (a) at (0,0) {$a$};
        \node[draw,circle,inner sep=4px] (X) at (1,0) {$X$};
        \node[factor] (b1) at (2,0.5){$b_1$};
        \node[factor] (bm) at (2,-0.5){$b_m$};
        \node[draw,circle,inner sep=2px] (Y1) at (-1,0.5){$Y_1$};
        \node[draw,circle,inner sep=2px] (Yn) at (-1,-0.5){$Y_n$};
        % \node[anchor=center] at (-1,0){$\scalebox{1}[1]{\vdots}$};
        % \node[anchor=center,draw] at (2,0){$\scalebox{1}[1]{\vdots}$};
        \draw (a) -- (X);
        \draw (b1) -- (X);
        \draw (bm) -- (X);
        \draw (Y1) -- (a);
        \draw (Yn) -- (a);

        % \draw[arr0,red,arrows={->[harpoon,swap]}]
        %     (1.6,0.7) to[in=23,out=-80] node[left, pos=0]{$m_{b_1 \!\sto \!X}$\!\!} (X);
        % \draw[arr0,red,arrows={->[harpoon,swap]},dashed]
        %     (0.5,0.7) to[in=0,out=-90] node[left, pos=0]{$m_{X \!\sto a}$\!} (a);
        % \draw[arr0,blue,arrows={->[harpoon,swap]}]
        %     (0.5,-0.7) to[in=180,out=90] node[right, pos=0]{\!$m_{a \sto \!X}$} (X);
        \begin{scope}[transform canvas={yshift=2px},green!70!black]
        \draw[arr,arrows={->[harpoon,swap]}]
            (b1) to node[above=-0.5pt,rotate=15]{$m_{b_1 \!\stogreen \mskip-2muX}$\!\!} (X);
        \draw[arr,arrows={->[harpoon,swap]}]
            (bm) to node[above=-0.5pt,rotate=-5,pos=0.3]{$m_{b_m \!\stogreen \mskip-2muX}$\!\!} (X);
        \draw[arr,arrows={->[harpoon,swap]},densely dotted]
            (X) to node[above]{$m_{X \!\stogreen a}$\!} (a);
        \end{scope}

        \begin{scope}[transform canvas={yshift=-2px},blue]
        \draw[arr,arrows={->[harpoon,swap]}]
        % \draw[blue,arrows={->[harpoon,swap]}]
            (Y1) to node[below=-0.5pt, rotate=-5,pos=0.35]{$m_{Y_{\!1} \!\stoblue a}$\!\!} (a);
        \draw[arr,arrows={->[harpoon,swap]}]
            (Yn) to node[below, rotate=17]{$m_{Y_{\!n} \!\stoblue a}$\!\!} (a);
        \draw[arr,arrows={->[harpoon,swap]},densely dotted]
            (a) to node[below]{\!$m_{a \stoblue \mskip-2muX}$} (X);
        \end{scope}
        % \draw (current bounding box.north west) rectangle (current bounding box.south east);
    \end{tikzpicture}~\raisebox{1ex}.
\end{center}%

This is only a schematic,
    but the PDG $\Msg$ can be made to look similar to it.
Adding a variable $X^{a}$ for every pair
    % $(a \in \Ar, X \in \mat X_a)$,
    $(X,a)$ with $X \in \mat X_a$
along with edges asserting that $X^{a} = X$,
we obtain the equivalent PDG in the main body of the paper:
\begin{center}
\begin{tikzpicture}[xscale=2,center base]
    % \node[factor] (a) at (0,0) {$a$};
\begin{scope}[os1/.style={outer sep=1pt},dpad0/.append style={fill=gray!50!black,text=white,font=\mathversion{bold}}]
    % \node[] (a) at (0,0) {$a$};
    % \node[dpad0,os1] (aX) at (0.22,0){\scalebox{0.7}{$X^{(a)}$}};
    \node[dpad0,os1] (aX) at (0.22,0){\scalebox{0.7}{$X^{a}$}};
    \node[dpadded,os1] (X) at (1,0) {$X$};
    % \node[factor] (b1) at (2,0.5){$b_1$};
    % \node[factor] (bm) at (2,-0.5){$b_m$};
    \node[dpad0,os1] (b1X) at (1.8,0.5){\scalebox{0.7}{$X^{b_1}$}};
    \node[dpad0,os1] (bmX) at (1.8,-0.5){\scalebox{0.7}{$X^{b_m}$}};
    % \node[draw,circle,inner sep=2px] (Y1) at (-1,0.5){$Y_1$};
    % \node[draw,circle,inner sep=2px] (Yn) at (-1,-0.5){$Y_n$};
    \node[dpadded,os1] (Y1) at (-1.1,0.5){$Y_1$};
    \node[dpadded,os1] (Yn) at (-1.1,-0.5){$Y_n$};
    % \node[dpad0,os1] (aY1) at (-0.2,0.3){\scalebox{0.7}{$Y_{1}^{(a)}$}};
    % \node[dpad0,os1] (aYn) at (-0.2,-0.3){\scalebox{0.7}{$Y_{n}^{(a)}$}};
    \node[dpad0,os1] (aY1) at (-0.2,0.3){\scalebox{0.7}{$Y_{1}^{a}$}};
    \node[dpad0,os1] (aYn) at (-0.2,-0.3){\scalebox{0.7}{$Y_{n}^{a}$}};
\end{scope}
    % \node[anchor=center] at (-1,0){$\scalebox{1}[1]{\vdots}$};
    % \node[anchor=center,draw] at (2,0){$\scalebox{1}[1]{\vdots}$};
    % \draw (a) -- (X);
    % \draw[double equal sign distance,shorten <=0,shorten >=0] (aX.center) to (X.center);
    % \draw[double equal sign distance,shorten <=0,shorten >=0] (aX) to (X);
\begin{scope}[every path/.append style={gray!50,double equal sign distance}]
    \draw (aX) to (X);
    \draw (aY1) to (Y1);
    \draw (aYn) to (Yn);
    \draw (b1X) to (X);
    \draw (bmX) to (X);
\end{scope}
    % \draw (b1) -- (X);
    % \draw (bm) -- (X);
    % \draw (Y1) -- (a);
    % \draw (Yn) -- (a);

    % red arrows X->a
\begin{scope}[green!70!black]
    \draw[arr0,arrows={->[harpoon,swap]}]
        (1.5,0.8) to[in=20,out=-80] node[left, pos=0.1]{$m_{b_1 \!\stogreen \mskip-2muX}$\!} ([yshift=1px]X.24);
    \draw[arr0,arrows={->[harpoon,swap]}]
        (1.6,0.1) to[in=-23,out=-95] node[right, pos=0.25]{$m_{b_m \!\stogreen \mskip-2muX}$\!} ([yshift=1px]X.-15);
    \draw[arr0,arrows={->[harpoon,swap]},dashed]
        (0.6,0.7) to[in=0,out=-90] node[left, pos=0.1]{$m_{X \!\stogreen a}$\!} ([yshift=2px]aX.east);
\end{scope}
    % blue arrows a -> X
\begin{scope}[blue]
    \draw[arr0,arrows={->[harpoon,swap]}]
        (-0.85,-0.1) to[in=160,out=110] node[right, pos=0.1]{$m_{Y_1 \!\stoblue a}$\!} ([yshift=-2px]aY1.175);
    \draw[arr0,arrows={->[harpoon,swap]}]
        (-0.8,-0.9) to[in=-157,out=100] node[right, pos=0.1]{$m_{Y_n \!\stoblue a}$\!} ([yshift=-1px]aYn.-166);

    \draw[arr0,arrows={->[harpoon,swap]},dashed]
        (0.6,-0.7) to[in=180,out=90] node[right, pos=0.1]{\!$m_{a \stoblue \mskip-2muX}$} ([yshift=-2px]X.west);
\end{scope}
\begin{scope}[blue!50!black]
    \coordinate (amerge) at (-70:0.6);
    \draw[arr0,shorten <=0] (amerge) to[out=130,in=-90] (aX);
    \draw[arr0,shorten <=0] (amerge) to[out=130,in=-45] (aY1);
    \draw[arr0,shorten <=0] (amerge) to[out=130,in=-10] (aYn);
    \draw[arr0,-,shorten >=0] (0.3,-0.9) to[in=-50,out=90]
        node[left]{$\phi_a$} (amerge);
\end{scope}
\end{tikzpicture}.
\end{center}

We now define the views.
% To a first approximation
Modulo a small subtlety,
the following is essentially true:
% In particular,
Equation
\eqref{eq:X->a}
adjusts the parameters of
$C_{X \sto a} := \{ m_{X \sto a} \}$ so as to
minimize 1-inconsistency in context
$A_{X \sto a} := \{ m_{b \sto X} \}_{b \in \partial X \setminus a} \cup \{m_{X \sto a}\}$,
while
% $m_{a \sto X}$
% the mesage from $a$ to $X$
\eqref{eq:a->X}
% is the one that minimizes the 1-inconsistency of
adjusts
$C_{a \sto X} := \{ m_{a \sto X} \}$
so as to minimize the 1-inconsistency in
context
$A_{a \sto X} := \{ \phi_a, m_{a \sto X} \} \cup \{ m_{Y \sto a} \}_{Y \in \mat X_a \setminus X}$.

The only wrinkle is that we do not want to attend to
    the structural aspect of a message $e$ that we are updating---%
    that is, we must select $\varphi$ so as to ignore its causal weight $\alpha_e$.
% Intuitively: although all of the input messages summarize causal information,
%     we're trying to capture that information with a distribution.
% Thus, it's not appropriate to attend to the causal structure
%     of the edges that we're
% Intuitively: when we are updating some message (say $m_{X \sto a}$), we
Intuitively: when we are updating some message $e$, we
are interested in summarizing information in the other messages
(both observational and causal information), purely with an observation.
% Put another way, we do not wa
% This means that the foci correspond to elements of
% \[
%     \mat L := \bigcup_{a\in \Ar,X \in \mat X_a}\Big\{a \sto X,\, X \sto a,\, X\Big\},
% \]
% such that $\mat F := \{ (\varphi_l, \chi_l) : l \in \mat L \}$.

More precisely, the foci
\[
    \mat F :=  \bigg\{ (\varphi_j, \chi_j) ~:~ j \in \bigcup_{\substack{a\in \Ar \\ X \in \mat X_a}}\Big\{a \sto X,\, X \sto a,\, X\Big\},\bigg \}
\]
are indexed by messages and variables, and defined as follows.
% such that $\mat F := \{ (\varphi_l, \chi_l) : l \in \mat L \}$.
% This means that for each focus
% \[
% (\varphi,\chi) \in \mat F ~~:= \bigcup_{a\in \Ar,X \in \mat X_a}\{a \sto X, X \sto a, X\},
% \]
The attention mask $\varphi_j$ is given by:
\[
    \varphi_j(a) :=  \begin{cases}
        \binom 11 & \text{ if } a \in A_{j} \setminus C_{j} \\
        \binom 10 & \text{ if } a \in C_{j} \\
        \binom 00 & \text{otherwise}
    \end{cases},
\]
where $\binom{\phi_1}{\phi_2}$ scales $\beta_a$ by $\phi_1$ and $\alpha_a$ by $\phi_2$.
Finally,
% for each focus $l \in \mat F$, define
full control over $C_j$ means defining
\[
\chi_j(a) := \begin{cases}
    \infty & \text{ if } a \in C_{j} \\
    0 & \text{ otherwise}.
\end{cases}
\]
With these definitions, \cref{prop:bp} follows easily.
%
Continue to \cref{appendix:proofs} for proofs!

\onecolumn
\section{Proofs}
    \label{appendix:proofs}

First, some extra details for \cref{thm:cvx}.
By parameteriations $\mathbb P$ log-concave, we mean that, for every $a \in \Ar$, and $(s,t) \in \V(\Src a,\Tgt a)$, the function
$$
    \theta \mapsto -\log \p_a^{\,\theta}(\Tgt a =t\mid\Src a = a) \quad: \Theta_a \to [0,\infty]
$$
is convex.
This is true for many families of distributions of interest.
For example, if $\Src a, \Tgt a$ is discrete, and the cpd is parameterized
by stochastic matrices $\mat P = [p_{s,t}] \in [0,1]^{\V(\Src a, \Tgt a)}$, then
\[
    - \log \p_a^{\mat P}(\Tgt a =t | \Src a=s) = - \log (p_{s,t})
\]
which is clearly convex in $\mat P$.

To take another example: if $\p_a$ is linear Gaussian, i.e.,
$\p_a(T|S) = \mathcal N(T | \mat A s + b,  \sigma^2)$, parameterized by
$(\mat A, b, 1/\sigma^2)$, then
\begin{align*}
    - \log \p_a^{(\mat A, b, \sigma^2)}(t|s)
    &= -\frac12 \log \frac{2\pi}{\sigma^2}  + \frac12 \left(\frac{t- \mat A s + b}{\sigma}\right)^2
\end{align*}
which is convex in $(\mat A, b, \frac1{\sigma^2})$.  Now, for the proof.


\textbf{\cref{thm:cvx}}. \textit{If $\mathbb P$ is log-concave, then
for small enough $\gamma$,
the map $\theta \mapsto \aar*{\varphi \odot( \Ctx + \dg M(\theta))}_\gamma$ is convex.}

\begin{proof}
    By definition,
    \begin{align*}
        \aar[\Big]{\varphi \odot( \Ctx + \dg M(\theta))}_\gamma
        &= \inf_\mu  \Big\{
            \OInc_{\,\Ctx}(\mu) + \gamma \SDef_{\Ctx}(\mu)
            + 
            \gamma \SDef_{\dg M(\theta)}(\mu) + \OInc_{\dg M(\theta)}(\mu)  
        \Big\}.
    \end{align*}
    Only the final term actually depends on $\theta$, though---recall that $\SDef_{\dg M(\theta)}$ 
    depends only on the structure of the hypergraph (and the weights $\balpha$),
    and not the parameters of the cpds. 
    Thus, we can write $F(\mu)$ for the first three terms. 
        
    For all of our examples, and indeed, 
    if $\gamma$ is chosen small enough, 
    the sum of the two terms is convex in $\mu$ \cite{pdg-aaai}.
    Then we have
    \begin{align*}
        \aar[\Big]{\varphi \odot( \Ctx + \dg M(\theta))}_\gamma
            &= \inf_\mu  \left( F(\mu) + \Ex_\mu \left[ \sum_{\ed a ST}\beta_a \log \frac{\mu(T|S)}{\p_a^\theta(T|S)} \right] \right) \\
            &= \inf_\mu  \Bigg( F(\mu) + \Ex_\mu \left[ \,\sum_{\ed a ST} \beta_a\log \frac{\mu(T|S)}{\lambda(T|S)} \right] +
            \underbrace{
                \Ex_\mu \left[\,\sum_{\ed a ST} \beta_a \log \frac{\lambda(T|S)}{\p_a^\theta(T|S)} \right]
            }_{\text{third term}}
            \Bigg)
    \end{align*}
    The second term is then entropy (relative to the base distribution), which is
        convex in $\mu$. The first term, $F(\mu)$, is convex in $\mu$ as well, and neither depend on $\theta$. The final term is linear in $\mu$.
    Since $\mathbb P$ is log-convex in $\theta$, 
    we know that $(\log\frac{\lambda(t|s)}{\p_a^\theta(t|s)})$ is convex in $\theta$.
    It follows that the third term is a positive linear combination
        of expectations that are all convex in $\theta$, and hence itself convex in $\theta$.
    Because the first two terms do not depend on $\theta$ and are convex in $\mu$,
        they are jointly convex in $(\mu,\theta)$.
    And, as we have seen, the third term is linear in $\mu$ and convex in $\theta$, so it is also jointly convex in $(\mu, \theta)$.
    Thus, the sum of all three terms in the infemum is jointly convex in $(\theta, \mu)$. 
    Taking an infemum over $\mu$ pointwise, the result is still convex in $\theta$     \cite{boyd2004convex}.
    % Since $\varphi \ge 0$ and $\bbeta  > 0$ this is effectively just the inconsistency of a PDG.
\end{proof}

\textbf{\cref{prop:bp}.}\textit{
If \textsc{Refocus} selects a view non-deterministically from
$\{ a\sto\mskip-2mu X, X\! \sto a, X\}_{X \in \X, a \in \partial X}$
with $\varphi, \chi$ as above, and $\gamma=1$, then
% with attention and control sets as above and
% $\gamma=1$, then
% \[ \Big\{
%     \begin{pmatrix}
%         % \chi = \infty \mathbbm 1[m_{X \sto a}]\\
%         % \phi = \infty\mathbbm1 [\{ m_{b \sto X} \}_{b \in \partial X \setminus a} \cup \{m_{X \sto a}\}]
%         C =  \{ m_{X \sto a} \}\\
%         A = \{ m_{b \sto X} \}_{b \in \partial X \setminus a} \cup \{m_{X \sto a}\}
%     \end{pmatrix}
%     ~\Big|~ a \in \Ar, X \in \mat X_a \Big\},
% \]
the possible runs of
\textsc{LIR}$(
    \dg M_\Phi, \Msg
    + \dg B
     )$
are precisely those of BP for different message schedules.}
\begin{proof}
When $\gamma=1$, and $\alpha, \beta = 1$ for all of the input factors, then the optimal
distribution $\mu^*$ that realizes the infemum is just the product of factors. It follows that any distribution that has those marginals will minimize the observational inconsistency.

The different orders that the \eqref{eq:X->a}, and \eqref{eq:a->X} can be ordered
for different adjacent pairs $(a, X)$ correspond to both the message passing schedules, and to the possible view selections of LIR.
\end{proof}


\end{subappendices}
