    \label{chap:LIR}

% \begin{abstract}
In this chapter, 
we present a generic algorithm
    for learning and
    approximate inference
    across a broad class of statistical models,
    that unifies many approaches in the literature.
Our algorithm,
    called local inconsistency resolution (LIR),
    has an intuitive epistemic interpretation.
It is based on the theory of
    probabilistic dependency graphs (PDGs),
    an expressive class of graphical models
        rooted in information theory,
        which can capture inconsistent beliefs.
    % can be viewed as a generic recipe for
    % learning and approximate inference.
% \end{abstract}

\section{Introduction}
% What causes you to update your beliefs?
% To reconsider your point of view?
% How do you show someone that they are mistaken?
% By identifying and resolving inconsistencies.
% What causes a person to change their beliefs?
% Often it is to resolve inconsistencies, amongst one's beliefs and observations.
% How do you change a person's mind?
% One way is to present evidence that contradicts their beliefs; another is to point out inconsistencies between the beliefs they already hold.
What causes a person to change their mind?
% One theory suggests
% Some say
According to some,
    % it is a response to inconsistency:
    it is a response to internal conflict:
    the result of
    discovering new information that contradicts our beliefs, or
    becoming aware of discrepancies between beliefs we already hold
    \citep{festinger1962cognitive}.
% The process is often a gradual one: inconsistencies are dealt with incrementally, and one at a time.
% However, people do not change their minds all at once, keeping in mind everything they know;
% However, it is impossible to respond to this inconsistency all at once, keeping in mind everything you know;
% Indeed, we can only respond to the ones we are aware of, and inconsistencies can be difficult to detect.
% Inconsistency
Inconsistencies
    can be difficult to detect, however
    \citep{selman1996generating}, and indeed can only
    be resolved once we are aware of them.
% Further complicating things,
% Typically
% Further complicating our lives, some parts of the picture are beyond our control;
% A further complication is that some things may be
% Some things may also be
Some things are also
 beyond our control;
    for example, we might receive conflicting information
        from two trusted sources and be unable to resolve
        their disagreement.
    % impossible to respond to this inconsistency all at once, keeping in mind everything you know;
% So in practice, we resolve inconsistencies \emph{locally}---little by little, and one at a time.
So in practice, we resolve inconsistencies \emph{locally}---%
    % by looking at only a small part of the picture,
    % and changing another part of it.
    % attending to and addressing
    little by little, and looking at
        only a small part of the picture at a time.
    % ---little by little, and one at a time.

This can have externalities; fixing one inconsistency can easily create
    others out of view.
Furthermore, some inconsistencies
    are not local in nature,
    and can only be seen when considering many components at once.
% Nevertheless, this process of locally resolving inconsistency can be quite useful.
% Nevertheless,
Yet despite its imperfections,
    this process of locally resolving inconsistency  can be quite useful.
    % And, as we have formalized it in the probabilistic setting,
    As we shall soon see,
    it is a powerful recipe for learning and approximate inference.
% In fact, we will show that it naturally captures a broad class of
% We provide a formalization of it, which we use to show how
%
% We provide a formalization of this process, and show
% We formalize the intuition above
% We provide a formalization of this process
We formalize the process
    in the language of probability and
    convex optimization,
    and show that many popular techniques in the literature
    % can be viewed as local inconsistency resolution.
    arise naturally as instances of it.
    % Finally, we propose a new algorithm
% We argue that this intuition applies not only to people, but also to a
    % broad class of AI systems from the last 20 years.
% This paper describes a general process by which this occurs,

% Historically, much of the belief revision literature has focused on
% conflicts between propositions and entrenchment \citep{agm,...}.


Our approach leans heavily on the theory of PDGs
        % \citep{pdg-aaai}.
        (\cref{chap:PDG-repr}).
% In addition, PDG semantics
% As illustrated in \cref{chap:one-true-loss},
As we have seen, there is a natural way
% information-theoretic way
    % of measuring the inconsistency of a PDG,
    to measure how inconsistent a PDG is;
    we saw in \cref{chap:one-true-loss} that
    many standard loss functions
    can be viewed as measuring the inconsistency of a PDG that
    describes the appropriate situation.
        % \citep{one-true-loss}.
% \vfull{
%     Recently, techniques have been developed to calculate this inconsistency
%     in polynomial time for bounded tree-width,
%     although it scales exponentially with the tree-width of the graph \citep{pdg-infer}.
%     As we move to variables that are continuous variables of large dimension,
%         it becomes intractable to calculate this global inconsistency
%         even for small graphs---the log evidence
%         of a latent variable model \citep{one-true-loss} can be
%         represented as a PDG inconsistency, for example.
% }
% Our work
% We provide an algorithm that
%
   % implicit when we regard inconsistency as a loss function,
   % and generalizing it to training procedures other than .
   % allowing us to describe details about the training process other than
   % ``somehow minimize this global loss''.
%
% It can be intractable to calculate for .
% Calculating the inconsistency of a PDG can be intractable.
% Calculating a PDG's inconsistency can be intractable.
% Our algorithm
We introduce an algorithm
    to operationalize
    the process of adjusting parameters
    to resolve this inconsistency.

% Calculating a PDG's inconsistency can be intractable.
In general, even just calculating a PDG's degree of inconsistency is intractable.
% In general, calculating a PDG's degree of inconsistency can be intractable.
% Computing the degree of inconsistency can be intractable.
% \citet{one-true-loss} shows how variational inference can
%     be understood as the adoption of extra beliefs to
%     % overapproximate the overall inconsistency in a way that is easier to calculate.
%     % in a tractable way.
%     get an overapproximation that is easier to calculate.
% Variational inference can be understood as
Much of variational inference can be understood as
    adopting extra beliefs to
    % overapproximate the overall inconsistency in a way that is easier to calculate.
    % in a tractable way.
    % overapproximate it in a way that is easier to calculate \citep{one-true-loss}.
    minimize an over\-approximation of it that is easier to calculate \citep{one-true-loss}.
% Our algorithm allows for this kind of
% Our approach extends this by also enabling the opposite:
Our approach can capture this, but also enables the opposite:
    % also enables the opposite:
% This is one use of our algorithm,   but we also enable the opposite:
    focusing on small parts of the graph at a time to
     % get
    address tractable under\-approximations of the global inconsistency.
% We show how this technique specializes and suggests a
% Thus, it also has more potential for parallelization.
% Thus, it enjoys benefits such as higher potential for parallelization.
% Thus, it many many instances
This makes it more suitable for distributed settings,
    and more amenable to parallelization.
% Thus, it is potentially easier to parallelize.
% This makes it more natural in distributed settings.
% It is also very expressive.
% Our algorithm, called \emph{local inconsistency resolution} (LIR),
% But perhaps the most surprising aspect of our algorithm,
%     called \emph{local inconsistency resolution} (LIR),
%     is how much .
%
The algorithm, which we call \emph{local inconsistency resolution} (LIR),
    is quite expressive,
    and naturally reduces to a wide variety of learning and inference algorithms in the literature.
    % under the appropriate modeling assumptions.
This observation suggests a generic approach to learning and
    inference in models with arbitrary structure.
%
% Yet often, by placing additional confidence in some parts of the picture,
% or restricting our attention to some small part of it, the inconsistency
% can be calculated more easily.



%% CALL FOR PAPERS WANTS:
% Inference and generating methods for graphs, time series, text, video, and other structured modalities.
% >> will present this in the case study?
%
% Unsupervised representation learning of high dimensional structured data.
% >> a generic algorithm for representation learning:
% >>    describe structure.  Invent or randomize variables + memory architecture.  Choose set of views,
% >>    always including views and
%
% Scaling and accelerating inference and generative models on structured data.
% Uncertainty quantification in AI systems.
%
% Thus, our work may be viewed in several lights.
%     % - a very general probabilistic inference algorithm for structured data;
% On one hand, it is a generic inference algorithm.
%     - a broad class of natural optimizations for inference algorithms;
%     - a generic way of learning


% Many algorithms can be cast in terms of locally minimizing inconsistency.

% \section{Background and Preliminaries}
% \section{Technical Preliminaries}
\commentout{%
\section{Mathematical Preliminaries}
% \section{Mathematical Formalism}
% Each variable $X$ can take on values from a set $\V\!X$ of possible values.
We write $\V\! X$ for the set of values that a variable $X$ can take on,
and $\Delta \V \! X $ for the set of distributions over $\V\!X$.
A conditional probability distribution (cpd) is a map
% (cpd)
$p(Y|X) : \V \!X \to \Delta\mskip-1mu \V \mskip-1mu Y$.
% is a distribution over the values of $Y$ for each value of $X$.
%
% \begin{defn}
    A \emph{directed hypergraph}
    % $(N, \mathcal A)$ is a set $N$ of nodes, and a set $\mathcal A$ of \emph{arcs},
    $(N, \mathcal A)$ is a set of nodes $N$ and a set of arcs $\mathcal A$,
    % $(N, \mathcal A)$ is a set of nodes $N$ and a set of arcs $\Ar$,
    % each $a \in \mathcal A$ of which is associated with
    % so that $a \in \Ar$ is associated
    % to each $a \in\! \Ar$ of which we associate
    each $a \in \mathcal A$ of which
    is associated with
    % comes with
    a set $\Src a \subseteq N$ of source nodes,
    and $\Tgt a \subseteq N$ target nodes.
    We also write $\ed {\scriptstyle a}{S}{T} \in \Ar$ to specify an
    arc $a$ together with its sources $S = \Src a$ and targets $T = \Tgt a$.
% \end{defn}
}%

\section{Parametric PDGs}
The main tool of this chapter is an alternate parametric version of a PDG.
% Just as a neural network is a 
Just as a neural network can be thought of as a flexible functions (or even cpds) that can be adjusted with internal parameters, so too would we like to model PDGs whose cpds are parametric.
For this reason, we
% \paragraph{Geometry.}
% \textbf{Preliminaries: Geometry.}
% We will need various parameter spaces $\Theta$; to simplify the presentation, assume that parameter spaces are a convex subsets of $\mathbb R^n$ (not necessarily of the same dimension).
will require a parameter space $\Theta_a$ for each (hyper)arc $a \in \Ar$.
% To simplify the presentation,
For simplicity, assume that each $\Theta_a$ is a convex subset of $\mathbb R^n$ (not necessarily of the same dimension).
%
\commentout{
Given a manifold $\Theta$ and a differentiable map $P : \Theta \to \Delta \V\! X$,
     the Fisher Information Matrix
$\mathcal I(\theta)
    % = [ \Ex_{x\sim P(\theta)}[ \frac{\partial^2}{\partial \theta_i \partial \theta_j}  \log p_\theta(x) ]]_{i,j}
$
at each $\theta \in \Theta$
 % forms such a Riemannian metric;
gives rise to a Riemannian metric;
thus the mere fact that $\Theta$ parametrizes a family of
probability distributions is enough to make it a Riemannian manifold.
% by $\mat u, \mat v \mapsto \mat u^{\sf T} \mathcal I(\theta) \mat v$.
Moreover, $\mathcal I(\theta)$ is particularly natural in a probabilistic context;
    up to a multiplicative constant, it is the \emph{only} such metric on $\Theta$ that is invariant under sufficient statistics, \citep{chentsov}. \citep{amari2016information}
}%
%
% \subsection{PDGs}
% \paragraph{Probabilistic Dependency Graphs.}
% \textbf{Parametric Probabilistic Dependency Graphs.}
% PDG is a directed graph whose arcs carry probabilistic and causal information, weighted by confidence.
% We now introduce an equally expressive variant, whose explicit parametric nature will prove useful for our purposes.
This allows us to formally define a \emph{parametric} variant of a PDG, as promised in \cref{sec:alt-pdgs}.
% whose explicit parametric nature will prove useful.
% ---%
    % yet it too is essentially equivalent to the others.
% We give yet another variant
% parametric variant of a PDG, which is essentially
    % equivalent (see \cref{appendix:internalization})

% \begin{defn}
%     A PDG $\dg M \!=\! (\X\mskip-2mu, \Ar, \mathbb P, \balpha, \bbeta )$
%     is a directed hypergraph
%         $(\X\mskip-2mu, \Ar)$
%     whose nodes correspond to
%     variables,
%     % together with
%     % probabilities $\mathbb P$
%     %     and
%     %     confidence vectors
%     %     $\balpha \!=\! [\alpha_a]_{a \in \Ar},\bbeta \!=\! [\beta_a]_{a \in \Ar}$,
%     %     so that
%     each arc
%     % $\ed aST \in \Ar$
%     $a \in \Ar$ of which is associated with:
%     % arc $a \in \Ar$ is associated with:
%
%     \begin{itemize}[nosep,itemsep=2pt]
%         \item (subsets of) variables $\Src a, \Tgt a \subset \X$, indicating the respective source and target variables of the edge;
%         % For example,
%         %     $$\Src L = \{A, B\} \ed L{}{} \{C\} = \Tgt L$$
%         %  intuitively represents a joint dependence of $C$ on the variables $A$ and $B$;
%         % \item A manifold $\Theta_a$ of parameters
%         \item
%         a conditional probability distribution
%         {%\subafalse
%             $\p_a(\Tgt a | \Src a)$},
%         on the target variables given
%         the values of the source variables,
%         \item a weight
%         $\beta_a \in \mathbb R \cup \{\infty\}$
%         of confidence in
%         the conditional probability distribution
%         {%\subafalse
%             $\p_a
%             %(\Tgt a | \Src a)
%             $},
%         % \discard{(as measured by the number of independent observations that support $\p_a$), }
%         and
%         \item
%         a weight $\smash{\alpha_a \in \mathbb R}$
%         indicating
%         confidence in the functional dependence of
%         {%\subafalse
%         $\Tgt a\mskip-2mu$ on $\Src a\mskip-2mu$}
%         expressed by $a$.
%         % \discard{
%         % (as measured by the expected number of independent causal mechanisms corresponding to $a$,
%         % that determine $\Tgt a$ given $\Src a$).%
%         % }
%     \end{itemize}
%     % In aggregate, $\balpha = [\alpha_a]_{a \in \Ar}$ and $\bbeta = [\beta_a]_{a \in \Ar}$
%     % are vectors over $\Ar$.
% \end{defn}

% We are particularly interested in PDGs whose cpds we can adjust smoothly.
% In the present paper, we are particularly interested in PDGs whose cpds we can adjust smoothly.
% To that end, we introduce the notion of a \emph{parametric PDG family}
% $\dg P(\Theta)$,
% which is defined the same way, except that each arc $a \in \Ar$ is also associated
% with a manifold $\Theta_a$, and $\p_a$ not itself a cpd, but rather a map from $\Theta_a$ to cpds.
% In other words, we have a cpd
%     % $\p_a^{\theta_a}(\Tgt a | \Src a)$
%     $\p_a(\Tgt a | \Src a; \theta_a)$
%     for each $\theta_a \in \Theta_a$;
%      % ison $\Tgt a$ given $\Src a$,
% thus,
% %
% In addition, we require that there be a special default parameter setting $0_a \in \Theta_a$, such that $\p_a(\Tgt a | \Src a=s; 0_a) \propto \lambda_{\Tgt a}$, for every $s \in \V\Src a$.


\begin{defn}
        \label{defn:ppdg}
    % A \emph{parametric PDG family}
    % A \emph{parametric probabilistic dependency graph} (PPDG)
    A \emph{Parametric Probabilistic Dependency Graph} (PPDG)
    % $\dg P \!=\! (\X\mskip-2mu, \Ar, \mathbb P, \balpha, \bbeta )$
    $
    \dg M(\Theta) \!=\!
    (\X\mskip-2mu, \Ar, \Theta, \mathbb P, \balpha, \bbeta )$
    is a directed hypergraph
        $(\X\mskip-2mu, \Ar)$
    whose nodes correspond to
    variables,
    each arc
    $a \in \Ar$
    % $\ed aST \in \Ar$
     of which is associated with:
    % arc $a \in \Ar$ is associated with:

    \begin{itemize}[nosep,itemsep=2pt,left=0pt]
        % \item (subsets of) variables $\Src a, \Tgt a \subset \X$, indicating the respective source and target variables of the edge;
        \item a parameter space $\Theta_a \subseteq \mathbb R^n $, with a default value $\theta^{\text{init}}_a$.
        % \item
        % a conditional probability distribution
        % {%\subafalse
        %     $\p_a(\Tgt a | \Src a)$},
        % on the target variables given
        % the values of the source variables,
        \item
        a map
        $\p_a : \Theta_a \times \V\Src a \to \Delta \V \Tgt a$
        % $\p_a : \Theta_a \subafalse \times \V\Src a \to \Delta \V \Tgt a$
        % {%\subafalse
        %     $\p_a(\Tgt a | \Src a)$},
        that gives a cpd
        % $\p_{a,\theta}(\Tgt a | \Src a)$
        $\p_{a}^{\,\theta}(\Tgt a | \Src a)$
        over $a$'s targets given its sources,
        % $\p_{a}^{\,\theta}\subafalse(\Tgt a | \Src a)$
        for every $\theta \in \Theta_a$,
        % such that  $\p_{a}^{\,0}(s) \propto \lambda_{\Src a}$;
        % on the target variables given the the source variables.
    %     \item a confidence
    %     % $\beta_a \in \mathbb R \cup \{\infty\}$
    %     $\beta_a \in \mathbb R \cup \{\infty\}$
    %     in the cpd
    %     {
    %         %\subafalse
    %         $\p_a(\Tgt a | \Src a)$}%
    %     % \discard{(as measured by the number of independent observations that support $\p_a$), }
    %     % .
    %     ,
    %     and
    % \item
    %     a confidence $\smash{\alpha_a \in \mathbb R}$
    %     in the functional dependence of
    %     {%\subafalse
    %     $\Tgt a\mskip-2mu$ on $\Src a\mskip-2mu$}
    %     expressed by $a$.
    %     % \discard{
    %     % (as measured by the expected number of independent causal mechanisms corresponding to $a$,
    %     % that determine $\Tgt a$ given $\Src a$).%
    %     % }
    \item confidences
    % $\beta_a \in \mathbb R \cup \{\infty\}$
    $\smash{\alpha_a \in \mathbb R}$
    in the functional dependence of
    {%\subafalse
    $\Tgt a\mskip-2mu$ on $\Src a\mskip-2mu$}
    expressed by $a$,
    % and $\beta_a \in \mathbb R \cup \{\infty\}$
    and $\beta_a \in [0,\infty]$
    in the cpd $\p_a$\,.
    \end{itemize}
    A PDG is the object obtained by fixing the parameters; thus,
    a
    % joint setting
    choice of
        $\theta \in \Theta := \prod_{a \in \Ar} \Theta_a$ yields a PDG
    $\dg M = \dg M(\theta)$.
    %
    % We call a PDG \emph{unweighted} if $\balpha=\bbeta=\mat 1$.
    \qedhere
\end{defn}
% \vspace{-0.2ex}

%
% This
Clearly, a PDG is the
special case of a PPDG in which every $\Theta_a = \{ \theta_a^{\text{init}} \}$ is a singleton.
%  recovers the definition given by \citet{pdg-infer};
% is clearly a PDG;
Conversely, a PPDG may be viewed as a PDG by adding each $\Theta_a$ as
    a variable, as depicted in \cref{fig:adv}.


% \citet{one-true-loss} argues that this inconsistency measure
    % $\aar{- }_\gamma$
    % \eqref{eq:inconsistency}
As argued in \cref{chap:one-true-loss}, PDG inconsistency
    is a ``universal'' loss function, and
     specializes to standard loss functions in a wide variety of standard situations.
It follows that, at an abstract level,
    much of machine learning can be viewed as inconsistency resolution.
%
% We take this idea one step further, operationalizing the resolution process.
We take this idea a few steps further, by 
    (1) operationalizing the resolution process with parametric PDGs,
    % and making it even more expressive with an approximate, local variant
    and 
    (2) allowing for a heuristic approach that only resolves inconsistencies \emph{locally}.
% The latter dramatically expands what computations can be expressed.
    % significantly  expanding what can be expressed.
%
% We operationalize resolution process, and then extend this idea to allow for locality.
    % how even more can be done if we allow
%
    % can be seen as \emph{local} resolution.
% We then demonstrate that several very different historically important
    % procedures are special cases.
     % of our algorithm.
    % of an algorithm we call \emph{local inconsistency resolution}.
% A surprisingly broad class of algorithms already
% \begin{itemize}
%     [nosep]
%     \item Belief updating (Conditioning, Jeffrey's rule)
% \end{itemize}
% However, global inference

\section{Local Inconsistency Resolution (LIR)}

\paragraph{Geometric Preliminaries}
To describe how those parameters evolve over time, we will need some additional geometric concepts.
A \emph{vector field} over $\Theta$ is a differentiable
    map $X$ assigning to each $\theta \in \Theta$ a vector $X_\theta \in \mathbb R^n$.
The \emph{gradient} of a twice differentiable map $f : \Theta \to \mathbb R$,
    % written $\nabla_\theta f(\theta)$,
    which we write $\nabla_\Theta f(\Theta)$, is a vector field.
Given a vector field  $X$ and an initial point $\theta_0 \in \Theta$, there is a unique trajectory $y(t)$ that solves the ODE
\[
\frac{\mathrm d }{\mathrm d t}y(t) = X_{y(t)}
% $,  $
,\qquad
y(0) = \theta_0.
\]
We adopt the notation $\exp_{\theta_0}( X ) := y(1)$,
    standard in differential geometry,
    % which is standard in differential geometry,
    for a compact description of 
    this trajectory
    % compactly refer to this curve, which is standard in many contexts
    \citep{lee.smooth-manifolds}.
At first glance,
% Although on the surface
% Although it may seem
$\exp$ only gives us access to $y(1)$,
    but
    % it is easy to show
     it is easily verified
     that $\exp_{\theta_0}(t X) = y(t)$.
% Although it may not be obvious, $\exp$ indeed allows us to describe the entire trajectory, because $\exp_{p_0}(tX) = p(t)$.
So altogether, the map $t\mapsto \exp_\theta(t \nabla_\Theta f(\Theta))$ is the smooth path beginning at $\theta$ that follows the gradient of $f$. It is commonly known as \emph{gradient flow}.
Our algorithm will be a version of gradient flow to reduce inconsistency, but modulated by two kinds of locality: attention, and control. 

% Two knobs to control the model. The first one controls what you're looking at and what you get to change.
% We now describe the local inconsistency reduction algorithm.
% There are two distinct senses in which inconsistency resolution can be done locally:
%     we can either restrict how much we can see, or how much we can do.
% There are two distinct senses in which inconsistency can be resolved locally:
%     we can restrict what we can see, or what we can do.
% There are two aspects of resolving inconsistency: what
% We can either
\paragraph{Attention and Control.}
There are two distinct senses in which inconsistency resolution can
    be \emph{local}: we can restrict what we can see, or what we can do about it.
Correspondingly, there are two ``focus'' knobs for our algorithm:
    one that restricts our attention to the inconsistency of a subset of arcs $\attn \subseteq \Ar$,
    % and one that restricts our control to the parameters of a subset
    and another that restricts our control to (only) the parameters of
    % a subset
    arcs
    $\ctrl
    % (\subseteq A)
     \subseteq \Ar$
     % \subseteq A$
      as we resolve that inconsistency.
% \begin{enumerate}[label=(\alph*)]
% (a)
% \item
    % restrict our attention to a subset $A \subseteq \Ar$ of arcs,
    % restrict our attention to the inconsistency of a subset of arcs $A \subseteq \Ar$,
    %     % trying to reduce inconsistency of the sub-PDG,
    %     or
    %     % taking a the inconsistency of a
    %     \label{item:attn-restrict}
    % % (b)
    % % \item
    %     restrict our control to the parameters
    %     % $\Theta_C := \prod_{a \in C}\Theta_a$ of a subset $C \subseteq \Ar$ of arcs.
    %     of arcs $C \subseteq \Ar$.
    %     \label{item:ctrl-restrict}
% \end{enumerate}
% In the first case,
% Both make the problem more tractible.
% Restricting attention \ref{item:attn-restrict}
% In other words,
The former makes for an underestimate of the inconsistency that is easier to calculate, while
% restricting control \ref{item:ctrl-restrict}
the latter
makes for an easier optimization problem.
% These restrictions are not just cheap approximations, though:
These locality restrictions are not just about making calculations easier, however:
    they are also appropriate modeling assumptions for
    actors that cannot see and control everything at once.

Attention and control need not be black or white.
% More general than selecting $A,C \subseteq \Ar$,
% A generalization of $A,C \subseteq \Ar$ is
A more general approach is to choose
    % an attention mask $\varphi \in \mathbb R^{\Ar}$ and
    % a control mask $\chi \in [0,\infty]^{\Ar}$.
    an \emph{attention mask} $\varphi \in \mathbb R^{\Ar}$ and
    a \emph{control mask} $\chi \in [0,\infty]^{\Ar}$;
% \newmaterial{%
    % these are continuous  which one attends fully to precisely the subset $A$ while controlling the subset $C$. 
% }%
% Large $\varphi(a)$ makes $a$ salient while $\varphi(a) \!=\! 0$ keeps it out of mind;
Large $\varphi(a)$ makes $a$ salient, while $\varphi(a) = 0$
    keeps it out of the picture.
Similarly, large $\chi(a)$ gives significant freedom to change $a$'s parameters,
small $\chi(a)$ affords only minor adjustments, and $\chi(a) \!=\! 0$
    % means we cannot change them at all.
    prevents change altogether.
Either mask can then be applied to
    % an array with an axis indexed by $\Ar$, via
    % a vector over $\Ar$ by
    % a vector indexed by $\Ar$, via
    a tensor that has an axis corresponding to $\Ar$, via
    pointwise multiplication ($\odot$).
% $\gamma$ may be viewed $\varphi$
% So, $\chi$ controls locality in the sense of ``local search''.
%
% So overall, the algorithm goes like this.
    % We recieve input in the form of a PDG $\Ctx$, and
    % a memory layout $\dg M(\Theta)$.
% We then repeatedly select an attention mask $\varphi$ and a control mask $\chi$, and update
% Once we select attention and control masks,
% % We then repeatedly select attention and control masks, and update
%     we update the parameters we can control so as to reduce the inconsistency of
%         what is in view.
% In more detail:

% At a high level, the algorithm proceeds as follows.
% The full algorithm, formalized in \cref{algo:LIR},
% The aim of LIR is to
% The LIR algorithm
\paragraph{The Algorithm.}
    LIR
    modifies
    the parameters $\theta$ of a
PPDG $\MThetadense$ so as to make it more consistent with its context.
% , as follows.
% The procedure is as follows.
It proceeds as follows.
First,
    receive context in the form of a PDG $\Ctx$%
    % recieve context as a PDG $\Ctx$%
    , and
    initialize mutable memory $\MThetadense$%
    % to $\theta^{\text{init}}$%
    .
    % a PPDG that whose parameters we will be changing to reduce its inconsistency with $\Ctx$.
In each iteration,
    choose $\gamma$ (which can be viewed as attention to structure),
    an attention mask $\varphi$ over the arcs of $\MThetadense + \Ctx$,
    and
    a control mask $\chi$ over the arcs of $\MThetadense$.
%
% Use methods of \citet{pdg-infer} to calculate
Calculate
% the inconsistency of $\varphi\odot(\dg M(\Theta) + \Ctx)$,
$\aar{\varphi\odot(\dg M(\theta) + \Ctx)}_\gamma$, the inconsistency of
    the combined context and memory, weighted by attention.
(For discrete PDGs, this can be done with the methods of \cref{chap:infer}.)
Then mitigate this local inconsistency
    by updating
     mutable memory
     $\theta$
      via (an approximation to) gradient flow,
    changing $a$'s parameters in proporition to control $\chi(a)$.
        % context and memory $\Ctx + \dg M(\theta)$
The procedure is fully formalized in \cref{algo:LIR}.
% This is fully formalized in \cref{algo:LIR}.
\medskip

% To describe the algorithm formally in full generality, we also need to des
% If $\dg M(\Theta)$ is a parametric PDG family, then we write
% $\dg M.\Ar$
% $\Ar(\dg M)$
% for the set of arcs in the underlying PDG, and
% $\dg M.\boldsymbol\theta$

\begin{algorithm}
    \singlespacingplus
    \caption{Local Inconsistency Resolution (LIR)}
	\label{algo:LIR}
	\begin{algorithmic}
        % \STATE \textbf{Input:}
        %     mutable memory $\dg M(\Theta) $,
        %     $\gamma \ge 0$,
        % \STATE ~~~~ procedure $\textsc{Refocus}: () \to PDG \times [0, \infty]^{\Ar(\dg M)}$
        % \STATE \textbf{Input:}
        %     context $\Ctx : $ PDG
        % \STATE ~~~~
        %     mutable memory $\Mm(\Theta) : $ PPDG
        \STATE \textbf{Input:}
            context $\Ctx$,~ mutable memory $\MThetadense$.

		% \STATE \textbf{Variables:}
            % state $\theta \in \Theta$,
            % PDG $\Ctx$.
        % \smallskip

        \STATE Initialze $\theta^{(0)} \gets \theta^{\text{init}}$;
        % \STATE Initialze $\theta \gets 0$;

        \FOR{$t = 0, 1, 2, \ldots$}
            \STATE $\Ctx \gets \textsc{Refresh}(\Ctx)$;
                \hfill{\color{gray}\small\texttt{//optional}}
            % \STATE $\varphi(\Ctx), \chi(\dg M)
            \STATE $\varphi, \chi, \gamma
                \gets \textsc{Refocus}()$;
                % \hfill{\color{gray}\small\texttt{//get loci of focus, control}}
                % Choose attention mask $\varphi: (\dg I + \dg M).\Ar \to \mathbb R$
            \STATE $\theta^{(t+1)} \!\gets \exp_{\theta^{(\mskip-1mut\mskip-1mu)}}\!
            % \STATE $\theta \gets \exp_{\theta}
                \Big\{\! {-} \chi \odot \nabla_{\!\Theta}
                \aar[\Big]{ \varphi\odot \!\big( \Ctx + \MThetadense \big) \!}_{\!\!\gamma} \Big\}$;
                % \aar[\big]{ \Ctx + \varphi\odot \dg M(\theta) }_{\!\gamma} \Big\}$;
        \ENDFOR
	\end{algorithmic}
    % \small\color{gray}
    % Issues. Choice of $\Ctx$ includes choice of $\varphi$, if $\varphi : \Ar(\dg M + \Ctx) \to \mathbb R$.
    % But if $\varphi : \Ar(\dg M) \to \mathbb R$, they're not overlapping, and $\chi,\varphi: \mathbb R^{\Ar(\dg M)}$ look more like ``duals''.
    % Of course, we can't then eliminate the refresh instruction.  I also don't like the interpretation as much: if your focus changes in a static environment, I want that to be reflected in $\varphi$, not changing context.
\end{algorithm}
% In order to fully define LIR, we must
% Before we can run \cref{algo:LIR},
% Before we can run it,
In order to
    % execute
    % \cref{algo:LIR},
    execute this procedure,
we must say something about
how the choice of $(\varphi,\chi,\gamma)$ is made.
Thus, we must supply an additional procedure \textsc{Refocus}
to select attention and control masks.
%
% we must supply two more procedures:
% and
% Different choices result in different algorithms;
    % for our purposes,
We focus mostly on the case where $\gamma$ is fixed, and
    \textsc{Refocus}
    chooses non-deterministically
    from a fixed set
    of attention/control mask pairs
    $(\varphi, \chi) \in \mathbf{F}
    % = \{ (\varphi_i,\chi_i) \}
    $,
which we call \emph{foci}.
\cref{algo:LIR} also allows us to a select a second procedure, \textsc{Refresh},
    which makes it easier to model receiving new information
    in online settings.
% The other procedure, \textsc{Refresh} is the identity by default, and can be removed in theory by making supplying an infinitely large initial context containing every possible result of refresh.
% The other procedure,
% \---that is, procedures that continually receive inputs.

The ODE on the last line of \cref{algo:LIR}, which is
    an instance of gradient flow, may be approximated with an
    inner loop running an iterative gradient-based optimization algorithm.
But it also does something interesting at the two extremes.
At on extreme, if \textsc{Refocus} produces small $\chi$
    then it is well-approximated by a single gradient descent step of size $\chi$;
    thus, when control values are small, LIR performs local gradient-based search.
At the oppposite extreme, if $\chi$ is infinite in every component,
    then the final line
% then $\chi=\infty$ finds
    % a parameter setting that globally minimizes inconsistency
 reduces to solving an optimization problem
\[
    \theta^{(t+1)} \gets \arg\min_{\theta}
        \aar[\big]{ \varphi\odot( \Ctx + \dg M(\theta)) }_{\!\gamma}\,
        ,
        % ~~~\text{because of}
\]
at least in 
% the (common) case in which the parameterizations $\mathbb P$ are unconditional and log-concave.
% % (The assignment is non-deterministic if )
% This is because of the following result.
a many cases of interest, because of the following result:

 % so long as the parameterizations $\mathbb P$ are log-concave, because of
% This is because of
\begin{linked}{prop}{logccave}
    % If $\,\mathbb P$ is log-concave, then
    If $\dg M(\Theta)$ is a PPDG whose parameterizations
    $\mathbb P$ are either constant or unconditional and log-concave, then
    for small enough $\gamma$, the map $\theta \mapsto \aar*{\varphi \odot( \Ctx + \dg M(\theta))}_\gamma$ is convex.%
        % \footnote{All proofs can be found in the appendix.}
    % and strictly so for $\gamma > 0$.
\end{linked}
% \begin{proof}
%     It is known that $p \mapsto \aar{\dg M + p}$ is convex (see appendix),
%     and also that $\aar{\dg M + p}$
%     % Let $\varphi_C$ be components of $\varphi$ that act on the arcs of $\dg M$.
% \end{proof}
%
%
% We now consider some important special cases.
In the remaining sections, we give a sample of
some historically important algorithms that are instances of LIR.

% We now consider some special cases of \cref{algo:LIR}.

%%%%%
% If $\Ctx$ is a PDG with variables $\X$,
% and our memory $\dg M$ contains a single joint distribution $\mu(\X)$
% then $\textsc{LIR}(\Ctx, \dg M, \gamma)$ equals $\bbr{\Ctx}^*_\gamma$, the optimal distribution
%%%%%
% If $\Ctx$ is a PDG with discrete variables $\X$,
% $\dg M(\Theta)$ consists of a single joint distribution $\mu(\X)$
%     parameterized as a vector $[0,1]^{\V\!\X}$,
% and \textsc{Refocus} always produces constant $\chi,\gamma$,
% then $\textsc{LIR}(\Ctx, \dg M)$ equals $\bbr{\chi\odot\Ctx}^*_\gamma$,
% the optimal distribution, albeit in a roundabout manner.
%%%%%
\commentout{
    If $\dg M$ is a PDG with discrete variables $\X$
    and we regard $\mu(X)$
    $\dg M(\Theta)$ consists of a single joint distribution $\mu(\X)$
        parameterized as a vector $[0,1]^{\V\!\X}$,
    and \textsc{Refocus} always produces the same constant $\phi$ and $\gamma$,
    then $\textsc{LIR}(\Ctx, \dg M)$ equals $\bbr{\chi\odot\Ctx}^*_\gamma$,
    the optimal distribution, albeit in a roundabout manner.
}
% in which (i.e., every $A_t = \Ar$) amounts to \emph{global} inconsistency reduction, i.e.,
% the universal training objective of \citep{one-true-loss}. which is \#P-hard.
% In this case, supposing $\bbeta \ge 0$,
% then the global inconsistency $\aar{\dg M}
% $ can only decrease in time
% It is worth noting that LIR does not always converge.

% \section{Local Inconsistency Resolution in Machine Learning}
\section{Learning, Inference, and the Pursuit of Consistency}

In \cref{chap:one-true-loss}, we found that standard loss fuctions can be viewed as the inconsistency of a PDG.
This observation is suggestive, but doesn't say anything about what should be do once you calculate the degree of inconsistency. 
% The primary use of a loss function is to train statistical models using data. 
The primary motivation for the LIR algorithm has been to operationalize
    learning algorithms---which are the typical use case for loss functions in machine learning.
Unsurprisingly, LIR does indeed operationalize standard learning algorithms. 
What may be more surprising is that other tasks in AI and machine learning---such as inference and forming adversarial examples---are also captured by local inconsistency resolution. 

\subsection{LIR in the Classification Setting}
Consider a parametric classifier $p_\theta(Y|X)$, perhaps
arising from a neural network whose final layer is a softmax.
Suppose $\V Y$ is a finite set of classes.
If $\V\!X$ is itself a manifold (such as the space of images), we can regard a value $x \in \V\!X$ as parameterizing a deterministic cpd, written
\tikz[center base]{\node[dpadinline](X) {$X$}; \draw[arr1, <<-](X.-165) to node[above,inner sep=1pt,pos=0.7]{$x$} +(-0.6,0);}.
% Viewing $\Theta$ as a variable, the discriminator
% as a conditional probability $p(Y| X, \Theta)$.
Together with a labeled sample $(x,y)$,
%  and parameter setting $\theta$,
    we get a PPDG
%
% \[ %%%>>  EXPLICIT PARAMETERIZATION; THETA IS A VARIABLE  <<%%
%     \dg M := ~~
%     \begin{tikzpicture}[center base]
%         \node[dpad1] (X) at (0,0) {$X$};
%         \node[dpad1] (Y) at (2,0) {$Y$};
%         \node[dpad1] (T) at (1, 0.7){$\Theta$};
%         \cmergearr[arr1]XTY{1.2,0}
%         \draw[arr1, <<-] (T) to node[above,pos=0.65]{$\theta$} +(-1,0);
%         \draw[arr1, <<-] (X) to node[above,pos=0.65]{$x$} +(-1,0);
%         \draw[arr1, <-] (Y) to node[above,pos=0.65]{$y$} +(1,0);
%         \node[anchor=south,inner sep=2pt] at (0.8,0) {$p$};
%     \end{tikzpicture},
% \]
$
    \dg M(\theta) :=
    \begin{tikzpicture}[center base,scale=1.3]
        % \node[dpad1] (X) at (0,0) {$X$};
        % \node[dpad1] (Y) at (1.5,0) {$Y$};
        % \draw[arr1, <<-] (X) to node[above,pos=0.65]{$x$} +(-1,0);
        % \draw[arr1, <-] (Y) to node[above,pos=0.65]{$y$} +(1,0);
        \node[dpadinline] (X) at (0,0) {$X$};
        \node[dpadinline] (Y) at (1.1,0) {$Y$};
        \draw[arr1, <<-] (X) to node[above,pos=0.65]{$x$} +(-0.85,0);
        \draw[arr1, <-] (Y) to node[above,pos=0.65,inner sep=2pt]{$y$} +(0.75,0);
        \draw[arr1] (X) to
            node[above] {$p_\theta$} (Y);
    \end{tikzpicture}
$
whose observational inconsistency is
$
    \aar{\dg M}_0 = - \log p_\theta(y|x)
$, the standard training objective for such a classifier \citep{one-true-loss}.
% This inconsistency is irreducible, since removing any of
% If $X$ and $\Theta$ are expressive enough, any of the four edges will make the inconsistency disappear
% All four components play a major role in giving rise to this inconsistency.
Each cpd plays major role in this inconsistency.
% Fixing all of $\dg M$ as our context,
% Fixing uniform global attention $\varphi = 1$,

What happens when we resolve this inconsistency
by modifying the parameters associated to different arcs?

\begin{itemize}
    \item Adjusting $\theta$
    % (i.e, $C = \{\theta\}$)
    amounts to training the network in the standard way---that is, changing the parameters little by little to minimize cross entropy.
        % In more detail, $\chi$
        % In this case, the value $\chi$ of the control mask corresponds to the number of optimization iterations.
    In this case, the value $\chi$ of the control mask corresponds roughly
    to the product of the learning rate and the number of optimization iterations.

    \item Adjusting $y$
    % (i.e., $C = \{y\}$)
    is like a  forward pass, in that it changes the label $y$ to match the distribution $p_\theta(Y|x)$.
    that the model predicts over labels, given the input $x$. 
        % In particular, $\chi=\infty$ corresponds to selection $y(
        % If there is also a qualitative edge that states that $Y$ is deterministic

    \item Adjusting $x$
    % (i.e. $C = \{x\}$ )
    creates an adversarial example.
    That is, it makes incremental changes to the input $x$
        until the (fixed) network assigns it label $y$.
    % amounts to constructing an adversarial example---that is, fixing the network
    % If $X$ represents images, for example, this means fixing the network and making small adjustments to the image until the network assigns it the desired label.
\end{itemize}

% \textbf{classification}
% Furthermore, these .
% with memory $\dg M(\Theta) = p$ and context
%  $y_i$ for every possible
% \textbf{adversarial attack in practice.}
% Take mutable memory to also include a special arc $\ed {\mathit{adex}}{}X$,
% and that is the focus
% \textbf{A more realistic classification setting.}
\textbf{Stochastic Gradient Descent (SGD).} \label{sec:SGD}
Take the mutable state to be the classifier $p$ as before.
Define $\textsc{Refresh}$ so that it draws a batch of samples $\{(x_i,y_i)\}_{i=1}^m$,
and returns a PDG with a single arc describing their emperical distribution $d(X,Y)$;
% Define $\textsc{Refocus}$ so that it sets $\varphi = \{d \mapsto \infty, p \mapsto 1\}$
%     (corresponding to high confidence in the data, default confidence in the still-training classifier)
let $\textsc{Refocus}$ be such that $\varphi(d) = \infty$
    (reflecting high confidence in the data).
If $\eta := \chi(p) \varphi(p)$ is small, then
    LIR is SGD with batch size $m$ and learning rate $\eta$.


% \textbf{Adversarial training views.}
% \textbf{SGD: batches as context.}
\begin{figure}
    \centering
    % \tikzset{atkv/.style={red!50!white},defv/.style={green!50!gray}}
    \tikzset{atkv/.style={green!70!black},defv/.style={blue}}
    \phantom{a}\hfill
    \begin{tikzpicture}[center base,scale=1.6]
        % \node[dpad1] (X) at (0,1) {$X$};
        % \node[dpad1] (X') at (0,0) {$X'$};
        \node[dpadded] (X) at (0,0) {$X$};
        \node[dpadded] (Y) at (1.3,0) {$Y$};

        \draw[arr1,-,shorten >=3pt,transform canvas={yshift=-1pt}, defv,dashed,ultra thick] (X) to (Y);
        \draw[arr1,-,shorten >=3pt,transform canvas={yshift=0.75pt}, atkv] (X) to (Y);
        \draw[arr1] (X) to node[above] {$p$} (Y);
        % \draw[arr1] (X) to node[right] {$\mathcal N$} (X');
        % \node[dpad1] (T) at (1, 0.7){$\Theta$};
        % \cmergearr[arr1]XTY{1.2,0}
        % \draw[arr1, <<-] (T) to node[above,pos=0.65]{$\theta$} +(-1,0);
        \coordinate (xend) at ($(X)+(-1.1,0.5)$);
        \draw[arr1,-,shorten <=5pt,transform canvas={yshift=0.75pt}, atkv] (X) to (xend);
        \draw[arr1, <-] (X) to
            %  node[left,pos=0.85]{$\mathcal N(x,1)$}
             node[above,pos=0.65]{$x$}
             (xend);
        \coordinate (x'end) at ($(X)+(-1.1,-0.5)$);
        \draw[arr1,-,ultra thick,shorten <=4pt,transform canvas={yshift= 1pt}, atkv, dashed] (X) to (x'end);
        \draw[arr1,-,shorten <=3pt,transform canvas={yshift= -0.75pt}, defv] (X) to +(x'end);
        \draw[arr1, <<-] (X) to node[above,pos=0.65]{$x'$} (x'end);
        \coordinate (yend) at ($(Y)+(1,0.5)$);
        \coordinate (y'end) at ($(Y)+(1,-0.5)$);
        \draw[arr1,-,shorten <=6pt,transform canvas={yshift=-0.75pt}, defv] (Y) to (yend);
        \draw[arr1, <<-] (Y) to node[above,pos=0.65]{$y$} (yend);
        \draw[arr1,-,shorten <=6pt,transform canvas={yshift=0.75pt}, atkv] (Y) to (y'end);
        \draw[arr1, <<-] (Y) to node[above,pos=0.65]{$y'$} (y'end);
    \end{tikzpicture}
    % $\cong$
    \hfill
    \begin{tikzpicture}[center base,scale=1.6]
        \def\outspray{.24}
        \def\inspray{.18}
        \def\yspray{.5}
        \node[dpadded] (X) at (0,-0.6) {$X$};
        \node[dpadded] (Y) at (1.4,0) {$Y$};
        \node[dpadded] (T) at (0, 0.6){$\Theta_p$};
        % \draw[arr1,-,shorten >=3pt,transform canvas={yshift=-1pt}, defv,dashed,ultra thick] (X) to (Y);
        % \draw[arr1,-,shorten >=3pt,transform canvas={yshift=0.75pt}, atkv] (X) to (Y);
        % \draw[arr1] (X) to node[above] {$p$} (Y);
        \mergearr[arr1]XTY
        \node[above right=0pt and 0pt of center-XTY]{$p$};
        \coordinate (tend) at ($(T)+(-1.1,\outspray)$);
        \draw[arr1,-,ultra thick,shorten <=7pt,transform canvas={yshift= -1.0pt}, defv, dashed] (T.160) to (tend);
        \draw[arr1,-,shorten <=3pt,transform canvas={yshift= 0.7pt}, atkv] (T.160) to (tend);
        \draw[arr1, <<-] (T.160) to node[left,pos=0.9]{$\theta$} (tend);
        \coordinate (nend) at ($(T)+(-1.1,-\inspray)$);
        % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=-0.7pt}, defv] (T.-160) to (nend);
        \draw[arr1, <-, defv] (T.-160) to
            node[below,inner sep=2px,pos=0.85]{\scalebox{0.8}{$\mathcal N(0,1)$}} (nend);
        \coordinate (xend) at ($(X)+(-1.1,\inspray)$);
        % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=0.7pt}, atkv] (X.160) to (xend);
        \draw[arr1, <-, atkv] (X.160) to
            % node[above,pos=0.65]{$x$} (xend);
            node[above,inner sep=1px,pos=0.85]{\scalebox{0.8}{$\mathcal N(x,1)$}} (xend);
        \coordinate (x'end) at ($(X)+(-1.1,-\outspray)$);
        \draw[arr1,-,ultra thick,shorten <=6pt,transform canvas={yshift=1.0pt}, atkv, dashed] (X.-160) to (x'end);
        \draw[arr1,-,shorten <=3pt,transform canvas={yshift= -0.7pt}, defv] (X.-160) to (x'end);
        \draw[arr1, <<-] (X.-160) to node[left,pos=0.9]{$x'$} (x'end);
        \coordinate (yend) at ($(Y)+(1,\yspray)$);
        \coordinate (y'end) at ($(Y)+(1,-\yspray)$);
        % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=-0.7pt}, defv] (Y.35) to (yend);
        \draw[arr1, <<-,defv] (Y.35) to node[above,pos=0.65]{$y$} (yend);
        % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=0.7pt}, atkv] (Y.-35) to (y'end);
        \draw[arr1, <<-,atkv] (Y.-35) to node[below,pos=0.65]{$y'$} (y'end);
    \end{tikzpicture}
    \hfill
    \phantom{a}
    %
    \caption[Two Illustrations for adversarial training: with a PPDG and a PDG]
        {Two illustrations of adversarial training.
         Left: the PPDG obtained by
            including a perturbed input $x'$ and target $y'$ to the
            classification setting.
        Right: the PDG obtained by making the parameters for $p$ explicit,
            together with a Gaussian prior $\Theta_p \sim \mathcal N(0,1)$ over them.
        Both are colored with two foci: the blue focus trains the network,
            and the green one creates adversarial examples. Dashes indicate control.
        }
    \label{fig:adv}
\end{figure}
\textbf{Adversarial training.}
% Now, include in the mutable state
% Let's now extend our mutible state to include
% Now consider an extended PDG
Suppose we want to slightly alter $x$ to obtain $x'$ that is classified as $y'$ instead of $y$.
% Adding $x'$ and $y'$ to $\dg M$ and relaxing $\p_x$ to be a Gaussian centered $x$ rather than a point mass,
By adding arcs corresponding to $x'$ and $y'$ to $\dg M$,
    and relaxing the cpd $\p_x$ associated with $x$ to be a Gaussian centered $x$ rather than a point mass,
    we get the PPDG on the left of \cref{fig:adv}.
% \[
% \begin{tikzpicture}[center base]
%     % \node[dpad1] (X) at (0,1) {$X$};
%     % \node[dpad1] (X') at (0,0) {$X'$};
%     \node[dpad1] (X) at (0,0) {$X$};
%     \node[dpad1] (Y) at (2,0) {$Y$};
%
%     \draw[arr1,-,shorten >=3pt,transform canvas={yshift=-1pt}, blue,dashed,ultra thick] (X) to (Y);
%     \draw[arr1,-,shorten >=3pt,transform canvas={yshift=0.75pt}, red] (X) to (Y);
%     \draw[arr1] (X) to node[above] {$p$} (Y);
%     % \draw[arr1] (X) to node[right] {$\mathcal N$} (X');
%     % \node[dpad1] (T) at (1, 0.7){$\Theta$};
%     % \cmergearr[arr1]XTY{1.2,0}
%     % \draw[arr1, <<-] (T) to node[above,pos=0.65]{$\theta$} +(-1,0);
%     \coordinate (xend) at ($(X)+(-1.1,0.5)$);
%     \draw[arr1,-,shorten <=2pt,transform canvas={yshift=0.75pt}, red] (X) to (xend);
%     \draw[arr1, <-] (X) to
%         %  node[left,pos=0.85]{$\mathcal N(x,1)$}
%          node[above,pos=0.65]{$x$}
%          (xend);
%     \coordinate (x'end) at ($(X)+(-1.1,-0.5)$);
%     \draw[arr1,-,ultra thick,shorten <=4pt,transform canvas={yshift= 1pt}, red, dashed] (X) to (x'end);
%     \draw[arr1,-,shorten <=4pt,transform canvas={yshift= -0.75pt}, blue] (X) to +(x'end);
%     \draw[arr1, <<-] (X) to node[above,pos=0.65]{$x'$} (x'end);
%     \coordinate (yend) at ($(Y)+(1,0.5)$);
%     \coordinate (y'end) at ($(Y)+(1,-0.5)$);
%     \draw[arr1,-,shorten <=2pt,transform canvas={yshift=-0.75pt}, blue] (Y) to (yend);
%     \draw[arr1, <<-] (Y) to node[above,pos=0.65]{$y$} (yend);
%     \draw[arr1,-,shorten <=2pt,transform canvas={yshift=0.75pt}, red] (Y) to (y'end);
%     \draw[arr1, <<-] (Y) to node[above,pos=0.65]{$y'$} (y'end);
% \end{tikzpicture}
% \]
% Taking a view of
An iteration of LIR
whose focus is the edges marked in green (with control over the dashed green edge)
is then an adversarial attack with Euclidean distance \citep{biggio2013advattk}.
% Taking the blue focus, on the other hand,
The blue focus, by contrast, ``patches'' the adversarial example by
    adjusting the model parameters to again classify it correctly.
Thus, LIR
 % that alternates between the two and
% with $\mat F = \{$green, blue$\}$
that alternates between the two foci,
in which $\textsc{Refresh}$ selects a fresh $(x,y,x'=x)$ from the dataset
    and target label $y'$,
% refreshes $(x,x',y,y')$
% amounts to
is
adversarial training, a standard defense against adversarial attacks \citep{goodfellow2014explaining}.
%

The ML community's focus on adversarial examples
    may appear to be a cultural phenomenon,
    % but in some sense it is .
    % but at a mathematical level, it is no accident.
    % but at a mathematical level,
    but mathematically,
     it is no accident.
At this level of abstraction, there is no difference between
    model parameters and inputs.
% We can highlight this visually by observing that, after adding L2 regularization \citep{one-true-loss}, translating the previous diagram to an ordinary PDG gives us the symmetrical:
% Making the parameterization of $p$ explicit
Indeed, if we make the parameterization of $p$ explicit
and add L2 regularization
% (i.e., a Gaussian prior $\Theta \sim \mathcal N(0,1)$),
% (in the form of a Gaussian prior $\Theta \sim \mathcal N(0,1)$),
(i.e., a Gaussian prior over $\Theta_p$),
% By making the parameterization of $p$ explicit
the symmetry
% between parameters and inputs
becomes striking (\cref{fig:adv}, right).
This may help explain why,
    even outside of adversarial contexts,
    % even in contexts,
% it is just as sensible to train the inputs, as the network \citep{FNNS}.
it can be just as sensible to train an input, as a model \citep{FNNS}.
% (S, right.)
% \[
%     \begin{tikzpicture}[center base,atkv/.style={red!50!white},defv/.style={green!50!gray}]
%         \def\spray{.24}
%         \node[dpad1] (X) at (0,0) {$X$};
%         \node[dpad1] (Y) at (2,0.5) {$Y$};
%         \node[dpad1] (T) at (0, 1){$\Theta$};
%         % \draw[arr1,-,shorten >=3pt,transform canvas={yshift=-1pt}, defv,dashed,ultra thick] (X) to (Y);
%         % \draw[arr1,-,shorten >=3pt,transform canvas={yshift=0.75pt}, atkv] (X) to (Y);
%         % \draw[arr1] (X) to node[above] {$p$} (Y);
%         \mergearr[arr1]XTY
%         \coordinate (tend) at ($(T)+(-1.1,\spray)$);
%         \draw[arr1,-,ultra thick,shorten <=3pt,transform canvas={yshift= -1.5pt}, defv, dashed] (T.160) to (tend);
%         \draw[arr1,-,shorten <=3pt,transform canvas={yshift= 1.0pt}, atkv] (T.160) to (tend);
%         \draw[arr1, <<-] (T.160) to node[above,pos=0.65]{$\theta$} (tend);
%         \coordinate (nend) at ($(T)+(-1.1,-\spray)$);
%         % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=-0.7pt}, defv] (T.-160) to (nend);
%         \draw[arr1, <-, defv] (T.-160) to node[left,pos=0.95]{$\mathcal N(0,1)$} (nend);
%         \coordinate (xend) at ($(X)+(-1.1,\spray)$);
%         % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=0.7pt}, atkv] (X.160) to (xend);
%         \draw[arr1, <-, atkv] (X.160) to
%             % node[above,pos=0.65]{$x$} (xend);
%             node[left,pos=0.95]{$\mathcal N(x,1)$} (xend);
%         \coordinate (x'end) at ($(X)+(-1.1,-\spray)$);
%         \draw[arr1,-,ultra thick,shorten <=3pt,transform canvas={yshift=2pt}, atkv, dashed] (X.-160) to (x'end);
%         \draw[arr1,-,shorten <=3pt,transform canvas={yshift= -1.0pt}, defv] (X.-160) to (x'end);
%         \draw[arr1, <<-] (X.-160) to node[below,pos=0.65]{$x'$} (x'end);
%         \coordinate (yend) at ($(Y)+(1,\spray)$);
%         \coordinate (y'end) at ($(Y)+(1,-\spray)$);
%         % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=-0.7pt}, defv] (Y.35) to (yend);
%         \draw[arr1, <<-,defv] (Y.35) to node[above,pos=0.65]{$y$} (yend);
%         % \draw[arr1,-,shorten <=3pt,transform canvas={yshift=0.7pt}, atkv] (Y.-35) to (y'end);
%         \draw[arr1, <<-,atkv] (Y.-35) to node[below,pos=0.65]{$y'$} (y'end);
%     \end{tikzpicture}
% \]

% \section{The EM Algorithm (and VI) as LIR}
% \section{The EM Algorithm and Variational Inference as LIR}
\subsection{The EM Algorithm as LIR}
Suppose we have a generative model $p(Z,X | \Theta)$
describing the probability over an observable variable $X$ and a latent one $Z$.
% Let $x$ be an observation of $X$, as before.
%
Given an observation $X{=}x$,
the standard approach for trying to learn the parameters despite
the missing data is called the EM algorithm. It iteratively computes
\[
    \theta^{(t+1)}_{\text{EM}}
            = \arg\max_{\theta} \Ex\nolimits_{z\sim p(Z|x,\theta_{\text{EM}}^{(t)})}[ \,\log p(x, z | \theta) \,].
\]
% \[
%     \dg M := ~~
%     \begin{tikzpicture}[center base]
%         \node[dpad1] (Z) {$Z$};
%         \node[dpad1,above=.3 of Z] (X) {$X$};
%         \coordinate (A) at ($ (X)!.5!(Z) + (-0.7,0)$);
%         \node[dpad1,left=.5 of A] (T) {$\Theta$};
%         % \coordinate (A) at ($ (X)!.5!(Z) + (-1.1,0)$);
%         \draw[arr1] (T) -- node[above]{$p$} (A) -- (X);
%         \draw[arr1] (T) -- (A) -- (Z);
% %
%         \draw[arr2, <<-] (X) --  node[above,pos=0.8]{$ x$} ++(0.9, 0);

%         \begin{scope}[red]
%             \draw[arr2, <<-] (T) --  node[above,pos=0.8]{$\theta$} ++(-0.9, 0);
%             \draw[arr2, <-] (Z) --  node[above,pos=0.8]{$q$} ++(0.9, 0);
%         \end{scope}
% % 			\draw[arr2, <-] (Z) -- node[above,pos=0.6]{$ q^{\{\beta =\infty\}}$} ++(-0.9, 0);%
%         % \ar[r,"p"] \& Z \ar[r,"p", bend left] \& X \ar[l,"q", bend left] \& \ar[l, two heads, "x"']
%     \end{tikzpicture}
% \]
% \vspace{-2.5ex}

\begin{prop}
    % If there are no local minima,
    % If $\theta \mapsto p_\theta(x,Z)$ is log-convex,
    %  then
    % Under mild assumptions,
    % alternating between focus $\theta$ and focus $q$ with $\chi=\infty$
    % LIR on the PDG
    % \[
    LIR$\Bigg(\!
        \begin{tikzpicture}[center base,scale=1.3]
			\node[dpad0] (X) {$X$};
			\draw[arr2, <<-] (X) --  node[above,pos=0.8]{$ x$} +(-0.9, 0);
		\end{tikzpicture}
        ~,~
        \begin{tikzpicture}[center base,scale=1.3]
			\node[dpad0] (Z) {$Z$};
			\node[dpad0,left=.5 of Z] (X) {$X$};
			\coordinate (A) at ($ (X)!.5!(Z) + (0,0.8)$);
			\draw[arr1] (A) -- node[left, inner sep=3pt]{$p$} ++(0,-0.35) -- (Z);
			\draw[arr1] (A) -- ++(0,-0.35) -- (X);
%
			% \draw[arr2, <<-] (X) --  node[above,pos=0.8]{$ x$} ++(0.9, 0);
			\draw[arr2, <-] (Z) --
                node[above,pos=0.65, inner sep=2pt]{$q$}
                node[below,pos=0.7, inner sep=2pt]{${\color{gray}\scriptscriptstyle(\infty)}$}
                ++(0.9, 0);%
			%\scriptstyle q^{\{\beta =\infty\}}
			% \ar[r,"p"] \& Z \ar[r,"p", bend left] \& X \ar[l,"q", bend left] \& \ar[l, two heads, "x"']
		\end{tikzpicture}
    \Bigg)$,
        % \]
    % in which \textsc{Refocus} fixes $\varphi, \gamma = \mat 1$
    in which \textsc{Refocus} fixes $\varphi = \mat 1$
    and alternates between
    % $\chi = \infty \mathbbm1_{\{\theta\}}$ and $\mathbbm1_{\{q\}}$
    full control of $p$ and $q$,
    implements EM, in that
    $\theta_{\text{EM}}^{(t)} = \theta_{\text{LIR}}^{(2t)}$
    % for all $t \in \mathbb N$
    .
    % $ \theta^{(t+1)}
\end{prop}

% At a technical level, this is essentially a restatement of a theorem
% This result was first observed by
% A result like this was first observed by
This result is closely related to one due to
\citet{neal1998view},
who view it as an intuitive explanation of why the EM
algorithm works.  Indeed, it is obvious in this form that
every adjustment reduces the overall inconsistency.
The result can also be readily adapted to an entire dataset by replacing $x$ with a high confidence empirical distribution, or batched with the same technique in \cref{sec:SGD}.
It also captures \emph{fractional EM} when $\chi < \infty$.
 % (\citeyear{neal1998view})
% By monotonicity, it is also clear that

This form of the EM algorithm is closely related to variational inference.
Indeed, analogous choices applied to the
    % analysis of \citet{one-true-loss}
    other PDGs analyzed in \cref{sec:variational}
% to obtain a concrete training algorithm for variational autoencoders.
yields the usual training algorithm for variational autoencoders (VAEs).

\subsection{Generative Adversarial (Actor-Critic) Training as LIR}

\def\pdata{p_{\mathrm{data}}}
\def\real{{\mathrm{real}}}
\def\fake{{\mathrm{fake}}}
LIR also subsumes more complex training procedures such as the one used to train
\emph{generative adversarial networks} (GANs) \citep{goodfellow2020generative}.
% The general idea is that
The goal is to train a network $G$ to generate images that cannot be distinguished from real ones.
    % , by simultaneously training discriminator network $D$ that attempts to do so. 
%
% In a GAN, we have a generator $G$ that represents a distribution over fake images,
% an empirical distribution of real images, and a discriminator that
% aims to classify images as real or fake.
% The training process involves a second network,
More precisely, 
\oldnewmaterial{%
represent the generator network with a distribution $G(X_\fake)$ over fake images,
and suppose we have training data in the form of an empirical distribution $\pdata(X_\real)$ over real images. 
}%
Define $X$ to be either an image $X_{\fake}\sim G$ or from a dataset
    $X_\real \sim \pdata$, based on the outcome a fair coin flip $C$.
The key component in the architecture is a second network,
called the \emph{discriminator}, 
    which aims to classify an image as either real or fake---or, in other words, 
    predicts the coin flip $C$ from the $X$. 
% A discriminator $D$ then predicts $C$ from $X$.
Thus, we model it as a conditional distribution $D(C \mid X)$. 
%
% Consider the PDG
% Finally, the generator believes that $C$ is not determined by $X$,
%     which can be captured with a purely qualitative edge with $\alpha=-1$.
% These components are summarized by the PDG
% The generator also has a belief that, even given $X$, the coin is equally likely
Finally, the generator aims to fool the discriminator, so that it can do no better than chance. Thus, the generator would like to be consistent with a distribution $e(C|X)$ indicating that the coin is equally likely to be heads as tails even knowing the value of the image $X$. 
    %  even given $X$, the coin is equally likely
    % heads as tails (call this $e$).
This state of affairs is summarized as the PPDG 
% , where ``!'' means $\beta=\infty$, i.e, full confidence.
%
% \vspace{-1ex}
\[
    % \dg M:=
    \dg M(\Theta) := \quad
    \begin{tikzpicture}[center base,Dcolor/.style={green!70!black},Gcolor/.style={blue},scale=1.4]
        % \node[dpad1](Z) at (0, 0.6){$Z$};
        \node[dpad1](Xfake) at (0,1) {$X_{\fake}$};
        \node[dpad1](Xreal) at (0,0){$X_{\real}$};
        \node[dpad1](X) at (2.1,0){$X$};
        % \node[dpad1](C) at (2,1){Real / Fake};
        % \node[dpad1](C) at (2.5,1.1){$\mathrm{Real / Fake}$};
        \node[dpad1](C) at (2.4,1.0){$C$};

        % \draw[arr, <-] (Z) to node[above,pos=0.6]{$\cal N!$} +(-1.2,0);
        %%%%% G %%%%%%
        \draw[arr1,-,line width=1.3pt,transform canvas={yshift=1pt},dashed,Gcolor, shorten <=5pt] (Xfake) to +(-1.5,0);
        \draw[arr1, <-] (Xfake) to node[above,pos=0.6]{$G$}
            % node[below,pos=0.7,inner sep=1.5pt]{\color{gray}\scriptsize$(\beta{=}1)$}
            +(-1.5,0);
        \draw[arr1, <-] (Xreal) to 
        node[above,pos=0.6]{$\pdata$}
        node[below,pos=0.7, inner sep=2pt]{${\color{gray}\scriptscriptstyle(\infty)}$}
            +(-2,0);
        % \draw[arr] (X.0) to[out=0,in=-80,looseness=1.6] node[right]{$D$} (C.-35);
        % \draw[arr] (X.10) to[out=10,in=-90,looseness=1.4] node[left]{$\frac12$} (C.-45);
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%% D %%%%%%%%%%%%%%%%%%
        \draw[arr,-,line width=1.3pt,transform canvas={xshift=1pt}, Dcolor,dashed]
                   (X.-2) to[out=0,in=-20,looseness=2.21] (C.-28);
        \draw[arr] (X.0) to[out=0,in=-20,looseness=2.2] node[right]{$D$} (C.-30);
        % \draw[arr] (X.10) to[out=10,in=-45,looseness=1.6] node[left]{$\frac12$} (C.-45);
        %%%%%%%%%%%%%%%%%%%%%%%%%%% e %%%%%%%%%%%%%%%%%%%%
        \draw[arr1,-,line width=1.1pt,transform canvas={xshift=0.5pt}, Gcolor,shorten >=3pt]
            (X.10) to[out=10,in=-55,looseness=1.6] (C.-50);
        \draw[arr1,-,line width=1.1pt,transform canvas={xshift=-0.8pt}, Dcolor,shorten >=3pt,shorten <=1pt]
            (X.10) to[out=10,in=-55,looseness=1.6] (C.-50);
        \draw[arr1]
            % (X)
            (X.10)
            to[out=10,in=-55,looseness=1.6]
            % node{\scalebox{0.8}{\contour{white}{\small$50/50$}}}
            node[left,pos=0.6,inner sep=4pt]{$e$}
            (C.-50);
        % \draw[arr] (X) to[] node[]{\small50/50} (C);
        % % \cmergearr[->>]{Xfake}{Xreal}{X}{1.2,0.1}
        \coordinate (ctr) at (1.2,0.1);
        \draw[arr1,-,shorten >=0pt] (Xfake) to[bend right=8] (ctr);
        \draw[arr1,-,shorten >=0pt] (Xreal) to[bend left=5] (ctr);
        \draw[arr1,-,shorten >=0pt] (C) to[in=150,out=180,looseness=1.5](ctr);
        \draw[arr1,->>,shorten <=0pt] (ctr) to[bend right=10](X);
        \draw[arr1, <-] (C) to 
            node[above,pos=0.7]{$50/50$} 
            node[below,pos=0.7, inner sep=2pt]
                {${\color{gray}\scriptscriptstyle(\infty)}$}
            +(1.5,0);
\end{tikzpicture}
\]
in which ``50/50'' is the label of the cpd describing the fair coin flip, and $\Theta$ consists of joint settings of the parameters of the two networks ($G$ and $D$). 

Traditionally, the GAN training objective is written as a 2-player minimax game
$
    \min_{G} \min_{D}  \mathcal L^{\text{GAN}}(G,D)
$, where
\[
\mathcal L^{\text{GAN}}(G,D) := 
    % \Ex_{\vphantom{|}x \mathrlap{\sim \pdata}}
    \Ex_{\vphantom{|}x \sim \pdata}
    \;[\, \log D(x) \, ] + \Ex_{\vphantom{|} x' \sim G} \, [\,\log (1- D(x')) \,],
\]
and $D(x)$ stands for the conditional probability $D(C{=}1 \mid X={x})$. 
The standard theoretical justification of this loss function 
is based on the following observations \citep{goodfellow2020generative}:
\begin{enumerate}[nosep]
    \item For fixed $G$, the optimal discriminator predicts $D^*(x) = \frac{G(X)}{G(x) + \pdata(x)}$;
    \item For the optimal generator, $\mathcal L^{\text{GAN}}$ becomes equivalent to the Jesnen-Shanon divergence \citep{MENENDEZ1997307} between $\pdata$ and $G$.
    \item There is a unique global Nash Equilibrium, in which $G(X) = \pdata(X)$ and $D(C | X) = e(C|X)$. 
\end{enumerate}
% Note that this equilibrium coincides with 
Note that this equilibrium is the one place where the PDG above has zero inconsistency. 
We now turn to describe the modeling process in terms of LIR, which involves alternating between two focuses: one for the generator (shown above in blue, plus the black context), and one for the discriminator (shown above in green, plus the black context). 

% Now consider the two perspectives: the generator's perspective $G$, with
% \textbf{The Discriminator's View.}
% % According to the discriminator,
% \textbf{The Generator's View.}
% The generator controls t

\textbf{The Discriminator's Focus.}
While the focus is on discrimination, 
    we have control over $D$, and attend to
    everything but $e$.
That inconsistency of this PDG is what might be called
    the discriminator's objective:
    the expected KL divergence from $D$ to the optimal discriminator.
Unfortunately, this may not be easy to calculate directly. 
If $D$ also disbelieves that any image is equally likely to be fake as real
    (by choosing $\varphi(e) = -1$),
% (i.e., $\begin{matrix}\varphi(e) = -1\\ \varphi(D) =+1\end{matrix}$),
then the inconsistency becomes $-\mathcal L^{\text{GAN}}$, which is easier to calculate.
% $D(C|X)$ to the
% true probability of $C$ given $X$.
% while the generator has control over $G$.

\textbf{The Generator's Focus.}
When the focusis on generation, we have control over $G$.
If we ignore $D$ attends only to $e$, then inconsistency
    is the Jensen-Shannon Divergence between $G$ and $\pdata$,
    coinciding with a standard argument for GANs. 
However, this is difficult to calculate directly.
If the generator also disbelieves the discriminator $D$
    (i.e., $ \varphi(D) =-1$),
    then the inconsistency becomes $+\mathcal L^{\text{GAN}}$,
    which, again, is easier to calculate.

Standard practice is to use small $\chi(G)$ and large $\chi(D)$,
so that the discriminator is well-adapted to the generator,
a strategy which is provably more stable
    \citep{two-scale-GAN-Heusel2017}.
Intuitively, we should the discriminator should be more maneuverable than the generator; the training signal for $G$ comes entirely from $D$, so it is not helpful for $G$ to updates faster than $D$.
In our language, this means selecting a focus in which the discriminator's focus entails a higher degree of control over $D$ than the generator's focus has over $G$.
% It is common to select small $\chi(D)$ and large $\chi(G)$
% Intuitively, because learning to generate is harder than learning to discriminate, 

\section{Message Passing Algorithms as LIR}

% Traditional graphical models,
At a quantitative level, traditional graphical models (i.e., the subject of \citet{KF09}) can all be represented as factor graphs.
% Nearly every standard graphical model can be viewed
    % as a factor graph, 
Moreover, they all admit the same
    (approximate) inference procedure, known
    variously as (loopy) belief propagation \citep{KF09},
    the generalized distributive law \citep{aji2000gendistriblaw},
    and the sum-product algorithm \citep{kschischang2001factor}.
This procedure turns out to be 
    a special case of LIR in which all PDGs involved represent factor graphs
    (i.e., have $\balpha = \bbeta, \gamma=1$, as shown in \cref{sec:factor-graphs}).
% which correspond to PDGs that have equal weighting in .

% \paragraph{Belief Propogation.}
% % Let $\X$ be a set of variables,
% Consider a factor 
Recall (from \cref{sec:prelim-fgs}) that a
\emph{factor graph} over a set of variables $\X$ is a set of factors
% Let $\X$ be a set of variables, and consider a set of factors
$\Phi = \{ \phi_a : \mathbf X_a \to \mathbb R_{\ge 0}\}_{a \in \Ar}$,
where each $\mathbf X_a \subseteq \mathcal X$ is called the \emph{scope} of $a$.
Conversely, for $X \in \X$, let
$\partial X
 := \{a \in \Ar : X \in \mat X_a\}
$ be the set of factors whose scopes contain the variable $X$.
As we have seen, $\Phi$ specifies a distribution
$\Pr_\Phi(\X) \propto \prod_a \phi_a(\mat X_a)$, and
% $\Pr_\Phi = \frac1Z \prod_a \phi_a(\mat X_a)$, where $Z$ is a normalization constant,and $\log Z$ is  and
corresponds to a PDG
%
% The PDG that corresponds to
% this factor graph is $\Phi$ is
\[
% $
    \dg M_\Phi = \Big\{ \begin{tikzpicture}[center base,scale=1.3]
        \node[dpadinline] (X) {$\mathbf X_a$};
        \draw[arr,<-] (X) to node[above, pos=0.65,inner sep=3pt]{$\propto \smash{\phi_a}$}
            node[below,pos=0.7,inner sep=1.5pt]{\color{gray}\scriptsize$(\alpha,\beta{=}1)$} +(-1.4,0);
    \end{tikzpicture}~\Big\}_{a \in \Ar}
% $
\]
% ~\parbox{0.45\linewidth}{\!\!that specifies the same\\\hspace{1em}distribution, when $\gamma=1$.}
that specifies the same joint distribution $\Pr_{\Phi}$, when
    observation and structure are weighted equally (i.e., $\gamma=1$).
% be the corresponding PDG.
% There are several different variants of belief propagation;
% each can be written as an instance of LIR.

% \paragraph{Sum Product Belief Propogation.}
% The state for standard sum-product belief
Sum-product belief propagation \citep{kschischang2001factor}
    aims to approximate marginals of $\Pr_{\Phi}$
    with only local computations: messages sent between factors and
        the variables they have in scope.
% passing messages between adjacent variables and factors.
 % so the state of the algorithm consists of a pair of messages
The state of the algorithm is a collection of ``messages''
% $m_{X \sto a}$ and $m_{a \sto X}$
$
\{ m_{X \sto a}, m_{a \sto X}\}_{X \in \X, a \in \partial X},
$, 
two for each pair $(a, X)$ of factor $a$ and variable $X$ in the scope of $X$.
Each message is an (unnormalized) distribution over the correspondig variable ($X$).
% each of which are (unnormalized) distributions over $X$,
% for each pair $(a, X)$ with $a \in \partial X$,
% for each pair $(X, a \in \partial X)$.
% $\nu_X(X)$ for each variable $X$, and a joint distribution $\nu_a(\mat X_a)$ for each
% factor $a \in \Ar$.
% $m_{i}
% Concretely, the messages passed are given by:
\def\Msg{\dg{M\mskip-1mus\mskip-2mu g}}
% Together, we can regard this collection of messages as a PDG, which we call $\Msg$.
% Just as with the original factor graph,
    % $\Msg$.
Together, the messages form a PDG which we will denote $\Msg$,
in the same way as
the original factor graph.
After initializing with uniform messages,
    belief propagation repeatedly recomputes:
%
\begin{align}
    m_{X \!\sto a}(x)
        % &:=
        &:\propto
        \prod_{{b \in \partial X\setminus a}} m_{b\sto\! X} (x)
        % \qquad &
        \label{eq:X->a}
        \\
    m_{a \sto\mskip-2mu X}(x)
        % &:=
        &:\propto
        ~~\sum_{\mathclap{\mat y \in \V(\mat X_a \setminus X)}}~~ \phi_a(x, \mat y)
        ~\prod_{\mathclap{Y \in \mat X_a \setminus X}}~
                m_{Y \!\sto a} (Y(\mat y)),
        \label{eq:a->X}
\end{align}
where $Y(\mat y)$ is the value of $Y$ in the joint setting $\mat y$.
Finally, variable marginals $\{b_X\}_{X \in \X}$,
which we regard as another PDG, $\dg B$, are computed from the messages according to
$
    b_X(x) \propto \prod_{a \in \partial X} m_{a \sto X}(x)
$.
Observe that every calculation is a (marginal of) a product of factors,
    and thus amounts to inference
        % on a local factor graph of the appropriate context.
        in some ``local'' factor graph.
% On the factor graph, each node aggregates
% The process can be visualized as propagation along the edges of the factor graph.
% The standard illustration of it is not so different from the PDG $\Msg$
The traditional depiction of messages
    moving between variables and factors,
    intended to illustrate equations (\ref{eq:X->a}) and (\ref{eq:a->X}),
    looks something like the following \citep{kschischang2001factor}:
    \begin{center}
        \begin{tikzpicture}
                % [xscale=1.8]
                [xscale=2,yscale=1.3]
            \node[lirfactor] (a) at (0,0) {$a$};
            \node[draw,circle,inner sep=4px] (X) at (1,0) {$X$};
            \node[lirfactor] (b1) at (2,0.5){$b_1$};
            \node[lirfactor] (bm) at (2,-0.5){$b_m$};
            \node[draw,circle,inner sep=2px] (Y1) at (-1,0.5){$Y_1$};
            \node[draw,circle,inner sep=2px] (Yn) at (-1,-0.5){$Y_n$};
            % \node[anchor=center] at (-1,0){$\scalebox{1}[1]{\vdots}$};
            % \node[anchor=center,draw] at (2,0){$\scalebox{1}[1]{\vdots}$};
            \draw (a) -- (X);
            \draw (b1) -- (X);
            \draw (bm) -- (X);
            \draw (Y1) -- (a);
            \draw (Yn) -- (a);
    
            % \draw[arr0,red,arrows={->[harpoon,swap]}]
            %     (1.6,0.7) to[in=23,out=-80] node[left, pos=0]{$m_{b_1 \!\sto \!X}$\!\!} (X);
            % \draw[arr0,red,arrows={->[harpoon,swap]},dashed]
            %     (0.5,0.7) to[in=0,out=-90] node[left, pos=0]{$m_{X \!\sto a}$\!} (a);
            % \draw[arr0,blue,arrows={->[harpoon,swap]}]
            %     (0.5,-0.7) to[in=180,out=90] node[right, pos=0]{\!$m_{a \sto \!X}$} (X);
            \begin{scope}[transform canvas={yshift=2px},green!70!black]
                \let\stogreen\sto
            \draw[arr,arrows={->[harpoon,swap]}]
                (b1) to node[above=-0.5pt,rotate=15]{$m_{b_1 \!\stogreen \mskip-2muX}$\!\!} (X);
            \draw[arr,arrows={->[harpoon,swap]}]
                (bm) to node[above=-0.5pt,rotate=-5,pos=0.3]{$m_{b_m \!\stogreen \mskip-2muX}$\!\!} (X);
            \draw[arr,arrows={->[harpoon,swap]},densely dotted]
                (X) to node[above]{$m_{X \!\stogreen a}$\!} (a);
            \end{scope}
    
            \begin{scope}[transform canvas={yshift=-2px},blue]
                \let\stoblue\sto
            \draw[arr,arrows={->[harpoon,swap]}]
            % \draw[blue,arrows={->[harpoon,swap]}]
                (Y1) to node[below=-0.5pt, rotate=-5,pos=0.35]{$m_{Y_{\!1} \!\stoblue a}$\!\!} (a);
            \draw[arr,arrows={->[harpoon,swap]}]
                (Yn) to node[below, rotate=17]{$m_{Y_{\!n} \!\stoblue a}$\!\!} (a);
            \draw[arr,arrows={->[harpoon,swap]},densely dotted]
                (a) to node[below]{\!$m_{a \stoblue \mskip-2muX}$} (X);
            \end{scope}
            % \draw (current bounding box.north west) rectangle (current bounding box.south east);
        \end{tikzpicture}~\raisebox{1ex}.
    \end{center}%
    
    % This is only a schematic,
    \newmaterial{%
    In this schematic representation, the dotted message $m_{X \!\sto a}(x)$, for instance, is calculated as a product of incoming messages to $X$ from other factors \eqref{eq:X->a}.
    While this diagram is only a schematic,
        the corresponding PDG $\Msg$, which represents
        the intermediate state of the algorithm,
    }%
    can be made to look similar to it.
    Adding a variable $X^{a}$ for every pair
        % $(a \in \Ar, X \in \mat X_a)$,
        $(X,a)$ with $X \in \mat X_a$
    along with edges asserting that $X^{a} = X$,
    we obtain an equivalent PDG, shown in \cref{fig:bp-lir},
        that does not look so different.
    \begin{figure}
        \centering
    \begin{tikzpicture}[center base,
            % [xscale=2,center base]
            xscale=2.4,yscale=1.5]
        % \node[lirfactor] (a) at (0,0) {$a$};
    \begin{scope}[os1/.style={outer sep=1pt},dpad0/.append style={fill=gray!50!black,text=white,font=\mathversion{bold}}]
        % \node[] (a) at (0,0) {$a$};
        % \node[dpad0,os1] (aX) at (0.22,0){\scalebox{0.7}{$X^{(a)}$}};
        \node[dpad0,os1] (aX) at (0.22,0){\scalebox{0.7}{$X^{a}$}};
        \node[dpadded,os1] (X) at (1,0) {$X$};
        % \node[lirfactor] (b1) at (2,0.5){$b_1$};
        % \node[lirfactor] (bm) at (2,-0.5){$b_m$};
        \node[dpad0,os1] (b1X) at (1.8,0.5){\scalebox{0.7}{$X^{b_1}$}};
        \node[dpad0,os1] (bmX) at (1.8,-0.5){\scalebox{0.7}{$X^{b_m}$}};
        % \node[draw,circle,inner sep=2px] (Y1) at (-1,0.5){$Y_1$};
        % \node[draw,circle,inner sep=2px] (Yn) at (-1,-0.5){$Y_n$};
        \node[dpadded,os1] (Y1) at (-1.1,0.5){$Y_1$};
        \node[dpadded,os1] (Yn) at (-1.1,-0.5){$Y_n$};
        % \node[dpad0,os1] (aY1) at (-0.2,0.3){\scalebox{0.7}{$Y_{1}^{(a)}$}};
        % \node[dpad0,os1] (aYn) at (-0.2,-0.3){\scalebox{0.7}{$Y_{n}^{(a)}$}};
        \node[dpad0,os1] (aY1) at (-0.2,0.3){\scalebox{0.7}{$Y_{1}^{a}$}};
        \node[dpad0,os1] (aYn) at (-0.2,-0.3){\scalebox{0.7}{$Y_{n}^{a}$}};
    \end{scope}
        % \node[anchor=center] at (-1,0){$\scalebox{1}[1]{\vdots}$};
        % \node[anchor=center,draw] at (2,0){$\scalebox{1}[1]{\vdots}$};
        % \draw (a) -- (X);
        % \draw[double equal sign distance,shorten <=0,shorten >=0] (aX.center) to (X.center);
        % \draw[double equal sign distance,shorten <=0,shorten >=0] (aX) to (X);
    \begin{scope}[every path/.append style={gray!50,double equal sign distance}]
        \draw (aX) to (X);
        \draw (aY1) to (Y1);
        \draw (aYn) to (Yn);
        \draw (b1X) to (X);
        \draw (bmX) to (X);
    \end{scope}
        % \draw (b1) -- (X);
        % \draw (bm) -- (X);
        % \draw (Y1) -- (a);
        % \draw (Yn) -- (a);
    
        % red arrows X->a
    \begin{scope}[green!70!black]
        \draw[arr0,arrows={->[harpoon,swap]}]
            (1.5,0.8) to[in=20,out=-80] node[left, pos=0.1]{$m_{b_1 \!\stogreen \mskip-2muX}$\!} ([yshift=1px]X.24);
        \draw[arr0,arrows={->[harpoon,swap]}]
            (1.6,0.1) to[in=-23,out=-95] node[right, pos=0.25]{$m_{b_m \!\stogreen \mskip-2muX}$\!} ([yshift=1px]X.-15);
        \draw[arr0,arrows={->[harpoon,swap]},dashed]
            (0.6,0.7) to[in=0,out=-90] node[left, pos=0.1]{$m_{X \!\stogreen a}$\!} ([yshift=2px]aX.east);
    \end{scope}
        % blue arrows a -> X
    \begin{scope}[blue]
        \draw[arr0,arrows={->[harpoon,swap]}]
            (-0.85,-0.1) to[in=160,out=110] node[right, pos=0.1]{$m_{Y_1 \!\stoblue a}$\!} ([yshift=-2px]aY1.175);
        \draw[arr0,arrows={->[harpoon,swap]}]
            (-0.8,-0.9) to[in=-157,out=100] node[right, pos=0.1]{$m_{Y_n \!\stoblue a}$\!} ([yshift=-1px]aYn.-166);
    
        \draw[arr0,arrows={->[harpoon,swap]},dashed]
            (0.6,-0.7) to[in=180,out=90] node[right, pos=0.1]{\!$m_{a \stoblue \mskip-2muX}$} ([yshift=-2px]X.west);
    \end{scope}
    \begin{scope}[blue!50!black]
        \coordinate (amerge) at (-70:0.6);
        \draw[arr0,shorten <=0] (amerge) to[out=130,in=-90] (aX);
        \draw[arr0,shorten <=0] (amerge) to[out=130,in=-45] (aY1);
        \draw[arr0,shorten <=0] (amerge) to[out=130,in=-10] (aYn);
        \draw[arr0,-,shorten >=0] (0.3,-0.9) to[in=-50,out=90]
            node[left]{$\phi_a$} (amerge);
    \end{scope}
    \end{tikzpicture}
    $\subseteq \Msg +
        {\color{blue!50!black}\dg M_\Phi}
    $.
    \caption[a schematic of belief propagation as LIR]
        {a schematic depiction of belief propogation as LIR. 
            At the center are a variable $X$, and a variable $\mat X^{a}$ that corresponds to a factor $a$ which depends on $X$. 
            The blue arcs correspond to a focus that computes the message from $a$ to $X$, while the green arcs correspond a focus that computes the message from $X$ to $a$.
        The faint gray double lines are deterministic cpds that enforce equality between the variables they connect.}
        \label{fig:bp-lir}
\end{figure}

    Indeed, it can be shown that equations (\ref{eq:X->a}) and (\ref{eq:a->X}) minimize the inconsistency of
    the dotted components in their appropriate contexts  (shown in green and blue above).

    % Formally, let's 
    % We now define the views.
    This means that LIR with the appropriate views computes belief propagation. 
    To show this formally, we need a few definitions.
    % To a first approximation
    % Modulo a small subtlety,
    % the following is essentially true:
    % In particular,
    Equation
    \eqref{eq:X->a}
    adjusts the parameters of
    $C_{X \sto a} := \{ m_{X \sto a} \}$ so as to
    minimize 1-inconsistency in context
    $A_{X \sto a} := \{ m_{b \sto X} \}_{b \in \partial X \setminus a} \cup \{m_{X \sto a}\}$,
    while
    % $m_{a \sto X}$
    % the mesage from $a$ to $X$
    \eqref{eq:a->X}
    % is the one that minimizes the 1-inconsistency of
    adjusts
    $C_{a \sto X} := \{ m_{a \sto X} \}$
    so as to minimize the 1-inconsistency in
    context
    $A_{a \sto X} := \{ \phi_a, m_{a \sto X} \} \cup \{ m_{Y \sto a} \}_{Y \in \mat X_a \setminus X}$.
    Simply put, this means looking at either the blue or the green sub-PDG of 
    
    %
    The only wrinkle is that we do not want to attend to
        the structural aspect of a message $e$ that we are updating---%
        that is, we must select $\varphi$ so as to ignore its causal weight $\alpha_e$.
    % Intuitively: although all of the input messages summarize causal information,
    %     we're trying to capture that information with a distribution.
    % Thus, it's not appropriate to attend to the causal structure
    %     of the edges that we're
    % Intuitively: when we are updating some message (say $m_{X \sto a}$), we
    Intuitively: when we are updating some message $e$, we
    are interested in summarizing information in the other messages
    (both observational and causal information), purely with an observation.
    % Put another way, we do not wa
    % This means that the foci correspond to elements of
    % \[
    %     \mat L := \bigcup_{a\in \Ar,X \in \mat X_a}\Big\{a \sto X,\, X \sto a,\, X\Big\},
    % \]
    % such that $\mat F := \{ (\varphi_l, \chi_l) : l \in \mat L \}$.
    
    More precisely, the foci
    \[
        \mat F :=  \bigg\{ (\varphi_j, \chi_j) ~:~ j \in \bigcup_{\substack{a\in \Ar \\ X \in \mat X_a}}\Big\{a \sto X,\, X \sto a,\, X\Big\},\bigg \}
    \]
    are indexed by messages and variables, and defined as follows.
    % such that $\mat F := \{ (\varphi_l, \chi_l) : l \in \mat L \}$.
    % This means that for each focus
    % \[
    % (\varphi,\chi) \in \mat F ~~:= \bigcup_{a\in \Ar,X \in \mat X_a}\{a \sto X, X \sto a, X\},
    % \]
    The attention mask $\varphi_j$ for focus $j$ is given by:
    \[
        \varphi_j(a) :=  {\singlespacing\begin{cases}
            \binom 11 & \text{ if } a \in A_{j} \setminus C_{j} \\
            \binom 10 & \text{ if } a \in C_{j} \\
            \binom 00 & \text{otherwise}
        \end{cases}},
    \]
    where $\binom{\phi_1}{\phi_2}$ scales $\beta_a$ by $\phi_1$ and $\alpha_a$ by $\phi_2$.
    Finally,
    % for each focus $l \in \mat F$, define
    full control over $C_j$ means defining
    % \[
    % \chi_j(a) := 
    % \begin{cases}
    %     \infty & \text{ if } a \in C_{j} \\
    %     0 & \text{ otherwise}.
    % \end{cases}
    % \]
    $\chi_j(a) := \infty \cdot \mathbbm 1[ a \in C_j]$. 
    With these choices
    % \cref{prop:bp} follows easily.
    in place, it's not hard to show that \cref{algo:LIR} amounts to belief propagation.
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
    
    
% \begin{quotation}\it
%     the message sent from a vertex $v$ on an edge $e$ is the product of the local function at $v$, with all messages recieved at $v$ (other than from $e$),
%     summarized for $e$'s variable node. \citep{kschischang2001sumproduct}
% \end{quotation}
% \vskip
% The PDG $\Msg$ is not so different from this schematic:
% \begin{center}
%     % \vspace{-1ex}
%     % $\begin{matrix}\Msg\\+
%     %     {\color{blue!50!black}\dg M_\Phi}
%     % \end{matrix}\!\cong\,$
%     \begin{tikzpicture}[xscale=2,center base]
%         % \node[lirfactor] (a) at (0,0) {$a$};
%     \begin{scope}[os1/.style={outer sep=1pt},dpad0/.append style={fill=gray!50!black,text=white,font=\mathversion{bold}}]
%         % \node[] (a) at (0,0) {$a$};
%         % \node[dpad0,os1] (aX) at (0.22,0){\scalebox{0.7}{$X^{(a)}$}};
%         \node[dpad0,os1] (aX) at (0.22,0){\scalebox{0.7}{$X^{a}$}};
%         \node[dpadded,os1] (X) at (1,0) {$X$};
%         % \node[lirfactor] (b1) at (2,0.5){$b_1$};
%         % \node[lirfactor] (bm) at (2,-0.5){$b_m$};
%         \node[dpad0,os1] (b1X) at (1.8,0.5){\scalebox{0.7}{$X^{b_1}$}};
%         \node[dpad0,os1] (bmX) at (1.8,-0.5){\scalebox{0.7}{$X^{b_m}$}};
%         % \node[draw,circle,inner sep=2px] (Y1) at (-1,0.5){$Y_1$};
%         % \node[draw,circle,inner sep=2px] (Yn) at (-1,-0.5){$Y_n$};
%         \node[dpadded,os1] (Y1) at (-1.1,0.5){$Y_1$};
%         \node[dpadded,os1] (Yn) at (-1.1,-0.5){$Y_n$};
%         % \node[dpad0,os1] (aY1) at (-0.2,0.3){\scalebox{0.7}{$Y_{1}^{(a)}$}};
%         % \node[dpad0,os1] (aYn) at (-0.2,-0.3){\scalebox{0.7}{$Y_{n}^{(a)}$}};
%         \node[dpad0,os1] (aY1) at (-0.2,0.3){\scalebox{0.7}{$Y_{1}^{a}$}};
%         \node[dpad0,os1] (aYn) at (-0.2,-0.3){\scalebox{0.7}{$Y_{n}^{a}$}};
%     \end{scope}
%         % \node[anchor=center] at (-1,0){$\scalebox{1}[1]{\vdots}$};
%         % \node[anchor=center,draw] at (2,0){$\scalebox{1}[1]{\vdots}$};
%         % \draw (a) -- (X);
%         % \draw[double equal sign distance,shorten <=0,shorten >=0] (aX.center) to (X.center);
%         % \draw[double equal sign distance,shorten <=0,shorten >=0] (aX) to (X);
%     \begin{scope}[every path/.append style={gray!50,double equal sign distance}]
%         \draw (aX) to (X);
%         \draw (aY1) to (Y1);
%         \draw (aYn) to (Yn);
%         \draw (b1X) to (X);
%         \draw (bmX) to (X);
%     \end{scope}
%         % \draw (b1) -- (X);
%         % \draw (bm) -- (X);
%         % \draw (Y1) -- (a);
%         % \draw (Yn) -- (a);

%         % red arrows X->a
%     \begin{scope}[green!70!black]
%         \draw[arr0,
%             % arrows={->[harpoon,swap]}
%             ]
%             (1.5,0.8) to[in=20,out=-80] node[left, pos=0.1]{$m_{b_1 \!\stogreen \mskip-2muX}$\!} ([yshift=1px]X.24);
%         \draw[arr0,
%             % arrows={->[harpoon,swap]}
%             ]
%             (1.6,0.1) to[in=-23,out=-95] node[right, pos=0.25]{$m_{b_m \!\stogreen \mskip-2muX}$\!} ([yshift=1px]X.-15);
%         \draw[arr0,
%             % arrows={->[harpoon,swap]},
%                 dashed]
%             (0.6,0.7) to[in=0,out=-90] node[left, pos=0.1]{$m_{X \!\stogreen a}$\!} ([yshift=2px]aX.east);
%     \end{scope}
%         % blue arrows a -> X
%     \begin{scope}[blue]
%         \draw[arr0,
%             % arrows={->[harpoon,swap]}
%             ]
%             (-0.85,-0.1) to[in=160,out=110] node[right, pos=0.1]{$m_{Y_1 \!\stoblue a}$\!} ([yshift=-2px]aY1.175);
%         \draw[arr0,
%             % arrows={->[harpoon,swap]}
%             ]
%             (-0.8,-0.9) to[in=-157,out=100] node[right, pos=0.1]{\mbox{$m_{Y_n \!\stoblue a}$\!}} ([yshift=-1px]aYn.-166);

%         \draw[arr0,
%             % arrows={->[harpoon,swap]},
%             dashed]
%             (0.6,-0.7) to[in=180,out=90] node[right, pos=0.1]{\!$m_{a \stoblue \mskip-2muX}$} ([yshift=-2px]X.west);
%     \end{scope}
%     \begin{scope}[blue!50!black]
%         \coordinate (amerge) at (-70:0.6);
%         \draw[arr0,shorten <=0] (amerge) to[out=130,in=-90] (aX);
%         \draw[arr0,shorten <=0] (amerge) to[out=130,in=-45] (aY1);
%         \draw[arr0,shorten <=0] (amerge) to[out=130,in=-10] (aYn);
%         \draw[arr0,-,shorten >=0] (0.3,-0.9) to[in=-50,out=90]
%             node[left]{$\phi_a$} (amerge);
%     \end{scope}
% \end{tikzpicture}
% % \!\!\!\raisebox{-1em}.
% \raisebox{-1em}{
% \!\!\!\!\!
% $\subseteq\begin{matrix}\Msg\\+
%     {\color{blue!50!black}\dg M_\Phi}
%     \text{.\!}
% \end{matrix}$}
% \end{center}
%
% Computing
% Calculating
% $m_{X \sto a}$
% the message from $X$ to $a$
% Our observation here is that the message computations are
% shown above in orange and blue, respectively.
% Once the details are in place, we find that

% Thus, for any schedule of messages
\begin{prop} \label{prop:bp}
    If \textsc{Refocus} selects a focus non-deterministically from
    $\{ a\sto\mskip-2mu X, X\! \sto a, X\}_{X \in \X, a \in \partial X}$
    % (details in \cref{sec:bp-details})
    \unskip, then
    % with attention and control sets as above and
    % $\gamma=1$, then
    % \[ \Big\{
    %     \begin{pmatrix}
    %         % \chi = \infty \mathbbm 1[m_{X \sto a}]\\
    %         % \phi = \infty\mathbbm1 [\{ m_{b \sto X} \}_{b \in \partial X \setminus a} \cup \{m_{X \sto a}\}]
    %         C =  \{ m_{X \sto a} \}\\
    %         A = \{ m_{b \sto X} \}_{b \in \partial X \setminus a} \cup \{m_{X \sto a}\}
    %     \end{pmatrix}
    %     ~\Big|~ a \in \Ar, X \in \mat X_a \Big\},
    % \]
    the possible runs of
    \textsc{LIR}$(
        \dg M_\Phi, \Msg
        + \dg B
         )$
    are precisely those of belief propagation for different message schedules.
    % Further adopting a view with full control in $\beta$
\end{prop}

There are many variants of the belief propagation algorithm.
Some of them are generated by clustering factors together---%
    in the language of \citet{KF09},
    this amounts to selecting a cluster graph other than the ``Bethe cluster graph'' (\S11.3.5.2, ibid)
    as the basis for the message passing algorithm.
Our analysis immediately applies to these other cluster graphs as well. 
% The variational form of message passing
% With a slightly different formalism,
% There is also a well-established variational picture, in which message
% passing minimizes the Bethe approximation to the free energy.


% We suspect
\citet{minka2005divergence} offers a different perspective,
in which a broader class of message passing algorithms can be viewed as iteratively
adjusting some local context to minimize an $\alpha$-divergence.
% that are generated from the $\alpha$-divergences \citep{minka2005divergence}, because
% of the close relationship those divergences have with simple PDGs \citep{one-true-loss}.
% In particuar, he shows minimizing $\alpha$-divergences
We suspect that LIR generalizes this procedure as well---not only
because it is similar in spirit, but also because because
these divergences can be viewed as the degree of inconsistency
of a PDG containing two distributions
    (\cref{sec:statdist}).
    %  \citep{one-true-loss}.
% generalized belief propagation
    % \citep{generalized-bp} as well.

% \paragraph{Message Passing from Divergences}
% \citet{}
% \section{Adversarial Training as LIR}

% we also believe

%
% Here, various control comes in handy.
% \section{Diffusion as LIR}
% The diffusion objective turns out to be just the inconsistency of the appropriate PDG,
% i.e.,
% \[
%     .
% \]
% % but it has
% What makes this tractable, however, is that

% \section{Generic Generative Modeling with LIR}
% All of this suggests a general process by which
% Suppose we want to generate some structured data .
% The general idea


\section{Decision Making with LIR}

In this section, we show how a few standard decision rules can be viewed as inconsistency minimization. 
% 
% \paragraph{Expected Utility Maximization as LIR.}
In \cref{sec:reverse-engineer,sec:soft-constraint-widget} it was established that PDGs can represent expected cost,
% albeit with some strange 
albeit by articulating some questionable probabilistic beliefs. 
% You should care about it because we've already established this construction (sections 4.2.2 and 6.8) as a way of representing something like a utility or a cost.  Yes, it may seem strange, but if you buy into the perspective of the dissertation, this is because agents with preferences are already strange.  Thus, a natural question becomes: what does inconsistency resolution mean in this context? Perhaps surprisingly, it means expected utility maximization, at least if confident in one's beliefs. 
\newmaterial{%
For the moment, let us entertain the possibility that these strange probabilistic beliefs are in fact an appropriate way to encode preferences. 
% (Perhaps it looks strange because preferences themselves are strange.)
% A natural 
A natural question then emerges: what does it mean to make decisions to as to minimize inconsistency in this context? 
}%
In this section, we show that the standard picture of rational decision making---that is, expected utility maximization---can be viewed as the pursuit of consistency in this probabilistic model. 

Suppose we are trying to choose an action (i.e., control a variable $A$). 
To do so reasonably, we need to model some context. 
Let $S$ be a variable represnting the current state of the world,
and $O$ represent the outcome after taking our action. 
%
To complete the standard picture, let's further assume that 
 we have 
\begin{enumerate}[nosep]
\item 
    some understanding of how our action $A$ and the state $S$ determine the outcome $O$, say in the form of a cpd $\tau(O \mid S,A)$, 
\item 
a belief $p(S)$ about the current state of the world,
and 
\item  a utility function $u : \V O \to \mathbb R$ on possible outcomes. 
\end{enumerate}
It is easy to model this information with a PDG:

\begin{center}
    \begin{tikzpicture}
        \node[dpadded] (S) at (-0.4,0) {$S$};
        \draw[arr2, <-] (S) -- node[above]{$p$} ++(-1.5, 0); 

        \node[dpadded] (O) at (2,0) {$O$};
        \node[dpadded] (U) at (4,0) {$U$};
        \draw[arr2, ->>] (O) -- node[above, pos=0.35]{$u$}  (U);
        \node[dpadded] (A) at (0.5,1.3) {$A$};
            \mergearr[arr1,->] SAO
        \node[below=1pt of center-SAO]{$\tau$};
        % \node[dpad1] (T) at (5.5,0) {$\Tru$};
    \end{tikzpicture}
\end{center}


% All that's missing is a connection 
% However, this is just a description of what
What's missing is any sort of encoding of the fact that higher utilities
are ``better'' than lower ones. 
For this, we can add a ``soft constraint'' (see \cref{sec:constraint-widget}) that is violated less at higher utilities than lower ones.
At a technical level, recall that this means 
(1) including the variable $\Tru$ that can technically take on values $\V \Tru = \{ \truf, \trut\}$, yet happens to always on the value $\trut$,
and  
(2) adding a cpd $b(\Tru \mid U)$ encoding a constraint violation that is more serious the lower the value of $U$. 
Let 
$\dg M_{(p, \tau, u, b)}$
represent the PDG above, augmented with such a soft constraint $b$. 

Intuitively, imagine that there is a part of you that you cannot control, 
    which we will call ``Faith''.
Faith
    engages in wishful thinking: 
    she disbelieves outcomes that are undesirable,
    creating epistemic conflict if she sees outcomes of low utility. 
(Technically, Faith refers to the sub-PDG consisting of $u$, $b$, and $\Tru{=}\trut$.)
If you have no control over Faith, but still have confidence and pay attention to her,
then it turns out that selecting actions to minimize your combined inconsistency
is a decision rule that interpolates between
\begin{enumerate}[nosep,label={(\alph*)}]
    \item 
    maximizing expected utility (when $\beta_p \gg \beta_b$), and
    \item 
    maximizing maximum utility (when $\beta_p \ll \beta_b$),
\end{enumerate}
where  $\beta_p$ is your confidence in your prior probabilistic belief $p(S)$, and $\beta_b$ is the confidence of the soft constraint $b$ (i.e., your ``degree of faith'').

\begin{linked}{prop}{eu+maxmax}
    % So long as $b(\Tru{=}\trut \mid U{=}u)$ is strictly increasing in $u$, 
    Suppose $b(\Tru{=}\trut \mid U{=}u) = k \cdot \exp( u )$ for some constant $k$. 
    \begin{enumerate}[nosep]
        \item 
    If $\beta_p = \infty$ and $\beta_b < \infty$, then 
    the action(s) $a \in \V\! A$ that minimize the inconsistency
    are those that maximize expected utility.
    Formally,
    for all $\gamma \ge 0$, 
    \[
        \argmin_{a \in \V\!A} \aar[\big]{\dg M_{(p,\tau,u,b)} + A{=}a}_\gamma 
        = \argmax_{a \in \V\! A} 
            \Ex_{\substack{s\sim p \\ o \sim \tau|s,a}} \big[ u(o) \big].
    \]
    \item 
    If $\beta_p < \infty$ and $\beta_b = \infty$, then 
    the action(s) $ a \in \V\! A$ that minimize overall inconsistency are
    those that can lead to the best possible outcome, i.e., 
    \[
        \argmin_{a \in \V\!A} \aar[\big]{\dg M_{(p,\tau,u,b)} + A{=}a}_\gamma 
        = \argmax_{a \in \V\! A} \;
            \max_{s \in \V S} \;
            \Ex_{o \sim \tau|s,a} [ u(o) ].
    \]
    \end{enumerate}
\end{linked}

We suspect it will also be possible to implement other decision rules such as minimax or maximin, but like GAN training, these objectives that look like ``two-player games'' require two different focuses of LIR. 
We leave the details and implications of this to future work.

We conclude our discussion of decision theory with a higher-level obseravation. 
When there is conflict between your preferences, beliefs, and actions
\newmaterial{%
(in the sense that a PDG that represents all three is inconsistent),
}%
there are, in principle, three possible resolutions.
You can change your action to maximize your expected utility, which is thought of as the rational thing to do.
You can also change your preferences to become happy with how things currently are, which is perhaps a more zen perspective. 
These two resolutions are two halves of the Serenity Prayer \citep{serenity-prayer}:

\begin{quotation}\it
    Oh, God, give us the courage to change what must be altered, serenity to accept what can not be helped, and insight to know the one from the other.
\end{quotation}

There is also a third way to resolve the inconsistency, which is arguably significantly worse: you can change your beliefs about what state you are in. 
Unfortunately, this kind of self delusion only creates more inconsistency with future observations, and is seldom a good idea.  
This is one more reason that it can be beneficial to carefully select only small local of the picture to control while resolving inconsistencies.


\section{Discussion and Future Work}

These examples are only the beginning.
Our initial investigations suggest that
opinion dynamics models,
the training process for diffusion models,
and much more, are all naturally captured by LIR.
%
The surprising generality of LIR begs some theoretical questions.
What assumptions are needed to prove that it reduces overall inconsistency,
    as is often the case?
    % Do simple choices ever lead to non-standard algorithms?
What are the most natural choices we could make to produce an efficient
    % non-standard algorithm?
    algorithm that is not already widely used?
    Might that algorithm itself prove useful?
% How expressive is this kind computation?

% These results also
It also suggests a novel approach to structured generative modeling:
assemble an enormous PDG by merging together
    many variables, existing models, priors,
    constraints, and data of all shapes and sizes.
Then train new models to predict variables from one another,
    using LIR (with random refocusing on small tractable parts, say).
Is this effective?  We are excited to find out!

% \clearpage
% \newpage

% \bibliography{lir-refs}
% \bibliographystyle{icml2023}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\begin{subappendices}
% \appendix

% \section{Details on Belief Propogation} \label{sec:bp-details}
% The usual schematic illustration of belief propagation \citep{kschischang2001factor}
% looks something like:
% \begin{center}
%     \begin{tikzpicture}
%             % [xscale=1.8]
%             [xscale=2,yscale=1.3]
%         \node[lirfactor] (a) at (0,0) {$a$};
%         \node[draw,circle,inner sep=4px] (X) at (1,0) {$X$};
%         \node[lirfactor] (b1) at (2,0.5){$b_1$};
%         \node[lirfactor] (bm) at (2,-0.5){$b_m$};
%         \node[draw,circle,inner sep=2px] (Y1) at (-1,0.5){$Y_1$};
%         \node[draw,circle,inner sep=2px] (Yn) at (-1,-0.5){$Y_n$};
%         % \node[anchor=center] at (-1,0){$\scalebox{1}[1]{\vdots}$};
%         % \node[anchor=center,draw] at (2,0){$\scalebox{1}[1]{\vdots}$};
%         \draw (a) -- (X);
%         \draw (b1) -- (X);
%         \draw (bm) -- (X);
%         \draw (Y1) -- (a);
%         \draw (Yn) -- (a);

%         % \draw[arr0,red,arrows={->[harpoon,swap]}]
%         %     (1.6,0.7) to[in=23,out=-80] node[left, pos=0]{$m_{b_1 \!\sto \!X}$\!\!} (X);
%         % \draw[arr0,red,arrows={->[harpoon,swap]},dashed]
%         %     (0.5,0.7) to[in=0,out=-90] node[left, pos=0]{$m_{X \!\sto a}$\!} (a);
%         % \draw[arr0,blue,arrows={->[harpoon,swap]}]
%         %     (0.5,-0.7) to[in=180,out=90] node[right, pos=0]{\!$m_{a \sto \!X}$} (X);
%         \begin{scope}[transform canvas={yshift=2px},green!70!black]
%         \draw[arr,arrows={->[harpoon,swap]}]
%             (b1) to node[above=-0.5pt,rotate=15]{$m_{b_1 \!\stogreen \mskip-2muX}$\!\!} (X);
%         \draw[arr,arrows={->[harpoon,swap]}]
%             (bm) to node[above=-0.5pt,rotate=-5,pos=0.3]{$m_{b_m \!\stogreen \mskip-2muX}$\!\!} (X);
%         \draw[arr,arrows={->[harpoon,swap]},densely dotted]
%             (X) to node[above]{$m_{X \!\stogreen a}$\!} (a);
%         \end{scope}

%         \begin{scope}[transform canvas={yshift=-2px},blue]
%         \draw[arr,arrows={->[harpoon,swap]}]
%         % \draw[blue,arrows={->[harpoon,swap]}]
%             (Y1) to node[below=-0.5pt, rotate=-5,pos=0.35]{$m_{Y_{\!1} \!\stoblue a}$\!\!} (a);
%         \draw[arr,arrows={->[harpoon,swap]}]
%             (Yn) to node[below, rotate=17]{$m_{Y_{\!n} \!\stoblue a}$\!\!} (a);
%         \draw[arr,arrows={->[harpoon,swap]},densely dotted]
%             (a) to node[below]{\!$m_{a \stoblue \mskip-2muX}$} (X);
%         \end{scope}
%         % \draw (current bounding box.north west) rectangle (current bounding box.south east);
%     \end{tikzpicture}~\raisebox{1ex}.
% \end{center}%

% This is only a schematic,
%     but the PDG $\Msg$ can be made to look similar to it.
% Adding a variable $X^{a}$ for every pair
%     % $(a \in \Ar, X \in \mat X_a)$,
%     $(X,a)$ with $X \in \mat X_a$
% along with edges asserting that $X^{a} = X$,
% we obtain the equivalent PDG in the main body of the paper:
% \begin{center}
% \begin{tikzpicture}
%         % [xscale=2,center base]
%         [xscale=2.4,yscale=1.5]
%     % \node[lirfactor] (a) at (0,0) {$a$};
% \begin{scope}[os1/.style={outer sep=1pt},dpad0/.append style={fill=gray!50!black,text=white,font=\mathversion{bold}}]
%     % \node[] (a) at (0,0) {$a$};
%     % \node[dpad0,os1] (aX) at (0.22,0){\scalebox{0.7}{$X^{(a)}$}};
%     \node[dpad0,os1] (aX) at (0.22,0){\scalebox{0.7}{$X^{a}$}};
%     \node[dpadded,os1] (X) at (1,0) {$X$};
%     % \node[lirfactor] (b1) at (2,0.5){$b_1$};
%     % \node[lirfactor] (bm) at (2,-0.5){$b_m$};
%     \node[dpad0,os1] (b1X) at (1.8,0.5){\scalebox{0.7}{$X^{b_1}$}};
%     \node[dpad0,os1] (bmX) at (1.8,-0.5){\scalebox{0.7}{$X^{b_m}$}};
%     % \node[draw,circle,inner sep=2px] (Y1) at (-1,0.5){$Y_1$};
%     % \node[draw,circle,inner sep=2px] (Yn) at (-1,-0.5){$Y_n$};
%     \node[dpadded,os1] (Y1) at (-1.1,0.5){$Y_1$};
%     \node[dpadded,os1] (Yn) at (-1.1,-0.5){$Y_n$};
%     % \node[dpad0,os1] (aY1) at (-0.2,0.3){\scalebox{0.7}{$Y_{1}^{(a)}$}};
%     % \node[dpad0,os1] (aYn) at (-0.2,-0.3){\scalebox{0.7}{$Y_{n}^{(a)}$}};
%     \node[dpad0,os1] (aY1) at (-0.2,0.3){\scalebox{0.7}{$Y_{1}^{a}$}};
%     \node[dpad0,os1] (aYn) at (-0.2,-0.3){\scalebox{0.7}{$Y_{n}^{a}$}};
% \end{scope}
%     % \node[anchor=center] at (-1,0){$\scalebox{1}[1]{\vdots}$};
%     % \node[anchor=center,draw] at (2,0){$\scalebox{1}[1]{\vdots}$};
%     % \draw (a) -- (X);
%     % \draw[double equal sign distance,shorten <=0,shorten >=0] (aX.center) to (X.center);
%     % \draw[double equal sign distance,shorten <=0,shorten >=0] (aX) to (X);
% \begin{scope}[every path/.append style={gray!50,double equal sign distance}]
%     \draw (aX) to (X);
%     \draw (aY1) to (Y1);
%     \draw (aYn) to (Yn);
%     \draw (b1X) to (X);
%     \draw (bmX) to (X);
% \end{scope}
%     % \draw (b1) -- (X);
%     % \draw (bm) -- (X);
%     % \draw (Y1) -- (a);
%     % \draw (Yn) -- (a);

%     % red arrows X->a
% \begin{scope}[green!70!black]
%     \draw[arr0,arrows={->[harpoon,swap]}]
%         (1.5,0.8) to[in=20,out=-80] node[left, pos=0.1]{$m_{b_1 \!\stogreen \mskip-2muX}$\!} ([yshift=1px]X.24);
%     \draw[arr0,arrows={->[harpoon,swap]}]
%         (1.6,0.1) to[in=-23,out=-95] node[right, pos=0.25]{$m_{b_m \!\stogreen \mskip-2muX}$\!} ([yshift=1px]X.-15);
%     \draw[arr0,arrows={->[harpoon,swap]},dashed]
%         (0.6,0.7) to[in=0,out=-90] node[left, pos=0.1]{$m_{X \!\stogreen a}$\!} ([yshift=2px]aX.east);
% \end{scope}
%     % blue arrows a -> X
% \begin{scope}[blue]
%     \draw[arr0,arrows={->[harpoon,swap]}]
%         (-0.85,-0.1) to[in=160,out=110] node[right, pos=0.1]{$m_{Y_1 \!\stoblue a}$\!} ([yshift=-2px]aY1.175);
%     \draw[arr0,arrows={->[harpoon,swap]}]
%         (-0.8,-0.9) to[in=-157,out=100] node[right, pos=0.1]{$m_{Y_n \!\stoblue a}$\!} ([yshift=-1px]aYn.-166);

%     \draw[arr0,arrows={->[harpoon,swap]},dashed]
%         (0.6,-0.7) to[in=180,out=90] node[right, pos=0.1]{\!$m_{a \stoblue \mskip-2muX}$} ([yshift=-2px]X.west);
% \end{scope}
% \begin{scope}[blue!50!black]
%     \coordinate (amerge) at (-70:0.6);
%     \draw[arr0,shorten <=0] (amerge) to[out=130,in=-90] (aX);
%     \draw[arr0,shorten <=0] (amerge) to[out=130,in=-45] (aY1);
%     \draw[arr0,shorten <=0] (amerge) to[out=130,in=-10] (aYn);
%     \draw[arr0,-,shorten >=0] (0.3,-0.9) to[in=-50,out=90]
%         node[left]{$\phi_a$} (amerge);
% \end{scope}
% \end{tikzpicture}.
% \end{center}

% We now define the views.
% % To a first approximation
% Modulo a small subtlety,
% the following is essentially true:
% % In particular,
% Equation
% \eqref{eq:X->a}
% adjusts the parameters of
% $C_{X \sto a} := \{ m_{X \sto a} \}$ so as to
% minimize 1-inconsistency in context
% $A_{X \sto a} := \{ m_{b \sto X} \}_{b \in \partial X \setminus a} \cup \{m_{X \sto a}\}$,
% while
% % $m_{a \sto X}$
% % the mesage from $a$ to $X$
% \eqref{eq:a->X}
% % is the one that minimizes the 1-inconsistency of
% adjusts
% $C_{a \sto X} := \{ m_{a \sto X} \}$
% so as to minimize the 1-inconsistency in
% context
% $A_{a \sto X} := \{ \phi_a, m_{a \sto X} \} \cup \{ m_{Y \sto a} \}_{Y \in \mat X_a \setminus X}$.

% The only wrinkle is that we do not want to attend to
%     the structural aspect of a message $e$ that we are updating---%
%     that is, we must select $\varphi$ so as to ignore its causal weight $\alpha_e$.
% % Intuitively: although all of the input messages summarize causal information,
% %     we're trying to capture that information with a distribution.
% % Thus, it's not appropriate to attend to the causal structure
% %     of the edges that we're
% % Intuitively: when we are updating some message (say $m_{X \sto a}$), we
% Intuitively: when we are updating some message $e$, we
% are interested in summarizing information in the other messages
% (both observational and causal information), purely with an observation.
% % Put another way, we do not wa
% % This means that the foci correspond to elements of
% % \[
% %     \mat L := \bigcup_{a\in \Ar,X \in \mat X_a}\Big\{a \sto X,\, X \sto a,\, X\Big\},
% % \]
% % such that $\mat F := \{ (\varphi_l, \chi_l) : l \in \mat L \}$.

% More precisely, the foci
% \[
%     \mat F :=  \bigg\{ (\varphi_j, \chi_j) ~:~ j \in \bigcup_{\substack{a\in \Ar \\ X \in \mat X_a}}\Big\{a \sto X,\, X \sto a,\, X\Big\},\bigg \}
% \]
% are indexed by messages and variables, and defined as follows.
% % such that $\mat F := \{ (\varphi_l, \chi_l) : l \in \mat L \}$.
% % This means that for each focus
% % \[
% % (\varphi,\chi) \in \mat F ~~:= \bigcup_{a\in \Ar,X \in \mat X_a}\{a \sto X, X \sto a, X\},
% % \]
% The attention mask $\varphi_j$ is given by:
% \[
%     \varphi_j(a) :=  {\singlespacing\begin{cases}
%         \binom 11 & \text{ if } a \in A_{j} \setminus C_{j} \\
%         \binom 10 & \text{ if } a \in C_{j} \\
%         \binom 00 & \text{otherwise}}
%     \end{cases},
% \]
% where $\binom{\phi_1}{\phi_2}$ scales $\beta_a$ by $\phi_1$ and $\alpha_a$ by $\phi_2$.
% Finally,
% % for each focus $l \in \mat F$, define
% full control over $C_j$ means defining
% % \[
% % \chi_j(a) := 
% % \begin{cases}
% %     \infty & \text{ if } a \in C_{j} \\
% %     0 & \text{ otherwise}.
% % \end{cases}
% % \]
% $\chi_j(a) := \infty \cdot \mathbbm 1[ a \in C_j]$. 
% With these definitions, \cref{prop:bp} follows easily.
%
% Continue to \cref{lir-appendix:proofs} for proofs!

% \onecolumn
\section{Proofs}
    \label{lir-appendix:proofs}

First, some extra details for \cref{prop:logccave}.
The parametrizations $\mathbb P$ are log-concave iff for every $a \in \Ar$, and $(s,t) \in \V(\Src a,\Tgt a)$, the function
$$
    \theta \mapsto -\log \p_a^{\,\theta}(\Tgt a =t\mid\Src a = a) \quad: \Theta_a \to [0,\infty]
$$
is convex (meaning that $\log \mathbb P$ without the negation is concave).
This is true for many families of distributions of interest.
For example, if $\Src a, \Tgt a$ is discrete, and the cpd is parameterized
by stochastic matrices $\mat P = [p_{s,t}] \in [0,1]^{\V(\Src a, \Tgt a)}$, then
\[
    - \log \p_a^{\mat P}(\Tgt a =t | \Src a=s) = - \log (p_{s,t})
\]
which is clearly convex in $\mat P$.

To take another example: if $\p_a$ is linear Gaussian, i.e.,
$\p_a(T|S) = \mathcal N(T | \mat A s + b,  \sigma^2)$, parameterized by
$(\mat A, b, 1/\sigma^2)$, then
\begin{align*}
    - \log \p_a^{(\mat A, b, \sigma^2)}(t|s)
    &= -\frac12 \log \frac{2\pi}{\sigma^2}  + \frac12 \left(\frac{t- \mat A s + b}{\sigma}\right)^2
\end{align*}
which is convex in $(\mat A, b, \frac1{\sigma^2})$.  Now, for the proof.


% \textbf{\cref{thm:cvx}}. \textit{If $\mathbb P$ is log-concave, then
% for small enough $\gamma$,
% the map $\theta \mapsto \aar*{\varphi \odot( \Ctx + \dg M(\theta))}_\gamma$ is convex.}
\recall{prop:logccave}

\begin{lproof} 
        \label{proof:logccave}
    By definition,
    \begin{align*}
        \aar[\Big]{\varphi \odot( \Ctx + \dg M(\theta))}_\gamma
        &= \inf_\mu  \Big\{
            \OInc_{\,\Ctx}(\mu) + \gamma \SDef_{\Ctx}(\mu)
            + 
            \gamma \SDef_{\dg M(\theta)}(\mu) + \OInc_{\dg M(\theta)}(\mu)  
        \Big\}.
    \end{align*}
    Only the final term actually depends on $\theta$, though---recall that $\SDef_{\dg M(\theta)}$ 
    depends only on the structure of the hypergraph (and the weights $\balpha$),
    and not the parameters of the cpds. 
    Thus, we can write $F(\mu)$ for the first three terms. 
        
    For all of our examples, and indeed, 
    if $\gamma$ is chosen small enough, 
    we have seen that the sum of the two terms is convex in $\mu$.
    Then we have
    \begin{align*}
        &\aar[\Big]{\varphi \odot( \Ctx + \dg M(\theta))}_\gamma
        \\&\quad= \inf_\mu  \left( F(\mu) + \Ex_\mu \left[ \sum_{\ed a ST}\beta_a \log \frac{\mu(T|S)}{\p_a^\theta(T|S)} \right] \right) \\
            &\quad= \inf_\mu  \Bigg( F(\mu) + \Ex_\mu \left[ \,\sum_{\ed a ST} \beta_a\log \frac{\mu(T|S)}{\lambda(T|S)} \right] +
            \underbrace{
                \Ex_\mu \left[\,\sum_{\ed a ST} \beta_a \log \frac{\lambda(T|S)}{\p_a^\theta(T|S)} \right]
            }_{\text{third term}}
            \Bigg).
    \end{align*}
    The second term is then entropy (relative to the base distribution), which is
        convex in $\mu$. The first term, $F(\mu)$, is convex in $\mu$ as well, and neither depend on $\theta$. The final term is linear in $\mu$.
    Since $\mathbb P$ is log-convex in $\theta$, 
    we know that $(\log\frac{\lambda(t|s)}{\p_a^\theta(t|s)})$ is convex in $\theta$.
    It follows that the third term is a positive linear combination
        of expectations that are all convex in $\theta$, and hence itself convex in $\theta$.
    Because the first two terms do not depend on $\theta$ and are convex in $\mu$,
        they are jointly convex in $(\mu,\theta)$.
    And, as we have seen, the third term is linear in $\mu$ and convex in $\theta$, so it is also jointly convex in $(\mu, \theta)$.
    Thus, the sum of all three terms in the infemum is jointly convex in $(\theta, \mu)$. 
    Taking an infemum over $\mu$ pointwise, the result is still convex in $\theta$ \citep{boyd2004convex}.
    % Since $\varphi \ge 0$ and $\bbeta  > 0$ this is effectively just the inconsistency of a PDG.
\end{lproof}

\textbf{\cref{prop:bp}.}\textit{
If \textsc{Refocus} selects a view non-deterministically from
$\{ a\sto\mskip-2mu X, X\! \sto a, X\}_{X \in \X, a \in \partial X}$
with $\varphi, \chi$ as above, and $\gamma=1$, then
% with attention and control sets as above and
% $\gamma=1$, then
% \[ \Big\{
%     \begin{pmatrix}
%         % \chi = \infty \mathbbm 1[m_{X \sto a}]\\
%         % \phi = \infty\mathbbm1 [\{ m_{b \sto X} \}_{b \in \partial X \setminus a} \cup \{m_{X \sto a}\}]
%         C =  \{ m_{X \sto a} \}\\
%         A = \{ m_{b \sto X} \}_{b \in \partial X \setminus a} \cup \{m_{X \sto a}\}
%     \end{pmatrix}
%     ~\Big|~ a \in \Ar, X \in \mat X_a \Big\},
% \]
the possible runs of
\textsc{LIR}$(
    \dg M_\Phi, \Msg
    + \dg B
     )$
are precisely those of BP for different message schedules.}
\begin{proof}
When $\gamma=1$, and $\alpha, \beta = 1$ for all of the input factors, then the optimal
distribution $\mu^*$ that realizes the infemum is just the product of factors. It follows that any distribution that has those marginals will minimize the observational inconsistency.
%
The message passing equations \eqref{eq:X->a} and \eqref{eq:a->X}, instantiated for various pairs $(a,X)$, 
% can be executed
can be executed in different orders. 
Since each focus corresponds directly to an instance of an equation, it is easy to see that the different message passing schedules correspond directly to the possible nondeterministic choices of focus in LIR. 
% correspond to both the message passing schedules and to the possible view selections of LIR.
\end{proof}

\recall{prop:eu+maxmax}
\begin{lproof}\label{proof:eu+maxmax}
    Since the choice $a$ is deterministic, the value of $A$ is determined; likewise the value of $\Tru$ is also fixed. 
    Similarly, as $u$ is a deterministic function, the value of $U$ is determined according to $u$.
    Thus we need only consider distributions $\mu(S, O)$ in our minimization; the other variables can be found as a function of these.
    However, we also know that $\mu(O\mid S) = \tau(O|S,A{=}a)$ since it is given with high confidence. 
    Therefore it suffices to restrict our search to distributions over the variable $S$.
    
    To simplify notation, let 
    $ EU(s,a) := \Ex_{o \sim \tau (O|s,a)} [ u(o) ] + \log k$
    be the expected utility of an action, shifted by the constant $\log k$.
    Note that
    \[
        \log \frac{1}{b(\Tru{=}\trut\mid U{=}u(o))}
        = - \log (k \cdot \exp( u(o) ))
        = - u(o) - \log k,
    \]
    which in expectation over $\tau(O|s,a)$, is $ - EU(s,a)$. 
    % which happens to be equal to 
    % $\log \frac{1}{b(\Tru{=}\trut\mid U{=}u(o))} =  \log \exp u(o) $
    % We then calculate that
    With this in mind, we calculate:
    \begin{align*}
        \aar[\big]{\dg M_{p,\tau,u,b}+A{=}a}_{\gamma}
        &= \inf_{\mu(S)} 
            \beta_p \Ex_{s \sim \mu} \Big[ \log \frac {\mu(s)}{p(s)} 
            + \frac{\beta_b}{\beta_p} \Ex_{o\sim \tau|a,s}\Big[\log \frac1{b(\Tru{=}\trut\mid U{=}u(o))}\Big]
            \\
        &= \inf_{\mu(S)} 
            \beta_p \Ex_{s \sim \mu} \Big[ \log \frac {\mu(s)}{p(s)} 
            + \log \circ \exp \Big(- \frac{\beta_b}{\beta_p} EU(s,a) \Big)
            \\
        &= \inf_{\mu(S)} 
            \beta_p \Ex_{s \sim \mu} \Big[ \log \frac {\mu(s)}{1}\cdot\frac{\exp(-\frac{\beta_b}{\beta_p} EU(s,a))}{p(s)} \cdot \frac ZZ \Big]
            \\
        &= \inf_{\mu(S)} 
            \beta_p \Ex_{s \sim \mu} \Big[ \log \frac {\mu(s)}{1}\cdot\frac{Z}{p(s)\cdot \exp(\frac{\beta_b}{\beta_p} EU(s,a))} \cdot \frac 1Z \Big]
                \numberthis\label{eq:save-eu-calc}
    \end{align*}
    At this point, we can use the same trick used repeatedly in the proving results of \cref{chap:one-true-loss}: take $Z$ to be the normalization constant needed to regard the middle fraction as the inverse of a probability distribution $\nu$. 
    Once we do so, we are left with an infimum over a KL divergence $\kldiv{\mu}{\nu}$ plus the expectation of a constant:
    \begin{align*}
        \aar[\big]{\dg M_{p,\tau,u,b}+A{=}a}_{\gamma}
            &= \beta_p \log \frac1 Z
            = - \log \sum_{s} p(s) \exp\Big( + \frac{\beta_b}{\beta_p} EU(s,a)\Big).
    \end{align*}
    We now look at the two extreme cases. 
    
    When $\beta_p \to \infty$, then
        the ratio $\frac{\beta_b}{\beta_p}$ becomes small, 
        and we can use the fact that $\exp( \epsilon) \approx 1+\epsilon$
        for small $\epsilon$
    to find that the inconsistency of interest is
    approximately 
    \begin{align*}
        \approx 
        - \beta_p \log \sum_{s} p(s) \Big[ 1 + \frac{\beta_b}{\beta_p} EU(s,a) \Big]
        = - \beta_p \log( 1 +  \frac{\beta_b}{\beta_p} EU(s,a) )
        \approx
        - \beta_b EU(s,a)
    \end{align*}
    Alternatively, more directly, when $\beta_p = \infty$, the optimal distribution must be $\mu = p$, and so the inconsistency is immediately $ - \Ex_{s \sim p} [ \beta_b EU(s,a)]$. 
    Either way, minimizing this quantity amounts to maximizing espected utility, as the two differ by a negative affine transformation.
    
    At the other extreme, when $\beta_b \to \infty$, 
    we can write our expression in terms of 
    $\mathrm{LSE} \{ x_1, \ldots, x_n \} := \log \sum_{i=1}^n \exp( x_i)$ (LogSumExp)
    which is a smooth approximation to a $\max$.  (In a moment, we will negate its arguments and the final output, using it as an approximation to a $\min$, instead.)
    Picking back up from \eqref{eq:save-eu-calc} and letting $t := \frac{\beta_b}{\beta_p}$, we find
    \begin{align*}
        \aar[\big]{\dg M_{p,\tau,u,b}+A{=}a}_{\gamma}
            &= - \beta_b \cdot \frac{-1}{t} \mathop{\mathrm{LSE}}\limits_{s \in \V S} \Big[-t \cdot \Big( \frac1t \log \frac1{p(s)} -  EU(s,a)\Big) \Big]. 
    \end{align*}
    Using the standard fact
    \unskip\footnote{
        Letting $m := \min_i x_i$, observe that, for all $t > 0$, we have
        $\exp(-tm) \le \sum_i \exp(-t x_i) \le n \exp(- tm)$.
        Apply a logarithm and multiply by $-\frac{1}{t}$ to get the promised result.}
    that 
    \[
        \min_{i \in [n]} x_i - \frac{1}{t} \log n \le 
            \frac{-1}{t} \mathop{\mathrm{LSE}}\limits_{i \in [n]} ( - t x_i ) < \min_{i \in [n]} x_i,
    \]
    we find that, in our case,
    \[
    M - \frac{1}{t} \log |\V S|
    ~~\le~~ \frac{1}{\beta_b} \aar[\big]{\dg M_{p,\tau,u,b}+A{=}a}_{\gamma}
    ~~\le~~ M
    \]
    where $M := \min_{s \in \V S}  ( -  EU(s,a)  + \frac{1}{t} \log \frac 1{p(s)} )$.
    In particular, when $\beta_b \to \infty$, meaning $t \to \infty$, the gap between the upper and lower bounds shrinks to zero,
    and the resulting inconsistency becomes
    $ - \min_{s \in \V S} ( - EU(s,a) ) = \max_{s \in \V S)} EU(s,a)$,
    proving the result.
\end{lproof}

\end{subappendices}
