    \label{chap:PDG-cat}

In this chapter, we investigate a more abstract perspective on PDGs, that was part of the original inspiration for PDGs in the first place. 
At first Bayesian networks appear to be almost diagrams in the categorical sense---if the BN is a 

% \section{Preliminaries}
\section{A Primer on Category Theory}
Category theory is a mathematical interlingua that captures the essential form of many arguments across mathematics. 
Sometimes (lovingly) called ``abstract nonsense'', category theory is often seen as extremely abstract meta-mathematics.  
% But it can be much more concrete and easier to understand than the traditional mathematical disciplines that reference it. 
Nevertheless, the basics more concrete and simpler than one might imagine.
% from the fields that algebraic topology.
At its core, it's essentially just the mathematic underpinnings of typed composition.  

\begin{defn}[category]
    A \emph{category} $\mathcal C$ consists of four pieces of data:
    \begin{itemize}[left=0pt,topsep=0pt]
        \item a collection of \emph{objects} $\ob_{\cal C}$;
        \item a collection of \emph{morphisms}
            $\Hom_{\mathcal C}(X,Y)$,  also written $\mathcal C(X,Y)$, for each pair of objects $(X,Y) \in \ob_{\mathcal C}^2$;
        \item a \emph{composition} operator 
        $\circ_{X,Y,Z}: \mathcal C(Y,Z) \times \mathcal C(X,Y) \to \mathcal C(X,Z)$ for each triple $(X,Y,Z) \in \ob_{\cal C}^3$, that is written inline (i.e., $f \circ g$ instead of $\circ(f,g)$), and is associative, i.e., $(f \circ g) \circ h = f \circ (g \circ h)$;
        \item a special \emph{identity element} $\id_{X} \in \mathcal C(X,X)$ for each object $X \in \mathcal C$, satisfying $\id_X \circ f = f$ and $g \circ \id_X = g$ for any morphism $f \in \mathcal C(Y,X)$ or $g \in \mathcal C(X,Y)$ for some $Y \in \ob_{\mathcal C}$.
        \qedhere
    \end{itemize}    
    % Note that the subscript $X,Y,Z$ on $\circ_{X,Y,Z}$ is almost always dropped and inferred from context, and the elements of $\mathcal C(X,Y)$ are referr
    % This means we are effectively instead  a polymorphic fuction $\circ : \forall X,Y,Z \in \ob_{\mathcal C}.~\mathcal C(Y,Z) \times \mathcal C(X,Y) \to \mathcal C(X,Z)$.
\end{defn}

Common examples of categories include those in \cref{tab:common-cats}.
\begin{table}
    \centering
    \begin{tabular}{ccc}\toprule
Category $\mathcal C$  & Objects $\ob \mathcal C$  & Morphisms $\Hom_{\mathcal C}$ \\ \midrule
$\Set$ & sets & functions \\
$\Rel$ & sets & relations \\
$\Top$ & topological spaces & continuous maps \\
$\Diff$ & 
    % \parbox{4cm}{smooth manifolds \\ (possibly with boundary or corners)} 
    \makecell{smooth manifolds\\ (with boundary or corners)}
    & smooth maps \\
\bottomrule
\end{tabular}
    \caption{A few of the most recognizable categories}
    \label{tab:common-cats}
\end{table}
% \begin{itemize}[left=0pt,noitemsep,topsep=0pt]
% \item    
%  $\Set$, the category whose objects are sets and whose morphisms are functions between them,  

% \item 
% $\Top$, the category whose objects are topological spaces and whose morphisms are continuous maps,

% \item 
% $\Rel$, the category whose objects are sets, and whose morphisms are relations, and

% \item $\Diff$, the category of smooth manifolds (possibly with boundary or corners) and differentiable maps. 

% \end{itemize}

% These categories are called \emph{concrete categories}, 
%     meaning that each one categories can be viewed as sets and
Each of these cateog 
But categories can also be much more combinatorial in nature. 
We will be much more interested in dinkier categories.
Here are some more extreme kinds of categories:

\begin{itemize}[]
    \item A category with one object is just a monoid---observe that $\circ$ is associative and has an identity. 
     % In this case, all information is in the set of morphisms from the unique object to itself, and the way they compose. 
    
    \item At the opposite extreme, a category with only identity morphisms is just a collection of objects. 
    
    \item A category with at most one morphism between any two objects is a preorder---in this case we write $a \le b$ iff there is a morphism from object $a$ to object $b$; the relation is reflexive because of the identity, and transitive because of composition. 
\end{itemize}

% What will be most relevant for our purposes is a construction 
The kinds of categories we are most interested in, however, are the ones generated by directed graphs. 

\begin{defn}[free category generated by a graph]
    \label{def:freecat}
    If $G = (N, A)$ is a directed (multi) graph with nodes $N$ and arrows $A$, the \emph{free category generated by $G$} is the category 
    % $\mathrm{Paths}(N,A)$
    $G^*$,  whose objects are the elements of $N$, and whose set of morphisms from $x$ to $y$, for $x,y \in N$, is the collection of paths from $x$ to $y$.
    That is, 
    \begin{align*}
        \ob_{G^*} &= N, \\
            G^*(x,y) &= \Big\{\text{ sequences }
                \langle a_1, \ldots, a_n\rangle ~\Big|~
                \begin{array}{c}
                n \in \mathbb N,\;\;
                n>0 \Rightarrow (\Src{a_1} = x \land \Tgt{a_n} = y),\\
                % n>1 \Rightarrow (
                \forall i \in \{1, \ldots,n-1\}.~\Tgt{a_i} = \Src{a_{i+1}} 
                % x = x_1, x_n = y,  x_
                \end{array}
                \Big\},
    \end{align*}
    with composition given by sequence concatenation, and the identity being the empty sequence. 
    % , generated by the arrows of $A$ and identity self-loops from variables to themselves. 
    % 
    % \begin{align*}
    %     \mathrm{ob}~
    % \end{align*}
\end{defn}

% Observe that the notation $A^*$ does not depend on $N$. 

The superscript-star notation has some standard meanings throughout mathematics, and this construction in \cref{def:freecat} reduces to several of them in the appropriate contexts. 

% interstingly the construction of \cref{def:freecat} recovers several
% of these standard notion.
    
\begin{itemize}
    \item 
A (multi) graph $G = (\{\ast\}, A)$ with one vertex can be identified with its arc set $A$. Every arrow has the same type ($\ast \to \ast$), and so a path is a sequence 
% $a_1.a_2.\,...\,.a_n$
$\langle a_1, a_2, ..., a_n\rangle$
where each $a_i \in A$. 
So in this case, $G^*$ (as given by \cref{def:freecat}) coincides
with the familiar set of strings $A^*$ over the alphabet $A$.

    \item
Let $R \subseteq V \times V$ be a binary relation on $V$.
Then the transitive closure of $R$, often denoted $R^*$,
    is the reachability relation generated by $R$. 
That is, $(u,v) \in R^*$ if and only if there is a path $\langle u{=}u_1,\, \ldots, u_n{=}v\rangle$ with each $(u_i, u_{i+1}) \in R$.
%
    % \item 
    % We now revisit the previous example from an alternate perspective. 
Equivalently, we can view $R$ as a graph $G = (V,R)$ by regarding each $(i,j) \in R$ as an arrow $i \to j$. The free category $G^*$ generated by these arrows (per \cref{def:freecat}) has an arrow from $u$ to $v$ (i.e., $G^*(u,v) \ne \emptyset$) iff $(u,v) \in R^*$.

\item 
% Continuing the preivous example, given a matrix
% If a  matrix $A \in \overline{\mathbb N}^{n\times n}$
A (directed) (multi) graph $G$ on $n$ vertices also has an adjacency matrix 
$A := \mathbb A_G \in \mathbb N^{n\times n}$.  Square matrices over a semiring
also have notion of a star, given by:
\[
    A^* = \sum_{n=0}^\infty A^n  \in \overline{\mathbb N}^{n\times n},
    \qquad \text{ where } \overline{\mathbb N} := \mathbb N \cup \{\infty\}.
\]
And, yet again, we have $\#G^*(i,j) = (\mathbb A_G^*)_{i,j}$
\end{itemize}


\temphide{
What about hypergraphs?
% Let $(N, \mathcal A)$ be a directed hypergraph. 
The free category generated by a directed hypergraph
$(N, \mathcal A)$
 % has objects equal to the powerset of $N$, 
is the free category generated by 

% How should we define the composition of two hyperarcs? 

\begin{defn}
    
\end{defn}
}


The second category of interest to us is that of sets and probabilistic functions between them. 
We can use the theory in \cref{ssec:fin-prob,ssec:gen-prob} to make this precise.

\begin{defn}
    % [stochastic category]
    [category of measurable spaces and Markov kernels]
    Let $\Stoch$ be the category whose
    objects are measureable topological spaces with a base measure, and 
    whose morphisms are Marov kernels that are absolutely continuous with respect to the base measure.
    Concretely,
    the objects of $\Stoch$ are pairs $(\mathcal X, \lambda)$, where $\mathcal X$ is a mesurable topological space, and $\lambda$ is a strictly positive and $\sigma$-finite measure on $\mathcal X$.
    % \begin{align*}
    %     \mathrm{ob}~\Stoch &= \Big\{ \text{pairs}~(\mathcal X, \lambda),
    %         ~\text{where $\mathcal X$ is a mesurable space, and $\lambda$ is a measure on $\mathcal X$} \Big\}%\\
    % \end{align*}
        % \Stoch(\lambda(X), \mu(Y)) &= \Big\{ \text{Markov Kernels $p(Y|X)$ such that} \\
        % 
        %     & \Big\}    
        % \end{align*}
    % and for objects $(X,\mathcal F_X \lambda_X)$ and
     % $(\mathcal Y = (Y,\mathcal G), \mu_X)$, 
    The collection of morphisms from 
    $(X,\mathcal F_X, \lambda_X)$ to $(Y, \mathcal F_Y, \lambda_Y)$ is the 
    set of Markov Kernels $\kappa : \mathcal X \to \mathcal Y$
    such that 
    % $\mu \ll \kappa(x)$ for all $x$.        
    $\kappa(x,-) \ll \lambda_Y$ for all $x$.        
    %   $\Stoch(, (\mathcal Y, \mu))$ from $(\mathcal X, \lambda)$ to $(\mathcal X, \mu)$
    % are Markov Kernels $\kappa : \mathcal X \to \mathcal Y$ such that 
    The reason we require this is so that the Radon-Nikodym derivative
    $\frac{\mathrm d \kappa(x)}{\mathrm d\lambda}$, i.e., the unique 
    $\mathcal F_Y$-measurable function satisfying
    \[ 
        \forall x. \forall A \in \mathcal F_Y.\qquad
        \kappa(x,A) = \int_{A} \frac{\mathrm d \kappa(x)}{\mathrm d\lambda} \mathrm d \lambda~,
        \qquad\text{exists.}
    \]
    
    Composition in $\Stoch$ is given by Lebesgue Integration: for Markov Kernels $p(Y|X) : \mathcal X \to \mathcal Y$ and $q(Z|Y) : \mathcal Y \to \mathcal Z$, define
    $(p \circ q) : \mathcal X \to \mathcal Z$  (i.e., $p \circ q : X \times \mathcal F_Z \to [0,1]$) by:
    \begin{align*}
        (p\circ q)(x, U) :=& \int_{\mathcal Y}
            q(-, U)
            \mathrm d p(x,-).
            % \overbrace{q(-, U)}^{\mathcal Y\text{-measurable function}}
            % \mathrm d \overbrace{p(x,-)}^{\text{a measure of }\mathcal Y}
    \end{align*}
    This typechecks because $q(-,U)$ is a $\mathcal Y$-measurable function, and $p(x,-)$ is a measure on $\mathcal Y$. 
    We must also prove that the result is a Markov Kernel, which we do below. 
    % for $U \in \mathcal F_Z$
    The identities are given by
    \begin{align*}
    \mathrm{id}_{\mathcal X}(x, U) &= \begin{cases}
            1 & \text{ if }x \in U \\ 0 & \text{otherwise}
        \end{cases}.
    \end{align*}
\end{defn}












% \subsection{A PDG is }
% \subsubsection*{A Categorical Definition of a PDG}
% \section{PDGs as Diagrams in the Category Sub-stochastic Maps}
\section{A Categorical Picture of PDGs}
Note that $\V$ is implicit in $\mathbb P$. 
The two can be expressed as a functor, which is arguably the most
compact definition of an (unweighted) PDG.  
% Let $\Stoch_\Delta$ be the Kliesli category formed
%
% \begin{defn}
    An \emph{unweighted PDG} $\langle\V, \mathcal P\rangle$ 
    over a structure $(\N, \Ar)$ is just a functor
    % $\mathrm{Paths}(\N,\Ar)$
    \begin{align*}
        {\mathbb P}  : 
            % \mathrm{Paths}(\N, \Ar)
            \Ar^* \to  \Stoch
    \end{align*} 
    whose action on objects $\N$ is $X \mapsto \V X$, and whose action on the generating morphisms $\ed aXY \in \Ar$
    is written
    % $p\ssub a(Y|X)$.
    $\p_a(Y|X)$. 
We drop the the symbol $\mathcal V$ in this context, using only the symbol $\mathbb P$, because $\mathcal V$ can be recovered by the action on the identity morphisms of $\Ar^*$. 
    % It is a functor between 
% \end{defn}   
% 
Given small category $J$ 
(such as the free category generated by a graph),
a functor $F : J \to \mathcal C$
is often called a \emph{diagram} of $\mathcal C$ (of shape J).
Therefore, an unweighted PDG is a diagram of the $\Stoch$, of shape
generated by its underlying hyprgraph.
In addition to probabilities, a PDG also contains 
confidences $\bbeta = \{ \beta_a \}_{a \in \Ar}$ about the reliability of those probabilities. 

% To get a clean categorical picture, we make two observations.
We now pursue a clean categorical picture of quantitative PDGs.
% It turns out that $\alpha$ can be replaced with edges of negative
% $\beta$ ---  and indeed, in the general setting of measurable spaces, something like this is necessary, because the entropy is not well-defined in this setting; we must use relative entropy anyway.
At a quantitative level, positive structural weights can be captured by negative observational weights.
This is because the gradient of $-\hat\nabla_\mu \H_\mu(Y|X)$, the gradient of the structural loss corresponding to a hyperarc $\ed aXY$, is the same as $+\hat\nabla\kldiv{\mu(X,Y)}{\mu(X) \lambda_Y(Y)}$, the gradient of the observational loss corresponding to a uniform distribution.
Furthermore, the weight $\beta_a$ may be absorbed into the cpd $\p_a$ by  dropping the requirement that measures be normalized. 
This is because the pair $(p(Y|X), \beta)$ as a
% subdistribution
can be encoded
\unskip\footnote{%\color{gray}
However, there is no way 
to combine $\beta$ with $p$ that results in a quantity $q$ (independent of $\mu$) that can be plugged directly into the ordinary expression for KL divergence:
\[
    \beta \log \frac \mu p = \log \frac \mu q \quad\implies\quad
        q = \mu^{1-\beta}p^\beta.
\]
% [Zhu \& Rower] suggest that this is not the appropriate notion of relative entropy, for unsigned measures; instead, one should use $\kldiv \mu\nu := \int \log \frac{\mathrm d\mu}{\mathrm d\nu} \mathrm d\mu + \int \mathrm d\mu - \int \mathrm d\nu$, but even with this 
}
as a single conditional measure
$
    (1-e^{-\beta}) p(Y|X)
    % (e^{\beta}) p(Y|X)
    % \beta p(Y|X)
$
losslessly, because $p(Y|X)$ can be reconstituted by renormalizing, and $\beta = - \log (1-k)$ can be recovered from the normalization constant $k$.
The only exception is when $\beta=0$, but in this case the cpd does not matter sematically, and so if anything it is a bonus that this representation identifies all cpds supplied with confidence $\beta=0$.

Furthermore, with this representation, the effect of composition is very compelling.  Suppose we compose $p(Y|X)$ with confidence $\beta_1$ with $q(Z|Y)$ with confidence $\beta_2$, where both $\beta_1, \beta_2 \in [0,\infty]$.
Then the composite 
\begin{align*}
    r(Z|X) &= \int_Y   (1-e^{-\beta_2}) p(Z|Y) (1-e^{-\beta_1}) \mathrm d p(Y|X) \\
    &= (1-e^{-\beta_1} - e^{-\beta_2} + e^{-\beta_1-\beta_2})~(q \circ p)(Z|X) \\
    &\approx (1 - \exp(-\min\{ \beta_1, \beta_2\}))~(q \circ p)(Z|X).
\end{align*}
In particular, the composite will be fully trusted iff both components are $\beta_1 = \beta_2 = \infty$, and if either has confidence zero, then the composite will also. 
\unskip\footnote{Keep in mind that, even if $p(Y|X)$ and $q(Z|Y)$ are both marginals of a shared distribution $\mu(X,Y,Z)$, and this is known with extreme confidence, their composite will only be correct if the information is somehow ``independent''. 
This is where I think $\alpha$ should enter the picture, ideally.}
% taking a mapping from $\V \Src a$ to un-normalized measures over $\V \Tgt a$
% To summarize, all data in a PDG can be captured by a single 
As a result, all data in a PDG ($\Ar$, $\N$, $\balpha$, $\V$, $\mathbb P$, $\bbeta$) may be specified together with a single functor 
\begin{equation}
    \dg M : \Ar^* \to \Meas_{\underline \Delta}.
\end{equation}
In other words, a PDG 
% $\dg M : \Ar^* \to \Meas_\Delta$
is a \emph{diagram}, in the usual categorical sense, of conditional (sub)distributions between measurable spaces. 

% So, it makes sense to ask a number of things:
This connection induces a number of category-theory flavored questions about PDGs:
\begin{enumerate}
    \item A PDG $\dg M : \mathcal A^* \to \Meas_{\underline\Delta}$ is a diagram in the category of subprobability distributions. When does it have a limit? What about a colimit? What do limits and colimits of PDGs mean? 

    \item If PDGs are functors, what are the natural transformations between them? What do they mean? 
    % \item Perhaps relatedly, what are the representable PDGs? 
    \item How does inconsistency arise in this categorical picture? 
    \item Can we study qualitative PDGs separately in this picture? Why are $\balpha$ combined with $\bbeta$ if the former are purely qualitative? 
    
    
    \item 
    PDGs can be given semantics in more than one way, in principle --- relative entropy is a natural choice, but, even then, it can be used in either direction.  Yet this functorial definition of a PDG does not contain this information.
    So is there any way it can possibly interact with relative entropy that defines the semantics? 
    If so, what is the categorical picture of the role of relative entropy?
    \begin{itemize}
        \item For general loss functions (e.g., reverse KL),
         how does this picture interact with confidence functions?  
    \end{itemize}
\end{enumerate}



\subsection{Limits}

A \emph{cone} $(X, \phi)$ over a diagram $D : \mathcal J \to \mathcal C$ is an object $X \in \ob \mathcal C$, together with an indexed family of morphisms $\phi_J : X \to D J$ for each object $J \in \ob \mathcal J$, such that, for all morphisms $f \in \mathcal J(A,B)$, 
% $D(f) \circ

... 
\TODO


% A \emph{limit} is a final cone---that is, a cone $(X, )

\begin{center}
\begin{tikzpicture}[scale=1.2]
    \begin{scope}[every node/.style={outer sep=0pt,inner sep=1pt}]
    \node (dots-left) at (-2, 0) {$\cdots$};
    \node (X1) at (-1,0) {$X_1$};
    \node (X2) at (0,-0.2) {$X_2$};
    \node (X3) at (1,0) {$X_3$};
    \node (dots-right) at (2,0) {$\cdots$};
    \end{scope}
    
    \begin{scope}[red!30!gray!70!black]
    \node (V) at (-0.5,1.2){$V$};
    \draw[arr2] (V) -- (X1);
    \draw[arr2] (V) -- (X2);
    \draw[arr2] (V) -- (X3);
    
    \node[below left=0.05 and 0.1 of V,inner sep=0pt, outer sep=0pt] {$\scriptstyle\forall$};
    \end{scope}
    
    \begin{scope}[blue!50!gray!60!white]
    \node (L) at (0.5,1){{$L$}};
        \node[right=1pt of L,anchor=west,blue!30!gray]{$\scriptstyle\cong X_1 X_3$};
    \draw[line width=4pt,draw=white] (L) -- (X1);
    \draw[line width=4pt,draw=white] (L) -- (X2);
    \draw[line width=4pt,draw=white] (L) -- (X3);
    \draw[arr2] (L) -- (X1);
    \draw[arr2] (L) -- (X2);
    \draw[arr2] (L) -- (X3);
    
        \draw[arr2, densely dashed] (V) -- node[above]{$\scriptstyle\exists!$} (L);
    \end{scope}
    \draw[arr2, ->>] (X1) -- (X2);
    \draw[arr2, ->](X3) -- (X2);
    \end{tikzpicture}
\end{center}

\begin{defn}
    Let $\dg M$ be a PDG, with variables $\X$. 
    The \emph{local marginal polytope}
     % $\mathbb L(\dg M)$
    % consists of all collections of ma
    \begin{equation}
        \mathbb L(\dg M) := 
            \Big\{ 
                \bmu = 
                \{ \mu_X \in \Delta \V\! X  
                \}_{X \in \X}
            ~\Big|~
                \forall \ed aST \in \Ar.~ \mu_T = \p_a \circ \mu_S 
            \Big\}.
    \end{equation}
    consists of all indexed sets of marginals (one per variable) that are  consistent with all conditional probabilities. 
    % We mention that, if $\Ar$ contains structural arcs 
\end{defn}


\begin{example}
\begin{enumerate}
    \item  Suppose $\dg M$ is the PDG repesentation of a joint distribution $\mu$, then $\lim \dg M = \mu$
    \item if $\dg M$ has a single arc $\Ar = X{\to }X$ associated with a cpd $q(X|X)$, then $\lim \dg M = \mathrm{Ext} \{ \mu : \mu = q \circ \mu \}$ is the set of extreme points over the stationary distributions of $q$.  
\end{enumerate}    
\end{example}


Our next results are sensitive to the particulars of the PDG encoding as a functor. Let $\dg M^{+\X} := \dg M \cup \{ \mat X \to \mat Y \}_{\mat Y \subset \mat Y \subset \X}$
be the PDG $\dg M$ augmented with additional structure describing the relationships between all subsets of variables. That is, $\Ar^*$, as the free hypergraph over nodes $2^\N$, complete with structural coherence maps.
\unskip\footnote{
Note that these cohrerence maps do not include hyperarcs on these variables to put them back together, e.g., $\{ \{X\}, \{Y\}\} \to \{X,Y\}$.
Including such a map is appropriate only if one believes $X$ and $Y$ are independent.
}
Let $\dg M^{\mathit{hold}}$ be the PDG where $p(\mat Y|\mat X)$ is really attached to a hyperarc $\mat X \to \mat X \cup \mat Y$, implicitly the identity along $\mat X \setminus \mat Y$. 
Write $\dg M^{+\X,\mathit{hold}}$ for the PDG with both alterations.

\begin{theorem}
    Suppose $\dg M$ is a PDG in which every arc has full confidence. Then, 
    \begin{enumerate}[itemsep=0pt,topsep=0pt]
        \item $\mathrm{Cones}(\dg M, 1) \cong \mathbb L(\dg M)$;
        \item $\mathrm{Cones}(\dg M^{+\X,\mathit{hold}}, 1) 
            \cong \SD{\dg M}$. 
        
        \item $\lim \dg M^{+\X,\mathit{hold}} = \mathrm{Ext} \SD{\dg M}$, where $\mathrm{Ext}(S)$ is the set of extreme points of a set $S$ (e.g., the vertices of $S$, when $S$ is a polytope). 
            
        % \item Let $A \subseteq \N$ be an object of $\Ar^*$. 
        % The set $\mathrm{Cones}(\dg M, A)$ of cones over $\dg M$ with vertex $A$ consists of 
        \item $\lim \dg M = \mathop{\mathrm{Ext}} \mathbb L(\dg M)$. 
    \end{enumerate}
\end{theorem}

\begin{proof}
    Part 1 is immediate; it just points out that the local marginal polytope, defined in the graphical models literature, is the limit in this context.
        
    % Part 2 is also straightforward.
    Now for part 2. 
     A cone over $\dg M^{+\X,\mathit{hold}}$ with vertex 1 is a collection of distributions $\{ \mu_X(X) \}_{X \in \X}$ such that, for all $\ed aST \in \Ar$, 
    $\mu_T(S,T) = \p_a(T|S) \mu_S(S)$. (This familiar notation is not problematic if $S \cap T = \emptyset$, but otherwise we mean $\mu_T(S,T) = \int_{S} \p_a(S,T|s') \mu_S(s')\,\mathrm ds'$, properly overwriting variables in $S$ according to $p$).
    In particular, $\dg M^{+\X,\mathit{hold}}$ has downprojections, so the cone data must satisfy $\mu_{\mat Y}(\mat Y) = \mu_{\mat X}(\mat Y)$ whenever $\mat Y \subseteq \mat X \subseteq \X$. In particular, this means all variables are determined by the particular marginal $\mu_{\X}$, pointing to the joint variable $\X$, which is present in $\dg M^{+\X}$ and $\dg M^{+\X,\mathit{hold}}$.  
    Such a distribution (and its induced marginals) creates a cone over 1 only if it matches the appropriate conditional probability distributions for these other arcs. 
    When $\Src a \cap \Tgt a = \emptyset$ for all $a$, that corresponds precisely to the requirement that $\mu$ matches all of the conditional marginals of $\mathbb P$ (i.e., $\mu \in \SD{\dg M}$). 
    On the other hand, if $\Src a \cap \Tgt a \ne \emptyset$ for some $a$, 
    e.g., for a self loop $p(X|X)$, 
    
    
    
\end{proof}

\commentout{%
\subsection{Colimits}
The colimits are arguably even more interesting: they summarize the most general thing that is known by all variables.

There's always a co-cone with vertex $1$, and there's a unique way to f
}%

\subsection{Natural Transformations}

Suppose $\dg M_1, \dg M_2 : \Ar^* \to \Meas_{\underline \Delta}$ are two PDGs generated by the same (hyper)graph $\Ar$.
What is a natural transformation $\eta: \dg M_1 \Rightarrow \dg M_2$? 

By definition, it is a collection of stochastic maps $\{ \eta_X :  \dg M_1(X) \to \dg M_2(X) \}_{X \in \N}$,
\unskip\footnote{
    Normally, we have been using the notation $\V_1(X)$ and $\V_2(X)$ for this concept, but for now we'll try this more traditional notation, and see if that works better.
}
satisfying the property that, for all $a : S \to T \in \Ar$,
\unskip\footnote{
    We have to verify this for all $a \in \Ar^*$, technically, but because $\Ar^*$ is a free category, it suffices to check it only for the generating arcs $a \in \Ar$.
}
the diagram
\[
\begin{tikzcd}[column sep=4em]
    \dg M_1 (X) 
        \ar[r,"\dg M_1(a)"]
        \ar[d,"\eta_X"]
    & \dg M_1 (Y)
        \ar[d,"\eta_Y"]
    \\
    \dg M_2 (X)
        \ar[r,"\dg M_2(a)"]
    & \dg M_2 (Y)
\end{tikzcd}
\qquad
%
\left(
~~
\parbox{2.2cm}{or, in the original notation,}
\qquad
\begin{tikzcd}
\V_1 X
    \ar[r,"\p_a"]
    \ar[d,"\eta_X"']
& \V_1 Y
    \ar[d,"\eta_Y"]
\\
\V_2 X
    \ar[r,"\p_a^2"]
& \V_2 Y
\end{tikzcd}
\quad
\right)
\]
commutes.  This is a diagram in the category $\Meas_{\underline\Delta}$. 

\temphide{%
I immediately have questions:

% \begin{quote}
\begin{enumerate}[itemsep=0pt]
\item What does the space of natural transformations from $\dg M_1$ to itself look like? 
\item What if $\dg M_1$ and $\dg M_2$ differ only in $\bbeta$? What is the effect of different encodings?
\item In what situations is there a natural transformation from one PDG into the other? 
\item What about for certain special PDGs?  What are some special PDGs with structure $\Ar$? 
\end{enumerate}
% \end{quote}


\fadeout{
\def\pdgunit{\dg I}
Fix a PDG $\dg M : \Ar^* \to \Meas_\Delta$. 
For a a measurable space $W$, let $\boldsymbol\Delta_W$ be the functor assigning each $N \in \ob \Ar^*$ to the measurable space $W$, and each arc $a$ to the identity map $\id_{W} : W \to W$. 
Can we characterize the natural transformations from $\pdgunit$ to $\dg M$?
}

}% end temphide


\paragraph{The relationship between relations and probabilities.}
There is a map $\Supp_X : \Delta X \to 2^X$ that takes a probability measure to its support set.  In fact, it is a natural transformation
\begin{center}
\begin{tikzcd}[column sep=huge]
\textbf{FinSet}
  \arrow[bend left=50]{r}[pos=0.4,name=U,label=above:$\Delta$]{}
  \arrow[bend right=50]{r}[pos=0.35,name=D,label=below:$2^{(-)}$]{} &
\textbf{Set}
  \arrow[shorten <=5pt,shorten >=4pt,Rightarrow,to path={(U) -- node[right] {$\Supp$} (D)}]{}
\end{tikzcd},
    \qquad
    since the diagram
    \qquad
\begin{tikzcd}
    \Delta X \ar[r,"\delta\! f"]
        \ar[d,"\Supp"']& \Delta Y \ar[d,"\Supp"] \\
    2^X \ar[r,"\bar f"] & 2^Y
\end{tikzcd}
    % \qquad 
    % \parbox{9cm}{
    % 
    % }
\end{center}
commutes for all $f : X \to Y$,%
\unskip\footnote{\textit{Proof:} 
% \begin{align*}
$
    y \in \Supp(\delta\!f(\mu)) 
    \iff y \in f^{-1}( \Supp(\mu))
    \iff y \in \bar f(\Supp(\mu))
$.
}
where
$\bar f(S) = \{ f(x) : x \in S\}$, often simply written as just $f$ to indicate the obvious extension of $f$ itself to subsets of $X$, is the application of the functor $2^{(-)}$ on $f$.


(( Can we use this to say something about how this interacts with IDef? What about $\H_\mu(Y|X)$ vs $\H_{\Supp \mu}(Y|X)$? ))
