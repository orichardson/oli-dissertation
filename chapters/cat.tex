% \subsection{A PDG is }
\subsubsection*{A Categorical Definition of a PDG}
Note that $\V$ is implicit in $\mathbb P$. 
The two can be expressed as a functor, which is arguably the most
compact definition of an (unweighted) PDG.  
% Let $\Stoch_\Delta$ be the Kliesli category formed
%
% \begin{defn}
    An \emph{unweighted PDG} $\langle\V, \mathcal P\rangle$ 
    over a structure $(\N, \Ar)$ is just a functor
    % $\mathrm{Paths}(\N,\Ar)$
    \begin{align*}
        {\mathbb P}  : 
            % \mathrm{Paths}(\N, \Ar)
            \Ar^* \to  \Stoch
    \end{align*} 
    whose action on objects $\N$ is $X \mapsto \V X$, and whose action on the generating morphisms $\ed aXY \in \Ar$
    is written
    % $p\ssub a(Y|X)$.
    $\p_a(Y|X)$. 
We drop the the symbol $\mathcal V$ in this context, using only the symbol $\mathbb P$, because $\mathcal V$ can be recovered by the action on the identity morphisms of $\Ar^*$. 
    % It is a functor between 
% \end{defn}   
% 
Given small category $J$ 
(such as the free category generated by a graph),
a functor $F : J \to \mathcal C$
is often called a \emph{diagram} of $\mathcal C$ (of shape J).
Therefore, an unweighted PDG is a diagram of the $\Stoch$, of shape
generated by its underlying hyprgraph.
In addition to probabilities, a PDG also contains 
confidences $\bbeta = \{ \beta_a \}_{a \in \Ar}$ about the reliability of those probabilities. 

% To get a clean categorical picture, we make two observations.
We now pursue a clean categorical picture of quantitative PDGs.
% It turns out that $\alpha$ can be replaced with edges of negative
% $\beta$ ---  and indeed, in the general setting of measurable spaces, something like this is necessary, because the entropy is not well-defined in this setting; we must use relative entropy anyway.
At a quantitative level, positive structural weights can be captured by negative observational weights.
This is because the gradient of $-\hat\nabla_\mu \H_\mu(Y|X)$, the gradient of the structural loss corresponding to a hyperarc $\ed aXY$, is the same as $+\hat\nabla\kldiv{\mu(X,Y)}{\mu(X) \lambda_Y(Y)}$, the gradient of the observational loss corresponding to a uniform distribution.
Furthermore, the weight $\beta_a$ may be absorbed into the cpd $\p_a$ by  dropping the requirement that measures be normalized. 
This is because the pair $(p(Y|X), \beta)$ as a
% subdistribution
can be encoded
\unskip\footnote{%\color{gray}
However, there is no way 
to combine $\beta$ with $p$ that results in a quantity $q$ (independent of $\mu$) that can be plugged directly into the ordinary expression for KL divergence:
\[
    \beta \log \frac \mu p = \log \frac \mu q \quad\implies\quad
        q = \mu^{1-\beta}p^\beta.
\]
[Zhu \& Rower] suggest that this is not the appropriate notion of relative entropy, for unsigned measures; instead, one should use $\kldiv \mu\nu := \int \log \frac{\mathrm d\mu}{\mathrm d\nu} \mathrm d\mu + \int \mathrm d\mu - \int \mathrm d\nu$, but even with this 
}
as a single conditional measure
$
    (1-e^{-\beta}) p(Y|X)
    % (e^{\beta}) p(Y|X)
    % \beta p(Y|X)
$
losslessly, because $p(Y|X)$ can be reconstituted by renormalizing, and $\beta = - \log (1-k)$ can be recovered from the normalization constant $k$.
The only exception is when $\beta=0$, but in this case the cpd does not matter sematically, and so if anything it is a bonus that this representation identifies all cpds supplied with confidence $\beta=0$.

Furthermore, with this representation, the effect of composition is very compelling.  Suppose we compose $p(Y|X)$ with confidence $\beta_1$ with $q(Z|Y)$ with confidence $\beta_2$, where both $\beta_1, \beta_2 \in [0,\infty]$.
Then the composite 
\begin{align*}
    r(Z|X) &= \int_Y   (1-e^{-\beta_2}) p(Z|Y) (1-e^{-\beta_1}) \mathrm d p(Y|X) \\
    &= (1-e^{-\beta_1} - e^{-\beta_2} + e^{-\beta_1-\beta_2})~(q \circ p)(Z|X) \\
    &\approx (1 - \exp(-\min\{ \beta_1, \beta_2\}))~(q \circ p)(Z|X).
\end{align*}
In particular, the composite will be fully trusted iff both components are $\beta_1 = \beta_2 = \infty$, and if either has confidence zero, then the composite will also. 
\unskip\footnote{Keep in mind that, even if $p(Y|X)$ and $q(Z|Y)$ are both marginals of a shared distribution $\mu(X,Y,Z)$, and this is known with extreme confidence, their composite will only be correct if the information is somehow ``independent''. 
This is where I think $\alpha$ should enter the picture, ideally.}
% taking a mapping from $\V \Src a$ to un-normalized measures over $\V \Tgt a$
% To summarize, all data in a PDG can be captured by a single 
As a result, all data in a PDG ($\Ar$, $\N$, $\balpha$, $\V$, $\mathbb P$, $\bbeta$) may be specified together with a single functor 
\begin{equation}
    \dg M : \Ar^* \to \Meas_{\underline \Delta}.
\end{equation}
In other words, a PDG 
% $\dg M : \Ar^* \to \Meas_\Delta$
is a \emph{diagram}, in the usual categorical sense, of conditional (sub)distributions between measurable spaces. 

% So, it makes sense to ask a number of things:
This connection induces a number of category-theory flavored questions about PDGs:
\begin{enumerate}
    \item A PDG $\dg M : \mathcal A^* \to \Meas_{\underline\Delta}$ is a diagram in the category of subprobability distributions. When does it have a limit? What about a colimit? What do limits and colimits of PDGs mean? 

    \item If PDGs are functors, what are the natural transformations between them? What do they mean? 
    % \item Perhaps relatedly, what are the representable PDGs? 
    \item How does inconsistency arise in this categorical picture? 
    \item Can we study qualitative PDGs separately in this picture? Why are $\balpha$ combined with $\bbeta$ if the former are purely qualitative? 
    
    
    \item 
    PDGs can be given semantics in more than one way, in principle --- relative entropy is a natural choice, but, even then, it can be used in either direction.  Yet this functorial definition of a PDG does not contain this information.
    So is there any way it can possibly interact with relative entropy that defines the semantics? 
    If so, what is the categorical picture of the role of relative entropy?
    \begin{itemize}
        \item For general loss functions (e.g., reverse KL),
         how does this picture interact with confidence functions?  
    \end{itemize}
\end{enumerate}



\subsection{Limits}

\begin{defn}
    Let $\dg M$ be a PDG, with variables $\X$. 
    The \emph{local marginal polytope}
     % $\mathbb L(\dg M)$
    % consists of all collections of ma
    \begin{equation}
        \mathbb L(\dg M) := 
            \Big\{ 
                \{ \mu_X \in \Delta \V X  
                \}_{X \in \X}
            ~\Big|~
                \forall \ed aST \in \Ar.~ \mu_T = \p_a \circ \mu_S 
            \Big\}.
    \end{equation}
    consists of all marginals over the variables $\N$ that are locally consistent with all arcs. 
    % We mention that, if $\Ar$ contains structural arcs 
\end{defn}

\begin{tikzpicture}[scale=1.2]
\begin{scope}[every node/.style={outer sep=0pt,inner sep=1pt}]
\node (dots-left) at (-2, 0) {$\cdots$};
\node (X1) at (-1,0) {$X_1$};
\node (X2) at (0,-0.2) {$X_2$};
\node (X3) at (1,0) {$X_3$};
\node (dots-right) at (2,0) {$\cdots$};
\end{scope}

\begin{scope}[red!30!gray!70!black]
\node (V) at (-0.5,1.2){$V$};
\draw[arr2] (V) -- (X1);
\draw[arr2] (V) -- (X2);
\draw[arr2] (V) -- (X3);

\node[below left=0.05 and 0.1 of V,inner sep=0pt, outer sep=0pt] {$\scriptstyle\forall$};
\end{scope}

\begin{scope}[blue!50!gray!60!white]
\node (L) at (0.5,1){{$L$}};
    \node[right=1pt of L,anchor=west,blue!30!gray]{$\scriptstyle\cong X_1 X_3$};
\draw[line width=4pt,draw=white] (L) -- (X1);
\draw[line width=4pt,draw=white] (L) -- (X2);
\draw[line width=4pt,draw=white] (L) -- (X3);
\draw[arr2] (L) -- (X1);
\draw[arr2] (L) -- (X2);
\draw[arr2] (L) -- (X3);

    \draw[arr2, densely dashed] (V) -- node[above]{$\scriptstyle\exists!$} (L);
\end{scope}
\draw[arr2, ->>] (X1) -- (X2);
\draw[arr2, ->](X3) -- (X2);
\end{tikzpicture}

\begin{example}
\begin{enumerate}
    \item  Suppose $\Ar = \{ \to X\}$, and its unique arc is 
\end{enumerate}    
\end{example}


Our next results are sensitive to the particulars of the PDG encoding as a functor. Let $\dg M^{+\X} := \dg M \cup \{ \mat X \to \mat Y \}_{\mat Y \subset \mat Y \subset \X}$
be the PDG $\dg M$ augmented with additional structure describing the relationships between all subsets of variables. That is, $\Ar^*$, as the free hypergraph over nodes $2^\N$, complete with structural coherence maps.
\unskip\footnote{
Note that these cohrerence maps do not include hyperarcs on these variables to put them back together, e.g., $\{ \{X\}, \{Y\}\} \to \{X,Y\}$.
Including such a map is appropriate only if one believes $X$ and $Y$ are independent.
}
Let $\dg M^{\mathit{hold}}$ be the PDG where $p(\mat Y|\mat X)$ is really attached to a hyperarc $\mat X \to \mat X \cup \mat Y$, implicitly the identity along $\mat X \setminus \mat Y$. 
Write $\dg M^{+\X,\mathit{hold}}$ for the PDG with both alterations.

\begin{theorem}
    Suppose $\dg M$ is a PDG in which every arc has full confidence. Then, 
    \begin{enumerate}[itemsep=0pt,topsep=0pt]
        \item $\mathrm{Cones}(\dg M, 1) \cong \mathbb L(\dg M)$;
        \item $\mathrm{Cones}(\dg M^{+\X,\mathit{hold}}, 1) 
            \cong \SD{\dg M}$. 
        
        \item $\lim \dg M^{+\X,\mathit{hold}} = \mathrm{Ext} \SD{\dg M}$, where $\mathrm{Ext}(S)$ is the set of extreme points of a set $S$ (e.g., the vertices of $S$, when $S$ is a polytope). 
            
        % \item Let $A \subseteq \N$ be an object of $\Ar^*$. 
        % The set $\mathrm{Cones}(\dg M, A)$ of cones over $\dg M$ with vertex $A$ consists of 
        \item $\lim \dg M = \mathop{\mathrm{Ext}} \mathbb L(\dg M)$. 
    \end{enumerate}
\end{theorem}

\begin{proof}
    Part 1 is immediate; it just points out that the local marginal polytope, defined in the graphical models literature, is the limit in this context.
        
    % Part 2 is also straightforward.
    Now for part 2. 
     A cone over $\dg M^{+\X,\mathit{hold}}$ with vertex 1 is a collection of distributions $\{ \mu_X(X) \}_{X \in \X}$ such that, for all $\ed aST \in \Ar$, 
    $\mu_T(S,T) = \p_a(T|S) \mu_S(S)$. (This familiar notation is not problematic if $S \cap T = \emptyset$, but otherwise we mean $\mu_T(S,T) = \int_{S} \p_a(S,T|s') \mu_S(s')\,\mathrm ds'$, properly overwriting variables in $S$ according to $p$).
    In particular, $\dg M^{+\X,\mathit{hold}}$ has downprojections, so the cone data must satisfy $\mu_{\mat Y}(\mat Y) = \mu_{\mat X}(\mat Y)$ whenever $\mat Y \subseteq \mat X \subseteq \X$. In particular, this means all variables are determined by the particular marginal $\mu_{\X}$, pointing to the joint variable $\X$, which is present in $\dg M^{+\X}$ and $\dg M^{+\X,\mathit{hold}}$.  
    Such a distribution (and its induced marginals) creates a cone over 1 only if it matches the appropriate conditional probability distributions for these other arcs. 
    When $\Src a \cap \Tgt a = \emptyset$ for all $a$, that corresponds precisely to the requirement that $\mu$ matches all of the conditional marginals of $\mathbb P$ (i.e., $\mu \in \SD{\dg M}$). 
    On the other hand, if $\Src a \cap \Tgt a \ne \emptyset$ for some $a$, 
    e.g., for a self loop $p(X|X)$, 
    
    
    
\end{proof}

\subsection{Colimits}
The colimits are arguably even more interesting: they summarize the most general thing that is known by all variables.

There's always a co-cone with vertex $1$, and there's a unique way to f

\subsection{Natural Transformations}

Suppose $\dg M_1, \dg M_2 : \Ar^* \to \Meas_{\underline \Delta}$ are two PDGs generated by the same (hyper)graph $\Ar$.
What is a natural transformation $\eta: \dg M_1 \Rightarrow \dg M_2$? 

By definition, it is a collection of stochastic maps $\{ \eta_X :  \dg M_1(X) \to \dg M_2(X) \}_{X \in \N}$,
\unskip\footnote{
    Normally, we have been using the notation $\V_1(X)$ and $\V_2(X)$ for this concept, but for now we'll try this more traditional notation, and see if that works better.
}
satisfying the property that, for all $a : S \to T \in \Ar$,
\unskip\footnote{
    We have to verify this for all $a \in \Ar^*$, technically, but because $\Ar^*$ is a free category, it suffices to check it only for the generating arcs $a \in \Ar$.
}
the diagram
\[
\begin{tikzcd}[column sep=4em]
    \dg M_1 (X) 
        \ar[r,"\dg M_1(a)"]
        \ar[d,"\eta_X"]
    & \dg M_1 (Y)
        \ar[d,"\eta_Y"]
    \\
    \dg M_2 (X)
        \ar[r,"\dg M_2(a)"]
    & \dg M_2 (Y)
\end{tikzcd}
\qquad
%
\left(
~~
\parbox{2.2cm}{or, in the original notation,}
\qquad
\begin{tikzcd}
\V_1 X
    \ar[r,"\p_a"]
    \ar[d,"\eta_X"']
& \V_1 Y
    \ar[d,"\eta_Y"]
\\
\V_2 X
    \ar[r,"\p_a^2"]
& \V_2 Y
\end{tikzcd}
\quad
\right)
\]
commutes.  This is a diagram in the category $\Meas_{\underline\Delta}$. 
I immediately have questions:

% \begin{quote}
\begin{enumerate}[itemsep=0pt]
\item What does the space of natural transformations from $\dg M_1$ to itself look like? 
\item What if $\dg M_1$ and $\dg M_2$ differ only in $\bbeta$? What is the effect of different encodings?
\item In what situations is there a natural transformation from one PDG into the other? 
\item What about for certain special PDGs?  What are some special PDGs with structure $\Ar$? 
\end{enumerate}
% \end{quote}


\fadeout{
\def\pdgunit{\dg I}
Fix a PDG $\dg M : \Ar^* \to \Meas_\Delta$. 
For a a measurable space $W$, let $\boldsymbol\Delta_W$ be the functor assigning each $N \in \ob \Ar^*$ to the measurable space $W$, and each arc $a$ to the identity map $\id_{W} : W \to W$. 
Can we characterize the natural transformations from $\pdgunit$ to $\dg M$?
}




\subsection{Additional Structure Preserved by PDGs}
\subsubsection{Monoidal Structure}

If we allow unions of variables at the qualitative level, this means we are working in a different category $\Ar^{**}$ whose objects $\ob \Ar^{**}$ are all subsets of $\N$, and equipped with down-projection morphisms, and joining hyperarcs.
Every PDG $\dg M : \Ar^* \to \Meas_{\underline\Delta}$ can be naturally lifted to a PDG 
$\dg M^{+\X}: \Ar^{**} \to \Meas_{\underline\Delta}$
in the obvious way: taking the additional joint variables to the appropriate product of measurable spaces, and treating their downprojections ($\mat X \tto \mat Y$, for $\mat X \supseteq \mat Y$) appropriately. 
We already saw one consequence of this change: the limits of such a PDG must be internally coherent in a certain way; the represent not the local marginal polytope, but the global marginal polytope.
% .  In particular, a cone over $1$ must contain a marginal $\mu_{\X}$ over all variables that must agree with all other marginals.  So, in this context, it suffices to summarize the limit with this one distribution. 

\paragraph{Variable Union as Monoidal Structure, for $\Ar^{*,\mathit{hold}}$}
Usually people call the a monoidal operation ``tensor'', but we will now define a monoidal operation that does not line up with the usual tensor product. 
On objects, which are sets of variables, $\odot$ behaves like union, and on morphisms, it simply multiplies densities.
% $\ovee$
\begin{align*}
    A \odot C &= A \cup C; \\
    \odot( p(B|A), q(D|C) )
    &:= p(B|A) \cdot q(D|C)\\
    &= (a_0, y, c_0) \mapsto 
        \Big(
        (b_0, z, d_0) \mapsto p(b_0, z|a_0,y) q(d_0,z|c_0,y)
        \Big).
\end{align*}
where $z$ gives the values of the common variables $A \cap C$, so that $a = (a_0, z)$ and $c = (c_0,z)$.

If $A$ and $C$ are disjoint, as are $B$ and $D$, then $\odot$ coincides with the tensor product $\otimes$. 
When they share variables, the resulting operation does something different: instead of having two distinct copies of eacn input and output, $p \odot q$ takes in only one copy of each shared sources, and produces a density over only one copy of their shared targets. 
For example $p(X) \odot p(X)$ is the subprobability $p(X)^2$. 
It is worth noting that a morphism $p(Y|X)$ is idempotent, in the sense that $p(Y|X) \odot p(Y|X)$ iff it is a deterministic function. 

Although $\odot$ is well-defined, it is not functorial in all cases,
\unskip\footnote{For example, $p(Y|X) \odot p(Y|X) = p^2(Y|X)$ which is a strict subprobability measure, while $(q \circ p)(Z|X)$ and $(r \circ p)(W|X)$ are both cpds, and $(q \circ p) \odot (r \circ p)$ will also be a cpd on $W \cup Z$, supposing that  $W$ and $Z$ are disjoint. So it cannot be the case that $(q \circ p) \odot (q \circ)$
}
and hence inadmissible as the basis of a monoidal structure for the full category of stochastic morphisms
.
However, if we restrict to the subcategory of purely generative morphisms---%
that is, arcs $a$ satisfying $\Tgt a \supseteq \Src a$---then $\odot$ becomes functorial. 

{\color{red} CAREFUL! This messes up the types of composition! If $f : X \to Y$ is converted to $f' : X \to XY$ and $g : Y \to Z$ is converted to $g': Y \to YZ$, then $g' \circ f'$ is not defined (because $XY \ne Y$) and so composition cannot proceed, without first including a forget/downprojection map!}

There are also many other properties that we must verify in order to get a symmetric monoidal category; we now verify them.

\begin{itemize}[wide,]
\item \textbf{Functoriality.}
% The left and right are symmetric.
% Suppose $p(Z|X) = f(Z|Y) \circ g(Y|X)$, and take any $h(B|A)$. 
% We need to show that $p \odot h = (f \odot h)$ 
We need to show that
\[
\left(
\begin{tikzcd}
    X_1 \ar[d,"f_1"'] \ar[dd,bend left,"p"] \\ 
    X_2 \ar[d,"f_2"'] \\
    X_3
\end{tikzcd}
\right) 
\odot
\left(
\begin{tikzcd}
    Y_1 \ar[d,"g_1"'] \ar[dd,bend left, "q"] \\ 
    Y_2 \ar[d,"g_2"'] \\
    Y_3
\end{tikzcd}
\right) 
= 
\begin{tikzcd}[row sep=5ex]
    X_1 \cup Y_1 \ar[d,"f_1 \odot g_1"description]  \\ 
    X_2 \cup Y_2 \ar[d,"f_2 \odot g_2"description] \\
    X_3 \cup Y_3
\end{tikzcd}.
\]
As mentioned above, this is not true in general. But we have assumed that $X_1 \subseteq X_2 \subseteq X_3$ and $Y_1 \subseteq Y_2 \subseteq X_3$.
To simplify notation, let's redefine $X_3 \gets X_3 \setminus X_2$ and $X_2 \gets X_2 \setminus X_1$. 
In this new notation, our goal becomes proving the commutativity of the following diagram:
\[
\left(
\begin{tikzcd}
    X_1 \ar[d,"f_1"']  \\ 
    X_2 \cup X_1\ar[d,"f_2"'] \\
    X_3 \cup X_2 \cup X_1
\end{tikzcd}
\right) 
\odot
\left(
\begin{tikzcd}
    Y_1 \ar[d,"g_1"'] \\ 
    Y_2 \cup Y_1 \ar[d,"g_2"'] \\
    Y_3 \cup Y_2 \cup Y_1
\end{tikzcd}
\right) 
= 
\begin{tikzcd}[row sep=5ex]
    X_1 \cup Y_1 \ar[d,"f_1 \odot g_1"description]  \\ 
    X_2 \cup X_1 \cup Y_2 \cup Y_1 \ar[d,"f_2 \odot g_2"description] \\
    X_3 \cup X_2 \cup X_1 \cup Y_3 \cup Y_2 \cup Y_1
\end{tikzcd}.
\]
% Let $X_2^- := X_2 \setminus X_1$, $X_3^- := X_3 \setminus X_2$, and define $Y_2^-, Y_3^-$ analogously.
% We can then define
We now compute
\begin{align*}
    (f_2 \circ f_1) (x_1'')(x_1,x_2,x_3) 
         &= \iint_{X_2,X_1} f_1(x_2',x_1'|x_1'') f_2(x_3,x_2,x_1|x_2',x_1') \mathrm d x_1' \mathrm d x_2' \\
         &= \iint_{X_2,X_1} f_1(x_2'|x_1'') 
            f_2(x_3|x_2) \delta(x_1) \mathrm d x_1' \mathrm d x_2' \\
\end{align*}

\item 


\end{itemize}


\subsubsection{(Pre)additivity}

\subsection{The Category of PDGs}

        
\section{Dependency Graphs for Other Monads}

The Big Questions:
\begin{enumerate}
    \item How does the monadic view of composition (bind / multiply), which describes composition in the underlying Kleisli category, interact with the ``scoring function semantics''? 
    \item Is there an important shared feature among the monads $T$ for which analogues of PDGs work out?  To set up the scoring semantics, it seems we need
    \begin{enumerate}
        \item a way to quantify ``degree of funtional dependence'' along an arc $S \to T$, in the limit object $\lim \mathcal M$, and
        \item a way to quantify degree of between $T(X,Y,Z)$ and $X \to T(Y)$.
        
        If there is an analogue of marginalization, then there is a map $T(X,Y,Z) \to T(X,Y)$, and there is a 
    \end{enumerate}
    
    \item Since most monads do not construct continuous geometry as nicely as the probability monad, relations are not continuous, there is no obvious analogue of parallel, symmetric, ``mixture composition''. Even when we give a loss function semantics (which we do below) this does not correspond to an obvious computational picture in the same way. 
\end{enumerate}

\subsection{Relational Dependency Graphs}

We will use $\mathcal P$ do denote the relational monad. 


\paragraph{Scoring Function Semantics.}
Suppose that instead of mapping $\ed aXY$ to a conditional probability
$\p_a(Y|X)$, we instead map it to a relation $R_a(X,Y)$. 

The analogue of a scoring function might operate
on a universal reation $U \subseteq \V\!\X$, and look something like:
\begin{align*}
    \Inc( U ) &= \sum_{\ed aXY \mathrlap{\in \Ar}} \beta_a 
        % \thickD(U(A,B) || R_a(A,B))
        \Big\Vert U(X,Y) - R_a(X,Y) \Big\Vert_1
        \\
    &\quad\text{where}\quad
    \Big\Vert U(X,Y) - R_a(X,Y) \Big\Vert_1 = 
        % \sum_{x,y} \mathbbm1[R(x,y) \exists \mat z.~ U(x,y,\mat z)] |
        \#\Big\{(x,y) \in \V(X,Y) \,\Big|\, R(x,y) \Leftrightarrow \exists\mat z.~ U(x,y,\mat z) \Big\},
\end{align*}
with each $\beta_a \in \mathbb N$. 

The analogue of a qualitative arrow $\ed aXY$, indicating that one attribute determines another, also requires a scoring function.
The fact that the argument to $\IDef{\Ar,\balpha}$ 
    is a joint distribution may not be critical,
    so long as we can find a suitable replacement notion of uncertainty along an arc.
%     not critical to the calculation. 
% What \emph{is} important is that $\mu$ is associated with a
% notion of uncertainty $\H_\mu(T|S)$ along each $\ed aST$.
% For example, $\mu$ could be a database, 
% n place of a joint distribution, we could have a 
% universal $D \subseteq \V\!\X$, with 
One possible analogue of conditional entropy might then be
\begin{align*}
    \H_U(Y|X) &= \log 
    % \#\{ 
    % \mat x \in D.~~  
        % S(\mat x) 
    % \}
    (\text{maximum \# of possible values of $Y$ given $X$}) \\
        &= \max_{x \in \V X}
            \log \# \{ y \in \V Y ~|~
                \exists 
                \omega \in U.~~
                \Src \omega = s,\; \Tgt \omega = t
            \}.
        \\
        &= \max_{x \in \V\! X} \log \#\{ y \in \V Y \mid \exists \mat z.~U(x,y,\mat z) \}.\numberthis\label{eq:rel-condl-ent-analogue}
\end{align*}
This shares an important property with conditional entropy: it is zero iff the value of $Y$ is determined by $X$ in the relation.
%
% If $U$ is empty, then this quantity will be is non-negative
% If there are 
It is undefined when $U$ is empty, and otherwise non-negative. 
The other important property of conditional entropy is monotonicity with respect to weakening.
\unskip\footnote{%
    Clearly an extra target to a hyperarc (i.e., extending $Y$ to $Y' = (Y, Z)$) can only make \eqref{eq:rel-condl-ent-analogue} larger. 
Perhaps less obviously: it also has the property that adding an extra source (i.e., extending $X$ to $X'= (X,Z)$) can only reduce the value of \eqref{eq:rel-condl-ent-analogue}.
This is because if the maximum is over joint pairs $(x,z)$, then only the maximum number of $\{ y : (x,y,z) \in U\}$ contribute, while all such $z$ are amalgamated and make the number larger in the case of an existential quantifier.
}

Altogether, the analogue of $\IDef{}$ is 
\begin{align*}
\IDef{\Ar}^*(U) :=
    \sum_{a \in \Ar} \max_{x \in \V\! X} \log \#\{ y \in \V Y \mid \exists \mat z.~U(x,y,\mat z) \}
    - \log \# U
\end{align*}

Here, as in the probabilistic case, there is an interpretation in terms of storage costs. 
Suppose $U$ is fixed and known. The first term is the number of bits needed to specify separately each target given the source (knowing that the result is in $U$), while the second is the number of bits needed to specify an element of $U$ directly.
The value is undefined iff $|U| = 0$ because, intuitively, it is impossible to specify a joint setting $\omega \in U$ if $U$ is empty. 


Now, for some examples.
\begin{example}
    Suppose $\Ar = \{ {\to} X, \; Y{\gets} \}$. Then 
    $\IDef{\Ar}^*(U(X,Y)) \ge 0$ with equality iff $U(X,Y) = U_X(X) \bowtie U_Y(Y)$, for some unary relations $U_X(X)$ and $U_Y(Y)$. 
\end{example}

More generally, it can be shown that, for target-partitinal hypergraphs $\Ar$ without sources, the quantity $\IDef{\Ar}^*(U)$  measures how far a joint relation $U$ is from decomposing independently along the specified arcs. 

\begin{prop}
    Let $\{\X_1, \ldots, \X_n\}$ be a partition of $\X$, and $\Ar = \{\to \X_i\}_{i =1}^n$ be the hypergraph consisting of a single hyperarc pointing to each partition, each without any sources. 
    In this case, $\IDef{\Ar}^*(U) \ge 0$, with equality iff
    $U = U_1 \bowtie \cdots\bowtie U_n$ is the natural join of $n$ subrelations $U_i \subseteq \V(\X_i)$. 
    (In addition, in that case it must be that each $U_i = \prod_{\X_i}(U)$ is the projection of $U$ onto the variables $\X_i$.)
)
\end{prop}

Let's turn to some more complicated examples. First, let's start with overlapping targets, and keep everything unconditional. 
It is easy to see that
\[
\IDef{[{\to}X{\gets}]}^*(U(X)) = \log |U|,
\]
which is defined when $U \ne \emptyset$, non-negative on this domain, and equal to zero if and only if $U(X)$ is a singleton. Thus, it captures determinism (almost) exactly the same way as in the probabilistic case. 
Now, for a more complicated example. 

\begin{example}
    In the probabilistic setting, all $\mu(X,Y)$ are compatible with the hypergraph $[{\to} X {\to}Y]$, intuitively because optimal codes for specifying $(x,y) \sim \mu$ directly take the same amount of information as optimal codes to first specify $x$, and then use a code dependent on $x$ to specify $y$. 
    
    In this relational setting, the two are not always the same.
    % \begin{enumerate}
    %     \item
         If $U = \{(x,y) : x \in S, y = f(x) \}$, then
        $\H_U(X) = \log |S|$ and
        $\H_U(Y|X) = 0$, since there is always precisely 1 $y$ for which $(x,y) \in U$. Thus $\IDef{[{\to}X{\to}Y]}^*(U) = 0$. 
        Under the cost-of-storage interpretation: this means when $U$ can be generated by selecting a subset of $S$ and applying a (known) function, then specifying a joint sample $(x,y)$ requires the same number of bits as specifying $x$.
        
    A generalization of this holds for what might be called ``$k$-multi-functional'' relationships, where $k \ge 1$ is some natural number. Suppose $f: X \to Y^k$ produces $k$ distinct values of $Y$ for each $x \in X$. 
    If $U = \{ (x,y) : x \in S, y \in f(x) \}$, then $\H_U(Y|X) = \log k$, but also $|U| = k |S|$. 
    So again $\IDef{[{\to}X{\to}Y]}^*(U) = 0$.
    This is becaus, again, it takes the same number of bits to first specify a value of $X$, and then use that value to specicfy a value of $Y$, as it does to specify $(x,y)$ together. 
    It can be shown that, as a function of non-empty relations $U$, the value of $\IDef{[{\to}X{\to}Y]}^*(U)$ is non-negative and zero precisely if $U$ is of the form described above. 
    % \end{enumerate}
    
    This is beause, when $f$ may produce a variable number of points depending on $x$, $\IDef{[{\to}X{\to}Y]}^*(U)$ will be positive overall. So, when $x$ is such that $|f(x)|$ is maximal, then specifying fist $x$ and then the appropriate $y$, is less efficient than specifying $(x,y)$ together. 
\end{example}

More generally, for directed graphs, we have an analogue of a conditional independence.

\begin{defn}
    Suppose $A,B,C \subseteq \X$. In a relation $R(\X)$, $A$ and $B$ are said to be conditionally independent given $C$ 
    (symbolically, $R \models A \CI B \mid C$)
    iff 
    % for all $c \in \V C$, 
    \[
    \forall (a,b,c) \in \V(A,B,C).\qquad
    R(a,b,c) ~\iff~ \Big(~\exists a' \in \V (A).~ R(a',b,c) \land \exists b' \in \V (B).~R(a,b',c) \Big)
    .
    \]
    We write $A \CI B$ to abbreviate $A \CI B \mid \emptyset$, i.e., the special case where there are no given variables.
    % $\forall c \in \V C.~R(a,b,c) \$
\end{defn}

\begin{prop}
    $R(A) \models A \CI A$
\end{prop}


\begin{prop}
    Suppose $A,B,C$ are sets of attributes. 
    $R \models A \CI B \mid C$ 
    iff 
    $R \models (A \setminus C) \CI (B \setminus C) \mid C$.
\end{prop}
\begin{proof}
    First, we claim that, for all $a,b,c \in \V(A,B,C)$, we have that $R(a,b,c) \iff R(a[A\setminus C], b[B\setminus C],c)$.
    If $\{a,b,c\}$ agree on shared values, then $R(a[A\setminus C], b[B\setminus C],c)$ must equal $R(a,b,c)$, by definition.
    On the other hand, if $\{a,b,c\}$ do not agree on shared values, then $R(a,b,c) = 0$, and there are three possibilities for conflict.
    % and by definition there is some variable $X \in A \cup B \cup C$ such that. 
    If this is because $a$ and $c$ conflict, then it is possible that $R(a[A\setminus C],b,c) = 1$. {\color{red} contradicing our claim!}
    
    Let $A' := A \setminus C$ and $B' := B \setminus C$. 
    
        
    Suppose $R \models A \CI B \mid C$, meaning that for all $a,b,c$, 
    \begin{align*}
        R(a,b,c)
         &\iff \exists a'' \in \V (A).~ R(a'',b,c) \land \exists b'' \in \V (B).~R(a,b'',c) \\
        &\iff \exists a'' \in \V (A).~ R(a''[A\setminus C],b[B\setminus C],c) \land a''[A\cap C] = c[A \cap C]
            \\&\qquad\quad\land \exists b'' \in \V (B).~R(a[A\setminus C],b''[B\setminus C],c) \land b''[B\cap C] = c[B \cap C] 
            \\
        &\iff \exists a' \in \V (A').~ R(a', b[B'], c) 
            \land \exists b' \in \V (B').~R(a[A'],b',c).
    \end{align*}
    % $R(a,b,c)$ if and only if $\exists a' \in \V (A).~ R(a',b,c) \land \exists b' \in \V (B).~R(a,b',c)$.
    {\color{red} incomplete; possibly untrue}
\end{proof}

\begin{prop}
    % Suppose $\X_1, \X_2, \X_3$ are disjoint.
    $R(\X_1) \bowtie S(\X_2) \models \X_1 \CI \X_2 \mid \X_1 \cap \X_2$.
\end{prop}
\begin{proof}
    
\end{proof}

\begin{prop}
    If $G$ is a directed acyclic graph, then 
    $\IDef{\Ar_G}^*(U) \ge 0$, with 
\end{prop}

\begin{example}
    Now, consider the 2-cycle
\end{example}


\bigskip

\paragraph{The relationship between relations and probabilities.}
There is a map $\Supp_X : \Delta X \to 2^X$ that takes a probability measure to its support set.  In fact, it is a natural transformation
\begin{center}
\begin{tikzcd}[column sep=huge]
\textbf{FinSet}
  \arrow[bend left=50]{r}[pos=0.4,name=U,label=above:$\Delta$]{}
  \arrow[bend right=50]{r}[pos=0.35,name=D,label=below:$2^{(-)}$]{} &
\textbf{Set}
  \arrow[shorten <=5pt,shorten >=4pt,Rightarrow,to path={(U) -- node[right] {$\Supp$} (D)}]{}
\end{tikzcd},
    \qquad
    since the diagram
    \qquad
\begin{tikzcd}
    \Delta X \ar[r,"\delta\! f"]
        \ar[d,"\Supp"']& \Delta Y \ar[d,"\Supp"] \\
    2^X \ar[r,"\bar f"] & 2^Y
\end{tikzcd}
    % \qquad 
    % \parbox{9cm}{
    % 
    % }
\end{center}
commutes for all $f : X \to Y$,%
\unskip\footnote{\textit{Proof:} 
% \begin{align*}
$
    y \in \Supp(\delta\!f(\mu)) 
    \iff y \in f^{-1}( \Supp(\mu))
    \iff y \in \bar f(\Supp(\mu))
$.
}
where
$\bar f(S) = \{ f(x) : x \in S\}$, often simply written as just $f$ to indicate the obvious extension of $f$ itself to subsets of $X$, is the application of the functor $2^{(-)}$ on $f$.


(( Can we use this to say something about how this interacts with IDef? What about $\H_\mu(Y|X)$ vs $\H_{\Supp \mu}(Y|X)$? ))
