% What is the problem I'm solving?

%%% POINTS TO TOUCH ON %%%
% -  communities are not self consistent.

\section{In Defense of Inconsistency}
% \section{In Defense of Our Villain}

\begin{center}
    \noindent\singlespacingplus
% \begin{quotation}
    \it
    ``A man with one watch always knows the time;\\
    a man with two watches is never sure.''
% \end{quotation}
\end{center}

% Science is built on simple tenet: if you want to someday understand things clearly, then it is important to acknowledge the possibility that your understanding today could be wrong.
%
% Science is built on a simple tenet: if you want to someday understand things clearly, then it is important to acknowledge the possibility that your understanding today could be wrong.
% Science :
% There is a central principle of scientific inquiry: 
A central principle of scientific inquiry is this:
    if you want to eventually understand things clearly, then you must take seriously the possibility that your understanding today might be wrong. 
And not only might you be wrong in the margins---wrong about the details of a calculation or under-informed---you could have a fundamentally misguided understanding of how the world works. You could be using meaningless concepts, confusing cause and effect, and misplacing your trust in those that seek to manipulate you. 
For those of us who prize logic and rationality (including, famously, Descartes \cite{descartes}), perhaps most terrifying of all is the possibility that your own internal thoughts might not even be self-consistent.
If there is one thing on which we can all agree, it is a contempt for inconsistency.
The foundational principles of microeconomics entitle us to our own tastes (and beliefs, to some extent), but only if they admit a consistent internal logic.
For without one, we risk being swindled \cite{RAT} or manipulated \cite{explosion}, if not humiliated \cite{logicalfalacy,emptysetthesis,politicians}.

That is why theoretical computer scientists are careful to prove things about clearly defined models,
and it is why applied computer scientists often build artificial agents that are \emph{consistent-by-construction}.
%
% When given a 
% Given a choice between two libraries for manipulating time and one, we choose the one. 
% All else being equal, we'd rather have one way of computing things than two.
We would rather have one definition of $X$ and one technique for computing $X$, than several different approaches and implementations. 
We would rather use one library for telling time than two. 
And no matter how many parameters we add, the goal is to parameterize a single coherent probability distribution. 
%
Why? Because computing things in just one way grants an invulnerability to our one common enemy: inconsistency.

Yet this invulnerability comes with shackles.
Would \emph{you} trade \emph{your} freedom to see things from two angles at once---to puzzle over paradoxes, to experience deep surprise and be challenged by internal conflict---for a guarantee of self-consistency?
%
In order to always present a consistent front, it is necessary to fully process new information before moving on. Mulling is prohibited, as
is waiting for clarification.
Half-baked ideas are a nonstarter.
This regimented consistency-first lifestyle works well if you are only mistaken in the margins, and need only to occasionally update a few parameters.
But what if you're wrong in a deeper way?
Can you imagine how hard it would be to learn to see things completely differently, without ever venturing into a confused state?
Imagine having to implement a substantial refactor of a large codebase, with an interface that only allows you to see code that compiles.
So if (through no fault of your own) you do happen to be wrong about how the world works---that is, really, fundamentally wrong---then taking on inconsistency may be worthwhile.

% By way of analogy, we all agree debt is bad, but would you willingly give up your freedom to take on debt?
% Many of the most powerful people and institutions in this world would not.
% Running a budget surplus works well, until you fall ill or need to buy a house.

In some sense, the opposite of inconsistency is \emph{redundancy}, which engineers and computer scientists agree is generally positive. 
% Another problem consistent by construction is that they also have no redundancy, which makes them fragile.
% Apparently unfazed by the prospect of representing inconsistency, the cryptographers distributed systems people 
But consistent-by-construction representations are also redundancy-free, by construction. 
For if we have two different representations that could conceivably agree, producing redundancy, then those two representations could conceivably be inconsistent with one another. 
Without opportunity for internal conflict, there can be no opportunity for internal agreement. 
% The only way to eliminate the possibility  opportunity for internal conflict, then there is also no opportunity for internal agreement. 

% For analogous reasons, 
Overlapping and redundant representations are incredibly useful in practice for guarding against catastrophic failure. 
For this reason, modern AI systems have been slipping away from the probabilistic consistency that they aspire to.
For instance, it is expensive and even undesirable to exactly model your training data; instead we try to do the opposite, by adding priors (also known as regularizers; see \cref{sec:regularizers}) to prevent this.
The priors are typically inconsistent with the data (which is arguably the point), although neither is ever modified. 
Still, the result is a far more robust learning process, both theoretically \cite{ftrl} and practically \cite{regularizers}.
%
% (maybe more?)
% Perhaps clearer examples arise when people use multiple networks together, such as  
Even clearer examples of inconsistency arise in variational inference
    (e.g., mean field approximation methods, variational autoencoders \cite{kingma2013autoencoding}),
    and when combining multiple networks 
        in more complicated ways \cite{}.
    In some cases, there is even a term representing ``consistency'' in the learning objective \cite{cycle-gan}. 
In all of these, the goal is to be (mostly) consistent, but seldom is it possible to actually get there,
and tolerating inconsistency is critical for the learning process.
%
These techniques are billed
% to our Bayesian overlords 
    as pragmatic ways of approximating
    % (i.e., cheap knockoffs) 
    % to the consistent thing we really mean.
    some presumably consistent we must have intended.
But why perpetuate these unrealistic rationality standards?
I argue that we should just call them what they are: (possibly) inconsistent belief states.

Doing so intelligently unlocks a beautiful theory of probabilistic modeling, that unifies a broad range of concepts, representations, and algorithms across artificial intelligence, and 
    % the theory of rational mind. 
    the study of rational belief and decision making.
The framework developed in this dissertation
gives us the ability to model, measure, and mitigate inconsistency.
    % comes with an enormous amount of 
This affords an agent an enormous amount of epistemic flexibility,
    making it far easier to make deep structural changes to its epistemic
    representations.
So, in an ironic twist, the possibility of internal inconsistency might ultimately be what saves you from being deeply wrong. 
%
Make no mistake: inconsistency is still bad.
% Indeed, it is the one true villain
% In fact, it is the only thing we view as bad.
Indeed, our measure of it is the only conception of ``bad'' needed to do much of machine learning (\cref{chap:one-true-loss,chap:LIR}).
There is no question that chronically exhibiting a high degree of inconsistency is problematic.
%
Yet, as with all internal conflict, it is better to engage with it and recognize it as a catalyst for positive change
than to avoid it entirely. 

% People, institutions, cultures, institutions of all shapes and sizes, 
%
Is really worse to be gullible than it is to be stuck and very wrong? 
It is said that a man with two watches never knows the time.  
But is a man with only one watch really better off?
And how are you supposed to use multiple watches, anyway?
% In the coming chapters, we will see. 
% We will soon see. 
% Join me, and together we will soon see. 
Let me show you. 


\section{Themes and Motifs}

\newmaterial{%
Before we get to the primary thread of the dissertation
(which we will overview in \cref{sec:overview-of-results}),
we first mention some recurring themes.
}% 


%
% \subsection{Equivalent Representations}
% \subsection{Equivalent Representations, Surprisingly}
\subsection{Equivalent Representations}

% Hilbert's Hotel \cite{hilbert-hotel} is an 

% Suppose I learn that 
When working with abstract representations, often it is the case that one class of models (representation $R_1$) seems clearly more general than another (representation $R_2$).
To prove this, you show that anything with representation $R_2$ can be converted to $R_1$; perhaps $R_2$ is even literally a subset of $R_1$. 
You convince yourself that the relationship is strict, because very little of $R_1$ is in the image of this transformation---and besides, how could you possibly represent a generic element of $R_1$ with $R_2$? 
Suppose that, sometimes later,
 you discover that there is in fact a much more subtle way of capturing $R_1$ with $R_2$. 
So, despite appearances, $R_1$ turns out to be equivalent to $R_2$. 

To illustrate, consider an example from \cref{chap:prelim}. 
Bipartite graphs are clearly a very special class of graphs, with special properties; not every graph is bipartite. 
Also, hypergraphs clearly are generalization of graphs; only the special case where hyperedges have two vertices correspond meaningfully to graphs. 
So, chaining these observations together, one might reasonably conclude that bipartite graphs are an \emph{extremely} special case of hypergraphs---yet, as we will see, bipartite graphs and hypergraphs are naturally isomorphic. 
% This pattern will be a recurring theme in this dissertation. 


% Here is another, more abstract example : verbs and nouns.
% \TODO


% Another example is that (fully) randomized PSEMs are equivalent to ordinary PSEMs (\cref{sec:causal}).
Probabilistic Dependency Graphs (PDGs), the central mathematical object whose theory that we develop in this dissertation, are a particularly special representation in this regard. 
Not only can PDGs capture a great many things 
% (\cref{chap:pdg-repr,chap:repr-tools})
\unskip, but they also capture most of the ``obvious generalizations'' of them that we could have wanted. 
If this abstract statement seems puzzling, recall that Turing machines (TMs) not only capture other notions of computation, but also capture many variants of TMs aimed at extending their expressive power: ``generalized'' TMs that have multiple tapes, multiple heads, tapes of higher dimension, and even non-determinism---are all (semantically) equivalent to ordinary Turing Machines. 

% \commentout{%

\subsection{Mathematical Precision for Informal Reasoning}

% definitional layering
 % good representation is that things that are actually different 

People---and I am certainly no exception---are often fast and loose with their notation and their math, especially when talking about complicated topics with many layers of definitions. 
%
The hallmark of a good formalism combined with good notation is that it supports a user in manipulate the concepts quickly and intuitively, yet at the same time is precise enough to withstand critical scrutiny.
%
Correspondingly, good representations, and tools for manipulating representations 
    have two critical properties:
\begin{enumerate}
    \item things that are different look different, and
    \item things that are the same look the same. 
\end{enumerate}
The first aids precision, and the latter aids ease of use. 
% In math, it is common to use 
% Often in mathematics, it is common to 
It is very common for computer scientists and mathematicians 
    % have an urge to prove their precision
    \unskip,
    in pursuit of precision,
    to focus on distinguishing things that are even superficially different.  
Failure to do so called ``abuse of notation''. 
%
We do it anyway, in service of making formalisms easier to work with. 
But in some deep way, the notation is not really being abused if the two objects are really two different views of the same thing.
% What is really important is 

The idea can be described more precisely as etiquette for \emph{implicit conversion}.
Whenever we define ways to implicitly regard objects of type $A$ to objects of type $B$, and a way of implicitly regarding objects of type
    $B$ as objects of type $C$, then 
    we have also defined way of implicitly converting an object of type $A$ to one of type $C$.
%
To avoid confusion, it is not strictly necessary to avoid implicit conversions---just to ensure that all ways implicitly converting $A$ to $C$ are equivalent. 
%
Doing so can be challenging, but it is often the best way to get representations that simultaneously attain both 1 and 2. 
%

Avoiding the challenge altogether----ensuring objects of different types of different types are given different symbols---is 
% a direct analogue of forcing yourself to be consistent by construction. 
\newmaterial{%
    one way of ensuring consistency by construction.
This practice is precise and grants a certain peace of mind, but makes it difficult to think. 
}%
%
In keeping with the theme of this dissertation, I have not shied away from defining the same symbol in more than one way---but I have tried to do so in a way that the definitions never disagree in any context. 
%
% After all, that is the role of a theorem: to give two different 
% symbol to have more than one formal meaning. 

% Two particularly successful representations, for which I have only the highest respect, are for are string diagrams for operating with symmetric monoidal categories, and 
% To my eyes, a primary strength of the formalism we develop in this dissertation 
% This is a sign that the formalism could be improved.
% The whole point of formalism is to support reasoning and thought, 
%     in a way that aids precision; if it is too cumbersome to use properly,
%     perhaps we would be better off with another formalism.
    
% Either it is too cumbersome to spea

The representation that lies at the heart of this thesis, called a probabilistic dependency graph (PDG), enables this kind of thinking in the probabilistic setting.
They enable us to manipulate probabilistic primitives in ways that look just like standard intuitive short-hand, yet the machinery underneath makes the reasoning completely precise.
Some of the key reasoning principles have congealed to form \cref{chap:reason},
but a keen reader will see examples of how PDGs formalize intuitive probabilistic reasoning throughout. 
But when we introduce probabilities, consistency is not just about ensuring paths are equal 
    (as is the case for implicit conversions); it's about something else. 
Making that something else precise is one objective of this thesis. 
% It is a powerful way of formalizing these kinds of thought patterns for probabilistic reasoning. 
% It aims to simultaneously 
%


% \subsection{A Division between Quantitative and Qualitative}
\subsection{Qualitative and Quantitative}

% \TODO[ TODO: fix this subsection; it is still very rough ]
% on two levels: a high-level structure
% There is an important conceptual division,
Broadly speaking, most modeling happens at two different levels: 
    at an abstract conceptual level, and a concrete 
The division between the two levels goes by different names in different communities. 
% Here, we will largely call the distinction one between \emph{qualitative } 
In computer science, perhaps the most common
    widely appreciated version of this phenomenon is 
    the difference between types and values.
A few others have are below.

\begin{center}
    \begin{tabular}{c|c}
        Qualitative & Quantitative \\ 
        Structural & Observational \\
            \hline
        Values & Types \\ 
        Theorems & Proofs \\
        Definitions & Examples \\
        Schemas & Instances 
    \end{tabular}
    \end{center}
    
Following the nomenclature of the graphical models community, we will refer to it principally as the divide between \emph{qualitative} and \emph{quantitative} information 
% (although both can be numerical)
\unskip, or alternatively as the divide between \emph{structural} and \emph{observational} information (see \cref{sec:pgm-prelim}).
The name of a variable, and the names of the variables on which it can depend are qualitative information; the set of values it can take, and its marginal distribution are quantitative information. 
%
Both kinds of information are important, and they are fundamentally very different. The former is about the graph structure, and the latter is about its annotation. Put differently,
% (as we will see in \cref{chap:PDG-cat}), 
the former is a conceptual shape, and the latter interprets that shape in the language of probabilities and confidences. 

% One unusual feature of PDGs is that the qualitative information has a numerical weight. 
% Different experts may have different relative strengths. For instance, one might want to trust one person to design a study to determine what causes what, and another person to independently design experiments aimed at determining the numerical probabilities. 

This interplay and division between these two kinds of information will be a recurring theme in this work.

% Understanding the difference between these two 
% Corresponding 
% We will see 

\commentout{
For expressive enough languages, the lines become blurred. Dependent types require one to think about values, leading to can get runtime errors at compile time. 
PDGs, too, blur this line.  We will see, for example, that it is possible to define variables that correspond to specific values of a variable (\cref{prop:individual-prob-1}).
}


\commentout{%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Not Nouns, but Verbs}

I grew up programming in Java. I loved the \href{https://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html}{kingdom of nouns}.

When I started to learn about the foundations of mathematics, 
I loved 

AI courses where about nouns. 
Then, in 20

\[
\text{Set Theory} : \text{Category Theory}
~~::~~
\text{GOFAI} : \text{ML Systems}
\]
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newmaterial{%
\section{Overview of Results}
    \label{sec:overview-of-results}

This thesis endeavors to show a broad unified picture of many representations, concepts, and algorithms used in modern AI systems.
At a high level, the idea is simple: represent everything in probabilistic terms, and then identify and resolve inconsistencies between them.
The details, of course, are more involved.
\Cref{chap:prelim} reviews the relevant background material, focusing particularly on those elements of the theory of probabilistic graphical models and information theory.
We also develop a perhaps unusual account of the notion of a variable, which provides a unified formal foundation for the standard notation and terminology across several different communities.
Both aspects serve as the building blocks for the theory that follows.
%
The rest of the material is based on three previously published conference papers
    \citep{pdg-aaai,one-true-loss,pdg-infer},
two workshop papers
    \citep{lir,mixlang},
several additional papers in earlier stages of the peer review process,
and a vast collection of original unpublished notes. 

% \citet{pdg-aaai}, \citet{one-true-loss}, \citet{LIR}, \citet{pdg-infer}
%

% \TODO[TODO]
% The remainder of the Thesis is laid out as follows. 
% In \cref{chap:prelim}, we will review the foundational building blocks or 

\Cref{part:univ-model} is about subjective representations of knowledge and uncertainty. 
%
\Cref{chap:pdg-repr} introduces the key mathematical object at the heart of this dissertation, called a \emph{probabilistic dependency graph (PDG)}. 
It is based on our AAAI paper of the same name \citep{pdg-aaai}, but augmented with significant elements from the other papers. 
In that chapter, we motivate by example the need for a representation with the kind of flexibility and modularity offered by PDGs, develop their formal syntax and semantics, and relate them to traditional graphical models---which are special cases. 
%
\cref{chap:repr-tools} takes the PDG representation further, showing how PDGs not only capture graphical models, but also other notions of uncertainty, such as credal sets \citep{Walley1991-SRIP} and Dempster-Shafer belief functions \citep{shafer1976mathematical}.  
In the process, we will develop a number of smaller tools, which we call \emph{widgets}, which capture small fragments of epistemic states as PDGs---enabling us to seamlessly convert things like on the values of variables, or on their probability ranges, to PDGs.
% Both \cref{chap:pdg-repr,chap:repr-tools} primarily use quantitative PDGs; we then turn to qualitative ones.
In \cref{chap:QIM}, we develop a concept that we call \emph{qualitative mechanism independence}, a generalization of independencies in other graphical models, that can also describe functional dependencies and also gives meaning to cyclic structures. 
As we shall see, this concept is closely tied to (and can be captured by) 
    the qualitative information in a PDG.
%
To summarize, \cref{part:univ-model} establishes PDGs as a sweeping generalization of classical knowledge and uncertainty representations, especially those that have formed the backbone of AI systems in the last 25 years. 


As we will see in \cref{part:univ-objective}, 
PDGs also turn out to capture not only more modern learned representations, 
    but also learning algorithms and objectives behind them. 
% While we have seen the benefits of being able to tolerate inconsistency, we have not yet 
Up until this point, we will have seen a few benefits of being able to tolerate inconsistency, but here we will begin to see the benefits of of measuring inconsistency precisely the way we have. 
In \cref{chap:one-true-loss}, 
% based on a paper called \textit{Loss as the Inconsistency of a Probabilistic Dependency Graph}
based on
% a paper titled
\textit{Loss as the Inconsistency of a Probabilistic Dependency Graph: Choose Your Model, not Your Loss Function} \citep{one-true-loss}, 
we will see that PDGs not only capture the part of a modern machine learning system typically thought of as the representation---i.e., the networks and their architectures---but at the same time, also the loss function used to train it.  
Previously, these two concepts were considered separate design considerations. 
In \cref{chap:one-true-loss}, we will see that a wide variety of losses can be viewed as measuring the inconsistency of a PDG that models the situation appropriately. 
%
At the same time, it also gives a principled way of selecting a loss function in new learning settings. 
%
From these results, it follows that much of machine learning can be viewed as resolving inconsistencies. 
%
In \cref{chap:LIR}, we operationalize this process, by giving a generic recipe for \emph{how} one might resolve inconsistencies:
% The subject of the chapter is a heuristic algorithm called the \emph{local inconsistency resolution (LIR) algorithm}, which gives one simple prescription:
% The recipe calls for a
focus your attention to some (small) view of the picture, and then make (small) changes to reduce the inconsistency of that picture. 
%
This process turns out to be a generalization of {belief propagation}, but one which applies to arbitrary PDGs, thereby also capturing techniques such as adversarial training and variational inference. 
% Perhaps surprisingly, training, inference, and the production of adversarial examples, are all instances of this simple recipe. 



\Cref{part:algo-logic-complexity} develops algorithms and reasoning techniques for PDGs.
% \Cref{part:algo-logic-complexity} develops useful algorithms and connections between the computational problems that PDGs pose.  
\Cref{part:univ-model} positions PDG semantics as a potentially quite useful generalization of traditional graphical models, but to actualize any of that potential, we need to able to do inference on them;
meanwhile, 
\cref{part:univ-objective} establishes calculating (and minimizing) a PDG's degree of inconsistency as an important problem of interest. 
In \cref{part:algo-logic-complexity}, we solve both problems (\cref{chap:infer}), characterize their computational complexity, and investigate the deep connection between them (\cref{chap:inc-infer-connection}). 
% We conclude by further developing a 
\Cref{chap:reason} then fleshes out 
    reasoning principle that appears in
    \cref{chap:one-true-loss,chap:QIM},
    called \emph{monotonicity of inconsistency}: 
    believing more things cannot make you any less inconsistent. 
This turns out to not only be the basis of an intuitive visual calculus for deriving important inequalities in the literature, but also forms the basis of a \emph{logic} that, unlike most logics, does not become trivial starting from inconsistent premises.


\Cref{part:foundations} develops foundational concepts of PDGs. 
In \cref{chap:conf}, we develop a generic framework for describing the notion of \emph{(learner's) confidence} that underlies the definition of a PDG---a concept that is fundamentally different from probability, but complements it.
In \cref{chap:relent-soup}, we describe several ways in which PDGs fit into the general framework developed in \cref{chap:conf}.
PDGs also have a rich foundation in category theory.
In fact, the PDG representation was inspired by categorical thinking. 
We describe this in \cref{chap:PDG-cat}.
% This categorical flavor is actually 
% \Cref{chap:PDG-cat} concludes by investigating the category theory of PDGs.
    % The departure between Bayesian Networks originally inspired the representation.
%
% Finally, we conclude with open questions in \cref{part:}
}