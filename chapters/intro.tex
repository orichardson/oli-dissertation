% What is the problem I'm solving?

%%% POINTS TO TOUCH ON %%%
% - 

\section{In Defense of Inconsistency}

If you ever want to be right, it is important to acknowledge that you might now be wrong.  And not only might you be wrong in the margins---wrong about the details of a calculation or under-informed---you could be fundamentally misguided about how the world works. You could be using meaningless concepts, confusing cause and effect, and misplacing your trust in people and institutions that seek to manipulate you. Worst of all, your own internal calculations may not even be self-consistent.

If there is one thing on which we can all agree, it is a contempt for inconsistency.
The foundations of microeconomics have taught us that we are entitled to our own tastes (and beliefs, to some extent), but only if they at least adhere to an internal logic.
Otherwise we can be easily cheated \cite{RAT}, manipulated \cite{explosion}, and humiliated \cite{logicalfalacy, emptysetthesis}.

That is why theoretical computer scientists are careful to prove things about clearly defined models,
and it is why applied computer scientists build artificial agents that are are consistent by construction:
  it grants an invulnerability to our one common enemy: inconsistency.

Yet this invulnerability comes with shackles.
Would \emph{you} trade your freedom to think in weird and backwards ways, to puzzle over paradoxes, to experience deep surprise and be challenged by internal conflict---for a guarantee of self-consistency?

In order to always present a consistent front, it is necessary to fully process new information before moving on. Mulling is prohibited, as
is waiting for clarification.
Half-baked ideas are a nonstarter.
This regimented consistency-first lifestyle works very well when your observations only tell you're mistaken in the margins and only need to update a few parameters here and there.
But what if you're wrong in a more fundamental way?
Can you imagine how hard it would be to reconceptualize things without ever venturing into an inconsistent state?
Imagine having to seriously refactor a large codebase, with an interface that only allows you to see code that compiles.
So if (by no fault of your own) you do happen to be wrong about how the world works---that is, really, fundamentally wrong---then taking on inconsistency may be worthwhile.

By way analogy, we all agree debt is bad, but would you willingly give up your freedom to take on debt?
Many of the most powerful people and institutions in this world would not.
Running a budget surplus works well, until you fall ill or need to buy a house.

For analogous reasons, modern AI systems have been slipping away from the consistency that they aspire to.
For instance, it is expensive and even undesirable to exactly model your training data; instead we try to do the opposite, by adding priors (also known as regularizers; see \cref{sec:regularizers}) to prevent this.
The priors are typically inconsistent with the data, which is arguably the point.
The result is a far more robust learning process, both theoretically \cite{ftrl} and practically \cite{regularizers}
%
% (maybe more?)
%
These techniques are billed as more efficient approximations to the consistent thing we really mean.
But I argue that we should just call them what they are: inconsistent belief states.


It is said that a man with two watches never knows the time.  But a man with only one watch

The  of inconsistency is redundancy.




\section{Themes and Motifs}

\subsection{Equivalent Representations}

% Suppose I learn that 
When working with abstract representations, often it is the case that one class of models (representation $R_1$) seems clearly more general than another (representation $R_2$).
To prove this, you show that anything with representation $R_2$ can be converted to $R_1$; perhaps $R_2$ is even literally a subset of $R_1$. 
You convince yourself that the relationship is strict, because very little of $R_1$ is in the image of this transformation---and besides, how could you possibly represent a generic element of $R_1$ with $R_2$? 
But later, you discover that there is in fact a much more subtle way of capturing $R_1$ with $R_2$. 
Thus $R_1$ turns out to be equivalent to $R_2$. 

To illustrate, consider an example that will soon appear in the mathematical preliminaries (\cref{chap:prelim}). 
Bipartite graphs are clearly a very special class of graphs, with special properties; not every graph is bipartite. 
Also, hypergraphs clearly are generalization of graphs; only the special case where hyperedges have two vertices correspond meaningfully to graphs. 
So, chaning these together, one might reasonably assume that bipartite graphs are an \emph{extremely} special case of hypergraphs---yet they are naturally isomorphic. 
% This pattern will be a recurring theme in this dissertation. 


%
%
% Another example is that (fully) randomized PSEMs are equivalent to ordinary PSEMs (\cref{sec:causal}).
Probabililistic Dependency Graphs (PDGs), the central mathematical object whose theory that we develop in this dissertation, are a particularly special representation in this regard. 
Not only can PDGs capture a great many things (\cref{chap:pdg-repr}), but there is no obvious way to generalize them that is not equivalent. 
There are numerous ways to define PDGs, an numerous seeming generalizations of them, all of which are equivalent. 
% This makes the representation 


%oli: don't talk about epistemic humility as a separate section if that's the hook.
% \subsection{Epistemic Humility}
% I distinguish between 

\subsection{A Division between Quantitative and Qualitative}

% on two levels: a high-level structure
There is an important conceptual division between concepts at two levels,
which goes by different names in different circles. 
% Here, we will largely call the distinction one between \emph{qualitative } 

\begin{center}
\begin{tabular}{c|c}
    Qualitative & Quantitative \\ 
    Structural & Observational \\
    Values & Types \\ 
    Theorems & Proofs \\
    Definitions & Examples \\
    Schemas & Instances 
\end{tabular}
\end{center}

The name of a variable, and the names of the variables on which it can depend are qualitative information; the set of values it can take, and its marginal distribution are quantitative information. 

Practically, one benefit of separating these modeling choices is that it allows us to  [[ divide among experts / reason using structure /  ]]

One of the biggest benefits of this separation is that it 

For expressive enough langauges, the lines become blurred. 
Dependent types require one to think about values, leading to can get runtime errors at compile time. 
PDGs, too, blur this line.  We will see that it is possible to define variables that correspond to specific values of a variable (\cref{?})

\commentout{
\subsection{Not Nouns, but Verbs}

I grew up programming in Java. I loved the \href{https://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html}{kingdom of nouns}.

When I started to learn about the foundations of mathematics, 
I loved 

AI courses where about nouns. 

Then, in 20

\[
\text{Set Theory} : \text{Category Theory}
~~::~~
\text{GOFAI} : \text{ML Systems}
\]
}%


\subsection{Mathematical Precision for Informal Reasoning}
People---and I am certainly no exception---are often fast and loose with their notation and their math, especially when talking about complicated topics with deep foundations. 

    
This is a sign that the formalism could be improved.
The whole point of formalism is to support reasoning and thought, 
    in a way that aids precision; if it is too cumbersome to use properly,
    perhaps we would be better off with another formalism.
    
Either it is too cumbersome to spea


\section{Overview of Results}
