% What is the problem I'm solving?

%%% POINTS TO TOUCH ON %%%
% -  communities are not self consistent.

\section{In Defense of Inconsistency}
% \section{In Defense of Our Villain}

\begin{center}
    \noindent\singlespacingplus
% \begin{quotation}
    \it
    ``A man with one watch always knows the time;\\
    a man with two watches is never sure.''
% \end{quotation}
\end{center}

% A core tenet of science: 
Science is built on simple tenet: if you want to someday understand things clearly, then it is important to acknowledge the possibility that your understanding today could be wrong.
% If you want to be right, it is important to acknowledge that you might now be wrong.
And not only might you be wrong in the margins---wrong about the details of a calculation or under-informed---you could have a fundamentally misguided understanding of how the world works. You could be using meaningless concepts, confusing cause and effect, and misplacing your trust in those that seek to manipulate you. 
For those of us who prize logic and rationality (including, famously, Descartes \cite{descartes}), perhaps most terrifying of all is the possibility that your own internal thoughts might not even be self-consistent.
If there is one thing on which we can all agree, it is a contempt for inconsistency.
The foundational principles of microeconomics entitle us to our own tastes (and beliefs, to some extent), but only if they admit a consistent internal logic.
For without one, we risk being swindled \cite{RAT} or manipulated \cite{explosion}, if not humiliated \cite{logicalfalacy,emptysetthesis,politicians}.

That is why theoretical computer scientists are careful to prove things about clearly defined models,
and it is why applied computer scientists build artificial agents that are are consistent by construction:
  it grants an invulnerability to our one common enemy: inconsistency.
%
Yet this invulnerability comes with shackles.
Would \emph{you} trade \emph{your} freedom to see things from two angles at once---to puzzle over paradoxes, to experience deep surprise and be challenged by internal conflict---for a guarantee of self-consistency?

In order to always present a consistent front, it is necessary to fully process new information before moving on. Mulling is prohibited, as
is waiting for clarification.
Half-baked ideas are a nonstarter.
This regimented consistency-first lifestyle works well if you are only mistaken in the margins, and need only to occasionally update a few parameters.
But what if you're wrong in a more fundamental way?
Can you imagine how hard it would be to learn to see things completely differently, without ever venturing into a confused state?
% being even a little bit inconsiste
Imagine having to implement a substantial refactor of a large codebase, with an interface that only allows you to see code that compiles.
So if (by no fault of your own) you do happen to be wrong about how the world works---that is, really, fundamentally wrong---then taking on inconsistency may be worthwhile.

% By way of analogy, we all agree debt is bad, but would you willingly give up your freedom to take on debt?
% Many of the most powerful people and institutions in this world would not.
% Running a budget surplus works well, until you fall ill or need to buy a house.

In some sense, the opposite of inconsistency is \emph{redundancy}, which engineers and computer scientists agree is generally positive. 
% Another problem consistent by construction is that they also have no redundancy, which makes them fragile.
% Apparently unfazed by the prospect of representing inconsistency, the cryptographers distributed systems people 
But consistent-by-construction representations are also redundancy-free, by construction. 
% The only way to make sure your beliefs cannot conflict is to only have one perspective on any 
For if there is no opportunity for internal conflict, then there is also no opportunity for internal agreement. 
% The only way to eliminate the possibility  opportunity for internal conflict, then there is also no opportunity for internal agreement. 

For analogous reasons, modern AI systems have been slipping away from the consistency that they aspire to.
For instance, it is expensive and even undesirable to exactly model your training data; instead we try to do the opposite, by adding priors (also known as regularizers; see \cref{sec:regularizers}) to prevent this.
The priors are typically inconsistent with the data (which is arguably the point), although neither is ever modified. 
Still, the result is a far more robust learning process, both theoretically \cite{ftrl} and practically \cite{regularizers}
%
% (maybe more?)
% Perhaps clearer examples arise when people use multiple networks together, such as  
Even clearer examples of inconsistency arise in variational inference,
    (e.g., mean field approximation methods, variational autoencoders \cite{kingma2013autoencoding})
    and when combining multiple networks 
        in more complicated ways \cite{}.
    In some cases, there is even a term reprsenting ``consistency'' in the learning objecctive \cite{cycle-gan}. 
In all of these, the goal is to be (mostly) consistent, but seldom is it possible to actually get there,
and tolerating inconsistency is critical for the learning process.
%
These techniques are billed
% to our Bayesian overlords 
as efficient approximations (i.e., cheap knockoffs) to the consistent thing we really mean.
But why perpetuate these unrealistic rationality standards?
I argue that we should just call them what they are: (possibly) inconsistent belief states.

Doing so intelligently unlocks a beautiful theory of probabilistic modeling, that unifies a broad range of concepts, representations, and algorithms across artificial intelligence, and the theory of rational mind. 
The framework developed in this dissertation
gives us
% The 
ability to model, measure, and mitigate inconsistency.
    % comes with an enormous amount of 
This affords an agent an enormous amount of epistemic flexibility,
    making it far easier to make deep structural changes to its epistemic
    representations.
So, in a sense, the possibility of internal inconsistency may be precisely what saves you from being deeply wrong. 
%
Make no mistake: inconsistency is still bad.
% Indeed, it is the one true villain
% In fact, it is the only thing we view as bad.
Indeed, our measure of it is the only conception of ``bad'' needed to do much of machine learning (\cref{chap:one-true-loss,chap:LIR}).
There is no question that chronically exhibiting a high degree of inconsistency is problematic.
%
Yet, as with all internal conflict, it is better to engage with it and recognize it as a catalyst for positive change, 
than to avoid it entirely. 

% People, institutions, cultures, institutions of all shapes and sizes, 
%
Is really worse to be gullible than it is to be stuck and very wrong? 
It is said that a man with two watches never knows the time.  
But is a man with only one watch really better off?
And how are you supposed to use multiple watches, anyway?
% In the coming chapters, we will see. 
% We will soon see. 
% Join me, and together we will soon see. 
Let me show you. 


\section{Themes and Motifs}



%
\subsection{Equivalent Representations}

% Suppose I learn that 
When working with abstract representations, often it is the case that one class of models (representation $R_1$) seems clearly more general than another (representation $R_2$).
To prove this, you show that anything with representation $R_2$ can be converted to $R_1$; perhaps $R_2$ is even literally a subset of $R_1$. 
You convince yourself that the relationship is strict, because very little of $R_1$ is in the image of this transformation---and besides, how could you possibly represent a generic element of $R_1$ with $R_2$? 
Suppose that, sometimes later,
 you discover that there is in fact a much more subtle way of capturing $R_1$ with $R_2$. 
So, despite appearances, $R_1$ turns out to be equivalent to $R_2$. 

To illustrate, consider an example that will soon appear in the mathematical preliminaries (\cref{chap:prelim}). 
Bipartite graphs are clearly a very special class of graphs, with special properties; not every graph is bipartite. 
Also, hypergraphs clearly are generalization of graphs; only the special case where hyperedges have two vertices correspond meaningfully to graphs. 
So, chaning these together, one might reasonably assume that bipartite graphs are an \emph{extremely} special case of hypergraphs---yet bipartite graphs and hypergraphs are naturally isomorphic. 
% This pattern will be a recurring theme in this dissertation. 


% Here is another, more abstract example : verbs and nouns.
% \TODO


% Another example is that (fully) randomized PSEMs are equivalent to ordinary PSEMs (\cref{sec:causal}).
Probabilistic Dependency Graphs (PDGs), the central mathematical object whose theory that we develop in this dissertation, are a particularly special representation in this regard. 
Not only can PDGs capture a great many things 
% (\cref{chap:pdg-repr,chap:repr-tools})
\unskip, but they also capture most of the ``obvious generalizations'' of them that we could have wanted. 
If this abstract statement seems puzzling, recall that Turing machines (TMs) not only capture other notions of computation, but also capture many variants of TMs aimed at extending their expressive power: ``generalized'' TMs that have multiple tapes, multiple heads, tapes of higher dimension, and even non-determinism---are all (semantically) equivalent to ordinary Turing Machines. 

% \commentout{%

\subsection{Mathematical Precision for Informal Reasoning}

% definitional layering
 % good representation is that things that are actually different 

People---and I am certainly no exception---are often fast and loose with their notation and their math, especially when talking about complicated topics with many layers of definitions. 
%
The hallmark of a good formalism combined with good notation is that it supports a user in manipulate the concepts quickly and intuitively, yet at the same time is precise enough to withstand critical scrutiny.
%
Correspondingly, good representations, and tools for manipulating representations 
    have two critical properties:
\begin{enumerate}
    \item things that are different look different, and
    \item things that are the same look the same. 
\end{enumerate}
The first aids precision, and the latter aids ease of use. 
% In math, it is common to use 
% Often in mathematics, it is common to 
It is very common for computer scientists and mathematicians 
    % have an urge to prove their precision
    \unskip,
    in pursuit of precision,
    to focus on distinguishing things that are even superficially different.  
Failure to do so called ``abuse of notation''. 
%
We do it anyway, in service of making formalisms easier to work with. 
But in some deep way, the notation is not really being abused if the two objects are really two different views of the same thing.
% What is really important is 

The idea can be described more precisely as etiquette for \emph{implicit conversion}.
Whenever we define ways to implicitly regard objects of type $A$ to objects of type $B$, and a way of implicitly regarding objects of type
    $B$ as objects of type $C$, then 
    we have also defined way of implicitly converting an object of type $A$ to one of type $C$.
%
To avoid confusion, it is not strictly necessary to avoid implicit conversions---just to ensure that all ways implicitly converting $A$ to $C$ are equivalent. 
%
Doing so can be challenging, but it is often the best way to get representations that simultaneously attain both 1 and 2. 
%

Avoiding the challenge altogether----ensuring objects of different types of different types are given different symbols---is a direct analogue of forcing yourself to be consistent by construction. It is very clear and precise, but makes it difficult to think. 
%
In keeping with the theme of this dissertation, I have not shied away from defining the same symbol in more than one way---but I have tried to do so in a way that the definitions never disagree in any context. 
%
% After all, that is the role of a theorem: to give two different 
% symbol to have more than one formal meaning. 

% Two particularly successful representations, for which I have only the highest respect, are for are string diagrams for operating with symmetric monoidal categories, and 
% To my eyes, a primary strength of the formalism we develop in this dissertation 
% This is a sign that the formalism could be improved.
% The whole point of formalism is to support reasoning and thought, 
%     in a way that aids precision; if it is too cumbersome to use properly,
%     perhaps we would be better off with another formalism.
    
% Either it is too cumbersome to spea

The representation that lies at the heart of this thesis, called a probabilistic dependency graph (PDG), enables this kind of thinking in the probabilistic setting.
They enable us to manipulate probabilistic primitives in ways that look just like standard intuitive short-hand, yet the machinery underneath makes the reasoning completely precise.
Some of the key reasoning principles have conjealed to form \cref{chap:reason},
but a keen reader will see examples of how PDGs formalize intuitive probabilistic reasoning throughout. 
% Howevef the reasoning is consistent, which we do not require by construction. 
But when we introduce probabilities, consistency is not just about ensure paths are equal;
 it's about something else. 
Making that something else precise is one objective of this thesis. 
% It is a powerful way of formalizing these kinds of thought patterns for probabilistic reasoning. 
% It aims to simultaneously 
%


%oli: don't talk about epistemic humility as a separate section if that's the hook.
% \subsection{Epistemic Humility}
% I distinguish between 

% \subsection{A Division between Quantitative and Qualitative}
\subsection{Qualitative and Quantitative}

% \TODO[ TODO: fix this subsection; it is still very rough ]
% on two levels: a high-level structure
% There is an important conceptual division,
Broadly speaking, most modeling happens at two different levels: 
    at an abstract conceptual level, and a concrete 
The division between the two levels goes by different names in different communities. 
% Here, we will largely call the distinction one between \emph{qualitative } 
In computer science, pehraps the most common
    widely appreciated version of this phenomenon is 
    the difference between types and values.
A few others have are below.

\begin{center}
    \begin{tabular}{c|c}
        Qualitative & Quantitative \\ 
        Structural & Observational \\
        Values & Types \\ 
        Theorems & Proofs \\
        Definitions & Examples \\
        Schemas & Instances 
    \end{tabular}
    \end{center}
    
Following the nomenclature of the graphical models community, we will refer to it principally as the divide between \emph{qualitative} and \emph{quantitative} information 
% (although both can be numerical)
\unskip, or alternatively as the divide between \emph{structural} and \emph{observational} information (see \cref{sec:pgm-prelim}).
The name of a variable, and the names of the variables on which it can depend are qualitative information; the set of values it can take, and its marginal distribution are quantitative information. 
%
Both kinds of information are important, and they are fundamentally very different. The former is about the graph structure, and the latter is about its annotation. Put differently,
% (as we will see in \cref{chap:PDG-cat}), 
the former is a conceptual shape, and the latter interprets that shape in the language of probabilities and confidences. 

% One unusual feature of PDGs is that the qualitative information has a numerical weight. 
% Different experts may have different relative strengths. For instance, one might want to trust one person to design a study to determine what causes what, and another person to independently design experiments aimed at determining the numerical probabilities. 

This line between these two kinds of information will be a recurring character in this work.

% Understanding the difference between these two 
% Corresponding 
% We will see 

\commentout{
For expressive enough languages, the lines become blurred. Dependent types require one to think about values, leading to can get runtime errors at compile time. 
PDGs, too, blur this line.  We will see, for example, that it is possible to define variables that correspond to specific values of a variable (\cref{prop:individual-prob-1}).
}


\commentout{%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Not Nouns, but Verbs}

I grew up programming in Java. I loved the \href{https://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html}{kingdom of nouns}.

When I started to learn about the foundations of mathematics, 
I loved 

AI courses where about nouns. 

Then, in 20

\[
\text{Set Theory} : \text{Category Theory}
~~::~~
\text{GOFAI} : \text{ML Systems}
\]
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\commentout{
\section{Overview of Results}

\TODO[TODO]
}