% What is the problem I'm solving?

%%% POINTS TO TOUCH ON %%%
% -  communities are not self consistent.

\section{In Defense of Inconsistency}
    \label{sec:defend-inconsist}
% \section{In Defense of Our Villain}

\begin{center}
    \noindent\singlespacingplus
% \begin{quotation}
    \it
    ``A man with one watch always knows the time;\\
    a man with two watches is never sure.'' \\
    \citep{segal-law}
% \end{quotation}
\end{center}

% Science is built on simple tenet: if you want to someday understand things clearly, then it is important to acknowledge the possibility that your understanding today could be wrong.
%
% Science is built on a simple tenet: if you want to someday understand things clearly, then it is important to acknowledge the possibility that your understanding today could be wrong.
% Science :
% There is a central principle of scientific inquiry: 
A central principle of scientific inquiry is this:
    if you want to eventually understand things clearly, then you must take seriously the possibility that your understanding today might be wrong. 
And not only might you be wrong in the margins---wrong about the details of a calculation or under-informed---you could have a fundamentally misguided understanding of how the world works. You could be using meaningless concepts, confusing cause and effect, and misplacing your trust in those that seek to manipulate you. 
For those of us who prize logic and rationality (including, famously, 
% Descartes \citeyear{descartes}
\citet{descartes}),
perhaps most terrifying of all is the possibility that your own internal thoughts might not even be self-consistent.
If there is one thing on which we can all agree, it is a contempt for inconsistency.
The foundational principles of microeconomics entitle us to our own tastes (and beliefs, to some extent), but only if they admit a consistent internal logic.
For without one, we risk being swindled \citep{sep-dutch-book} or misled 
% \citep{explosion},
\citep{priest1996paraconsistent}, 
if not humiliated \citep{finocchiaro1981fallacies,russell1902letter}.
% For middle reference, looking into Reinhardt cardinals and Kuen's theorem,
% or Frege


% That is why theoretical computer scientists are careful to prove things about clearly defined models, and it is why applied computer scientists often build artificial agents that are \emph{consistent by construction}.
% We would rather have one definition of $X$ and one technique for computing $X$, than several different approaches and implementations. 
% We would rather use one library for telling time than two. 
% And no matter how many parameters we add, the goal is to parameterize a single coherent probability distribution. 
% Why? Because computing things in just one way grants an invulnerability to our one common enemy: inconsistency.

% Computer scientists who build cognition from scratch, have capitalized on an opportunity to not only avoid contradictions, but  build frameworks where they're impossible by design. 
Computer scientists like us, who build cognition from scratch, have capitalized on an opportunity to eliminate contradictions in artificial agents altogether, by using epistemic representations that are consistent by design.
Theorists are careful to prove things about clearly defined models,
while applied computer scientists have long built artificial agents
\newmaterial{%
using representations that, by construction, cannot even encode inconsistency. 
}%
% using representations that make inconsistency impossible by design.
%
We would rather have one definition of $X$ and one technique for computing $X$ than several different approaches and implementations
    \newmaterial{that could, in principle, disagree}. 
We would rather use one library for telling time than two.
And no matter how many parameters we add, the goal is to parameterize a single coherent probability distribution. 
Why? Because computing things in just one way grants an invulnerability to our one common enemy: inconsistency.

Yet this invulnerability comes with shackles.
Would \emph{you} trade \emph{your} freedom to see things from two angles at once---to puzzle over paradoxes, to experience deep surprise and be challenged by internal conflict---for a guarantee of self-consistency?
%
In order to always present a consistent front, it is necessary to fully process new information before moving on. Mulling is prohibited, as
is waiting for clarification.
Half-baked ideas are a nonstarter.
This regimented consistency-first lifestyle works well if you are only mistaken in the margins, and need only to occasionally update a few parameters.
But what if you're wrong in a deeper way?
Can you imagine how hard it would be to learn to see things completely differently, without ever venturing into a confused state?
% Imagine having to implement a substantial refactor of a large codebase, with an interface that only allows you to see code that compiles.
Picture having to substantially refactor a large codebase with an interface that only allows you to see code that compiles.
So if (through no fault of your own) you do happen to be wrong about how the world works---that is, really, fundamentally wrong---then taking on inconsistency may be worthwhile.

% By way of analogy, we all agree debt is bad, but would you willingly give up your freedom to take on debt?
% Many of the most powerful people and institutions in this world would not.
% Running a budget surplus works well, until you fall ill or need to buy a house.

Engineers and computer scientists generally agree that \emph{redundancy}---by which we mean multiple independent approaches, tests, and safeguards that all point in the same direction---is highly desirable. 
But, in a sense, redundancy and inconsistency are opposite sides of the same coin. 
% In some sense, the opposite of inconsistency is \emph{redundancy}, which engineers and computer scientists agree is generally positive. 
% % Another problem consistent by construction is that they also have no redundancy, which makes them fragile.
% % Apparently unfazed by the prospect of representing inconsistency, the cryptographers distributed systems people 
% But consistent-by-construction representations are also redundancy-free, by construction. 
% For if we have two different representations that could conceivably agree, producing redundancy, then those two representations could conceivably be inconsistent with one another. 
\newmaterial{%
If we use only representations that avoid inconsistency 
    % by  making disagreement impossible,
    by preempting the possibility of two meaningfully distinct answers to a question 
        (using only one watch, so to speak),
    % then there are no two meaningfully distinct,
    % and so there can be no redundancy, either. 
    then we also cannot have meaningful agreement, 
        and hence no redundancy. 
}
% are also redundancy-free, by construction. 
Without opportunity for internal conflict, there can be no opportunity for internal agreement; 
    \newmaterial{%
it is the possibility of disagrement that makes agreement valuable. 
    }%
% The only way to eliminate the possibility  opportunity for internal conflict, then there is also no opportunity for internal agreement. 


Overlapping and redundant representations are incredibly useful in practice for guarding against catastrophic failure. 
\commentout{%
\newmaterial{%
The issue is particularly pronouncd in logic, as a single contradiction can leave a theory entirely trivial and detached from truth---an unforunate fact
that has lead some \citep{priest1996paraconsistent} to develop logics that can tolerate inconsistency. 
% But these \emph{paraconsistent} logics are far less 
% But ultimately, these logics are hard to use, 
But these approaches have never been popular in Artificial Intelligence (AI).
In AI, the brittleness was solved in another way: by accounting for uncertainty, meaning probability. 
This is extremely attractive, as this kind of epistemic modesty allows us to present a clear and consistent picture.
% You don't need to believe $P$ and $\lnot P$,
% In the Artificial Intelligence (AI), we addressed the brittleness   with something else: probability. 
% Uncertainty helps, but it does not fix the problem. 

\TODO

Yet recently, modern AI systems have recently been slipping further from even the probabilistic consistency to which they aspire.
}}%
For this reason, even modern AI systems have recently been slipping away from the probabilistic consistency to which they aspire.
%
For instance, it is expensive and even undesirable to exactly model your training data; instead we try to do the opposite, by adding priors (also known as regularizers; see \cref{sec:regularizers}) to prevent this.
The priors are typically inconsistent with the data (which is arguably the point), although neither is ever modified. 
Still, the result is a far more robust learning process, both theoretically 
% \citep{ftrl}
\citep{ftrl-lecturenotes1,ftrl-lecturenotes2}
 and practically \citep{girosi1995regularization}.
%
% (maybe more?)
% Perhaps clearer examples arise when people use multiple networks together, such as  
Even clearer examples of inconsistency arise in variational inference
    (e.g., mean field approximation methods, variational autoencoders \citep{kingma2013autoencoding}),
    and more generally when combining multiple networks in nontrivial ways.
    In some cases, there is even a term representing ``consistency'' in the training objective \citep{CycleGAN2017}, or the name of the approach \citep{learning-loc-glob-consistency,temp-cycle-consist-2019CVPR}. 
In each case, the goal is to be (mostly) consistent, but seldom is it possible to actually get there,
and tolerating inconsistency is critical to the learning process.
%
These techniques are billed
% to our Bayesian overlords 
    as pragmatic ways of approximating
    % (i.e., cheap knockoffs) 
    % to the consistent thing we really mean.
    % some presumably consistent we must have intended.
    some presumably consistent we must have intended.
But why perpetuate these unrealistic rationality standards?
I argue that we should just call them what they are: (possibly) inconsistent belief states.

Doing so intelligently unlocks a beautiful theory of probabilistic modeling, that unifies a broad range of concepts, representations, and algorithms across artificial intelligence, and 
    % the theory of rational mind. 
    the study of rational belief and decision making.
The framework developed in this dissertation
gives us the ability to model, measure, and mitigate inconsistency.
    % comes with an enormous amount of 
This affords an agent an enormous amount of epistemic flexibility,
    making it far easier to make deep structural changes to its epistemic
    representations.
So, in an ironic twist, the possibility of internal inconsistency might ultimately be what saves you from being deeply wrong. 
%
Make no mistake: inconsistency is still bad.
% Indeed, it is the one true villain
% In fact, it is the only thing we view as bad.
Indeed, our measure of it is the only conception of ``bad'' needed to do much of machine learning (\cref{chap:one-true-loss,chap:LIR}).
There is no question that chronically exhibiting a high degree of inconsistency is problematic.
%
Yet, as with all internal conflict, it is more productive to engage with it and recognize it as a catalyst for positive change
than it is to avoid it entirely. 

% People, institutions, cultures, institutions of all shapes and sizes, 
%
Is really worse to be gullible than it is to be stuck and very wrong? 
It is said that a man with two watches never knows the time.  
But is a man with only one watch really better off?
And how are you supposed to use multiple watches, anyway?
% In the coming chapters, we will see. 
% We will soon see. 
% Join me, and together we will soon see. 
Let me show you. 

\section{Overview of Results}
    \label{sec:overview-of-results}

This thesis endeavors to provide a broad unified picture of many representations, concepts, and algorithms used in modern AI systems.
At a high level, the idea is simple: represent everything in probabilistic terms, and then identify and resolve inconsistencies between them.
The details, of course, are more involved.
\Cref{chap:prelim} reviews the relevant background material, focusing particularly on those elements of the theory of probabilistic graphical models and information theory;
it also develops a (novel) account of \emph{variables} that unifies standard notation across different communities. 
% We also develop a perhaps unusual account of the notion of a variable, which provides a unified formal foundation for the standard notation and terminology across several different communities.
Both aspects serve as the building blocks for the theory that follows.
%
The rest of the material is based on three previously published conference papers
    \citep*{pdg-aaai,one-true-loss,pdg-infer},
two workshop papers
    \citep{LIR,mixture-langs},
additional papers in earlier stages of the peer review process
    \citep*{qim},
and a trove of original unpublished notes. 


\textbf{\newmaterial{A Universal Model.}}
\Cref{part:univ-model} is about subjective representations of knowledge and uncertainty. 
%
\Cref{chap:pdg-repr} introduces the key mathematical object at the heart of this dissertation, called a \emph{probabilistic dependency graph (PDG)}. 
It is based on our AAAI paper of the same name \citep{pdg-aaai}, but augmented with significant elements from the other papers. 
In that chapter, we motivate by example the need for a representation with the kind of flexibility and modularity offered by PDGs, develop their formal syntax and semantics, and relate them to traditional graphical models---which are special cases. 
%
\cref{chap:repr-tools} takes the PDG representation further, showing how PDGs not only capture graphical models, but also other notions of uncertainty, such as credal sets \citep{Walley1991-SRIP} and Dempster-Shafer belief functions \citep{shafer1976mathematical}.  
In the process, we will develop a number of smaller tools, which we call \emph{widgets}, which capture small fragments of epistemic states as PDGs---enabling us to seamlessly convert things like constraints on the values of variables, or on their probability ranges, to PDGs.
% Both \cref{chap:pdg-repr,chap:repr-tools} primarily use quantitative PDGs; we then turn to qualitative ones.
\Cref{chap:QIM},
    also based on 
    % joint work with Peters and Halpern \citep*{qim},
    an eponymous paper \citep*{qim},
    develops a concept that we call \emph{qualitative mechanism independence}.
The concept is a significant generalization of independencies in other graphical models that can also describe functional dependencies and give meaning to cyclic structures. 
As we shall see, qualitative mechanism independence is closely related to (and can be captured by) the qualitative information in a PDG.
%
To summarize, \cref{part:univ-model} establishes PDGs as a sweeping generalization of classical knowledge and uncertainty representations, especially those that have formed the backbone of AI systems in the last 25 years. 


\textbf{\newmaterial{A Universal Objective.}}
% As we will see in \cref{part:univ-objective}, 
In \cref{part:univ-objective}, we will see that
PDGs also turn out to capture more modern learned representations, 
    and even learning algorithms and objectives behind them. 
% While we have seen the benefits of being able to tolerate inconsistency, we have not yet 
Up until this point, we will have seen a few benefits of being able to tolerate inconsistency, but here we will begin to see the benefits of of measuring inconsistency precisely the way we have. 
In \cref{chap:one-true-loss}, 
% based on a paper called \textit{Loss as the Inconsistency of a Probabilistic Dependency Graph}
based on
% a paper titled
\textit{Loss as the Inconsistency of a Probabilistic Dependency Graph: Choose Your Model, not Your Loss Function} \citep{one-true-loss}, 
we will see that PDGs capture not only the part of a modern machine learning (ML) system typically thought of as the representation (i.e., the networks and their architectures), but also the loss function used to train that system.  
% Previously, these two concepts were considered separate design considerations, 
% In a more standard telling, these two concepts completely separate design considerations, and a loss function is about pragmatics, rather than truth. 
Previously the two aspects of an ML system were considered separate design choices, and the loss was a matter of pragmatics, not truth. 
But our findings suggest that 
% In \cref{chap:one-true-loss}, we argue that this is not the case, and 
there may be a ``universal'' way of getting at the ``correct'' loss function simply by laying out your (possibly inconsistent) beliefs. 
Indeed, a wide variety of standard losses can be viewed as measuring the inconsistency of a PDG that models the situation appropriately. 
% This framing also gives a principled way of selecting a loss function in new learning settings. 
%
From these results, it follows that much of machine learning can be viewed as resolving inconsistencies. 
%
In \cref{chap:LIR}, we operationalize this process, by giving a generic recipe for \emph{how} one might resolve inconsistencies:
% The subject of the chapter is a heuristic algorithm called the \emph{local inconsistency resolution (LIR) algorithm}, which gives one simple prescription:
% The recipe calls for a
focus your attention to some (small) view of the picture, and then make (small) changes to reduce the inconsistency of that picture. 
%
This idea turns out to be a generalization of {belief propagation} that applies to arbitrary PDGs, and that captures techniques such as adversarial training and variational inference in the process. 
% Perhaps surprisingly, training, inference, and the production of adversarial examples, are all instances of this simple recipe. 


\textbf{\newmaterial{Algorithms, Logic, and Complexity.}}
\Cref{part:algo-logic-complexity} develops algorithms and reasoning techniques for PDGs.
% \Cref{part:algo-logic-complexity} develops useful algorithms and connections between the computational problems that PDGs pose.  
\Cref{part:univ-model} positions PDG semantics as a potentially quite useful generalization of traditional graphical models, but to actualize any of that potential, we need to able to do inference on them;
meanwhile, 
\cref{part:univ-objective} establishes calculating (and minimizing) a PDG's degree of inconsistency as an important problem of interest. 
In \cref{part:algo-logic-complexity}, we solve both problems (\cref{chap:infer}), characterize their computational complexity, and investigate the deep connection between them (\cref{chap:inc-infer-connection}). 
Both \cref{chap:infer,chap:inc-infer-connection} are based on the full version of the PDG inference paper \citep*{pdg-infer}.
% We conclude by further developing a 
In \cref{chap:reason}, we then flesh out a
    reasoning principle that appears in both \cref{part:univ-model,part:univ-objective}
    (\cref{chap:one-true-loss,chap:QIM}),
    called \emph{monotonicity of inconsistency}: 
    believing more things cannot make you any less inconsistent. 
This turns out to not only be the basis of an intuitive visual calculus for deriving important inequalities in the literature, but also forms the basis of a \emph{logic} that, unlike most logics, does not become trivial starting from inconsistent premises.


\textbf{\newmaterial{Foundations.}}
\Cref{part:foundations} develops foundational concepts that underlie the theory of PDGs presented in the previous chapters. 
In \cref{chap:conf}, we develop a generic framework for describing the notion of \emph{(learner's) confidence} that underlies the definition of a PDG---a concept that is fundamentally different from probability, but complements it.
In \cref{chap:relent-soup}, we describe several ways in which PDGs fit into the general framework developed in \cref{chap:conf}.
PDGs also have a rich foundation in category theory.
In fact, the PDG representation was inspired by categorical thinking. 
We describe this in \cref{chap:PDG-cat}.
% This categorical flavor is actually 
% \Cref{chap:PDG-cat} concludes by investigating the category theory of PDGs.
    % The departure between Bayesian Networks originally inspired the representation.
%

% Finally, we conclude and discuss important open questions in \cref{part:conclusions} (\cref{chap:end}). 
Finally, we conclude with discussion and open questions in \cref{chap:end}. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Themes and Motifs}

% \oldnewmaterial{%
% Before we get to the primary thread of the dissertation,
% which we will overview in \cref{sec:overview-of-results},
% we first prime the reader to notice a few recurring themes.
% }% 

\newmaterial{
% Our last order of business before starting in earnest is to prime ourselves to notice some important recurring themes. 
Before embarking in earnest, we first prime ourselves to notice some important recurring themes. 
Some of these patterns have been important guiding principles;
    others surprised us by their recurrance. 
By recognizing these patterns, it may be easier to digest the material that succeeds them. 
}

\subsection{Mathematical Precision for Informal Reasoning}

% \commentout{People---and I am certainly no exception---are often fast and loose with their notation and their math, especially when talking about complicated topics with many layers of definitions. }%
%
The hallmark of a good formalism is that it supports a user in manipulate the concepts quickly and intuitively, yet at the same time is precise enough to withstand critical scrutiny.
%
Correspondingly, our representations representations (and tools for manipulating them)
    strive for two opposing virtues:
\begin{enumerate}[nosep]
    \item things that are different look different, and
    \item things that are the same look the same. 
\end{enumerate}
The first aids precision, and the latter aids ease of use. 
% In math, it is common to use 
% Often in mathematics, it is common to 
It is common for computer scientists and mathematicians 
    % have an urge to prove their precision
    \unskip,
    in pursuit of precision,
    to focus on distinguishing objects that are even superficially different.  
Failure to do so called ``abuse of notation''. 
%
% We do it anyway, in service of making formalisms easier to work with. 
But in some deep way, the notation is not really being abused if the two objects are really two different views of the same thing.
% What is really important is 

The idea can be described more precisely as etiquette for \emph{implicit conversion}: when one defines a way of regarding one type of mathematical object as another that is so transparent that it can be implied. 
% Whenever we define ways to implicitly regard objects of type $A$ to objects of type $B$, and a way of implicitly regarding objects of type
%     $B$ as objects of type $C$, then 
%     we have also defined way of implicitly converting an object of type $A$ to one of type $C$.
% %
% To avoid confusion, it is not strictly necessary to avoid implicit conversions---just to ensure that all ways implicitly converting $A$ to $C$ are equivalent. 
%
To avoid confusion, it is not strictly necessary to avoid implicit conversions---but it may require ensuring that all ways of implicitly converting an object of type $A$ to one of type $B$ are equivalent. 
%
Doing so can be challenging, but it is often the best way to get representations that simultaneously attain both virtues 1 and 2. 
%
%
Avoiding the challenge altogether---that is, ensuring objects of different types are given different symbols---is 
% To avoid the challenge altogether---i.e., to ensure objects of different types are given different symbols---is 
% a direct analogue of forcing yourself to be consistent by construction. 
    % one way of ensuring consistency by construction.
    % a way of eliminating contradictions by construction. 
    a way of ensuring consistency by construction. 
    % a way to ensure consistency by construction. 
This practice is precise and grants a certain peace of mind, but also makes it difficult to think. 
%
In keeping with the argument made in \cref{sec:defend-inconsist}, I have not shied away from defining the same symbol in more than one way---but I have tried to do so in a way that the definitions never disagree in any context. 
%
% After all, that is the role of a theorem: to give two different 
% symbol to have more than one formal meaning. 

% Two particularly successful representations, for which I have only the highest respect, are for are string diagrams for operating with symmetric monoidal categories, and 
% To my eyes, a primary strength of the formalism we develop in this dissertation 
% This is a sign that the formalism could be improved.
% The whole point of formalism is to support reasoning and thought, 
%     in a way that aids precision; if it is too cumbersome to use properly,
%     perhaps we would be better off with another formalism.
    
% Either it is too cumbersome to spea

% The representation that lies at the heart of this thesis, called a probabilistic dependency graph (PDG), enables this kind of thinking in the probabilistic setting.
PDGs, the representation at the heart of this thesis, enable this kind of thinking in the probabilistic setting.
They allow us to manipulate probabilistic primitives in ways that look just like standard intuitive short-hand, yet the machinery underneath makes the reasoning completely precise.
Some of the key reasoning principles have congealed to form \cref{chap:reason},
but a keen reader will see examples of how PDGs formalize intuitive probabilistic reasoning throughout. 
But when we introduce probabilities, consistency is not just about ensuring paths are equal 
    (as is the case for implicit conversions); it's about something else. 
Making that something else precise is one objective of this thesis. 
% It is a powerful way of formalizing these kinds of thought patterns for probabilistic reasoning. 
% It aims to simultaneously 
%


% \subsection{A Division between Quantitative and Qualitative}
% \subsection{Two Important Divisions}
% \subsection{Two Important Dichotomies}
\subsection{Qualitative and Quantitative Information}

% \TODO[ TODO: fix this subsection; it is still very rough ]
% on two levels: a high-level structure
% There is an important conceptual division,
Broadly speaking, most modeling happens at two different levels: 
    at an abstract conceptual level, and a concrete fully detailed one.
The division between the two kinds of objects
    goes by different names in different communities. 
% Here, we will largely call the distinction one between \emph{qualitative } 
% In computer science, perhaps the most common widely appreciated version of this phenomenon is the distinction between types and values. A few others have are below.
A few examples of the distinction we're getting at are given below.

\begin{center}
    \begin{tabular}{c|c}
        Qualitative & Quantitative \\ 
        Structural & Observational \\
            \hline
        Types & Values \\ 
        Theorems & Proofs \\
        Definitions & Examples \\
        Schemas & Instances 
    \end{tabular}
\end{center}

\newmaterial{%
Some readers might immediately recognize this division; 
% others may find it confusing or find it to be a false equivalence. 
    others may find the analogy between these concepts resonates less.
By the end of the thesis, hopefully we will gain some 
% for the analogy, and moreover, the importance of the underling distinction for probabilistic modeling. 
% for this distinction and its role in probabilistic modeling. 
appreciation for this concept and its role in probabilistic modeling. 
}%    
Following the nomenclature of the graphical models community, we will refer to it principally as the divide between \emph{qualitative} and \emph{quantitative} information 
% (although both can be numerical)
\unskip, or alternatively as the divide between \emph{structural} and \emph{observational} information.
% (see \cref{sec:pgm-prelim}).
% The name of a variable, and the names of the variables on which it can depend are qualitative information; the set of values it can take, and its marginal distribution are quantitative information. 
%
Both kinds of information are important, and they are fundamentally different.
% The former is about the graph structure, and the latter is about its annotation.
% Put differently,
% (as we will see in \cref{chap:PDG-cat}), 
% the former is a conceptual shape, and the latter interprets that shape in the language of probabilities and confidences. 
% One unusual feature of PDGs is that the qualitative information has a numerical weight. 
% Different experts may have different relative strengths. For instance, one might want to trust one person to design a study to determine what causes what, and another person to independently design experiments aimed at determining the numerical probabilities. 
The interplay between them will be a recurring theme.

% Understanding the difference between these two 
% Corresponding 
% We will see 

\commentout{
For expressive enough languages, the lines become blurred. Dependent types require one to think about values, leading to can get runtime errors at compile time. 
PDGs, too, blur this line.  We will see, for example, that it is possible to define variables that correspond to specific values of a variable (\cref{prop:individual-prob-1}).
}


\commentout{
\paragraph{Nouns and Verbs.}
The other important dichotomy we will see is the distinction between 
nouns and verbs. 


\TODO
}


\commentout{%%%%%%
\subsection{Not Nouns, but Verbs}

I grew up programming in Java. I loved the \href{https://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html}{kingdom of nouns}.

When I started to learn about the foundations of mathematics, 
I loved 

AI courses where about nouns. 
Then, in 20

\[
\text{Set Theory} : \text{Category Theory}
~~::~~
\text{GOFAI} : \text{ML Systems}
\]
}%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Equivalent Representations}

% Hilbert's Hotel \cite{hilbert-hotel} is an 

% Suppose I learn that 
When working with abstract representations, often it is the case that one class of models (representation $R_1$) seems clearly more general than another (representation $R_2$).
To prove this, you show that anything with representation $R_2$ can be converted to $R_1$; perhaps $R_2$ is even literally a subset of $R_1$. 
You convince yourself that the relationship is strict, because very little of $R_1$ is in the image of this transformation---and besides, how could you possibly represent a generic element of $R_1$ with $R_2$? 
% Suppose that, sometimes later,
Yet it is still possible that, sometime later,
you discover that there is in fact a much more subtle way of capturing $R_1$ with $R_2$. 
So, despite appearances, $R_1$ may turn out to be equivalent to $R_2$. 

\commentout{%
To illustrate, consider an example from \cref{chap:prelim}. 
Bipartite graphs are clearly a very special class of graphs, with special properties; not every graph is bipartite. 
Also, hypergraphs clearly are generalization of graphs; only the special case where hyperedges have two vertices correspond meaningfully to graphs. 
So, chaining these observations together, one might reasonably conclude that bipartite graphs are an \emph{extremely} special case of hypergraphs---yet, as we will see, bipartite graphs and hypergraphs are naturally isomorphic. 
}%
We will see this happen almost immediately even in the preliminary material: with hypergraphs and bipartite graphs (\cref{sec:prelim-graphs}), with variables, sets of variables, and random variables, (\cref{ssec:basic-vars,sec:var-randomvar}), and with causal models and randomized causal models (\cref{sec:causal}),
    % to say nothing of the novel math developed in this thesis.
before we even get into our own contributions. 
% This pattern will be a recurring theme in this dissertation. 
%
But even against this backdrop,
% Another example is that (fully) randomized PSEMs are equivalent to ordinary PSEMs (\cref{sec:causal}).
Probabilistic Dependency Graphs (PDGs), the central mathematical object whose theory that we develop in this dissertation, are particularly special in this regard. 
Not only can PDGs capture a great many things 
% (\cref{chap:pdg-repr,chap:repr-tools})
\unskip, but they also encompass many natural ``obvious generalizations'' of themselves (\cref{sec:alt-pdgs}). 
If this abstract statement seems puzzling, recall that Turing machines (TMs) not only capture other notions of computation, but also capture many natural variants of TMs that seem on the surface to make them more expressive.  
Yet ``generalized'' TMs with multiple tapes, multiple heads, tapes of higher dimension, and even non-determinism---are all (semantically) equivalent to ordinary Turing Machines. 

\oldnewmaterial{%
    % For PDGs, this is true not only 
    % PDGs push this idea even further.
PDGs push this idea even further.
% PDGs exhibit this kind of equivalence in an in.
Not only is it the case that there are many different ways of specifying a PDG that are all equivalent, 
but also there are also several different semantics (with seemingly different expressiveness), that also turn out to all be equivalent.     
}%
\newmaterial{%
% Because of this property, together 
This property, together with their ability to capture seemingly every other epistemic representation has led us to call PDGs, informally, \emph{a universal modeling language}.
\Cref{part:univ-model} aims to justify this rather bold terminology.
\Cref{part:univ-objective} explores a different aspect
    in which PDGs are universal: minimizing inconsistency appears to be a \emph{universal objective}. 
The two aspects of PDGs are deeply related, as we will see in \cref{part:algo-logic-complexity}.  

But first, we should review some math. 
}%
