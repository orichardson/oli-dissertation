    \label{chap:prelim}

\subsection{Basic Notation}
If $A$ is a finite set, we write $\#A$ or $|A|$ for its cardinality.
We will often be concerned with \emph{variables}, which intuitively correspond to aspects of the world or properties of some object. 
Mathematically, a variable has two aspects.
Qualitatively, a variable is just some unique identifier (the variable name),
    such as ``height''.
Quantitatively, a variable $X$ is also associated with a set $\V(X)$, or simply $\V\!X$, of possible values. 
For example, $\V(\text{height})$ might be the set of positive real numbers, 
    or the set $\{ \texttt{short}, \texttt{tall} \}$. 


\subsection{Relations}

Let $\mathcal X = \{ X_1, \ldots, X_n \}$ be variables, traditionally called atributes. 
A \emph{relation} $R(\mathcal A) = R(X_1, \ldots, X_n) \subseteq 
\V(X_1) \times \cdots \times \V(X_n)$, or equivalently, $R : \prod_{i=1}^n \V\!X_i \to \{0,1\}$, is a subset of joint values of attributes. 
The natural number $n$ is called the \emph{arity} of $R$. 


The \emph{natural join} of two relations $R(A,B)$ and $S(B,C)$ combines them in a particularly obvious way: $(a,b,c) \in R \bowtie S$ iff $(a,b) \in R$ and $(b,c) \in S$. 
More generally, we have

% \begin{defn}[natural join]
%     \[
\begin{equation}
    R(\mathcal X) \bowtie S(\mathcal Y)
        := 
        \Big\{
            \boldsymbol\omega \in  \V(\mathcal X \cup \mathcal Y)
            ~\Big|~
            \mathcal X(\boldsymbol\omega) \in R \land
            \mathcal Y(\boldsymbol\omega) \in S
        \Big\}
        % \qedhere
%     \]
% \end{defn}
\end{equation}

At one extreme, if $\mathcal X$ and $\mathcal Y$ are disjoint sets of attributes, then $R(\mathcal X) \bowtie S(\mathcal Y)$ coincides with the cartesian product of $R \subseteq \V \mathcal X$ and $S \subseteq \V\mathcal Y$.
At the opposite extreme, if $\mathcal X = \mathcal Y$ are the same set of variables, then $R(\mathcal X) \bowtie S(\mathcal X)$ coincides with the intersection of the subsets $R$ and $S$. 

Even when $A_1, \ldots, A_n \subseteq \X$ are not disjoint, we give a convenient extended syntax by defining the quantity $R(a_1, \ldots, a_n)$, where $a_i \in \V(A_i)$. 
Concretely, define $R(a_1,\ldots, a_n) := 0$ if when $\{a_1, \ldots, a_n\}$ do not agree on the value of some shared attribute (i.e., if $\exists X \in \X, \exists i,j \in [n]. ~X \in A_i \cap A_j \land X(a_i) \ne X(a_j)$).
When $\{a,b,c\}$ do agree on all values of shared attributes, let $\mat x$ denote the joint value of $A \cup B \cup C$ obtained from $(a,b,c)$ by removing redundant copies of variable values.
In this case, define $R(a,b,c) := R(\mat x)$. 
\subsection{Algebra}
\begin{defn}[Monoid]%
    A \emph{monoid} is a tuple $(S, *, e)$, where $S$ is a set, $* : S \times S \to S$ is a binary operation, and $e \in S$ is a distinguished identity element, such that:
    \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt,]
        \item (associativity) $\forall a, b,c \in S.~~(a*b)*c = a*(b*c)$;
        \item (identity) $\forall a \in S.~~a * e = a = e * a$.
    \end{itemize}
    A monoid is called \emph{commutative} if it also satisfies
    \begin{itemize}[nosep]
        \item (commutativity): $\forall a, b \in S.~~ a * b = b * a$,
    \end{itemize}
    and \emph{idempotent} if it satisfies
    \begin{itemize}[nosep]
        \item (idempotence): $\forall a \in S.~~ a + a = a$. 
    \end{itemize}
    An idempotent semiring defines partial order by $a \le b \iff a + b = b$. 
\end{defn}


\subsection{Graph Theory}
\begin{defn}
    A \emph{(directed) (multi)graph} $G = (N, A)$, or simply a \emph{graph}, is a set $N$ of nodes,
    and a collection $A$ of arcs, such that each $a \in A$ has a source node $\Src a \in N$ and a target node $\Tgt a \in N$.
    So, formally, the definition is
    $G = (N, A, \Src{}, \Tgt{})$, with $\Src{},\Tgt{}: A \to N$ often left implicit.
\end{defn}

\begin{defn}
    % [Undirected (Multi) Graph]
    An \emph{undirected} (multi)graph $G = (N,E)$ is a set $N$ of vertices (or nodes)
    and a set $E$ of edges,
    each element $e \in E$ of which
    corresponds to an unordered pair of vertices $\{u,v\}$.
    More formally, there is a map
    \[
        \iota: E \to~ \faktor{ V \times V \setminus \{(v,v):v \in N\} }{ \{(u,v)\sim(v,u)\mid (u,v) \in N \times N\} }~.
    \]
    implicit in the definition of $G$, which we will write $G =
    % (V,E{,~ \color{gray}\iota})$ 
     (N,E, \iota)$ 
    only when being extra careful.
    \qedhere
\end{defn}

It is common to identify a graph $H = (N,A)$ (or an undirected graph $G = (N,E)$) with its (symmetric) adjacency matrix
\[
    \mathbb A_H = \Big[ \#\Big\{ a \in A : \begin{array}{c}\Src a = u,\\ \Tgt a = v \end{array} \Big\} \Big]_{(u,v) \in N \times N}
    \qquad
    \mathbb A_G = \Big[
    \#\{ e \in E : \iota(e) = \{u,v\}         \Big]_{(u,v) \in N\times N}~~,
\]
in part because there is a natural bijection between
(undirected) multigraphs and (symmetric) square matrices
over the natural numbers.  
For example:
\[
    \begin{bmatrix}
        0 & 1 & 2 \\
        1 & 0 & 0 \\
        2 & 0 & 1
    \end{bmatrix}
    \qquad
    \leftrightsquigarrow
    \qquad
    \begin{tikzpicture}[center base]
        \begin{scope}
            % \node at (0:1) (a) {a};
            % \node at (120:1) (b) {b};
            % \node at (-120:1) (c) {c};
            \node at (0,0) (a) {a};
            \node at (1,0) (b) {b};
            \node at (-1,0) (c) {c};
        \end{scope}
        \draw[-] (a) to (b);
        % \draw (a) to[bend left] (b);
        % \draw (b) to[bend left] (a);
        
        \draw[-] (a) to[bend left] (c);
        \draw[-] (a) to[bend right] (c);
        
        \path[every loop/.style={}] (c) edge[loop above] (c);
    \end{tikzpicture}
\]

A (directed) graph has more information than an undirected one.
There are natural ways to convert between the two: one forgets the direction of arcs to turn a directed graph into an undirected one, and annotates each arc with arrows in both directions to make an undirected graph directed. These choices are essentially locked in if we want the correspondence with square matrices to hold properly. 
\[ 
\begin{tikzcd}
\text{(Directed) Graphs} \ar[r,bend left,"\mathit{forget}"]&
\text{Undirected Graphs}
\ar[l,bend left,"\mathit{annotate}"]
\end{tikzcd}
\]
in which
$\mathit{forget} \circ \mathit{annotate} = \id_{\text{Undirected Graphs}}$
but
$\mathit{anotate} \circ \mathit{forget}$ is not the identity on (directed) graphs. Technically, this makes $\mathit{forget}$ a \emph{retraction}, and $\mathit{annotate}$ a \emph{section}.

\begin{defn}
    A bipartite graph $G = (L, R, E)$ is a graph $(L \sqcup R, E)$ whose
    vertices are partiioned into two components $V = L \sqcup R$, 
    and whose edges $E \subset L \times R$ are only between $L$ and $R$. 
\end{defn}


\fadeout{ \begin{defn}
    A directed bipartite graph $G = (L, R, E)$ is a bipartite graph $(L, R, E)$ whose edges $E \subset (L \times R) \cup (R \times L)$ are directed. 
    \end{defn} }


\begin{defn}
    A \emph{hypergraph} $G = (V, \mathcal E)$ is a set $V$ of vertices,
    and a collection $\cal E$ of edges, which correspond to finite
    subsets of vertices. 
\end{defn}

Thus, a hypergraph is the generalization of an undirected graph in which the codomain of $\iota : \mathcal E \to 2^V$ is arbitrary subsets of $V$, not just those of cardinality 2. 


\begin{prop} 
There is a natural bijection between hypergraphs and bipartite graphs:
% \def\bitgr{\mathit{BG}}
\def\bigr{\mathit{bipart}}
% \def\bitgr{\mathit{HG}}
\def\hygr{\mathit{hyper}}
\begin{align*}
    \bigr(V, \mathcal E) &:=  (V, \mathcal E, \{(v, E) \in V \times \mathcal E : v \in E \} ) \\
    % (L, \{\{x \in L : \} \}) &\maspfrom (L, R, E)
    \hygr(L, R, E) &:= (L, \{\{v \in L : (v,r) \in E\} : r \in R\}),
\end{align*}    
    \[
        \bigr \circ \hygr = \mathrm{id}_{BG}
        \quad\text{and}\quad
        \hygr \circ \bigr = \mathrm{id}_{HG}.
    \]
\end{prop}

The consequences of this can be unintuitive.
It is common to think of bipartite graphs as a strict (particularly nice) special case of ordinary undirected graphs, which themselves are a strict (particularly easy to draw) strict special case of hypergraphs.  
By transitivity, one might expect bipartite graphs to naturally be an extremely strict special case of hypergraphs---yet in fact they are naturally isomorphic. 

\begin{defn}
    A \emph{directed} hypergraph $(N, \mathcal A)$ is a set $N$ of nodes,
    and a collection $\Ar$ of hyperarcs, each of which has a set $S_a \subset N$ of
    source variables and a set $T_a \subset N$ of target variables.        
\end{defn}

A directed hypergraph $(N, \mathcal A)$ can be equivalently defined as an (ordinary) directed graph $(2^N, \Ar)$ whose set of nodes is the powerset of some set $N$. 
% 
% \medskip
% \hrule
% \medskip
% 
\begin{defn}
The \emph{dual} of the hypergraph $G = (V, \mathcal E)$ is
\[
    \check G := (\mathcal E, \{\{e \in \mathcal E : v \in e\} : v \in V\})
    .\qedhere
\]
\end{defn}

\begin{defn}
    The \emph{dual} of a directed hypergraph $\mathcal H = (N, \Ar)$ is 
    \(
        \check{\mathcal H} := (\mathcal A, N)
    \),
    where
    % \[
    $
    \check{\Src n} = \{ a \in \Ar : n \in \Tgt a\} \quad\text{and}\quad
    \check{\Tgt n} = \{ a \in \Ar : n \in \Src a\}.
    % \qedhere
    % \]
    $
\end{defn}

% \begin{prop}
We now verify that $\check{\check{\mathcal H}} = \mathcal H$.
Observe that 
\begin{align*}
    \check{\check{\Src a}}
    &= \{ n \in N : a \in \check{\Tgt n} \} \\
    &= \{ n \in N : a \in \{ a' \in \Ar :  n \in \Src a' \} \} \\
    &= \{ n \in N : n \in \Src a\} 
    = \Src a;
\end{align*}
% \end{prop}
symetrically, $\Tgt a = \check{\check{\Tgt a}}$. 
% What does this look like?
% To visualize . 
% One might wonder why $\check\Src n$ is definde in terms of $\Tgt a$ instead of $\Src a$.

See \cref{fig:dhygraph-duals} for some visual illustrations.
\begin{figure}%
    \centering
% \begin{tabular}{c}
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node[] (X) at (0,1) {X};
        \node[] (Y) at (1,1) {Y};
        \node[] (Z) at (2,1) {Z};
    \end{scope}
        \draw[arr1,<-] (X) to node[above]{\small 1} +(-0.9,0);
        \draw[arr1] (X) to node[above,pos=0.35]{\small 2} (Y);
        \draw[arr1] (Y) to node[above,pos=0.35]{\small 3} (Z);
    % \end{tikzpicture}\\
    % \begin{tikzpicture}
        \node[draw,outer sep=2pt] (1) at (-0.5,0) {1};
        \node[draw,outer sep=2pt] (2) at (0.5,0) {2};
        \node[draw,outer sep=2pt] (3) at (1.5,0) {3};
        \draw[->,arr1] (1) to node[below,pos=0.35]{\small X} (2);
        \draw[->,arr1] (2) to node[below,pos=0.35]{\small Y} (3);
        \draw[arr1,] (3) to node[below,pos=0.35]{\small Z} +(0.9,0);
    \end{tikzpicture}
    \hfill
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node (A) at (0,0) {A};
        \node (B) at (1,0.6) {B};
        \node (C) at (1,-0.6) {C};
        \node (D) at (2,0) {D};
    \end{scope}
        \draw[arr1,<-] (A) to node[above]{\small 1} +(-0.9,0);
        \draw[arr1] (A) to node[above,pos=0.35]{\small 2} (B);
        \draw[arr1] (A) to node[below,pos=0.35]{\small 3} (C);
        \mergearr[arr1] BCD
        \node[above=1pt of center-BCD]{\small 4};
    % \end{tikzpicture}\\
    % \begin{tikzpicture}
    \begin{scope}[shift={(-0.5,-2)}]
        \node[draw,outer sep=2pt] (1) at (0,0) {1};
        \node[draw,outer sep=2pt] (2) at (1,0.6) {2};
        \node[draw,outer sep=2pt] (3) at (1,-0.6) {3};
        \node[draw,outer sep=2pt] (4) at (2,0) {4};
        \unmergearr[arr1] 123
        \node[above left=1pt and 1pt of center-123]{\small A};
        \draw[arr1] (2) to node[above,pos=0.35]{\small B} (4);
        \draw[arr1] (3) to node[below,pos=0.35]{\small C} (4);
        \draw[arr1,] (4) to node[above]{\small D} +(0.9,0);
    \end{scope}
    \end{tikzpicture}% \end{tabular}
    %
    \hfill
    %
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node[draw,outer sep=2pt] (X) at (0,0) {$X$};
        \node[draw,outer sep=2pt] (Y) at (1,0) {$Y$};
    \end{scope}
        \draw[arr1] (X) to node[above,pos=0.35]{\small 1} (Y);
        \draw[arr1,<-] (Y) to node[above]{\small 2} +(0.9,0);
        %
        \begin{scope}[shift={(0.5,-1.5)}]
        \node[draw,outer sep=2pt] (1) at (0,0) {1};
        \node[draw,outer sep=2pt] (2) at (1,0) {2};
        \draw[arr1,<-] (1) to node[above]{\small $X$} +(-0.9,0);
        % \draw[arr1] ()
        \coordinate (a) at (0.5,-0.5);
        \mergearr[arr1] 12a
        \node at (0.5,0) {\small $Y$};
        \end{scope}
    \end{tikzpicture}
    \caption[Examples of directed hypergraphs and their duals]
    {Examples of directed hypergraphs (first row) and their duals (second row).}
        \label{fig:dhygraph-duals}
\end{figure}%
We remark that
the left and center diagrams on the top can be viewed as (the hypergraphs corresponding to) qualitative Bayesian Networks, by regarding X,Y,Z and A,B,C,D as variables, and imagining that there is a (randomized) causal determination occuring along each arc. 
% The structures that arise from causal models
One can also imagine an analogue with cycles---resuling in perhaps a (randomized) causal model of the given shape.  But a causal model has one equation corresponding to each variable, and the corresponding hypergraphs thus has exactly one hyperarc leading to it. 
In the dual hypergraphs, one should view the nodes as processes and the arcs as wires. Such a hypergrah has precisely one hyperarc leading out of every node. When wires branch, one imagines a copy; when two arcs point to the same process (as in process 4, in the middle center), that process takes both of the wires as inputs.
In the duals of hypergraphs corresponding to causal models, there are no two-tailed arrows, which might be thought of as a ``merge''. 
Yet it is not clear how to merge the values of two variables, when they are not the same, in general---especially if we expect associativity and commutativity, as we do with \emph{copy}. 


\TODO[((What can be done with these objects? \\TODO: tie into the thesis elswehere or remove.))]



% \TODO[
%     Possible to have other identity morphisms? Require that, for each $X \in \N$, the identity morphism $\dg M(\ed {\mathrm{id}}XX)$ preserves the base measure $\lambda_X$, in addition to pre/post committing 
% ]

% \begin{fact}
%     All morphisms 
% \end{fact}

\subsection{Independencies}

\subsection{Information Theory}

\subsection{Graphical Models}

There are two aspects any graphical model: 
a ``qualitative/structural'' aspect, which describes influences between variables, and
a ``quantitative/observational'' aspect, 
    that annotates those influences with data.

A qualitative BN, for example, is a directed graph whose semantics are given in terms of independencies: any variable $X$ is independent of its non-descendents, given the values of its parents, $\mathbf{Pa}\, X$.  
A quantitative BN, then, includes both that directed graph, and also each variable $X$ to a conditional probability distribution $\Pr_X(X|\mathbf{Pa}\, X)$.

\section{An Ode to Relative Entropy}

In this dissertation, we will derive a 

Relative entropy is extremely special, and arguably exactly the
appropriate measure of discrepancy between belief and reality.  It has
a clear information theoretic interpretation as communication overhead
from an incorrect belief.  There are many nice axiomatizations of it;
some important ones include Renyi's, Tom Leinster's result showing
that it is the only function satisfying three trivial regularity
properties and the chain rule, and this characterization as the unique
functor from statistical maps to the additive monoid $[0,\infty]$, which
might be particularly persuasive to a category theorist.  It is the
only statistical divergence that is both a Bregman divergence and an
f-divergence.  Its Hessian is the Fisher metric, which is the unique
metric tensor that is invariant under sufficient statistics. In that
geometry, I have recently shown that gradient flow of relative entropy
with respect to its first (``belief'') argument amounts to
multiplicative interpolation of probability measures, while gradient
flow of relative entropy with respect to its second (``reality'')
argument amounts to additive interpolation. 
