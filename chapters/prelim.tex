    \label{chap:prelim}

Although the high-level ideas in this thesis are often intuitive and philosophical, 
% our primary vehicle for results is the 
% we make those ideas precise using some  
but in their most precise forms, they are often quite mathematical.
In this chapter, we build up a library of standard mathematical concepts needed to understand them. 

\section{Background, Notation, and Critical Preliminaries}

% Only \cref{sec:}
Let's start with the basics.
There are two kinds of equalities: definitions, and assertions. 
We distinguish between the two.
Namely, we write ``$A := B$'' when we want to (re)defines the symbol $A$ so stand for $B$,
but write ``$A = B$'' to assert that the (already defined) expressions $A$ and $B$ stand for the same object.

% \subsection
% \paragraph
% {Sets, Variables, and Vectors}
    \label{ref:prelim-basic}

\paragraph{Basic Concepts and Notation.}
A \emph{set} is a collection of unique elements, 
    such as 
    $\{1, \texttt{hello}, 7\}$,
    the empty set $\emptyset$, 
    the natural numbers $\mathbb N = \{0, 1, 2, \ldots \}$,
    or the set $\mathbb R$ of real numbers. 
We write $a \in A$, or equivalently $a : A$, 
    to indicate that $a$ is an element of the set $A$.
For brevity, we write $a,b \in A$ to indicate that both $a \in A$ and $b \in A$. 
We write $|A|$ or $\# A$ for the \emph{cardinality} of a set $A$;
    typically when we write this,  $A$ will be \emph{finite}, 
    meaning it contains $|A| \in \mathbb N$ distinct elements. 
When $n \in \mathbb N$ is a natural number, let
    $[n] := \{ 0, 1, \ldots, n-1 \}$ denote the set of the first $n$ natural numbers.
$A$ is a \emph{subset} of $B$ (written $A \subseteq B$) means that every element of $A$ is also an element of $B$, and $2^A$ is the set of all subsets of $A$.

We write $\forall x \in X.~\varphi(x)$ to indicate that a condition $\varphi$ is true \emph{for all} $x \in X$, and $\exists x \in X.~ \varphi(x)$ to indicate that there \emph{exists some} $x \in X$ such that $\varphi(x)$ is true.
We use standard ``set-builder'' notation $\{ x \in X ~|~ \varphi(x) \}$ to construct the subset of $X$ that satisfies the property $\varphi$. 
If $\varphi$ and $\psi$ are 


% Given two sets $A$ and $B$, there are a number of ways t 
There are a number of important ways to combine two sets $A$ and $B$ to produce a third set. 
% The \emph{product }
% The \emph{union} $A \cup B := \{ x : x \in B \}$  


A \emph{map}, or \emph{function} $f : A \to B$ is an object that, 
    given $a \in \Ar$, produces a value $f(a) \in B$. 
So, $A \to B$ (also written $B^A$), 
    is the set of functions from $A$ to $B$.
Observe: if $2 = \{ 0, 1\}$ represents a set with 2 elements, then 
    the elements of $2^A$ are functions assigning either $0$ (absence) or $1$ (presence) to each element of $A$---that is, they are subsets of $A$. 

If $I$ is a set, then an \emph{indexed set}
    $\mat a = \{ a_i \}_{i \in I}$ consists of a collection of objects, one for each $a_i$. 
If $\forall i.~a_i \in A$, then $\mat a$ is just a function $\mat a : I \to A$. 
% The set of indexed 
% Therefore, if each $a_i \in A_i$, then  $\mat a : \sqcup_{i \in I} A_i  
A \emph{tuple} is the special case of a set indexed by $[n]$ for some natural number $n \in \mathbb N$. 


% - intersections, unions, minus
% - products
% - sums
% - tuples
% - coproduct / disjoint union
% - "space" = set + topology or geoemtry
% - unit interval


\paragraph{Vectors.}
For us, a \emph{vector} is a map from a finite set $S$, called its 
\emph{shape}, to the extended reals $\Rext := \mathbb R \cup \{\infty\}$.
We write $\mat u := [u_i]_{i \in S}$ to define a vector $\mat u$ by its components.
% Note that $
Vectors of the same shape
% Suppose that $\mat u, \mat v \in \Rext^S$ are two vectors of 
can be added (+), partially ordered ($\le$), or multiplied ($\odot$) pointwise as usual.
$\mat 1$ denotes an all-ones vector, of a shape implied by context.
If $\mat u$ and $\mat v$ are both of shape $S$, their \emph{inner product}, denoted either $\mat u \cdot \mat v$ or $\mat u ^{\sf T} \mat v$,
    is the real number $\sum_{i \in S} u_i v_i$. 

\paragraph{Variables.}
    \label{ssec:basic-vars}
A less standard concept, but one on which this thesis revolves is that of a \emph{variable}.
Intuitively, a variable represents some feature of the world or some property of some object. 
Mathematically, a variable exists on two levels.
\emph{Qualitatively}, a variable is just some unique identifier (the variable name), such as ``Height'', or $X$. 
We typically use capital roman letters for variables.
\emph{Quantitatively}, a variable $X$ is also associated with a set $\V(X)$, or simply $\V\!X$, of possible values. 
For example, $\V(\text{Height})$ might be the set of positive real numbers, 
    or the set $\{ \texttt{short}, \texttt{tall} \}$. 
A \emph{binary variable} is one whose possible values are $\{0, 1\}$, and a 
    \emph{real variable} is one whose possible values are $\mathbb R$. 

We can regard sets of variables $\mat X$ as variables themselves, with
$\V \mat X = \Pi_{X \in \mat X} \V X$.
Note that, by this definition, the empty set is a variable, and it takes on a single value (i.e., the unique setting of no variables).  
This might seem strange, but there is precisely one way of selecting a value for each variable in $\emptyset$: simply do nothing. 
To see this another way, observe that if $\mat X = \{ X_1, \ldots, X_n \}$ is a set of binary variables,
then $|\V\mat X| = 2^n$, so in the special case of $n=0$, we have $|\V \emptyset| = 2^0 = 1$. 

Similarly, tuples $(X_1, \ldots, X_n)$ of variables and indexed sets $\{X_i\}_{i \in I}$ of variables, are also themselves variables,
with 
\begin{align*}
    \V(X_1, \ldots, X_n) &= \{ (x_1, \ldots, x_n) \mid \forall i \in [n]. x_i \in \V(X_i) \}
\end{align*}
 % \text{and}\quad
More generally, $
    \V\{ X_i \}_{i \in I} = \prod_{i \in I} \V(X_i).  
$
% \end{align*}
If $f : \V\!X \to S$ is a function, then $f(X)$ is a variable, with $\V(f(X)) = S$. 

Given a joint setting $\mat x \in \V(\mat X)$, we write either $\mat x[X]$ or  $X(\mat x)$ for the value of the variable $X$ in the joint setting $\mat x$. The former is familiar lookup notation from many programming languages, and the latter is random variable notation, which we will get to in the next section.


\section{Probability, and Other Representations of Uncertainty}

We first give the simpler picture of probability in the finite case, which is largely representative of one should think of probability in general (\cref{ssec:fin-prob}). 

\subsection{The Finite Case}
    \label{ssec:fin-prob}
% We first give the simpler picture of probability in the finite case; for now, assume the variables $X$ and $Y$ can take on only finitely many possible values (i.e., $|\V\!X|, |\V Y| \in \mathbb N$).
% In this section, let's assume that all variables can take on only finitely many possible values
%     (i.e., $|\V\!X|, |\V Y| \in \mathbb N$).
Suppose that $\Omega$ is a finite set. 
% A \emph{probability distriu} 
% Suppose $X$ and $Y$ are (finite) variables. 
A \emh{probability distribution} $\mu$ over $\Omega$ is essentially a function 
% $\mu : \V\!\X \to [0,1]$ such that $\sum_{x \in \V X} \mu(x) = 1$.
$\mu : \Omega \to [0,1]$ such that $\sum_{\omega \in \Omega} \mu(\omega) = 1$.
In this finite case, this means $\mu$ is a vector of shape 
% $\V\! X$,
$\Omega$ 
satisfying $\mu \ge 0$ and $\mu \cdot \mat 1 = 1$. 
% We write $\Delta \V\!X$ for the set of all distributions over (the values of) $X$,
% which is also known as the \emph{probability simplex}.
The \emph{probability simplex} $\Delta \Omega$ is defined as the set of all probability distributions over $\Omega$. 
For $U \subseteq \V\!X$, $\mu(U) = \sum_{x \in U} \mu(x)$. 
While we have just defined $\mu(U)$ for subsets $U$ of $\Omega$ in terms of $\mu(\omega)$ for elements $\omega \in \Omega$, we remark that, in the general case, it is the other way around: a distribution $\mu$ is defined as an object that assigns probability to subsets of $\Omega$ in a way that is compatible with intuitions from the finite case we have just described.

%% adding variables.
In this dissertation, the way we represent and manipulate probability distributions will make heavy use of the notion of a variable, which we have started to develop in the previous section.
Specifically, we will essentially always work with probability distributions over the values of a variable. This just a notational convenience, since we can always define a variable $W$ with possible values $\V W := \Omega$.  

This allows us to 
use notation standard notation and
write $\mu(X)$ in place of $\mu$ to emphasize that $\mu \in \Delta \V \! X$ in a compact way. 
%


%  $\mu(\mat X) = \mu(X_1, \ldots, X_n)$
A \emph{joint distribution} over a finite set of variables $\mat X = \{X_1, \ldots, X_n\}$ is just a distribution over joint settings of all variables---i.e, a distribution over $\V \mat X$. 
%TODO: the above is awkward.
If $\mu(\mat X)$ is a joint distribution and $X \in \mat X$, then we write $\mu(X)$ for its \emph{marginal} on $X$, which is given by
\[
    \mu(X{=}x) = \mu(X)(x) := \sum_{\substack{\mat x \in \V \mat X \\ \mat x[X] = x}} \mu(\mat x) 
\]

A \emph{conditional probability distribution (cpd)} on $Y$ given $X$ is a function $p : \V X \to \Delta \V Y$ that, for each $x \in \V\!X$, yields a probability distribution
% \[ 
$p(Y | X{=}x) = p(Y | x)  \in \Delta \V Y$.
% \]

Cpds have special notation; we refer to $p$ also as $p(Y|X)$. 

%marginals 

We write $\Delta S$ to denote the set of probability distributions over a finite set $S$.
A conditional probability distribution (cpd) $p(Y|X)$ is a map
$p : \V\mskip-1.5mu  X \to \Delta \V Y$ assigning to each $x \in \V\mskip-1.5mu X$ a
probability distribution 
% $p(Y|X{=}x) \in \Delta \V$.
$p(Y|x) \in \Delta \V Y$, which is shorthand for $p(Y|X{=}x)$.
% $p$ that assigns each $x \in \V\mskip-1.5mu X$ a probability distribution
% $p(Y|X{=}x) \in \Delta \V Y$.
Given a distribution $\mu$ over (the values of) a set of variables including $X$ and $Y$,
we write $\mu(X)$ for its marginal on $X$,
and $\mu(Y|X)$ for the cpd obtained by 
conditioning on $X$ and marginalizing to $Y$.



\subsection{The General Case}

A \emph{measurable space} $(\Omega, \mathcal F_\Omega)$ is a set $\Omega$,
called the \emph{outcome space}, together with a collection $\mathcal F_\Omega \subseteq 2^\Omega$ of subsets of $\Omega$, which considered \emph{measurable}, and called \emph{events}. 
The elements of $\Omega$, should be thought of \emph{possible outcomes} or \emph{possible worlds} for in the model.
The set $\mathcal F_\Omega$ of events must also be an algebra, be closed under intersection, union, and complement---%
that is, $U, V \in \mathcal F_\Omega$, then $U \cup V, U \cap V, \Omega - U \in \mathcal F_\Omega$.  
% One should think of $\mathcal A = 2^\Omega$ as consisting of every 
% For all but a few select places, one can ignore $\mathcal F_\Omega$ in this dissertation.
Often in probabilistic modeling, and in this dissertation in particular, the $\mathcal F_\Omega$.
When $\Omega$ is finite, we allow all subsets to be measured by defining $\mathcal F_\Omega := 2^\Omega$,
    and there are also standard choices when $\Omega = \mathbb R$, or when it is constructed o. 
Either way, we will mostly leave $\mathcal F_\Omega$ implicit, and referring to a measurable space simply as $\Omega$. 

A \emph{probability distribution} over a (measurable) space $(\Omega, \mathcal F)$ is a function $\Pr : \Omega \to [0,1]$
satisfying the three Kolmogorov axioms:
% \begin{enumerate}
%     \item 
    $\Pr(\emptyset) = 0$, 
    $\Pr(\Omega) = 1$, 
    and 
% \end{enumerate}

% See \cref{sec:meas-prob} for the more formal definition in terms of measure theory. 



\section{Graph Theory}
\begin{defn}
    A \emph{(directed) (multi)graph} $G = (N, A)$, or simply a \emph{graph}, is a set $N$ of nodes,
    and a collection $A$ of arcs, such that each $a \in A$ has a source node $\Src a \in N$ and a target node $\Tgt a \in N$.
    So, formally, the definition is
    $G = (N, A, \Src{}, \Tgt{})$, with $\Src{},\Tgt{}: A \to N$ often left implicit.
\end{defn}

\begin{defn}
    % [Undirected (Multi) Graph]
    An \emph{undirected} (multi)graph $G = (N,E)$ is a set $N$ of vertices (or nodes)
    and a set $E$ of edges,
    each element $e \in E$ of which
    corresponds to an unordered pair of vertices $\{u,v\}$.
    More formally, there is a map
    \[
        \iota: E \to~ \faktor{ V \times V \setminus \{(v,v):v \in N\} }{ \{(u,v)\sim(v,u)\mid (u,v) \in N \times N\} }~.
    \]
    implicit in the definition of $G$, which we will write $G =
    % (V,E{,~ \color{gray}\iota})$ 
     (N,E, \iota)$ 
    only when being extra careful.
    \qedhere
\end{defn}

It is common to identify a graph $H = (N,A)$ (or an undirected graph $G = (N,E)$) with its (symmetric) adjacency matrix
\[
    \mathbb A_H = \Big[ \#\Big\{ a \in A : \begin{array}{c}\Src a = u,\\ \Tgt a = v \end{array} \Big\} \Big]_{(u,v) \in N \times N}
    \qquad
    \mathbb A_G = \Big[
    \#\{ e \in E : \iota(e) = \{u,v\}         \Big]_{(u,v) \in N\times N}~~,
\]
in part because there is a natural bijection between
(undirected) multigraphs and (symmetric) square matrices
over the natural numbers.  
For example:
\[
    \begin{bmatrix}
        0 & 1 & 2 \\
        1 & 0 & 0 \\
        2 & 0 & 1
    \end{bmatrix}
    \qquad
    \leftrightsquigarrow
    \qquad
    \begin{tikzpicture}[center base]
        \begin{scope}
            % \node at (0:1) (a) {a};
            % \node at (120:1) (b) {b};
            % \node at (-120:1) (c) {c};
            \node at (0,0) (a) {a};
            \node at (1,0) (b) {b};
            \node at (-1,0) (c) {c};
        \end{scope}
        \draw[-] (a) to (b);
        % \draw (a) to[bend left] (b);
        % \draw (b) to[bend left] (a);
        
        \draw[-] (a) to[bend left] (c);
        \draw[-] (a) to[bend right] (c);
        
        \path[every loop/.style={}] (c) edge[loop above] (c);
    \end{tikzpicture}
\]

A (directed) graph has more information than an undirected one.
There are natural ways to convert between the two: one forgets the direction of arcs to turn a directed graph into an undirected one, and annotates each arc with arrows in both directions to make an undirected graph directed. These choices are essentially locked in if we want the correspondence with square matrices to hold properly. 
\[ 
\begin{tikzcd}
\text{(Directed) Graphs} \ar[r,bend left,"\mathit{forget}"]&
\text{Undirected Graphs}
\ar[l,bend left,"\mathit{annotate}"]
\end{tikzcd}
\]
in which
$\mathit{forget} \circ \mathit{annotate} = \id_{\text{Undirected Graphs}}$
but
$\mathit{anotate} \circ \mathit{forget}$ is not the identity on (directed) graphs.
% Technically, this makes $\mathit{forget}$ a \emph{retraction}, and $\mathit{annotate}$ a \emph{section}.

\begin{defn}
    A bipartite graph $G = (L, R, E)$ is a graph $(L \sqcup R, E)$ whose
    vertices are partiioned into two components $V = L \sqcup R$, 
    and whose edges $E \subset L \times R$ are only between $L$ and $R$. 
\end{defn}


% \fadeout{ \begin{defn}
%     A directed bipartite graph $G = (L, R, E)$ is a bipartite graph $(L, R, E)$ whose edges $E \subset (L \times R) \cup (R \times L)$ are directed. 
%     \end{defn} }


\begin{defn}
    A \emph{hypergraph} $G = (V, \mathcal E)$ is a set $V$ of vertices,
    and a collection $\Ed$ of \emph{hyperedges}, which correspond to finite
    subsets of $V$. 
\end{defn}

An ordinary undirected graph may be viewed as the special case in which every hyperedge contains  two vertices.

\begin{prop} 
There is a natural bijection between hypergraphs and bipartite graphs:
% \def\bitgr{\mathit{BG}}
\def\bigr{\mathit{bipart}}
% \def\bitgr{\mathit{HG}}
\def\hygr{\mathit{hyper}}
\begin{align*}
    \bigr(V, \mathcal E) &:=  (V, \mathcal E, \{(v, E) \in V \times \mathcal E : v \in E \} ) \\
    % (L, \{\{x \in L : \} \}) &\maspfrom (L, R, E)
    \hygr(L, R, E) &:= (L, \{\{v \in L : (v,r) \in E\} : r \in R\}),
\end{align*}    
    \[
        \bigr \circ \hygr = \mathrm{id}_{BG}
        \quad\text{and}\quad
        \hygr \circ \bigr = \mathrm{id}_{HG}.
    \]
\end{prop}

The consequences of this can be unintuitive.
It is common to think of bipartite graphs as a strict (particularly nice) special case of ordinary undirected graphs, which themselves are a strict (particularly easy to draw) strict special case of hypergraphs.  
By transitivity, one might expect bipartite graphs to naturally be an extremely strict special case of hypergraphs---yet in fact they are naturally isomorphic. 

\begin{defn}
    A \emph{directed} hypergraph $(N, \mathcal A)$ is a set $N$ of nodes,
    and a collection $\Ar$ of hyperarcs, each of which has a set $S_a \subset N$ of
    source variables and a set $T_a \subset N$ of target variables.        
\end{defn}

A directed hypergraph $(N, \mathcal A)$ can be equivalently defined as an (ordinary) directed graph $(2^N, \Ar)$ whose set of nodes is the powerset of some set $N$. 
% 
% \medskip
% \hrule
% \medskip
% 
\begin{defn}
The \emph{dual} of the hypergraph $G = (V, \mathcal E)$ is
\[
    \check G := (\mathcal E, \{\{e \in \mathcal E : v \in e\} : v \in V\})
    .\qedhere
\]
\end{defn}

\begin{defn}
    The \emph{dual} of a directed hypergraph $\mathcal H = (N, \Ar)$ is 
    \(
        \check{\mathcal H} := (\mathcal A, N)
    \),
    where
    % \[
    $
    \check{\Src n} = \{ a \in \Ar : n \in \Tgt a\} \quad\text{and}\quad
    \check{\Tgt n} = \{ a \in \Ar : n \in \Src a\}.
    % \qedhere
    % \]
    $
\end{defn}

% \begin{prop}
We now verify that $\check{\check{\mathcal H}} = \mathcal H$.
Observe that 
\begin{align*}
    \check{\check{\Src a}}
    &= \{ n \in N : a \in \check{\Tgt n} \} \\
    &= \{ n \in N : a \in \{ a' \in \Ar :  n \in \Src a' \} \} \\
    &= \{ n \in N : n \in \Src a\} 
    = \Src a;
\end{align*}
% \end{prop}
symetrically, $\Tgt a = \check{\check{\Tgt a}}$. 
% What does this look like?
% To visualize . 
% One might wonder why $\check\Src n$ is definde in terms of $\Tgt a$ instead of $\Src a$.

See \cref{fig:dhygraph-duals} for some visual illustrations.
\begin{figure}%
    \centering
% \begin{tabular}{c}
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node[] (X) at (0,1) {X};
        \node[] (Y) at (1,1) {Y};
        \node[] (Z) at (2,1) {Z};
    \end{scope}
        \draw[arr1,<-] (X) to node[above]{\small 1} +(-0.9,0);
        \draw[arr1] (X) to node[above,pos=0.35]{\small 2} (Y);
        \draw[arr1] (Y) to node[above,pos=0.35]{\small 3} (Z);
    % \end{tikzpicture}\\
    % \begin{tikzpicture}
        \node[draw,outer sep=2pt] (1) at (-0.5,0) {1};
        \node[draw,outer sep=2pt] (2) at (0.5,0) {2};
        \node[draw,outer sep=2pt] (3) at (1.5,0) {3};
        \draw[->,arr1] (1) to node[below,pos=0.35]{\small X} (2);
        \draw[->,arr1] (2) to node[below,pos=0.35]{\small Y} (3);
        \draw[arr1,] (3) to node[below,pos=0.35]{\small Z} +(0.9,0);
    \end{tikzpicture}
    \hfill
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node (A) at (0,0) {A};
        \node (B) at (1,0.6) {B};
        \node (C) at (1,-0.6) {C};
        \node (D) at (2,0) {D};
    \end{scope}
        \draw[arr1,<-] (A) to node[above]{\small 1} +(-0.9,0);
        \draw[arr1] (A) to node[above,pos=0.35]{\small 2} (B);
        \draw[arr1] (A) to node[below,pos=0.35]{\small 3} (C);
        \mergearr[arr1] BCD
        \node[above=1pt of center-BCD]{\small 4};
    % \end{tikzpicture}\\
    % \begin{tikzpicture}
    \begin{scope}[shift={(-0.5,-2)}]
        \node[draw,outer sep=2pt] (1) at (0,0) {1};
        \node[draw,outer sep=2pt] (2) at (1,0.6) {2};
        \node[draw,outer sep=2pt] (3) at (1,-0.6) {3};
        \node[draw,outer sep=2pt] (4) at (2,0) {4};
        \unmergearr[arr1] 123
        \node[above left=1pt and 1pt of center-123]{\small A};
        \draw[arr1] (2) to node[above,pos=0.35]{\small B} (4);
        \draw[arr1] (3) to node[below,pos=0.35]{\small C} (4);
        \draw[arr1,] (4) to node[above]{\small D} +(0.9,0);
    \end{scope}
    \end{tikzpicture}% \end{tabular}
    %
    \hfill
    %
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node[draw,outer sep=2pt] (X) at (0,0) {$X$};
        \node[draw,outer sep=2pt] (Y) at (1,0) {$Y$};
    \end{scope}
        \draw[arr1] (X) to node[above,pos=0.35]{\small 1} (Y);
        \draw[arr1,<-] (Y) to node[above]{\small 2} +(0.9,0);
        %
        \begin{scope}[shift={(0.5,-1.5)}]
        \node[draw,outer sep=2pt] (1) at (0,0) {1};
        \node[draw,outer sep=2pt] (2) at (1,0) {2};
        \draw[arr1,<-] (1) to node[above]{\small $X$} +(-0.9,0);
        % \draw[arr1] ()
        \coordinate (a) at (0.5,-0.5);
        \mergearr[arr1] 12a
        \node at (0.5,0) {\small $Y$};
        \end{scope}
    \end{tikzpicture}
    \caption[Examples of directed hypergraphs and their duals]
    {Examples of directed hypergraphs (first row) and their duals (second row).}
        \label{fig:dhygraph-duals}
\end{figure}%
We remark that
the left and center diagrams on the top can be viewed as (the hypergraphs corresponding to) qualitative Bayesian Networks, by regarding X,Y,Z and A,B,C,D as variables, and imagining that there is a (randomized) causal determination occuring along each arc. 
% The structures that arise from causal models
One can also imagine an analogue with cycles---resuling in perhaps a (randomized) causal model of the given shape.  But a causal model has one equation corresponding to each variable, and the corresponding hypergraphs thus has exactly one hyperarc leading to it. 
In the dual hypergraphs, one should view the nodes as processes and the arcs as wires. Such a hypergrah has precisely one hyperarc leading out of every node. When wires branch, one imagines a copy; when two arcs point to the same process (as in process 4, in the middle center), that process takes both of the wires as inputs.
In the duals of hypergraphs corresponding to causal models, there are no two-tailed arrows, which might be thought of as a ``merge''. 
Yet it is not clear how to merge the values of two variables, when they are not the same, in general---especially if we expect associativity and commutativity, as we do with \emph{copy}. 


\TODO[((What can be done with these objects? \\TODO: tie into the thesis elswehere or remove.))]



% \TODO[
%     Possible to have other identity morphisms? Require that, for each $X \in \N$, the identity morphism $\dg M(\ed {\mathrm{id}}XX)$ preserves the base measure $\lambda_X$, in addition to pre/post committing 
% ]

% \begin{fact}
%     All morphisms 
% \end{fact}

% \subsection{Independencies}


% \subsection{Graphical Models}
\section{A Review of Graphical Models}

There are two aspects any graphical model: 
a ``qualitative/structural'' aspect, which describes influences between variables, and
a ``quantitative/observational'' aspect, 
    that annotates those influences with data.

A qualitative BN, for example, is a directed graph whose semantics are given in terms of independencies: any variable $X$ is independent of its non-descendents, given the values of its parents, $\mathbf{Pa}\, X$.  
A quantitative BN, then, includes both that directed graph, and also each variable $X$ to a conditional probability distribution $\Pr_X(X|\mathbf{Pa}\, X)$.


\section{Information Theory}

Let $\mu$ be a probability distribution, and be $X,Y,Z$
%oli6:
% (sets of) random variables (taking on finitely many values).
    be (sets of) discrete random variables.
%oli16:
The \emph{entropy} of $X$ is the uncertainty in $X$, when it is distributed according to $\mu$, as measured by the number of bits of information needed (in expectation) needed to determine it, if the distribution $\mu$ is known.  It is given by 
\[
    \H_\mu(X) := \sum_{x \in \V(X)} \mu(X{=}x) \log \frac{1}{\mu(X{=}x)} \qquad= -\Ex_{\mu}[\log \mu(X)],
\]
and a few very important properties; chief among them, $\H_\mu(X)$ is non-negative, and equal to zero iff $X$ is a constant according to $\mu$. 
The ``joint entropy'' $\H(X,Y)$ is just the entropy of the combined variable $(X,Y)$ whose values are pairs $(x,y)$ for $x \in \V(X),y \in \V(Y)$; this is the same as the entropy of the variable $X \cup Y$ when $X$ and $Y$ are themselves sets of variables. 

The \emph{conditional entropy} of $Y$ given $X$
measures the uncertainty present in $Y$ if one knows the value of $X$
(think: the information in $Y$ but not $X$),
and is equivalently defined as any of the following three quantities:
\[
\H_\mu( Y | X) :=
        \quad
    \Ex_{\mu} [~\log \nicefrac1{\mu(Y | X)}~]
        \quad
    =\H_\mu(X,Y) - \H_\mu(X)
        \quad
    =\Ex_{x \sim \mu(X)} [~\H_{\mu \mid X{=}x}(Y)~]    
.
\]
The \emph{mutual information} $\I(X;Y)$,
and its conditional variant $\I(X;Y|Z)$, 
are given, respectively, by
\[
    \I_\mu(X;Y) :=
        \Ex_{\mu} \Big[ \log \frac{\mu(X,Y)}{\mu(X) \mu(Y)}\Big],
    \quad\text{and}\quad
    \I(X;Y|Z):= 
        \Ex_{\mu} \Big[ \log \frac{\mu(X,Y,Z)\mu(Z)}{\mu(X,Z) \mu(Y,Z)}\Big].
\]
The former is non-negative and equal to zero iff $\mu \models X \CI Y$, and the latter is non-negative and equal to zero iff $\mu \models X \CI Y \mid Z$. 
All of these quantities are purely ``structural'' or ``qualitative'' in the sense that they are invariant to relabelings of values, and 

Just as conditional entropy can be written as a linear combination of unconditional entropies, so too can conditional mutual information be written as a linear combination of unconditional mutual informations: $\I(X;Y|Z) = \I(X;(Y,Z)) - \I(X;Z)$.  
Thus conditional quantities are easily derived from the unconditional ones. But at the same time, the unconditional versions are clearly special cases of the conditional ones; for example, $\H_\mu(X)$ is clearly the special case of $\H(X|Z)$ when $Z$ is a constant (e.g., $Z = \emptyset$). 
Furthermore, entropy and mutual information are also interdefinable and generated by linear combinations of one another. 
It is easy to verify that 
$\I_\mu(X;Y) 
    % = \H_\mu(X,Y) - \H(Y|X) - \H(X|Y)
    = \H_\mu(X) + \H_\mu(Y) - \H(X,Y)
$
and 
$\I_\mu(X;Y|Z)
    = \H_\mu(X|Z) + \H_\mu(Y|Z) - \H(X,Y|Z)
$,
and thus mutual information is derived from entropy. 
Yet on the other hand, $\I_\mu(Y;Y) = \H_\mu(Y)$ and $\I_\mu(Y;Y|X) = \H_\mu(Y|X)$---thus entropy is a special case of mutual information. 


\subsection{An Ode to Relative Entropy}

In this dissertation, we will derive a 

Relative entropy is extremely special, and arguably exactly the
appropriate measure of discrepancy between belief and reality.  It has
a clear information theoretic interpretation as communication overhead
from an incorrect belief.  There are many nice axiomatizations of it;
some important ones include Renyi's, Tom Leinster's result showing
that it is the only function satisfying three trivial regularity
properties and the chain rule, and this characterization as the unique
functor from statistical maps to the additive monoid $[0,\infty]$, which
might be particularly persuasive to a category theorist.  It is the
only statistical divergence that is both a Bregman divergence and an
f-divergence.  Its Hessian is the Fisher metric, which is the unique
metric tensor that is invariant under sufficient statistics. In that
geometry, I have recently shown that gradient flow of relative entropy
with respect to its first (``belief'') argument amounts to
multiplicative interpolation of probability measures, while gradient
flow of relative entropy with respect to its second (``reality'')
argument amounts to additive interpolation. 



\begin{subappendices}    
\section{Primer on Advanded and Auxiliary Concepts}
The main body of this chapter has focused on material that is directly in the critical path to understanding every important result in this dissertation. 
 % 
 % the state of the field and building up a formal mathematical foundation on which the rest of the dissertation s.
%
At the other extreme, there is also some mathematical background needed only to understand a single chapter; that material is presented before it is needed. 
%
Here, we review some standard mathematical concepts that lie somewhere in between: these concepts play more minor roles than the ones already covered, and are common to a handful of chapters. 
%
%
We encourage a reader to quickly skim this section,
    and return to it if necessary. 


\subsection{Algebra}
\cref{chap:conf,chap:PDG-cat}

\begin{defn}[Monoid]%
    A \emph{monoid} is a tuple $(S, *, e)$, where $S$ is a set, $* : S \times S \to S$ is a binary operation, and $e \in S$ is a distinguished identity element, such that:
    \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt,]
        \item (associativity) $\forall a, b,c \in S.~~(a*b)*c = a*(b*c)$;
        \item (identity) $\forall a \in S.~~a * e = a = e * a$.
    \end{itemize}
    A monoid is called \emph{commutative} if it also satisfies
    \begin{itemize}[nosep]
        \item (commutativity): $\forall a, b \in S.~~ a * b = b * a$,
    \end{itemize}
    and \emph{idempotent} if it satisfies
    \begin{itemize}[nosep]
        \item (idempotence): $\forall a \in S.~~ a + a = a$. 
    \end{itemize}
    An idempotent semiring defines partial order by $a \le b \iff a + b = b$. 
\end{defn}

\subsection{Relations}

Let $\mathcal X = \{ X_1, \ldots, X_n \}$ be variables, traditionally called atributes. 
A \emph{relation} $R(\mathcal A) = R(X_1, \ldots, X_n) \subseteq 
\V(X_1) \times \cdots \times \V(X_n)$, or equivalently, $R : \prod_{i=1}^n \V\!X_i \to \{0,1\}$, is a subset of joint values of attributes. 
The natural number $n$ is called the \emph{arity} of $R$. 


The \emph{natural join} of two relations $R(A,B)$ and $S(B,C)$ combines them in a particularly obvious way: $(a,b,c) \in R \bowtie S$ iff $(a,b) \in R$ and $(b,c) \in S$. 
More generally, we have: 

% \begin{defn}[natural join]
%     \[
\begin{equation}
    R(\mathcal X) \bowtie S(\mathcal Y)
        := 
        \Big\{
            \boldsymbol\omega \in  \V(\mathcal X \cup \mathcal Y)
            ~\Big|~
            \mathcal X(\boldsymbol\omega) \in R \land
            \mathcal Y(\boldsymbol\omega) \in S
        \Big\}
        % \qedhere
%     \]
% \end{defn}
\end{equation}

At one extreme, if $\mathcal X$ and $\mathcal Y$ are disjoint sets of attributes, then $R(\mathcal X) \bowtie S(\mathcal Y)$ coincides with the cartesian product of $R \subseteq \V \mathcal X$ and $S \subseteq \V\mathcal Y$.
At the opposite extreme, if $\mathcal X = \mathcal Y$ are the same set of variables, then $R(\mathcal X) \bowtie S(\mathcal X)$ coincides with the intersection of the subsets $R$ and $S$. 

Even when $A_1, \ldots, A_n \subseteq \X$ are not disjoint, we give a convenient extended syntax by defining the quantity $R(a_1, \ldots, a_n)$, where $a_i \in \V(A_i)$. 
Concretely, define $R(a_1,\ldots, a_n) := 0$ if when $\{a_1, \ldots, a_n\}$ do not agree on the value of some shared attribute (i.e., if $\exists X \in \X, \exists i,j \in [n]. ~X \in A_i \cap A_j \land X(a_i) \ne X(a_j)$).
When $\{a,b,c\}$ do agree on all values of shared attributes, let $\mat x$ denote the joint value of $A \cup B \cup C$ obtained from $(a,b,c)$ by removing redundant copies of variable values.
In this case, define $R(a,b,c) := R(\mat x)$. 


\subsection{Geometry}

    
\end{subappendices}
