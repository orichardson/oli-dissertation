    \label{chap:prelim}

Although the high-level ideas in this thesis are often intuitive and philosophical, 
% our primary vehicle for results is the 
% we make those ideas precise using some  
but in their most precise forms, they are often quite mathematical.
In this chapter, we build up a library of standard mathematical concepts and notation needed to fully understand them. 


\section{Background, Notation, and Critical Preliminaries}

% Only \cref{sec:}
Let's start with the basics.
There are two kinds of equalities: definitions, and assertions. 
We distinguish between the two.
Namely, we write ``$A := B$'' when we want to (re)defines the symbol $A$ so stand for $B$,
but write ``$A = B$'' to assert that the (already defined) expressions $A$ and $B$ stand for the same object.

% \subsection
% \paragraph
% {Sets, Variables, and Vectors}
    \label{ref:prelim-basic}

% \paragraph{Basic Concepts and Notation.}
\paragraph{Sets and Maps.}
A \emph{set} is a collection of unique elements, 
    such as 
    $\{1, \texttt{hello}, 7\}$,
    the empty set $\emptyset$, 
    the natural numbers $\mathbb N = \{0, 1, 2, \ldots \}$,
    or the set $\mathbb R$ of real numbers. 
A \emph{singleton} is a set with precisely one element. 
We write $a \in A$, or equivalently $a : A$, 
    to indicate that $a$ is an element of the set $A$.
For brevity, we write $a,b \in A$ to indicate that both $a \in A$ and $b \in A$. 
We write $|A|$ or $\# A$ for the \emph{cardinality} of a set $A$;
    typically when we write this,  $A$ will be \emph{finite}, 
    meaning it contains $|A| \in \mathbb N$ distinct elements. 
When $n \in \mathbb N$ is a natural number, let
    $[n] := \{ 0, 1, \ldots, n-1 \}$ denote the set of the first $n$ natural numbers.
$A$ is a \emph{subset} of $B$ (written $A \subseteq B$) means that every element of $A$ is also an element of $B$, and $2^A$ is the set of all subsets of $A$.
% We write $\Set$ for the collection of all sets (which is not itself a set). 

There are a number of important ways to combine two sets $A$ and $B$ to produce a third set.  
% \begin{enumerate}
%     \item 
% \end{enumerate}
Their \emph{union}, denoted $A \cup B$, is the set of all elements contained in either $A$ or $B$ (or both),
while their \emph{intersection} $A \cap B := \{x \in A : x \in B\}$ is the set of elements they have in common.
The \emph{product} of $A$ and $B$ is the set $A \times B := \{ (a,b) : a \in A \land b \in B\}$. 
The \emph{disjoint union} of $A$ and $B$ is a variant of a union in which all elements are forced to be distinct, by ``tagging'' them with the set they come from, i.e., 
\[
    A \sqcup B := \{ (A, a) : a \in A \} \cup \{ (B, b) : b \in B \}. 
\]


We write $\forall x \in X.~\varphi(x)$ to indicate that a logical expression $\varphi(x)$ is true \emph{for all} $x \in X$, and $\exists x \in X.~ \varphi(x)$ to indicate that there \emph{exists some} $x \in X$ such that $\varphi(x)$ is true.
We use both of the two standard ``set-builder'' notations 
$\{ x \in X ~|~ \varphi(x) \}$ 
    and
    $\{ x \in X : \varphi(x) \}$ 
    to construct the subset of $X$ that satisfies the property $\varphi$;
    the choice between the colon and the bar is made to enhance readability in context.
We write $\lnot\varphi$ for the negation of an expression $\varphi$. 
If $\varphi$ and $\psi$ are both logical expressions, we write $\varphi \land \psi$ for their conjunction (which is true only if $\varphi$ and $\psi$ are both true), and $\varphi \lor \psi$ for their disjunction (which is true if and only if either $\varphi$ or $\psi$ are true).
The notation $\varphi \implies \psi$ means that $\psi$ holds whenever $\phi$ does, and can be viewed as an abbreviation for $\lnot \varphi \lor \psi$, 
and $\varphi \iff \psi$ to indicate that the two are logically equivalent (which can be viewed as an abbreviation for $(\varphi \implies \psi) \land (\psi \implies \varphi)$).
Speaking of logical equivalence, we often abbreviate ``if and only if'' by writing just ``iff''. 

% Given two sets $A$ and $B$, there are a number of ways t 
% The \emph{product }
% The \emph{union} $A \cup B := \{ x : x \in B \}$  


A \emph{map}, or \emph{function} $f : A \to B$ is an object that, 
    given $a \in \Ar$, produces a value $f(a) \in B$. 
The \emph{preimage} of a set $U \subseteq B$ is the set $\{a \in A \mid f(a) \in B \}$.
As one might hope from the notation, $A \to B$ 
    is the set of functions from $A$ to $B$; it is also written $B^A$. 
Observe: if $2 = \{ 0, 1\}$ represents a set with 2 elements, then 
    the elements of $2^A$ are functions assigning either $0$ (absence) or $1$ (presence) to each element of $A$---that is, they are subsets of $A$. 
We can \emph{compose} the functions $f : A \to B$ and $g : B \to C$
to form a new function $g \circ f : A \to C$, defined by $(g \circ f) := a \mapsto g(f(a))$. 

If $I$ is a set, then an \emph{indexed set}
    $\mat a = \{ a_i \}_{i \in I}$ consists of a collection of objects, one for each $a_i$. 
If $\forall i.~a_i \in A$, then $\mat a$ is just a function $\mat a : I \to A$. 
% The set of indexed 
% Therefore, if each $a_i \in A_i$, then  $\mat a : \sqcup_{i \in I} A_i  
A \emph{tuple} is the special case of a set indexed by $[n]$ for some natural number $n \in \mathbb N$. 



\paragraph{Vectors.}
For us, a \emph{vector} is a map from a finite set $S$, called its 
\emph{shape}, to the extended reals $\Rext := \mathbb R \cup \{\infty\}$.
We write $\mat u := [u_i]_{i \in S}$ to define a vector $\mat u$ by its components.
% Note that $
Vectors of the same shape
% Suppose that $\mat u, \mat v \in \Rext^S$ are two vectors of 
can be added (+), partially ordered ($\le$), or multiplied ($\odot$) pointwise as usual.
$\mat 1$ denotes an all-ones vector, of a shape implied by context.
If $\mat u$ and $\mat v$ are both of shape $S$, their \emph{inner product}, denoted either $\mat u \cdot \mat v$ or $\mat u ^{\sf T} \mat v$,
    is the real number $\sum_{i \in S} u_i v_i$. 

%%%%% TODO %%%%%
% - matrices, matrix multiplication


\paragraph{}

%%%% TODO %%%%
% - products
% - sums
% - converging sequence
% - unit interval
% - "space" = set + topology or geoemtry



\subsection{Variables}
    \label{ssec:basic-vars}
% A less standard concept, but one on which this thesis revolves is that of a \emph{variable}.
% A less standard concept, but one on which this thesis revolves is that of a \emph{variable}.
% Another standard concept, but one 
We now turn to the concept of a \emph{variable}.
Intuitively, a variable represents some feature of the world or some property of some object. 
Although variables are widely used across computer science---in programming languages, graphical models, causality, and probability theory, to name a few---there are actually several different closely related formalisms. 
We give a (perhaps somewhat opinionated) account here that simultaneously explicates many of approaches in the literature. 

% Our presentation of it is not standard, but largely because it incorprorates standard 
Mathematically, our notion of a variable exists on two levels.
\emph{Qualitatively}, a variable is just some unique identifier (the variable name), such as ``Height'', or $X$. 
We typically use capital roman letters for variables.
\emph{Quantitatively}, a variable $X$ is also associated with a set $\V(X)$, or simply $\V\!X$, of possible values. 
For example, $\V(\text{Height})$ might be the set of positive real numbers, 
    or the set $\{ \texttt{short}, \texttt{tall} \}$. 
A \emph{binary variable} is one whose possible values are $\{0, 1\}$, and a 
    \emph{real variable} is one whose possible values are $\mathbb R$. 
A \emph{constant} is a variable $X$ that can only take on one possible value (i.e., $|\V\! X| = 1$).

We can regard sets of variables $\mat X$ as variables themselves, with
$\V \mat X = \Pi_{X \in \mat X} \V\! X$.
However, when we do so, we must also be careful to remember $\mat X$ the variable has a special relationship with $\mat X$, the set of variables, a point we will soon return to. 
Note that, by this definition, the empty set is a variable, and it takes on a single value (i.e., the unique setting of no variables).  
This might seem strange, but there is precisely one way of selecting a value for each variable in $\varnothing$: simply do nothing. 
To see this another way, observe that if $\mat X = \{ X_1, \ldots, X_n \}$ is a set of binary variables,
then $|\V\mat X| = 2^n$, so in the special case of $n=0$, we have $|\V \varnothing | = 2^0 = 1$. 
Those with keen attention to typography may have noticed we have used a different symbol ($\varnothing$) for the empty set of variables, than we have for other empty sets $(\emptyset)$. 
This will be necessary to avoid a minor ambiguity, because the (standard) notation for probability that we adopt has a different meaning applied to a set of variables than it does applied to a set of possible outcomes. 

Similarly, tuples $(X_1, \ldots, X_n)$ of variables and indexed sets $\{X_i\}_{i \in I}$ of variables, are also themselves variables,
with
\begin{align*}
    \V(X_1, \ldots, X_n) &= \{ (x_1, \ldots, x_n) \mid \forall i \in [n]. x_i \in \V(X_i) \},
\end{align*}
 % \text{and}\quad
% More generally, 
and, more generally,
$
    \V\{ X_i \}_{i \in I} = \prod_{i \in I} \V(X_i).
$
% \end{align*}
The distinction between sets and indexed sets (such as tuples) of variables is only relevant when variables are not unique---a caveat that is far more relevant for the python implementation of this framework than it is for the mathematical development.
% Consequently, we convert between the two 
In the text here, we convert between the two equivalent representations as is convenient, often implicitly converting a set of variables to a tuple of variables with a specific ordering for the purposes of presentation.
% Later on, we will blur the line between sets and indexed sets,

Given a joint setting $\mat x \in \V(\mat X)$, we write either $\mat x[X]$ or  $X(\mat x)$ for the value of the variable $X$ in the joint setting $\mat x$. The former is familiar lookup notation from many programming languages, and the latter is random variable notation, which we will get to in the next section.

% Equality of variables 
Variable notation is designed to look intuitive. Yet beneath the surface, it can be very confusing, as it is quite unlike the standard mathematical notation for every other mathematical object.
This is especially true of the way variables interact with equality, and the symbol ``$=$''.
For instance, if $X$ is a variable and $x \in \V(X)$, then the notation $X{=}x$ does not necessarily stand for a false statement (even though $X$ and $x$ refer to different mathematical objects); instead, it represents the possibility that $X$ happens take on the value $x$, which we will soon see is called an \emph{event} in the context of probability. 
% In particular, it is neither true nor false, but rather a different mathematical object. 
% Worse, even if $X$ and $Y$ are both variables, then $X{=}Y$ intuitively represents the possibility that $X$ happens to equal $Y$, 
Even if $X$ and $Y$ are both variables, one might expect $X{=}Y$ to be true if and only if $X$ and $Y$ stand for the same variable, but here again it instead typically represents the possibility that $X$ and $Y$ happen to equal the same thing. 

By taking context into account, there is a nice way to rigorously remove this ambiguity, and also have all of our notation work out as we would hope. 
% If $f(\X)$
But before we resolve the issue, we must first explain a more general version of it. 
Whenever $X$ is a variable, and $f : \V\!X \to S$ is a function of the value of $X$, then we can regard $f(X)$ itself as a variable, with $\V(f(X)) = S$. 
The issue with equality we encountered in the previous paragraph is just the special case when we view ${=} : \V(X,Y) \to \{0,1\}$ as such a function. 
% However, when we do so, we must keep in mind that $f(X)$ and $X$ are related.
%
% The key is to keep in mind
However, when we do so we must also keep in mind that the variable $f(X)$ and the variable $X$ cannot simply take any values; they are related by the constraint $f$. 
An analogous constraint appears when we regard an (indexed) set of variables as a variable.
The solution, requires us to keep track constraints involved in the construction of the relevant variables; 
% for a (set of) variables $\X$, call this set of constraints $\Gamma(\X)$. 
if $X$ is a variable, let $\Gamma(X)$ denote this set of constraints. 
% if $X$ is a variable for a (set of) variables $\X$, call this set of constraints $\Gamma(\X)$. 
%
Now, when we write $X = Y$ we primarily mean the possibility that $X$ and $Y$ take the same value, which is a rich mathematical object (e.g., an event, or itself a variable). 
But at the same time, we can also implicitly convert that object to a truth value, in contexts where a truth value is appropriate.
Specifically, 
if the construction of $X$ involves constraints on the set of variables $\mat X$, and the construction of $Y$ involves constraints on the variables $\mat Y$, 
then we say $X{=}Y$ is \emph{true} iff $w[X] = w[Y]$ for all $w \in \V(\mat X \cup \mat Y)$ that satisfy the constraints of $\Gamma(X)$ and $\Gamma(Y)$.
A variable $X$ \emph{primitive} if $\Gamma(X)$ is empty, and we typically use the symbol $\X$ to denote the set of all primitive variables relevant to the current context.

The approach described above gives us what we want in essentially every context. 
It is always true that $X = X$, but not in general true that $X = X'$ unless $X$ and $X'$ must be equal by construction, or if $X$ and $X'$ are constants representing the same value. 
It is true that $f(X) = g(X)$ if and only if $f = g$.
If $X$ and $Y$ are real variables, then (it is true that) $X + Y = Y + X$.
At the same time, these expressions remain unambiguously meaningful for describing events (as we will soon see). And when expressions are \emph{true}, they are events that must necessarily occur.


% Clearly the full implications of this setup of variables can get rather tangled up
Most formalisms that involve variables avoid these issues by adopting only fragments of this picture, or sweeping ambiguities under the rug.
But it isn't necessary to do so; all of these standard concepts can coexist in one perfectly formal context.
The constraint-tracking resolution resolution to these issues gives just a taste of a broader approach to modeling, which lies at the heart of the theory of probabilistic dependency graphs. 
% gives a taste of the approach at the heart of the PDG formalism,   
% and how we can have our cake and eat it too. 
% Yet it turns out that all of this can be made precise with probabilistic dependency graphs, the mathematical representation of uncertainty that lies at the heart of this dissertation.  
The missing part, as one might expect, is the probability. 

\section{Probability, and Other Representations of Uncertainty}

% The material and notation in this chapter is all standard, but 
% The one we have chosen 
There are many different notations for probability in different community.
% In this section, we present a carefully 
Our approach simultaneously enables some of the most common notations
    and the most recognizable foundations. 
Thus, although all of the pieces are likely to be familiar, the precise formal way it fits together may not be. 
%
We start by giving the simpler picture of probability in the finite case (\cref{ssec:fin-prob}), which, for our purposes, is largely representative of how one should think of probability in the general case (which we return to \cref{ssec:gen-prob}).

\subsection{Probability, in the Finite Case}
    \label{ssec:fin-prob}
% We first give the simple picture of probability in which representations are finite. 
% in the finite case; for now, assume the variables $X$ and $Y$ can take on only finitely many possible
% Thus, we assume  values (i.e., $|\V\!X|, |\V Y| \in \mathbb N$).
% In this section, let's assume that all variables can take on only finitely many possible values
%     (i.e., $|\V\!X|, |\V Y| \in \mathbb N$).
Suppose that $\Omega$ is a finite set. 
A \emph{probability distribution} $\mu$ over $\Omega$ is essentially a function 
% $\mu : \V\!\X \to [0,1]$ such that $\sum_{x \in \V X} \mu(x) = 1$.
$\mu : \Omega \to [0,1]$ such that $\sum_{\omega \in \Omega} \mu(\omega) = 1$.
This means $\mu$ is a vector of shape 
% $\V\! X$,
$\Omega$ 
satisfying $\mu \ge 0$ and $\mu \cdot \mat 1 = 1$. 
% We write $\Delta \V\!X$ for the set of all distributions over (the values of) $X$,
% which is also known as the \emph{probability simplex}.
The \emph{probability simplex} $\Delta \Omega$ is defined as the set of all probability distributions over $\Omega$. 
For $U \subseteq \V\!X$, called an \emph{event},
    define $\mu(U) := \sum_{x \in U} \mu(x)$. 
% Specifically, the event $X{=}x$ 
% The prototypical event is of the form $X{=}x$. 
While we just defined $\mu$ on events $U$ of $\Omega$ in terms of $\mu(\omega)$ for elements $\omega \in \Omega$, we remark that, in general (i.e., beyond the finite case), it has to be the other way around: a probability distribution is defined as an object that assigns probability to events in a way that is compatible with intuitions from the finite case we have just described.


The standard way of updating a probability distribution based on new evidence is conditioning. Given a subset $A \subseteq \Omega$, the conditional measure
$\mu \mid A$ according to 
$(\mu \mid A)(x) := \frac{1}{\mu(A)} \mathbbm 1[x \in A] \mu(x)$.
% In other words, 
That quantity is also given a shorter and more common name: $\mu(x \mid A)$. 
From this definition, it is easy to recover the usual definition of conditional probability:
$\mu(U \mid A) = \mu(U \cap A) / \mu(A)$. 
There is just one caveat: this formula is not meaningful when $\mu(A) = 0$, and in this case, we leave the conditional probability undefined. Indeed, if $\mu(A) = 0$, then every answer for $\mu(U \mid A)$ is appropriate, in the sense that, for for every $p \in [0,1]$ there is a sequence of probability distributions $\{ \mu_k \}_{k \in \mathbb N}$ converging to $\mu$ that all $\mu(U \mid A) = p$ for all $k$. 

%% adding variables.
Our notation for representing and manipulate probability distributions will make heavy use of the notion of a variable, as developed in \cref{ssec:basic-vars}. 
Namely, we almost always work with probability distributions over the values of a variable. This just a notational convenience, since we can always define a variable $W$ with possible values $\V W := \Omega$.  
Working with variables confers us a number of benefits,
the first of which is 
% For now, we mention only that it allows us to 
to enable standard notation in the AI community:
given a distribution $\mu \in \Delta \V\!X$ over the values of $X$,
we also write $\mu(X)$ to specify $\mu$ and its type in a compact way.
% we write $\mu(X)$ in place of $\mu$ to emphasize that $\mu \in \Delta \V \! X$ in a compact way. 
% To a reader paying close attention, this definition might seem dangerous.
Since $\mu : \V W \to [0,1]$ is a function, we have already defined $\mu(W)$ to be a variable, but fortunately that variable encodes exactly the same information as $\mu$ itself.


%  $\mu(\mat X) = \mu(X_1, \ldots, X_n)$
A \emph{joint distribution} $\mu(\mat X) = \mu(X_1, \ldots, X_n)$ over a finite (indexed) set of variables $\mat X = \{X_1, \ldots, X_n\}$ is just a distribution over joint settings of all variables---i.e, a distribution over $\V \mat X$. 
%TODO: the above is awkward.
If $\mu(\mat X)$ is a joint distribution and $X \in \mat X$, then we write $\mu(X)$ for its \emph{marginal} on $X$, which is given by
\[
    \mu(X{=}x) = \mu(X)(x) := \sum_{\substack{\mat x \in \V \mat X \\ \mat x[X] = x}} \mu(\mat x) .
\]
It is called a marginal because of the special case of two variables: a joint distribution $\mu(X,Y)$ may be viewed as a grid of numbers summing to 1, whose rows correspond to values of $X$, and whose columns correspond to values of $Y$. 
If we were to write the sum $\sum_{y \in \V Y} \mu(x,y)$ of each row $x$ in the margin, this would give the marginal distribution on $X$. 


% The conditioned distribution $\mu|X{=}x$ is also written $\mu(\mat X \mid X{=}x)$, or, in abbreviated form as $\mu(Y|x)$ in contexts where the conditioning variable $X$ is unambiguous. 
To \emph{condition} a joint distribution $\mu(\mat X)$ on a variable $X \in \mat X$ is to form an indexed set of conditioned distributions $\mu(\mat X \mid X) = \{ ~(\mu|X{=}x) ~\}_{x \in \V\!X}$, one for each possible value of $X$. 
% The cpd $\mu(Y|X)$
We can also refer to such objects without reference to a joint distribution.
%
%
A \emph{conditional probability distribution (cpd)} on $Y$ given $X$ is a function $p : \V X \to \Delta \V Y$ that, for each $x \in \V\!X$, yields a probability distribution over $Y$,
which is written in any of the following equivalent notations:
\[ 
% $
p(x) ~=~ p|x ~=~ p(Y | X{=}x) ~=~ p(Y | x) \quad \in \Delta \V Y.
% $
\]
Cpds are also called \emph{channels} in information theory. 
Just like we write $\mu(X)$ for a distribution over the values of $X$, we
write $p(Y|X)$ for a cpd on $Y$ given $X$. 
In the finite case, cpds coincide with \emph{stochastic matrices}: matrices whose columns correspond to values of $Y$, rows correspond to values of $X$, and such that the sum of each row equals 1. 

We typically write $\X$ for the set of all variables relevant to a given context. If $X,Y \in \X$, then we write $\mu(Y|X)$ for the cpd given by first conditioning $\mu(\X)$ on $X$, and then marginalizing each conditional distribution to $Y$. 
% Now, $\mu(Y|X)$ too is a variable 
By regarding $\mu(Y|X)$ as a function of $Y$ and $X$, it too can be viewed a real-valued variable (just like $\mu(X)$). 
Moreover, the fact that $\mu(Y|X{=}x)$ is undefined can now be captured simply by not adding a constraint in $\Gamma(\mu(Y|X))$ connecting the value of $\mu(Y|X)$ to the value of $\mu(X,Y)$ in such cases. 
% This now explains 
% Observe that $\mu(\X \mid X)$ 
% We can deal with the 
% In this context, the un-definedness of $\mu(Y|X)$ 
% Recall that the components of $\mu(Y|X)$
We can now state \emph{chain rule} for probability, a simple but incredibly important fact:
\begin{equation}
    \mu(X,Y) = \mu(X) \mu(Y|X).
\end{equation}
% Unpacking our definition of what it means for it to be true that two variables are equal, 
This is a direct analogue the definition of a conditional probability when $X$ and $Y$ are events. 
% Yet this equation, unlike the definition of a conditional probability for events, holds 
Yet here, we no longer need to carve out a special caveat for conditioning on probability zero events; that case is handled automatically by implicit underlying constraints. 


%%%% TODO
% expectation


\subsection{Beyond Probability}
    \label{ssec:beyond-prob}

%%% TODO %%%
% sets of probability measures
% dempster-shafer belief functions

% \subsection{Probability, More Generally}
\subsection{Probability, In General}
    \label{ssec:gen-prob}

The time has come for us to bite the bullet and describe how probability actually works, when a finite representation is insufficient. 
The material here can be interesting and instructive,
    and it is certainly a necessary foundation for our results that talk about distributions over real numbers.
Still, we reiterate that the full account of measure theory needed to define probability in general is more than is necessary to get a deep conceptual understanding of the present work. 

\paragraph{Measures.}
A \emph{measurable space} $(\Omega, \mathcal F)$ is a set $\Omega$,
called the \emph{outcome space}, together with a collection $\mathcal F \subseteq 2^\Omega$ of subsets of $\Omega$, which considered \emph{measurable}, and called \emph{events}. 
The elements of $\Omega$, should be thought of \emph{possible outcomes} or \emph{possible worlds} for in the model.
The set $\mathcal F$ of events must also be an algebra, meaning that it contains $\Omega$, and is closed under complement and countable union---%
that is, $\Omega \in \mathcal F$, if $U \in mathcal F$ then $\Omega \setminus U \in \mathcal F$, and if $U_1, U_2, \ldots \in \mathcal F$, then $U_1 \cup U_2 \cup \cdots \in \mathcal F$.  
% One should think of $\mathcal A = 2^\Omega$ as consisting of every 
% For all but a few select places, one can ignore $\mathcal F_\Omega$ in this dissertation.
% Often in probabilistic modeling, and in this dissertation in particular, the $\mathcal F_\Omega$.
When $\Omega$ is finite, we allow all subsets to be measured by defining $\mathcal F := 2^\Omega$,
    and there is also a standard choice for most other spaces of interest, such as when $\Omega$ is a convex subset of real numbers, called its \emph{Standard Borel Space}. 
Either way, we will typically leave $\mathcal F$ implicit after this chapter, referring to a measurable space with the same symbol as its sample space $\Omega$. 

A \emph{measure} $\mu$ over a measurable space $(\Omega, \mathcal F)$ is a function $\mu : \mathcal F \to [0,\infty]$ with two additional properties: $\mu(\emptyset) = 0$, and,
for every countable collection $\{ U_i \}_{i \in \mathbb N}$
of pairwise disjoint measurable sets (i.e., where each $U_i \in \mathcal F$ and $U_i \cap U_j = \emptyset$),  we have that
$\sum_{i \in \mathbb N} \mu(U_i) = \lambda( \cup_{i \in \mathbb N}~U_i)$. 
A \emph{probability distribution} (or probability measure) is a measure $\mu$ with the additional property that $\mu(\Omega) = 1$. 
% satisfying the three Kolmogorov axioms:
% % \begin{enumerate}
% %     \item 
% \begin{align*}
%     &\Pr(\emptyset) = 0, \qquad
%     \Pr(\Omega) = 1, \\
%     \quad\text{and}\qquad&
%     \forall U, V \in \mathcal F.\quad 
%         U \cap V = \emptyset \implies \Pr(U \cup V) = \Pr(U) + \Pr(V).
% \end{align*}
%
Intuitively, these three properties are quite intuitive: $\emptyset$ is an event that cannot occur and hence has probability zero,
$\Omega$ is an event that always occurs and hence has probability 1, 
and if events cannot co-occur, then the probability that one of them occurs is the sum of their probabilities. 
% \end{enumerate}
% We need a few more definitions about measures. 

We will need two additional concepts about measures.
First, if $\mu$ and $\nu$ are measures over a space $(X, \mathcal F)$, 
and $\nu(U) = 0$ implies $\mu(U) = 0$ for all $U \in \mathcal F$, then
$\mu$ is \emph{absolutely continuous} with respect to $\nu$, also written $\nu \ll \mu$.
Second, if $\Omega$ comes with a topology $\mathcal T$ (see \cref{prelim:topo} for definitions) and $\mu(U) > 0$ for every non-empty open set $U \in \mathcal F \cap \mathcal T$, then $\mu$ is said to be \emph{strictly positive} (with respect to $\cal T$).

\paragraph{Statistics and (Random) Variables.} 
If $(\Omega, \mathcal F)$ and $(Y, \mathcal G)$ are two measurable spaces, then
a function $f : \Omega \to Y$ is said to be \emph{measurable} if $f(U) \in \mathcal F$ for all $U \in \mathcal G$. 
In a context where one is interested in probabilities over $\Omega$, 
a measurable function $X : \Omega \to Y$ is also called a \emph{random variable} or a \emph{statistic}.
Although the formalism may appear quite different, it is fundamentally very similar to our notion of a variable, a connection we explore in the next two paragraphs.

In this more general setting, we no longer require $\V(X)$ to be a finite set, but it still cannot be arbitrary---it has to be a measurable space. At a technical level, this is compatible with our previous discussion about variables, because all of our ways of constructing variables---through (indexed) sets of variables, and by applying functions---work equally well over measurable spaces. 
\begin{enumerate}
    \item Recall that we regard a tuple $\mat X = (X_1, \ldots, X_n)$ of variables as itself a variable, with $\V(X_1, \ldots, X_n) = \prod_{i=1}^n \V(X_i)$.
        But if each $\mathcal V(X_i)$ comes with an algebra $\mathcal F_i$ of events, then there is a natural product algebra $\mathcal F_{1:n}$ generated by complements and countable unions of events of the form $U = U_1 \times \cdots \times U_n$ with each $U_i \in \mathcal F_i$. 

    \item Similarly, recall that when $X$ is a variable and $f : \V X \to S$ is a function, we regard $f(X)$ as a variable. In this setting, we add only the minimal requirement that $S$ be a measurable space and that $f$ be a measurable function.  
\end{enumerate}

Whereas the traditional foundations of probability starts with a sample space $\Omega$ and defines random variables from it,  probabilistic modeling is done the other way around in practice: one starts with variables $\X$ of interest, and defines $\Omega := \V\!\X$ to be the joint settings of those variables. 
With that definition, a variable $X$ can be viewed as a random variable $X : \V\!\X \to \V\!X$ that selects the value of $X$ from the joint setting.  
Conversely, an arbitrary random variable $Z : \V\!\X \to \V Z$ can be included in the set of all variables; there is a natural 1-1 correspondence between distributions $\mu \in \Delta \V\!\X$ over the original variables, and ``extended'' distributions $\bar\mu \in \Delta \V(\X \cup Z)$ with the property that variable $Z$ and the function $Z$ applied to the other variables coincide (i.e., $\bar\mu(Z(\X) = Z)=1$).
%
%
Although the two perspectives seem to be equally expressive, we opt for the variable-based one. 
After all, we are interested in modeling epistemic states that undergo structural changes, and it is conceptually simpler to add and remove variables of interest, than it is to change sample spaces and figure out how to properly translate the random variables on them.  


\paragraph{The Radon-Nikodym Derivative.}
Suppose $\mu$ and $\nu$ are both measures over a measurable space $(X,\mathcal F)$, and $\mu \ll \nu$. 
The Radon-Nikodym theorem states that there is then a unique $\mathcal F$-measurable function $f$
such that for all $A \in \mathcal F$,
\[
    \mu(A) = \int_{A} f \,\mathrm d \nu.
\]
The function $f$ is called the \emph{Radon-Nikodym derivative} of $\mu$ with respect to $\nu$, and is denoted $\frac{\mathrm d\mu}{\mathrm d\nu} := f$.


\paragraph{Markov Kernels.}
Finally, we get to the precise notion of a conditional probability distribution (cpd) 
in this more careful framework regarding measurability.
If $(\Omega, \mathcal F)$, and $(\Omega', \mathcal G)$ are two measurable spaces, a \emph{Markov Kernel} $\kappa : \Omega \to \mathcal Y$, which we sometimes write as ``$\kappa(Y|X)$'', is a function $\kappa : X \times \mathcal G \to \mathbb R$, such that
\begin{enumerate}
    \item For every $x \in X$, the map $\kappa(x, -) : \mathcal U \to [0,1]$ is a probability measure. (So $\kappa$ is also a cpd.)
    \item For every $U \in \mathcal G$, the map $\kappa(-, U) : X \to [0,1]$ is a measurable function from $\mathcal X$ to the Borell space $[0,1]$.
    Or more explicitly: for every open set $S \subseteq [0,1]$, and $U \in \mathcal G$, we have that
    $\{x : \kappa(x,U) \in S\} \in \mathcal F$. 
\end{enumerate}


%%% TODO %%%%
% - densities
% - base measures
% - move discussion on random variables up to finite case
%  - integration
%

\section{Graph Theory}
\begin{defn}
    A \emph{(directed) (multi)graph} $G = (N, A)$, or simply a \emph{graph}, is a set $N$ of nodes,
    and a collection $A$ of arcs, such that each $a \in A$ has a source node $\Src a \in N$ and a target node $\Tgt a \in N$.
    So, formally, the definition is
    $G = (N, A, \Src{}, \Tgt{})$, with $\Src{},\Tgt{}: A \to N$ often left implicit.
\end{defn}

\begin{defn}
    % [Undirected (Multi) Graph]
    An \emph{undirected} (multi)graph $G = (N,E)$ is a set $N$ of vertices (or nodes)
    and a set $E$ of edges,
    each element $e \in E$ of which
    corresponds to an unordered pair of vertices $\{u,v\}$.
    More formally, there is a map
    \[
        \iota: E \to~ \faktor{ V \times V \setminus \{(v,v):v \in N\} }{ \{(u,v)\sim(v,u)\mid (u,v) \in N \times N\} }~.
    \]
    implicit in the definition of $G$, which we will write $G =
    % (V,E{,~ \color{gray}\iota})$ 
     (N,E, \iota)$ 
    only when being extra careful.
    \qedhere
\end{defn}

It is common to identify a graph $H = (N,A)$ (or an undirected graph $G = (N,E)$) with its (symmetric) adjacency matrix
\[
    \mathbb A_H = \Big[ \#\Big\{ a \in A : {\singlespacing \begin{array}{c}\Src a = u,\\ \Tgt a = v \end{array}} \Big\} \Big]_{(u,v) \in N \times N}
    \qquad
    \mathbb A_G = \Big[
    \#\{ e \in E : \iota(e) = \{u,v\}         \Big]_{(u,v) \in N\times N}~~,
\]
in part because there is a natural bijection between
(undirected) multigraphs and (symmetric) square matrices
over the natural numbers.  
For example:
\[
    \begin{bmatrix}
        0 & 1 & 2 \\
        1 & 0 & 0 \\
        2 & 0 & 1
    \end{bmatrix}
    \qquad
    \leftrightsquigarrow
    \qquad
    \begin{tikzpicture}[center base]
        \begin{scope}
            % \node at (0:1) (a) {a};
            % \node at (120:1) (b) {b};
            % \node at (-120:1) (c) {c};
            \node at (0,0) (a) {a};
            \node at (1,0) (b) {b};
            \node at (-1,0) (c) {c};
        \end{scope}
        \draw[-] (a) to (b);
        % \draw (a) to[bend left] (b);
        % \draw (b) to[bend left] (a);
        
        \draw[-] (a) to[bend left] (c);
        \draw[-] (a) to[bend right] (c);
        
        \path[every loop/.style={}] (c) edge[loop above] (c);
    \end{tikzpicture}
\]

A (directed) graph has more information than an undirected one.
There are natural ways to convert between the two: one forgets the direction of arcs to turn a directed graph into an undirected one, and annotates each arc with arrows in both directions to make an undirected graph directed. These choices are essentially locked in if we want the correspondence with square matrices to hold properly. 
\[ 
\begin{tikzcd}
\text{(Directed) Graphs} \ar[r,bend left,"\mathit{forget}"]&
\text{Undirected Graphs}
\ar[l,bend left,"\mathit{annotate}"]
\end{tikzcd}
\]
in which
$\mathit{forget} \circ \mathit{annotate}$
is the identity on undirected graphs, but
$\mathit{anotate} \circ \mathit{forget}$ is not the identity on (directed) graphs.
% Technically, this makes $\mathit{forget}$ a \emph{retraction}, and $\mathit{annotate}$ a \emph{section}.

\begin{defn}
    A bipartite graph $G = (L, R, E)$ is a graph $(L \sqcup R, E)$ whose
    vertices are partiioned into two components $V = L \sqcup R$, 
    and whose edges $E \subset L \times R$ are only between $L$ and $R$. 
\end{defn}


% \fadeout{ \begin{defn}
%     A directed bipartite graph $G = (L, R, E)$ is a bipartite graph $(L, R, E)$ whose edges $E \subset (L \times R) \cup (R \times L)$ are directed. 
%     \end{defn} }


\begin{defn}
    A \emph{hypergraph} $G = (V, \mathcal E)$ is a set $V$ of vertices,
    and a collection $\Ed$ of \emph{hyperedges}, which correspond to finite
    subsets of $V$. 
\end{defn}

An ordinary undirected graph may be viewed as the special case in which every hyperedge contains  two vertices.

\begin{prop} 
There is a natural bijection between hypergraphs and bipartite graphs:
% \def\bitgr{\mathit{BG}}
\def\bigr{\mathit{bipart}}
% \def\bitgr{\mathit{HG}}
\def\hygr{\mathit{hyper}}
\begin{align*}
    \bigr(V, \mathcal E) &:=  (V, \mathcal E, \{(v, E) \in V \times \mathcal E : v \in E \} ) \\
    % (L, \{\{x \in L : \} \}) &\maspfrom (L, R, E)
    \hygr(L, R, E) &:= (L, \{\{v \in L : (v,r) \in E\} : r \in R\}),
\end{align*}    
    \[
        \bigr \circ \hygr = \mathrm{id}_{BG}
        \quad\text{and}\quad
        \hygr \circ \bigr = \mathrm{id}_{HG}.
    \]
\end{prop}

The consequences of this can be unintuitive.
It is common to think of bipartite graphs as a strict (particularly nice) special case of ordinary undirected graphs, which themselves are a strict (particularly easy to draw) strict special case of hypergraphs.  
By transitivity, one might expect bipartite graphs to naturally be an extremely strict special case of hypergraphs---yet in fact they are naturally isomorphic. 

\begin{defn}
    A \emph{directed} hypergraph $(N, \mathcal A)$ is a set $N$ of nodes,
    and a collection $\Ar$ of hyperarcs, each of which has a set $S_a \subset N$ of
    source variables and a set $T_a \subset N$ of target variables.        
\end{defn}

A directed hypergraph $(N, \mathcal A)$ can be equivalently defined as an (ordinary) directed graph $(2^N, \Ar)$ whose set of nodes is the powerset of some set $N$. 
% 
% \medskip
% \hrule
% \medskip
% 
\begin{defn}
The \emph{dual} of the hypergraph $G = (V, \mathcal E)$ is
\[
    \check G := (\mathcal E, \{\{e \in \mathcal E : v \in e\} : v \in V\})
    .\qedhere
\]
\end{defn}

\begin{defn}
    The \emph{dual} of a directed hypergraph $\mathcal H = (N, \Ar)$ is 
    \(
        \check{\mathcal H} := (\mathcal A, N)
    \),
    where
    % \[
    $
    \check{\Src n} = \{ a \in \Ar : n \in \Tgt a\} \quad\text{and}\quad
    \check{\Tgt n} = \{ a \in \Ar : n \in \Src a\}.
    % \qedhere
    % \]
    $
\end{defn}

% \begin{prop}
We now verify that $\check{\check{\mathcal H}} = \mathcal H$.
Observe that 
\begin{align*}
    \check{\check{\Src a}}
    &= \{ n \in N : a \in \check{\Tgt n} \} \\
    &= \{ n \in N : a \in \{ a' \in \Ar :  n \in \Src a' \} \} \\
    &= \{ n \in N : n \in \Src a\} 
    = \Src a;
\end{align*}
% \end{prop}
symetrically, $\Tgt a = \check{\check{\Tgt a}}$. 
% What does this look like?
% To visualize . 
% One might wonder why $\check\Src n$ is definde in terms of $\Tgt a$ instead of $\Src a$.

See \cref{fig:dhygraph-duals} for some visual illustrations.
\begin{figure}%
    \centering
% \begin{tabular}{c}
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node[] (X) at (0,1) {X};
        \node[] (Y) at (1,1) {Y};
        \node[] (Z) at (2,1) {Z};
    \end{scope}
        \draw[arr1,<-] (X) to node[above]{\small 1} +(-0.9,0);
        \draw[arr1] (X) to node[above,pos=0.35]{\small 2} (Y);
        \draw[arr1] (Y) to node[above,pos=0.35]{\small 3} (Z);
    % \end{tikzpicture}\\
    % \begin{tikzpicture}
        \node[draw,outer sep=2pt] (1) at (-0.5,0) {1};
        \node[draw,outer sep=2pt] (2) at (0.5,0) {2};
        \node[draw,outer sep=2pt] (3) at (1.5,0) {3};
        \draw[->,arr1] (1) to node[below,pos=0.35]{\small X} (2);
        \draw[->,arr1] (2) to node[below,pos=0.35]{\small Y} (3);
        \draw[arr1,] (3) to node[below,pos=0.35]{\small Z} +(0.9,0);
    \end{tikzpicture}
    \hfill
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node (A) at (0,0) {A};
        \node (B) at (1,0.6) {B};
        \node (C) at (1,-0.6) {C};
        \node (D) at (2,0) {D};
    \end{scope}
        \draw[arr1,<-] (A) to node[above]{\small 1} +(-0.9,0);
        \draw[arr1] (A) to node[above,pos=0.35]{\small 2} (B);
        \draw[arr1] (A) to node[below,pos=0.35]{\small 3} (C);
        \mergearr[arr1] BCD
        \node[above=1pt of center-BCD]{\small 4};
    % \end{tikzpicture}\\
    % \begin{tikzpicture}
    \begin{scope}[shift={(-0.5,-2)}]
        \node[draw,outer sep=2pt] (1) at (0,0) {1};
        \node[draw,outer sep=2pt] (2) at (1,0.6) {2};
        \node[draw,outer sep=2pt] (3) at (1,-0.6) {3};
        \node[draw,outer sep=2pt] (4) at (2,0) {4};
        \unmergearr[arr1] 123
        \node[above left=1pt and 1pt of center-123]{\small A};
        \draw[arr1] (2) to node[above,pos=0.35]{\small B} (4);
        \draw[arr1] (3) to node[below,pos=0.35]{\small C} (4);
        \draw[arr1,] (4) to node[above]{\small D} +(0.9,0);
    \end{scope}
    \end{tikzpicture}% \end{tabular}
    %
    \hfill
    %
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node[draw,outer sep=2pt] (X) at (0,0) {$X$};
        \node[draw,outer sep=2pt] (Y) at (1,0) {$Y$};
    \end{scope}
        \draw[arr1] (X) to node[above,pos=0.35]{\small 1} (Y);
        \draw[arr1,<-] (Y) to node[above]{\small 2} +(0.9,0);
        %
        \begin{scope}[shift={(0.5,-1.5)}]
        \node[draw,outer sep=2pt] (1) at (0,0) {1};
        \node[draw,outer sep=2pt] (2) at (1,0) {2};
        \draw[arr1,<-] (1) to node[above]{\small $X$} +(-0.9,0);
        % \draw[arr1] ()
        \coordinate (a) at (0.5,-0.5);
        \mergearr[arr1] 12a
        \node at (0.5,0) {\small $Y$};
        \end{scope}
    \end{tikzpicture}
    \caption[Examples of directed hypergraphs and their duals]
    {Examples of directed hypergraphs (first row) and their duals (second row).}
        \label{fig:dhygraph-duals}
\end{figure}%
We remark that
the left and center diagrams on the top can be viewed as (the hypergraphs corresponding to) qualitative Bayesian Networks, by regarding X,Y,Z and A,B,C,D as variables, and imagining that there is a (randomized) causal determination occuring along each arc. 
% The structures that arise from causal models
One can also imagine an analogue with cycles---resuling in perhaps a (randomized) causal model of the given shape.  But a causal model has one equation corresponding to each variable, and the corresponding hypergraphs thus has exactly one hyperarc leading to it. 
In the dual hypergraphs, one should view the nodes as processes and the arcs as wires. Such a hypergrah has precisely one hyperarc leading out of every node. When wires branch, one imagines a copy; when two arcs point to the same process (as in process 4, in the middle center), that process takes both of the wires as inputs.
In the duals of hypergraphs corresponding to causal models, there are no two-tailed arrows, which might be thought of as a ``merge''. 
Yet it is not clear how to merge the values of two variables, when they are not the same, in general---especially if we expect associativity and commutativity, as we do with \emph{copy}. 


% \TODO[((What can be done with these objects? \\TODO: tie into the thesis elswehere or remove.))]
% \TODO[
%     Possible to have other identity morphisms? Require that, for each $X \in \N$, the identity morphism $\dg M(\ed {\mathrm{id}}XX)$ preserves the base measure $\lambda_X$, in addition to pre/post committing 
% ]

% \begin{fact}
%     All morphisms 
% \end{fact}

% \subsection{Independencies}


% \subsection{Graphical Models}
\section{A Review of Graphical Models}

There are two aspects any graphical model: 
a ``qualitative/structural'' aspect, which describes influences between variables, and
a ``quantitative/observational'' aspect, 
    that annotates those influences with data.

A qualitative BN, for example, is a directed graph whose semantics are given in terms of independencies: any variable $X$ is independent of its non-descendents, given the values of its parents, $\mathbf{Pa}\, X$.  
A quantitative BN, then, includes both that directed graph, and also each variable $X$ to a conditional probability distribution $\Pr_X(X|\mathbf{Pa}\, X)$.


\section{Information Theory}

% Perhaps surprisingly
Information theory is the study of when computational tasks
    (such as communicating in a noisy environment, 
        or )
    are possible, and when they are not. 
%
The field is vast and heterogenous, 
    yet much of it is built using a small set of 
    particularly nice ways of quantifying (un)certainty in a probabilistic
    context, which we will refer to throughout as \emph{information-theoretic primatives}.

Although not typically thought of in this way, 
    the basic information-theoretic quantities come in two flavors, mirroring the divide between qualitative and quantitative information that we saw in the presentation of graphical models. 


\subsection{Shannon Entropy and the Information Profile}

Let $\mu$ be a probability distribution, and be $X,Y,Z$
    be (sets of) discrete random variables.
The \emph{information content} $\I_{\mu}[ X{=}x ] := \log \frac1{\mu(X{=}x)}$ of an event $X{=}x$ quantifies one's surprisal at learning that $X{=}x$.
    
The \emph{entropy} of $X$ is the uncertainty in $X$, when it is distributed according to $\mu$, as measured by the number of bits of information needed (in expectation) needed to determine it, if the distribution $\mu$ is known.  It is given by 
\[
    \H_\mu(X) := \sum_{x \in \V(X)} \mu(X{=}x) \log \frac{1}{\mu(X{=}x)} \qquad= -\Ex_{\mu}[\log \mu(X)],
\]
and has several very important properties. 
Chief among them, $\H_\mu(X)$ is non-negative, and equal to zero iff $X$ 
    is a constant according to $\mu$. 
The ``joint entropy'' $\H(X,Y)$ is just the entropy of the combined variable $(X,Y)$ whose values are pairs $(x,y)$ for $x \in \V(X),y \in \V(Y)$; this is the same as the entropy of the variable $X \cup Y$ when $X$ and $Y$ are themselves sets of variables. 

The \emph{conditional entropy} of $Y$ given $X$
measures the uncertainty present in $Y$ if one knows the value of $X$
(think: the information in $Y$ but not $X$),
and is equivalently defined as any of the following three quantities:
\[
\H_\mu( Y | X) :=
        % \quad
    ~~
    \Ex_{\mu} [~\log \nicefrac1{\mu(Y | X)}~]
        % \quad
    ~~=~~\H_\mu(X,Y) - \H_\mu(X)
        % \quad
    ~~=\Ex_{x \sim \mu(X)} [~\H_{\mu \mid X{=}x}(Y)~]    
.
\]
The \emph{mutual information} $\I(X;Y)$,
and its conditional variant $\I(X;Y|Z)$, 
are given, respectively, by
\[
    \I_\mu(X;Y) :=
        \Ex_{\mu} \Big[ \log \frac{\mu(X,Y)}{\mu(X) \mu(Y)}\Big],
    \quad\text{and}\quad
    \I(X;Y|Z):= 
        \Ex_{\mu} \Big[ \log \frac{\mu(X,Y,Z)\mu(Z)}{\mu(X,Z) \mu(Y,Z)}\Big].
\]
The former is non-negative and equal to zero iff $\mu \models X \CI Y$, and the latter is non-negative and equal to zero iff $\mu \models X \CI Y \mid Z$. 
All of these quantities are purely ``structural'' or ``qualitative'' in the sense that they are invariant to relabelings of values, and 

Just as conditional entropy can be written as a linear combination of unconditional entropies, so too can conditional mutual information be written as a linear combination of unconditional mutual informations: $\I(X;Y|Z) = \I(X;(Y,Z)) - \I(X;Z)$.  
Thus conditional quantities are easily derived from the unconditional ones. But at the same time, the unconditional versions are clearly special cases of the conditional ones; for example, $\H_\mu(X)$ is clearly the special case of $\H(X|Z)$ when $Z$ is a constant (e.g., $Z = \emptyset$). 
Furthermore, entropy and mutual information are also interdefinable and generated by linear combinations of one another. 
It is easy to verify that 
$\I_\mu(X;Y) 
    % = \H_\mu(X,Y) - \H(Y|X) - \H(X|Y)
    = \H_\mu(X) + \H_\mu(Y) - \H(X,Y)
$
and 
$\I_\mu(X;Y|Z)
    = \H_\mu(X|Z) + \H_\mu(Y|Z) - \H(X,Y|Z)
$,
and thus mutual information is derived from entropy. 
Yet on the other hand, $\I_\mu(Y;Y) = \H_\mu(Y)$ and $\I_\mu(Y;Y|X) = \H_\mu(Y|X)$---thus entropy is a special case of mutual information. 

\TODO[ TODO: move information profile material here ]

\subsection{Relative Entropy}

The


\paragraph{An Ode to Relative Entropy}
In this dissertation, we will derive a 

Relative entropy is extremely special, and arguably exactly the
appropriate measure of discrepancy between belief and reality.  It has
a clear information theoretic interpretation as communication overhead
from an incorrect belief.  There are many nice axiomatizations of it;
some important ones include Renyi's, Tom Leinster's result showing
that it is the only function satisfying three trivial regularity
properties and the chain rule, and this characterization as the unique
functor from statistical maps to the additive monoid $[0,\infty]$, which
might be particularly persuasive to a category theorist.  It is the
only statistical divergence that is both a Bregman divergence and an
f-divergence.  Its Hessian is the Fisher metric, which is the unique
metric tensor that is invariant under sufficient statistics. In that
geometry, I have recently shown that gradient flow of relative entropy
with respect to its first (``belief'') argument amounts to
multiplicative interpolation of probability measures, while gradient
flow of relative entropy with respect to its second (``reality'')
argument amounts to additive interpolation. 



\begin{subappendices}    
\section{Primer on Advanded and Auxiliary Concepts}
The main body of this chapter has focused on material that is directly in the critical path to understanding every important result in this dissertation. 
 % 
 % the state of the field and building up a formal mathematical foundation on which the rest of the dissertation s.
%
At the other extreme, there is also some mathematical background needed only to understand a single chapter; that material is presented before it is needed. 
%
Here, we review some standard mathematical concepts that lie somewhere in between: these concepts play more minor roles than the ones already covered, and are common to a handful of chapters. 
%
%
We encourage a reader to quickly skim this section,
    and return to it if necessary. 


\subsection{Algebra}
\cref{chap:conf,chap:PDG-cat}

\begin{defn}[Monoid]%
    A \emph{monoid} is a tuple $(S, *, e)$, where $S$ is a set, $* : S \times S \to S$ is a binary operation, and $e \in S$ is a distinguished identity element, such that:
    \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt,]
        \item (associativity) $\forall a, b,c \in S.~~(a*b)*c = a*(b*c)$;
        \item (identity) $\forall a \in S.~~a * e = a = e * a$.
    \end{itemize}
    A monoid is called \emph{commutative} if it also satisfies
    \begin{itemize}[nosep]
        \item (commutativity): $\forall a, b \in S.~~ a * b = b * a$,
    \end{itemize}
    and \emph{idempotent} if it satisfies
    \begin{itemize}[nosep]
        \item (idempotence): $\forall a \in S.~~ a + a = a$. 
    \end{itemize}
    An idempotent semiring defines partial order by $a \le b \iff a + b = b$. 
\end{defn}

\subsection{Relations}

Let $\mathcal X = \{ X_1, \ldots, X_n \}$ be variables, traditionally called atributes. 
A \emph{relation} $R(\mathcal A) = R(X_1, \ldots, X_n) \subseteq 
\V(X_1) \times \cdots \times \V(X_n)$, or equivalently, $R : \prod_{i=1}^n \V\!X_i \to \{0,1\}$, is a subset of joint values of attributes. 
The natural number $n$ is called the \emph{arity} of $R$. 


The \emph{natural join} of two relations $R(A,B)$ and $S(B,C)$ combines them in a particularly obvious way: $(a,b,c) \in R \bowtie S$ iff $(a,b) \in R$ and $(b,c) \in S$. 
More generally, we have: 

% \begin{defn}[natural join]
%     \[
\begin{equation}
    R(\mathcal X) \bowtie S(\mathcal Y)
        := 
        \Big\{
            \boldsymbol\omega \in  \V(\mathcal X \cup \mathcal Y)
            ~\Big|~
            \mathcal X(\boldsymbol\omega) \in R \land
            \mathcal Y(\boldsymbol\omega) \in S
        \Big\}
        % \qedhere
%     \]
% \end{defn}
\end{equation}

At one extreme, if $\mathcal X$ and $\mathcal Y$ are disjoint sets of attributes, then $R(\mathcal X) \bowtie S(\mathcal Y)$ coincides with the cartesian product of $R \subseteq \V \mathcal X$ and $S \subseteq \V\mathcal Y$.
At the opposite extreme, if $\mathcal X = \mathcal Y$ are the same set of variables, then $R(\mathcal X) \bowtie S(\mathcal X)$ coincides with the intersection of the subsets $R$ and $S$. 

Even when $A_1, \ldots, A_n \subseteq \X$ are not disjoint, we give a convenient extended syntax by defining the quantity $R(a_1, \ldots, a_n)$, where $a_i \in \V(A_i)$. 
Concretely, define $R(a_1,\ldots, a_n) := 0$ if when $\{a_1, \ldots, a_n\}$ do not agree on the value of some shared attribute (i.e., if $\exists X \in \X, \exists i,j \in [n]. ~X \in A_i \cap A_j \land X(a_i) \ne X(a_j)$).
When $\{a,b,c\}$ do agree on all values of shared attributes, let $\mat x$ denote the joint value of $A \cup B \cup C$ obtained from $(a,b,c)$ by removing redundant copies of variable values.
In this case, define $R(a,b,c) := R(\mat x)$. 


\subsection{Geometry}

    
\end{subappendices}
