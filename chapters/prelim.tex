    \label{chap:prelim}

Although the high-level ideas in this thesis are often intuitive and philosophical, 
% our primary vehicle for results is the 
% we make those ideas precise using some  
% but in their most precise forms, they are often quite mathematical.
the most precise forms of those ideas are quite mathematical.
In this chapter, we build up a library of standard mathematical concepts and notation needed to fully understand them. 
A reader with a firm grasp of probability, graph theory, probabilistic graphical models, convex geometry, and information theory, should already be equipped to understand the later chapters;
these readers should feel free to skip to \cref{part:univ-model}.
Still, even for such readers,
the way these concepts fit together formally may be less familiar.
The notation is carefully selected to be precise,
    yet serve multiple purposes,
    and look indistinguishable 
    from various standard notations in different contexts. 
 
% to motivate
% to prime the reader 
% to see the most important themes and design choices that go into 
The chapter also serves a second purpose: 
    to introduce the kinds of reasoning and representation patterns that appear in later chapters.
%
% , 
% and to reconcile those representations and reasoning patterns with the 
% especially when it comes to the notion of a \emph{variable}. 
% The principle such pattern is the notion of a \emph{variable}, 
\oldnewmaterial{%
The principle such pattern is the concept of \emph{variable}, 
which has several closely related meanings.
One typically takes care to distinguish these different meanings, 
    but we do some extra leg work in this chapter to justify using them interchangeably. 
% The result is a simulacrum of the reasoning patterns that undergird 
% These patterns directly undergird the notion of a PDG.
In this chapter, we will get a chance to work with these ideas in a familiar context.
% We return to this in \cref{ssec:basic-vars}.
}%


% \section{Background, Notation, and Critical Preliminaries}
\section{Basic Concepts and Notation}

% Only \cref{sec:}
Let's start with the basics.
There are two kinds of equalities: definitions, and assertions. 
We distinguish between the two.
Namely, we write ``$A := B$'' to (re)define the symbol $A$ so as to stand for $B$,
but write ``$A = B$'' to assert that the (already defined) expressions $A$ and $B$ are equal.
%


% \subsection
% \paragraph
% {Sets, Variables, and Vectors}
    \label{ref:prelim-basic}

% \paragraph{Basic Concepts and Notation.}
\paragraph{Sets, Maps, Numbers, and Logic.}
A \emph{set} is a collection of unique elements, 
    such as 
    $\{1, \texttt{hello}, 7\}$,
    the empty set $\emptyset$, 
    the natural numbers $\mathbb N = \{0, 1, 2, \ldots \}$,
    or the set $\mathbb R$ of real numbers. 
A \emph{singleton} is a set with precisely one element. 
We write $a \in A$, or $a : A$, 
    to indicate that $a$ is an element of the set $A$
    (and $a \notin A$ to indicate otherwise).
For brevity, we write $a,b \in A$ to indicate that both $a \in A$ and $b \in A$. 
$A$ is a \emph{subset} of $B$ (written $A \subseteq B$) means that every element of $A$ is also an element of $B$, and $2^A$ is the set of all subsets of $A$.
% We write $\Set$ for the collection of all sets (which is not itself a set). 

There are a number of important ways to combine two sets $A$ and $B$ to produce a third set.  
% \begin{enumerate}
%     \item 
% \end{enumerate}
Their \emph{union} $A \cup B$ is the set of all elements contained in either $A$ or $B$ (or both),
while their \emph{intersection} 
% $A \cap B := \{x \in A : x \in B\}$ is the set of elements they have in common.
$A \cap B$ is the set of elements they have in common.
The \emph{product} of $A$ and $B$ is the set $A \times B := \{ (a,b) : a \in A \land b \in B\}$. 
The \emph{disjoint union} of $A$ and $B$ is a variant of a union in which all elements are forced to be distinct, by ``tagging'' them with the set they come from, i.e., 
% \[
$
    A \sqcup B := \{ (A, a) : a \in A \} \cup \{ (B, b) : b \in B \}. 
$
% \]
% If the two sets share a symbol, then we instead write
\newmaterial{%
The elements must be distinct even if the two sets share a symbol; 
    in this case, we instead tag elements based on whether they come
    from the left or the right:
$
    A \sqcup A = \{ \mathrm{inl}(a) : a \in A\} \cup \{ \mathrm{inr}(a) : a \in A\}.
$
}%

We write $|A|$ or $\# A$ for the \emph{cardinality} of a set $A$;
    when we write $\#A$, that means $A$ is \emph{finite}, 
    meaning it contains $\#A = |A| \in \mathbb N$ distinct elements. 
When $n \in \mathbb N$ is a natural number, let
    $[n] := \{ 0, 1, \ldots, n-1 \}$ denote the set of the first $n$ natural numbers.
When $a$ and $b$ are numbers (e.g., elements of $\mathbb N$ or $\mathbb R$), we denote their product by $a \cdot b$, or, more commonly, simply $a b$.
We will also often consider extended numbers that include positive infinity ($\infty$), and perform arithmetic with it by defining, for $r \in \mathbb R$,
\[
    \infty + r := \infty,
    \qquad 
    \infty \cdot r := \infty \text{ for }r > 0,
    \qquad
    \infty \cdot 0 := 0,
    % \quad \text{and } \infty + $
\]
and leaving $\infty \cdot r$ undefined when $r < 0$. 
We write $\Rext := \mathbb R \cup \{\infty\}$ for the set of extended real numbers, and $[0, \infty]$ for just the non-negative ones. 


We write $\forall x \in X.~\varphi(x)$ to indicate that a logical expression $\varphi(x)$ is true \emph{for all} $x \in X$, and $\exists x \in X.~ \varphi(x)$ to indicate that there \emph{exists some} $x \in X$ such that $\varphi(x)$ is true.
We use both of the two standard ``set-builder'' notations 
$\{ x \in X ~|~ \varphi(x) \}$ 
    and
    $\{ x \in X : \varphi(x) \}$ 
    to construct the subset of $X$ that satisfies the property $\varphi$;
    the choice between the colon and the bar is made to enhance readability in context.
We write $\lnot\varphi$ for the negation of an expression $\varphi$. 
If $\varphi$ and $\psi$ are both logical expressions, we write $\varphi \land \psi$ for their conjunction (which is true only if $\varphi$ and $\psi$ are both true), and $\varphi \lor \psi$ for their disjunction (which is true if and only if either $\varphi$ or $\psi$ are true).
In some contexts, it is standard (and may feel more natural) to use a comma instead of ``$\land$'' to conjoin logical expressions. 
The notation $\varphi \implies \psi$ means that $\psi$ holds whenever $\phi$ does, and can be viewed as an abbreviation for $\lnot \varphi \lor \psi$; 
the notation $\varphi \iff \psi$ means that $\varphi$ and $\psi$ are logically equivalent (which can be viewed as an abbreviation for $(\varphi \implies \psi) \land (\psi \implies \varphi)$).
Speaking of logical equivalence, we often abbreviate ``if and only if'' by writing just ``iff''. 

% Given two sets $A$ and $B$, there are a number of ways t 
% The \emph{product }
% The \emph{union} $A \cup B := \{ x : x \in B \}$  


A \emph{map}, or \emph{function} $f : A \to B$ is an object that, 
    given $a \in \Ar$, produces a value $f(a) \in B$. 
If $f, g : A \to B$ have the same behavior (i.e., $\forall a\in A.~f(a) = f(a)$),
    then they are considered equal, and we write $f = g$. 
Thus, a function can be specified with the notation $f = a \mapsto f(a)$,
    which allows us to talk about functions without naming them.
% The \emph{image} of a subset $U \subseteq A$ is the set $\im(f) := \{ f(b) : a \in A\} \subseteq B$ of possible results from applying $f$; meanwhile,
The \emph{preimage} of a set $V \subseteq B$ is the set 
$f^{-1}(V) := \{a \in A : f(a) \in V \} \subseteq A$ of inputs to $f$, whose corresponding outputs are in $V$.
We can \emph{compose} the functions $f : A \to B$ and $g : B \to C$
to form a new function $g \circ f : A \to C$, defined by $(g \circ f) := a \mapsto g(f(a))$. 
% We also regularly make use of the \emph{indicator function}
When $A \subseteq X$, it is often very useful to refer to the \emph{indicator function} for membership in $A$,
\[
\mathbbm1[A]  := x \mapsto {\singlespacing\begin{cases}
    1 & \text{ if } x \in A \\
    0 & \text { if } x \notin A
\end{cases}}
\quad: X \to \{0,1\}
\]

As one might hope from the notation, ``$A \to B$''
    is the set of functions from $A$ to $B$; it is also written $B^A$. 
It is worth pausing here to reflect on some implications of this notation.
First, if $1 = \{ \star \}$ is a singleton set, then an element $a \in A$ is really no different from a map $a \in A^1 = 1 \to A$. 
Second, if $2 = \{ 0, 1\}$ represents a set with 2 elements, then 
    the elements of $2^A$ are functions assigning either $0$ (absence) or $1$ (presence) to each element of $A$---that is, they are subsets of $A$. 
Finally, $|A^B| = |A|^{|B|}$. 


% \paragraph{Indexed sets, Tuples, and Sequeces.}
If $I$ is a set, then an \emph{indexed set}
    $\mat a = \{ a_i \}_{i \in I}$ consists of a collection of objects, one for each $i \in I$. 
If $\forall i.~a_i \in A$, then $\mat a$ is just a function $\mat a : I \to A$. 
% The set of indexed 
% Therefore, if each $a_i \in A_i$, then  $\mat a : \sqcup_{i \in I} A_i  
A \emph{tuple} is the special case of a set indexed by $[n]$ for some natural number $n \in \mathbb N$, and typically written with parentheses in order, as in $(a_1, \ldots, a_n)$.
A \emph{sequence} is the special case of a set indexed by $\mathbb N$, and is typically also written with parentheses, as in $(a_1, a_2, \ldots)$. 
\oldnewmaterial{%
For tuples and sequences, we sometimes use \emph{slice notation}, writing, for example, $a_{3:7}$ for the sub-sequence $(a_3, a_4, \ldots, a_7)$.
} 

%%%%%% TODO %%%%
% - convergence of sequences.
% - distances
% - min, max, argmin, argmax

\paragraph{Linear Algebra.}
For us, a \emph{vector} is a map from a finite set $I$, called its 
\emph{shape}, to the (possibly extended) reals.
To simplify and match standard notation, we write $\mathbb R^n$ instead of $\mathbb R^{[n]}$, and often write $\mat u = [u_i]_{i \in I}$ to specify a vector $\mat u$ by its components.
% Note that $
Given a number $\alpha \in \mathbb R$ and a vector $\mat u \in \Rext^I$,
    we can form the scaled vector 
    $\alpha \mat u := [ \alpha  u_i ]_{i \in I} = i \mapsto \mat \alpha \cdot \mat u(i)$
    % (unless some $u_i$ is infinite and $\alpha < 0$)
    \unskip;
    for this reason, $\alpha$ is called a \emph{scalar} in this context. 
Vectors of the same shape
% Suppose that $\mat u, \mat v \in \Rext^S$ are two vectors of 
can be added ($+$) or multiplied ($\odot$) pointwise as usual,
and we say that $\mat u \le \mat v$ iff $\mat u(i) \le \mat v(i)$ for all $i \in I$.
\newmaterial{%
We write $\mat u \propto \mat v$ iff there exists some a scalare $\alpha \in \mathbb R$ such that $\mat u = \alpha \mat v$.
}%
% $\mat 1$ denotes an all-ones vector, of a shape implied by context.
$\mat 1$ and $\mat 0$ denote all-ones and all-zeros vectors, of a shape implied by context.
If $\mat u$ and $\mat v$ are both of shape $I$, their \emph{inner product}, denoted either $\mat u \cdot \mat v$ or $\mat u ^{\sf T} \mat v$,
    is the real number $\sum_{i \in I} \mat u(i) \mat v(i)$. 
The latter notation will become clear once we introduce the notion of a \emph{matrix}. 

A map $f : \mathbb R^I \to \mathbb R^J$ is \emph{linear} iff
$f( \alpha \mat u + \beta \mat v) = \alpha f(\mat u) + \beta f(\mat v)$ for all $\mat u, \mat v \in \mathbb R^I$. 
There is a natural
1-1 correspondence between linear maps $f : \mathbb R^I \to \mathbb R^J$ and
special vectors $A \in \mathbb R^{J \times I}$ called \emph{matrices}---%
    a fact that is arguably the foundation of linear algebra.
% Specifically, the matrix $A = [a_{i,j}]_{(i,j) \in I\times J} \in \mathbb R^{I \times J}$ represents the function $[u_j]_{j \in J} \mapsto [ \sum_{j \in J} u_j a_{j,i} ]_{i \in I}$.
Specifically, the matrix $A \in \mathbb R^{J\times I}$ represents the function $\mat u \mapsto [ \sum_{j \in J} \mat u(j) A({j,i}) ]_{i \in I}$.
%
Two matrices $A \in \mathbb R^{I \times J}$ and $B \in \mathbb R^{J \times K}$
can be multiplied to form a matrix
% \[
$\displaystyle
    A B = \Big[ \sum_{j \in J} a_{i,j} \cdot b_{j,k} \Big]_{(i,k) \in I \times K}~;
$
% \]
remarkably, if $f_A : \mathbb R^{J} \to \mathbb R^{I}$ and $f_B : \mathbb R^{K} \to \mathbb R^J$ are the linear maps corresponding to $A$ and $B$, respectively, then
$A B$ is the matrix corresponding to $f_A \circ f_B$. 
Observe that a vector $\mu \in \mathbb R^I$ can be viewed as
    a matrix $\mat u \in \mathbb R^{I \times 1}$ for the purposes of composition; in this form, it is called a \emph{column vector}.
This establishes that linear maps can be viewed as vectors, 
    and vectors can be viewed as linear maps.
\oldnewmaterial{%
    The \emph{transpose} of a matrix $A \in \mathbb R^{I \times J}$ is 
    the matrix $A^{\sf T} \in \mathbb R^{J \times I}$ that results 
        from swapping the rows and columns of $A$. 
    In particular, the transpose $\mat u^{\sf T}$ of a column vector $\mat u \in \mathbb R^{I}$ 
        is a \emph{row vector} or \emph{dual vector}, 
        and represents a linear function $\mat v \mapsto \mat u^{\sf T} \mat v ~:~ \mathbb R^{I} \to \mathbb R$.
}%
% This is yet another instance
% The result is another 
% This illustrates a general guiding principle in this thesis: 
%     the nouns (here, the vectors), but rather the maps between sets. 

% For this reason, 
% A \emph{matrix} is a function $A \in \mathbb R^{S \times T}$

\textbf{Cones and Convexity.}
A subset $A \subseteq \Rext^I$ of extended real space is a \emph{convex set} iff
it contains all of the line segments between pairs of its elements---that is, if
it has the property that
$
(1-\lambda) \mat u + \lambda \mat v \in A
$
% it is close
for all $\mat u, \mat v \in A$ and $\lambda \in [0,1]$,
$(1-\lambda)$.
% Convex sets have some important relationships with probability distributions.
The probability simplex $\Delta \Omega$ is the prototypical convex set. 
The \emph{convex hull} of a subset of $U \subseteq \Rext^I$ is the smallest convex set that contains $U$. 
The \emph{extreme points} or \emph{vertices} of a convex set $A$ is the smallest subset 
% $\matrm{Ext}(A) 
of $A$ whose convex hull is $A$. 
% For instance, $\mathrm{Ext}(\Delta \Omega) = \{ \delta_{\omega} : \omega \in \Omega \}$. 
% If $A$ is a convex set with $|\mathrm{Ext}(A)| \in \mathbb N$,
% then $A$ is the image of a linear map ....
For instance, the convex hull of $\{ (0,0), (0,1), (1,0), (1,1) \}$
is the unit square $[0,1]^2$, and the extreme points of the unit square are those four points. 

%
A function $f : \Rext^I \to \Rext$ is \emph{convex} iff
\[
    \forall \mat u, \mat v \in \Rext^I,~\forall \lambda \in (0,1).\quad
    f( (1-\lambda) \mat u + \lambda \mat v ) \le (1-\lambda) f(\mat u) + \lambda f(\mat v).
\]
Recall that a linear map is one for which this relationship always holds with equality for all $\mat u$ and $\mat v$. 
A \emph{strictly convex} function, on the other hand, is one where it never does, except when $\mat u = \mat v$. 

% The two notions are closely related: a c 
Convex functions and convex sets are closely related. The set of points that lie above a function $f$
 % (its so-called \emph{epigraph} $\{ (\mat u, f(\mat u)) : \mat u \in \Rext^I$) 
is a convex set if and only if $f$ is convex. The level sets of a function $f$ (i.e., $\{ \mat u \in \Rext^I : f(\mat u) \le k \}$ for some $k \in \Rext$) are convex if $f$ is convex. And $\mat u \mapsto \infty \mathbbm 1[\mat u \in A]$ is a convex function iff $A$ is a convex set. 

% A \emph{conic combination} is a linear combination 
A \emph{cone} is something between a linear space and a convex set: a linear space is one closed under linear combination with arbitrary coefficients, while a cone is one closed under linear combination with non-negative coefficients, and a convex space is closed under linear combination with non-negative coefficients that sum to one.
It follows that linear spaces are cones, and cones are convex.



%%%% TODO %%%%
% - limits (?)
% - computational complexity / big O
%
% - products
% - sums
% - unit interval
% - "space" = set + topology or geometry



% \subsection{Variables}
\section{Variables}
    \label{ssec:basic-vars}
% A less standard concept, but one on which this thesis revolves is that of a \emph{variable}.
% A less standard concept, but one on which this thesis revolves is that of a \emph{variable}.
% Another standard concept, but one 
% We now turn to the concept of a \emph{variable}.
Intuitively, a \emph{variable} represents some feature of the world or some property of some object. 
Although variables are widely used across computer science---in programming languages, graphical models, causality, and probability theory, to name a few---the term is actually shared by several different formalisms. 
% We give a (perhaps somewhat opinionated) account here that simultaneously explicates many of approaches in the literature. 
The account presented here simultaneously explicates many of them.
 % approaches in the literature. 

% Our presentation of it is not standard, but largely because it incorporates standard 
Mathematically, our notion of a variable exists on two levels.
\emph{Qualitatively}, a variable is just some unique identifier (the variable name), such as ``Height'', or $X$. 
We typically use capital roman letters for variables.
\emph{Quantitatively}, a variable $X$ is also associated with a set $\V(X)$, or simply $\V\!X$, of possible values. 
For example, $\V(\text{Height})$ might be the set of positive real numbers, 
    or the set $\{ \texttt{short}, \texttt{tall} \}$. 
A \emph{binary variable} is one whose possible values are $\{0, 1\}$, and a 
    \emph{real variable} is one whose possible values are $\mathbb R$. 
A \emph{constant} is a variable $X$ that can only take on one possible value (i.e., $|\V\! X| = 1$).

\paragraph{Joint Variables.}
We can regard sets of variables $\mat X$ as variables themselves, with
$\V \mat X = \Pi_{X \in \mat X} \V\! X$.
However, when we do so, we must also be careful to remember $\mat X$ the variable has a special relationship with $\mat X$ the set of variables. 
By this definition, the empty set $\varnothing$ is a variable, and it takes on a single value (i.e., the unique setting of no variables).%
\footnote{%
    This might seem strange, but there is precisely one way of selecting a value for each variable in $\varnothing$: simply do nothing. To see this another way, observe that if $\mat X = \{ X_1, \ldots, X_n \}$ is a set of binary variables,
    then $|\V\mat X| = 2^n$, so in the special case of $n=0$, we have $|\V \varnothing | = 2^0 = 1$. 
    }
Those with keen attention to typography may have noticed we have used a different symbol ($\varnothing$) for the empty set of variables, than we have for other empty sets $(\emptyset)$. 
This will be necessary to avoid a minor ambiguity, because the (standard) notation for probability has a different meaning applied to a set of variables than it does applied to a set of possible outcomes. 

Similarly, tuples $(X_1, \ldots, X_n)$ of variables and indexed sets $\{X_i\}_{i \in I}$ of variables, are also themselves variables,
with
\begin{align*}
    \V(X_1, \ldots, X_n) &= \{ (x_1, \ldots, x_n) : \forall i \in [n].~~ x_i \in \V(X_i) \},
\end{align*}
 % \text{and}\quad
% More generally, 
and, more generally,
$
    \V\{ X_i \}_{i \in I} := \prod_{i \in I} \V(X_i).
$
% \end{align*}
% The distinction between sets and indexed sets (such as tuples) of variables is only relevant when variables are not unique---a caveat that is far more relevant for the python implementation of this framework than it is for the mathematical development.
% In the text here, we convert between the two equivalent representations as is convenient,
The distinction between sets and indexed sets (such as tuples) of variables is only relevant when variables are not unique.
% Consequently, we convert between the two 
In this text, we implicitly convert between the two representations as is convenient, and often view a set of variables as a tuple with a specific order for the sake of presentation.
% We often implicitly convert a set of variables to a tuple of variables with a specific ordering for the purposes of presentation.
% Later on, we will blur the line between sets and indexed sets,
%
Given a joint setting $\mat x \in \V(\mat X)$, we write either $\mat x[X]$ or  $X(\mat x)$ for the value of the variable $X$ in the joint setting $\mat x$. The former is familiar lookup notation from many programming languages, and the latter is random variable notation, which we will get to in the next section.

% Equality of variables 
\paragraph{Variable Equality, Truth, and Functions of Variables.}
Variable notation is designed to be intuitive and easy to read. 
Yet beneath the surface, it can be confusing, because it is unlike almost any other standard mathematical notation.
This is especially true of the way variables interact with equality, and the symbol ``$=$''.
For instance, if $X$ is a variable and $x \in \V\!X$, then we do not interpret $X{=}x$ as falsity (despite the fact that $X$ and $x$ refer to very different mathematical objects); instead, it represents the possibility that $X$ happens take on the value $x$, which we will soon see is called an \emph{event} in the context of probability. 
% In particular, it is neither true nor false, but rather a different mathematical object. 
% Worse, even if $X$ and $Y$ are both variables, then $X{=}Y$ intuitively represents the possibility that $X$ happens to equal $Y$, 
Furthermore, if $X$ and $Y$ are both variables, one might expect $X{=}Y$ to be true iff $X$ and $Y$ stand for the same variable, but here again it instead represents the possibility that $X$ and $Y$ happen to take on the same value. 
%
% By taking context into account, there is a nice way to rigorously remove this ambiguity, and also have all of our notation work out in the standard way. 
% % If $f(\X)$
% But before we resolve the issue, we must first explain a more general version of it. 
More generally, 
if $X$ is a variable and $f : \V\!X \to S$ is a function of the value of $X$, then we can regard $f(X)$ itself as a variable, with $\V(f(X)) = S$. 
The issue with equality we encountered in the previous paragraph is just the special case when we view ${=} : \V(X,Y) \to \{0,1\}$ as such a function. 
% However, when we do so, we must keep in mind that $f(X)$ and $X$ are related.
%
% The key is to keep in mind
However, when we do so we must also keep in mind that the variable $f(X)$ and the variable $X$ cannot simply take any values; they are related by the constraint $f$. 
An analogous constraint appears when we regard an (indexed) set of variables as a variable.

The solution requires us to keep track constraints involved in the construction of the relevant variables; 
% for a (set of) variables $\X$, call this set of constraints $\Gamma(\X)$. 
if $X$ is a variable, let $\Gamma(X)$ denote this set of constraints. 
% if $X$ is a variable for a (set of) variables $\X$, call this set of constraints $\Gamma(\X)$. 
%
Now, when we write $X = Y$ we primarily mean the possibility that $X$ and $Y$ take the same value, which is a rich mathematical object (e.g., an event, or itself a variable). 
But at the same time, we can also implicitly convert that object to a truth value, in contexts where a truth value is appropriate.
Specifically, 
if the construction of $X$ involves constraints on the set of variables $\mat X$, and the construction of $Y$ involves constraints on the variables $\mat Y$, 
then we say $X{=}Y$ is \emph{true} iff $w[X] = w[Y]$ for all $w \in \V(\mat X \cup \mat Y)$ that satisfy the constraints of $\Gamma(X)$ and $\Gamma(Y)$.
A variable $X$ \emph{primitive} if $\Gamma(X)$ is empty, and we typically use the symbol $\X$ to denote the set of all primitive variables relevant to the current context.

The approach described above gives us what we want in essentially every context. 
It is always true that $X = X$, but not in general true that $X = X'$ unless $X$ and $X'$ must be equal by construction, or if $X$ and $X'$ are constants representing the same value. 
It is true that $f(X) = g(X)$ if and only if $f = g$.
If $X$ and $Y$ are real variables, then (it is true that) $X + Y = Y + X$.
At the same time, these expressions remain unambiguously meaningful for describing events (as we will soon see). And when expressions are \emph{true}, they are events that must necessarily occur.


% Clearly the full implications of this setup of variables can get rather tangled up
Most formalisms that involve variables avoid these issues by adopting only fragments of this picture, or sweeping ambiguities under the rug.
But it isn't necessary to do so; all of these standard concepts can coexist in one perfectly formal context.
The constraint-tracking resolution resolution to these issues gives just a taste of a broader approach to modeling, which lies at the heart of the theory of probabilistic dependency graphs. 
% gives a taste of the approach at the heart of the PDG formalism,   
% and how we can have our cake and eat it too. 
% Yet it turns out that all of this can be made precise with probabilistic dependency graphs, the mathematical representation of uncertainty that lies at the heart of this dissertation.  
The missing part, as one might expect, is the probability. 

% \section{Probability, and Other Representations of Uncertainty}
\section{Probability}

% The material and notation in this chapter is all standard, but 
% The one we have chosen 
% There are several different notations for probability in different communities.
\newmaterial{%
Probability is a foundational concept broadly across computer science, and especially for us. 
% There are several different notation for probability
Unfortunately, its notation is almost as diverse as the the people who use it. 
}%
% }%
% In this section, we present a carefully 
Our approach simultaneously enables some of the most common notations
    and uses the standard foundations. 
    % and the most recognizable foundations. 
Thus, although all of the pieces are likely to be familiar, the precise formal way it fits together may not be. 
%
We start by giving the simpler picture of probability in the finite case (\cref{ssec:fin-prob}), which, for our purposes, is largely representative of how one should think of probability in the general case (which we return to \cref{ssec:gen-prob}).

\subsection{Probability in the Finite Case}
    \label{ssec:fin-prob}
% We first give the simple picture of probability in which representations are finite. 
% in the finite case; for now, assume the variables $X$ and $Y$ can take on only finitely many possible
% Thus, we assume  values (i.e., $|\V\!X|, |\V Y| \in \mathbb N$).
% In this section, let's assume that all variables can take on only finitely many possible values
%     (i.e., $|\V\!X|, |\V Y| \in \mathbb N$).
\paragraph{Measures and Conditioning.}
Suppose that $\Omega$ is a finite set. 
A \emph{probability distribution} $\mu$ over $\Omega$ is essentially a function 
% $\mu : \V\!\X \to [0,1]$ such that $\sum_{x \in \V X} \mu(x) = 1$.
$\mu : \Omega \to [0,1]$ such that $\sum_{\omega \in \Omega} \mu(\omega) = 1$.
This means $\mu$ is a vector of shape 
% $\V\! X$,
$\Omega$ 
satisfying $\mu \ge 0$ and $\mu \cdot \mat 1 = 1$. 
%
The distribution that places all mass on a single outcome $\omega \in \Omega$ is written $\delta_\omega$. 
% We write $\Delta \V\!X$ for the set of all distributions over (the values of) $X$,
% which is also known as the \emph{probability simplex}.
The \emph{probability simplex} $\Delta \Omega$ is defined as the set of all probability distributions over $\Omega$. 
The \emph{support} of a distribution $\mu$
    is the set $\Supp \mu := \{ \omega \in \Omega : \mu(\omega) > 0 \}$
    of outcomes that have positive probability.
For $U \subseteq \Omega$, called \emph{events},
    define $\mu(U) := \sum_{x \in U} \mu(x)$. 
% Specifically, the event $X{=}x$ 
% The prototypical event is of the form $X{=}x$. 
While we just defined $\mu$ on events $U$ of $\Omega$ in terms of $\mu(\omega)$ for elements $\omega \in \Omega$, we remark that, in general (i.e., beyond the finite case), it has to be the other way around: 
a probability distribution is an object that assigns probability to events in a way that is compatible with intuitions from the finite case we have just described.
\newmaterial{%
We return to this in \cref{ssec:gen-prob}.
}%


The standard way of updating a probability distribution based on new evidence is conditioning.
Given a subset $A \subseteq \Omega$, the conditional measure
$\mu \mid A$ is defined by
$(\mu \mid A)(x) := \frac{1}{\mu(A)} \mathbbm 1[x \in A] \mu(x)$.
% In other words, 
That quantity is also given a shorter and more common name: $\mu(x \mid A)$. 
From this definition, it is easy to recover the usual definition of conditional probability:
$\mu(U \mid A) = \mu(U \cap A) / \mu(A)$. 
There is just one caveat: this formula is not meaningful when $\mu(A) = 0$; in this case, we leave the conditional probability undefined.
% Indeed, if $\mu(A) = 0$, it is arguably appropriate to say $\mu(U \mid A) = p$ for \emph{every} $p\in[0,1]$, because there is a sequence of probability distributions $\{ \mu_k \}_{k \in \mathbb N}$ converging to $\mu$ that all $\mu_k(U \mid A) = p$ for all $k$. 
Indeed, if $\mu(A) = 0$, there is an equally strong argument supporting the notion that $\mu(U \mid A) = p$ for \emph{every} $p\in[0,1]$: for all $p \in [0,1]$, there is a sequence $\{ \mu_k \}_{k \in \mathbb N}$ of probability distributions converging to $\mu$ with the property that $\mu_k(U \mid A) = p$ for all $k$.


%% adding variables.
\paragraph{Probability via Variables.}
Our notation for representing and manipulate probability distributions will make heavy use of the notion of a variable, as developed in \cref{ssec:basic-vars}. 
Namely, we almost always work with probability distributions over the values of a variable. This just a notational convenience, since we can always define a variable $W$ with possible values $\V W := \Omega$.  
Working with variables confers us a number of benefits,
the first of which is 
% For now, we mention only that it allows us to 
to enable standard notation in the AI community:
given a distribution $\mu \in \Delta \V\!X$ over the values of $X$,
we also write $\mu(X)$ to specify $\mu$ and its type in a compact way.
% we write $\mu(X)$ in place of $\mu$ to emphasize that $\mu \in \Delta \V \! X$ in a compact way. 
% To a reader paying close attention, this definition might seem dangerous.
Since $\mu : \V W \to [0,1]$ is a function, we have already defined $\mu(W)$ to be a variable, but fortunately that variable encodes exactly the same information as $\mu$ itself.


%  $\mu(\mat X) = \mu(X_1, \ldots, X_n)$
A \emph{joint distribution} $\mu(\mat X) = \mu(X_1, \ldots, X_n)$ over a finite (indexed) set of variables $\mat X = \{X_1, \ldots, X_n\}$ is just a distribution over joint settings of all variables---i.e, a distribution over $\V \mat X$. 
%TODO: the above is awkward.
If $\mu(\mat X)$ is a joint distribution and $X \in \mat X$, then we write $\mu(X)$ for its \emph{marginal} on $X$, which is given by
\[
    \mu(X{=}x) = \mu(X)(x) := \sum_{\substack{\mat x \in \V \mat X \\ \mat x[X] = x}} \mu(\mat x) .
\]
It is called a marginal because of the special case of two variables: a joint distribution $\mu(X,Y)$ may be viewed as a grid of numbers summing to 1, whose rows correspond to values of $X$, and whose columns correspond to values of $Y$. 
If we were to write the sum $\sum_{y \in \V Y} \mu(x,y)$ of each row $x$ in the margin, this would give the marginal distribution on $X$. 


% The conditioned distribution $\mu|X{=}x$ is also written $\mu(\mat X \mid X{=}x)$, or, in abbreviated form as $\mu(Y|x)$ in contexts where the conditioning variable $X$ is unambiguous. 
To \emph{condition} a joint distribution $\mu(\mat X)$ on a variable $X \in \mat X$ is to form an indexed set of conditioned distributions $\mu(\mat X \mid X) = \{ ~(\mu|X{=}x) ~\}_{x \in \V\!X}$, one for each possible value of $X$. 
% The cpd $\mu(Y|X)$
We can also refer to such objects without reference to a joint distribution.
%
%
A \emph{conditional probability distribution (cpd)} on $Y$ given $X$ is a function $p : \V X \to \Delta \V Y$ that, for each $x \in \V\!X$, yields a probability distribution over $Y$,
which is written in any of the following equivalent notations:
\[ 
% $
p(x) ~=~ p|x ~=~ p(Y | X{=}x) ~=~ p(Y | x) \quad \in \Delta \V Y.
% $
\]
% Cpds are also called \emph{channels} in information theory. 
Just like we write $\mu(X)$ for a distribution over the values of $X$, we
write $p(Y|X)$ for a cpd on $Y$ given $X$. 
\newmaterial{%
If $p(Y|x)$ gives probability one to a single value of $Y$ for all $x \in \V\!X$, then $p(Y|X)$ is \emph{deterministic}, and has the same meaning as the function $f : X \to Y$ that maps $x$ to the value $y$ for which $p(Y|x)$ has probability one.  Conversely, given a function $f : X \to Y$, we write $\delta\!f(Y|X)$ for the corresponding deterministic cpd. 
}
In the finite case, cpds coincide with \emph{stochastic matrices}: matrices whose columns correspond to values of $Y$, rows correspond to values of $X$, and such that the sum of each row equals 1. 
We will use alternate notation (such as $\mat P_{Y|X}$) when we want to view $p(Y|X)$ as a matrix.

We typically write $\X$ for the set of all variables relevant to a given context. If $X,Y \in \X$, then we write $\mu(Y|X)$ for the cpd given by first conditioning $\mu(\X)$ on $X$, and then marginalizing each conditional distribution to $Y$. 
% Now, $\mu(Y|X)$ too is a variable 
By regarding $\mu(Y|X)$ as a function of $Y$ and $X$, it too can be viewed a real-valued variable (just like $\mu(X)$). 
% This now explains 
% Observe that $\mu(\X \mid X)$ 
% We can deal with the 
% In this context, the un-definedness of $\mu(Y|X)$ 
% Recall that the components of $\mu(Y|X)$
We can now state \emph{chain rule for probability}, a simple but incredibly important fact:
% \begin{equation}
    $\mu(X,Y) = \mu(X) \mu(Y|X)$.
% \end{equation}
% Unpacking our definition of what it means for it to be true that two variables are equal, 
This standard formula is typically viewed as shorthand for 
\[
    \forall (x,y) \in \V(X,Y).\quad \mu(X{=}x,Y{=}y) = \mu(X{=}x) \mu(Y{=}y|X{=}x),
\]
a view that also demands a caveat: ``whenever $\mu(X{=}x) > 0$, so that $\mu(Y{=}y|X{=}x)$ is defined''. 
% and is essentially just many closely related instances of the definition of the conditional probability of one event given another. 
% But the shorter version also has another benefit: 
However, since $\mu(X,Y)$, $\mu(X)$, and $\mu(Y|X)$ are all variables,
    the equality $\mu(X,Y) = \mu(X)\mu(Y|X)$ is simply true without caveats.
Why?  When $\mu(X{=}x) = 0$, then $\mu(Y{=}y|X{=}x)$ is undefined, meaning 
    it has no definitional constraint (i.e., $\Gamma(\mu(Y{=}y|X{=}x))$)---but no matter what value we select for $\mu(Y{=}y|X{=}x)$, the result still holds, because 
    % that value is later multiplied by zero.
    the question is whether or not that value times zero equals zero. 
% (Which it does.)
%    
% Moreover, the fact that $\mu(Y|X{=}x)$ is undefined can now be captured simply by not adding a constraint in $\Gamma(\mu(Y|X))$ connecting the value of $\mu(Y|X)$ to the value of $\mu(X,Y)$ in such cases. 
%
% Yet this equation, unlike the definition of a conditional probability for events, holds 
% Yet here, we no longer need to carve out a special caveat for conditioning on probability zero events; that case is handled automatically by implicit underlying constraints.

\paragraph{Variables and Random Variables.}
    \label{sec:var-randomvar}
In the context of a probability distribution $\mu \in \Delta \Omega$,
% a \emph{random variable} is a function $X : \Omega \to S$, where $S$ is a set called the domain of $X$. In service of unified notation, let $\V\!X := S$ denote the domain of the random variable $X$. 
a \emph{random variable} is a function $X : \Omega \to \V\!X$, where $\V\! X$ is just some set (unfortunately) called the \emph{domain} of $X$
    (even though $\Omega$ is the domain of the function).
% Although the formalism may appear quite different, it is fundamentally very similar to our notion of a variable, a connection we explore.
Despite the superficial similarity induced by our choice of notation, random variables and variables are different formal objects;
    hence the mantra, 
    ``a random variable is neither random nor a variable''. 
Yet at an even deeper level, variables and random variables are interchangeable, and fundamentally represent the same concept, as we now explore. 

Whereas the traditional foundations of probability start with a sample space $\Omega$ and defines random variables $X : \Omega \to \V\!X$ in terms of it, 
probabilistic modeling is often done the other way around in practice: one starts with variables $\X$ of interest, and defines $\Omega := \V\!\X$ to be the joint settings of those variables. 
Under that definition of $\Omega$ a variable $X \in \X$ can be viewed as a random variable $\hat X : \V\!\X \to \V\!X$ that selects the value of $X$ from a joint setting of all variables.  
% Conversely, 
The converse not well appreciated, 
    perhaps because it requires one to think about
    the constraints involved in defining a variable.
% Still, given 
% Given an arbitrary random variable $Z : $
If $\Omega = \Delta \V\!X$, and  
    $\hat Z : \V\!\X \to \V Z$
    is an arbitrary random variable, 
    we can regard $\hat Z$ as a variable $Z$, and include it in the set $\X' := \X \cup \{ \hat Z \}$ of all variables.
When we do so,  
there is a natural 1-1 correspondence between distributions $\mu \in \Delta \V\!\X$ over the original variables, and ``extended'' distributions 
$\bar\mu \in \Delta \V\!\X'$ in which variable $Z$ and the function $\hat Z$ applied to the other variables coincide (formally, where $\bar\mu(\hat Z(\X) = Z)=1$).

%
Although the two perspectives seem to be equally expressive, we opt for the one in which the variables $\X$ are primitive, rather than the one in which the sample space $\Omega$ is primitive. 
After all, we are interested in modeling epistemic states that undergo structural changes, and it is more natural to add and remove variables of interest, than it is to change sample spaces and figure out how to properly translate the relevant random variables on them.  
% We have already seen one benefit of this, in our compact discussion of 
We will see an even deeper connection between the two ways of thinking about (random) variables in \cref{chap:PDG-cat}.

\paragraph{%Dependence and 
    Independence.} 
Suppose that $X$, $Y$, and $Z$ are (random) variables and $\mu(X,Y,Z)$ is a distribution over them.
The variables $X$ and $Y$ are \emph{independent} (according to $\mu$) iff $\mu(X,Y) = \mu(X)\mu(Y)$.
Often it is the case that two variables are independent only when we control for something else.
The variable ${X}$ is \emph{conditionally independent of ${Y}$ given ${Z}$} 
(according to $\mu$),  denoted 
 % ${X} \CI_\mu {Z} \mid {Y}$, 
$\mu \models {X} \CI {Y} \mid {Z}$, 
only if $\mu(X | Z) \mu(Y | Z) = \mu(X,Y| Z)$,
or equivalently, if
$\mu(X,Z) \mu(Y,Z) = \mu(X,Y,Z) \mu(Z)$. 
% By multiplying both sides by $\mu(Z)^2$,
% If $X$ and $Y$ are not variables, but rather events, then they are independent iff the same formula holds. 
Precisely the same formulas define (conditional) independence
    when $X$,$Y$, and $Z$ are not events, but rather events
    (modulo the fact that conditioning on an event of probability zero is undefined).



%%% perhaps this can wait for QIM section...
% We say that $X$ \emph{determines} $Y$, and write $\mu \models X \tto Y$ only if
% for all $x \in \V\!X$, there is at most one value of $Y$ that has positive probability, i.e.,  $\#\{ y \in \V Y : \mu(X{=}x,Y{=}y) > 0 \} \le 1$.
% It is not hard to see that $X \tto Y$ if and only if there exists a function $f : \V\!X \to \V Y$ such that $\mu(f(X) = Y) = 1$. 


\paragraph{Expectation.%
    % and Utility
    } 
If $\mu$ is a probability over $\Omega$, and $X$ is random variable 
whose possible values are vectors 
% of a given shape(i.e., $\V \! X \subseteq \Rext^I$)
\unskip, then the \emph{expectation} of $X$ with respect to $\mu$ is intuitively its mean or average value.
Depending on which is clearer in context, 
    an expectation may be notated in either of the following two ways:
\[
    \Ex_{\omega \sim \mu} \Big[ X(\omega) \Big]
        = \Ex_{\mu}[ X ]  
        := \sum_{\omega \in \Omega} \mu(\omega) X(\omega).
\]
In some cases, we omit the brackets to reduce clutter and improve readability.
For a fixed random variable $X$, the expectation operator $\mu \mapsto \Ex_\mu[X] : [0,1]^{\V\! X} \to \Rext^I$ is a linear function of the probability distribution $\mu$.


Expectations and probabilities are closely related to convexity. 
% In a sense, the probability simplex $\Delta \Omega$ is the prototypical convex set. 
% \mathbbm
% For instance, if $A \in \Rext^{I \times J}$ is a matrix, then
% the image of $\mu(X) \mapsto \Ex_\mu[f]$.
For instance, 
the convex hull of $A \subseteq \Rext^I$ 
is set of possible expectations over the extreme points of $A$,
or more formally,
\[
    \mathrm{conv}(A) = \im 
        \Big( \Delta \mathrm{Ext}(A) \ni \mu \mapsto \Ex_{v \sim \mu} [ v ] \Big).
\]
Thus, if $A$ is convex, to specify $a \in A$, it suffices to specify a probability distribution over $\mathrm{Ext}(A)$.  
% Such distributions are called \emph{barycentric coordinates} for $A$.
The components of a distribution $\mu \in \Delta \mathrm{Ext}(A)$ are called \emph{barycentric coordinates} for the element $a = \Ex_{v \sim \mu}[v] \in A$.
%
\oldnewmaterial{%
    A cpd $p(Y|X)$ can be viewed as providing a barycentric coordinate system for 
    $\Delta \V Y$ whose vertices correspond to $\V\!X$. 
}
\newmaterial{%
This way of thinking about conditional probability distributions will at times be quite useful (e.g., in \cref{ssec:cvx-prob-sets}).
}

% For instance, $\mathrm{Ext}(\Delta \Omega) = \{ \delta_{\omega} : \omega \in \Omega \}$. 
% If $A$ is a convex set with $|\mathrm{Ext}(A)| \in \mathbb N$,
% then $A$ is the image of a linear map ....


%%% UTILITIES??
% The gold standard for decision making is expected utility maximization: an agent that has an internal probability $\mu \in \Delta \Omega$, a utility function $U: $
% 
% \TODO

\commentout{%
\subsection{Beyond Probability}
    \label{ssec:beyond-prob}
}%
%%% TODO %%%
% sets of probability measures
% dempster-shafer belief functions
% Probability is 

% \subsection{Probability, More Generally}
\subsection{Probability, in General}
    \label{ssec:gen-prob}

The time has come for us to bite the bullet and describe how probability actually works, when a finite representation is insufficient. 
The material here can be interesting and instructive,
    and it is certainly a necessary foundation for our results that talk about distributions over real numbers.
Still, we reiterate that the full account of measure theory needed to define probability in general is more than is necessary to get a deep conceptual understanding of the present work. 

\paragraph{Measures.}
A \emph{measurable space} $(\Omega, \mathcal F)$ is a set $\Omega$,
called the \emph{outcome space}, together with a collection $\mathcal F \subseteq 2^\Omega$ of subsets of $\Omega$ 
% that are considered \emph{measurable}, called \emph{events}. 
called \emph{events}, or \emph{measurable} sets. 
The elements of $\Omega$, should be thought of \emph{possible outcomes} or \emph{possible worlds} for in the model.
The set $\mathcal F$ of events must also be a \emph{$\sigma$-algebra}, meaning that it contains $\Omega$, and is closed under complement and countable union---%
that is, $\Omega \in \mathcal F$, if $U \in \mathcal F$ then $\Omega \setminus U \in \mathcal F$, and if $U_1, U_2, \ldots \in \mathcal F$, then $U_1 \cup U_2 \cup \cdots \in \mathcal F$.  
% One should think of $\mathcal A = 2^\Omega$ as consisting of every 
% For all but a few select places, one can ignore $\mathcal F_\Omega$ in this dissertation.
% Often in probabilistic modeling, and in this dissertation in particular, the $\mathcal F_\Omega$.
When $\Omega$ is finite, we allow all subsets to be measured by defining $\mathcal F := 2^\Omega$,
    and there is also a standard choice for most other spaces of interest, such as when $\Omega$ is a convex subset of real numbers, called its \emph{Standard Borel Space}. 
Either way, we will typically leave $\mathcal F$ implicit after this chapter, referring to a measurable space with the same symbol as its sample space $\Omega$. 

A \emph{measure} $\mu$ over a measurable space $(\Omega, \mathcal F)$ is a function $\mu : \mathcal F \to [0,\infty]$ with two additional properties: $\mu(\emptyset) = 0$, and,
for every countable collection $\{ U_i \}_{i \in \mathbb N}$
of pairwise disjoint measurable sets (i.e., where each $U_i \in \mathcal F$ and $U_i \cap U_j = \emptyset$),  we have that
$\sum_{i \in \mathbb N} \mu(U_i) = \lambda( \cup_{i \in \mathbb N}~U_i)$. 
A \emph{probability distribution} (or probability measure) is a measure $\mu$ with the additional property that $\mu(\Omega) = 1$. 
% satisfying the three Kolmogorov axioms:
% % \begin{enumerate}
% %     \item 
% \begin{align*}
%     &\Pr(\emptyset) = 0, \qquad
%     \Pr(\Omega) = 1, \\
%     \quad\text{and}\qquad&
%     \forall U, V \in \mathcal F.\quad 
%         U \cap V = \emptyset \implies \Pr(U \cup V) = \Pr(U) + \Pr(V).
% \end{align*}
%
These three properties are quite intuitive: $\emptyset$ is an event that cannot occur and hence has probability zero,
$\Omega$ is an event that always occurs and hence has probability 1, 
and if events cannot co-occur, then the probability that one of them occurs is the sum of their probabilities. 
% \end{enumerate}
% We need a few more definitions about measures. 

%%TODO: marginal measures, integration, expectation, in the general case.

%%%
\commentout{%%% MOVED BELOW!
We will need a few additional concepts about measures.
First, if $\mu$ and $\nu$ are measures over a space $(X, \mathcal F)$, 
and $\nu(U) = 0$ implies $\mu(U) = 0$ for all $U \in \mathcal F$, then
$\mu$ is \emph{absolutely continuous} with respect to $\nu$, also written $\nu \ll \mu$.
Second, if $\Omega$ comes with a topology $\mathcal T$ (see \cref{prelim:topo} for definitions) and $\mu(U) > 0$ for every non-empty open set $U \in \mathcal F \cap \mathcal T$, then $\mu$ is  \emph{strictly positive} (with respect to $\cal T$).}


\paragraph{Statistics and (Random) Variables.} 
If $(\Omega, \mathcal F)$ and $(Y, \mathcal G)$ are two measurable spaces, then
a function $f : \Omega \to Y$ is \emph{measurable} if $f^{-1}(U) \in \mathcal F$ for all $U \in \mathcal G$. 
In a context where one is interested in probabilities over $\Omega$, 
a measurable function $X : \Omega \to \V\!X$ is also called a \emph{random variable} or a \emph{statistic}.
% Although the formalism may appear quite different, it is fundamentally very similar to our notion of a variable, a connection we explore in the next two paragraphs.
\oldnewmaterial{%
For our purposes, measurable functions have two key properties:
(1) they are closed under composition (so $f \circ g$ is measurable if both $f$ and $g$ are), and
(2) if $f : (\Omega, \mathcal F) \to \Rext^n$ is a measureable function, $\mu$ is a measure on $(\Omega, \mathcal F)$, and $A \in \mathcal F$, then we can form the (Lebesgue) integral
    $\int_{A} f \mathrm d \mu$ to obtain an element of $\Rext^n$.%
\footnote{To be certain that nothing goes wrong, 
    it is typically also necessary to assert that either $f \ge 0$ or $\int_A |f_i| < \infty$ for all $i \in [n]$. 
    % but the integral may still be perfectly meaningful 
}
% One key property of measurable functions is that they are closed under composition. 
% Another key property of a measurable function whose 
% % The key property of measurable functions is that we can take their integral with respect to a measure.
% Formally, if $\mu$ is a measure over the measurable space $(\Omega, \mathcal F)$, $A \in \mathcal F$, and 
}%



% One way of 
\oldnewmaterial{%
% The last property of measures
% is a  of simplifying the theory for general measures,
We now present an important way of handling concerns about measurability, and bringing the formalism closer to the finite case.
The idea is to endow each measurable space with a fixed ``base measure'', and describe all measures on a space relative to that measure. 
Indeed, this is what we will need to develop the analogues of information theoretic quantities that extend beyond the finite case.
To make this precise, we need:
% which we will need to make use of information theory. 
% A common way of simplifying the theory for general measures,
}

\paragraph{The Radon-Nikodym Derivative.}
Suppose $\mu$ and $\nu$ are both measures over a measurable space $(\Omega,\mathcal F)$.
%  and $\mu \ll \nu$. 
If $\nu(U) = 0$ implies $\mu(U) = 0$ for all $U \in \mathcal F$, then
$\mu$ is \emph{absolutely continuous} with respect to $\nu$, also written $\mu \ll \nu$.
When $\mu \ll \nu$, the Radon-Nikodym theorem \citep{nikodym1930generalisation} 
states that there exists a unique 
% $\mathcal F$-measurable function $f$
    measurable function $f : \Omega \to \mathbb R$
such that, for all $A \in \mathcal F$,
$
    \mu(A) = \int_{A} f \,\mathrm d \nu.
$
This function $f$ is called the \emph{Radon-Nikodym derivative} of $\mu$ with respect to $\nu$, and is denoted $\frac{\mathrm d\mu}{\mathrm d\nu} := f$.


In this more general setting, we no longer require $\V\!X$ to be a finite set, but in exchange, we require some structure: $\V\!X$ has to be a measurable space (i.e., come equipped with a $\sigma$-algebra).
%
\oldnewmaterial{%
% From here on out, we will assume that for each variable $X$, the possible values $\V\!X$ not only comes with a $\sigma$-algebra $\mathcal F_X$ (making it a measurable space),
Furthermore, we will also assume that $\V\!X$ comes equipped with a measure $\lambda_X$ that we call the \emph{base measure}. 
In the case of a discrete variable $X$ that takes on finitely or countably many values, we take $\lambda_X$ to be the counting measure: for $U \subseteq \V\!X$ $\lambda_X(U) = |U|$ is the number of elements in $U$. 
When $\V\!X$ is a subset of real space $\mathbb R^n$, we use the \emph{Lebesgue measure}, which agrees with the standard notions of length, area, and volume.
}%


% At a technical level,
All of this new structure is compatible with our discussion of variables in \cref{ssec:basic-vars}.
This is because all of our ways of constructing variables---through (indexed) sets of variables, and by applying functions---work equally well over measurable spaces
\oldnewmaterial{(with a base measure)}.
\begin{enumerate}
\item
Recall that a tuple $\mat X = (X_1, \ldots, X_n)$ of variables can itself be viewed as itself a variable, with
    $\V\!\mat X = \prod_{i=1}^n \V \! X_i$.
$\V\!\mat X$ is a measurable space, because if each $\V(X_i)$ comes with a $\sigma$-algebra $\mathcal F_i$,
then $\V\!\mat X$ naturally inherits the product algebra $\mathcal F_{1:n}$ generated by complements and countable unions of events of the form $U = U_1 \times \cdots \times U_n$ with each $U_i \in \mathcal F_i$. 
\oldnewmaterial{%
Similarly, it inherits the unique product measure $\lambda_{\mat X}$
whose action on such primitive measurable sets is given by
$\lambda_{\mat X}(U_1 \times \cdots\times U_n) := \lambda_{X_1}(U_1)
        \lambda_{X_2}(U_2) \cdots \lambda_{X_n}(U_n)$.
}%

\item Similarly, recall that when $X$ is a variable and $f : \V X \to S$ is a function, we regard $f(X)$ as a variable. In this setting, we require that $S$ 
\oldnewmaterial{%
    come equipped with a $\sigma$-algebra $\mathcal G$ with respect to which $f$ is a measurable function.  
    The base measure $\lambda_{f(X)}$ is then the \emph{pushforward} of $\lambda_X$ through $f$, given by 
    $\lambda_{f(X)}(U) := \lambda_X( f^{-1}(U))$, for $U \in \mathcal G$.
}
\end{enumerate}

 
\oldnewmaterial{%
% Technically speaking $\mu(X)$ is not a random variable 
%
Why do we need all of this? 
Recall in the finite case, that when $X$ is a variable and $\mu \in \Delta \V\!X$, then $\mu(X) : \V\!\X \to \mathbb R$ was a random variable. 
In the more general setting, the appropriate analogue is 
    % is not $\mu(X)$, but rather 
    % the derivative
    $\frac{\mathrm d\mu(X)}{\mathrm d \lambda_X}$,
the Radon-Nikodym derivative
of $\mu$ with respect to the base measure, which is the random variable of interest.
%
For finite variables, this definition coincides with the \emph{probability mass function (pmf)} that we were using before;
for continuous variables, 
    it coincides with the \emph{probability density function (pdf)} of the distribution.
% So in particular, when we define 
Having such an analogue of a random variable corresponding to a measure $\mu$ will be particularly important for generalizing information-theoretic quantities to continuous variables (\cref{sec:prelim-infotheory}).
}%%%%%%%%%%
 
% Whereas the traditional foundations of probability starts with a sample space $\Omega$ and defines random variables from it,  probabilistic modeling is done the other way around in practice: one starts with variables $\X$ of interest, and defines $\Omega := \V\!\X$ to be the joint settings of those variables. 
% With that definition, a variable $X$ can be viewed as a random variable $X : \V\!\X \to \V\!X$ that selects the value of $X$ from the joint setting.  
% Conversely, an arbitrary random variable $Z : \V\!\X \to \V Z$ can be included in the set of all variables; there is a natural 1-1 correspondence between distributions $\mu \in \Delta \V\!\X$ over the original variables, and ``extended'' distributions $\bar\mu \in \Delta \V(\X \cup Z)$ with the property that variable $Z$ and the function $Z$ applied to the other variables coincide (i.e., $\bar\mu(Z(\X) = Z)=1$).
% %
% %
% Although the two perspectives seem to be equally expressive, we opt for the variable-based one. 
% After all, we are interested in modeling epistemic states that undergo structural changes, and it is conceptually simpler to add and remove variables of interest, than it is to change sample spaces and figure out how to properly translate the random variables on them.  



\paragraph{Markov Kernels.}
Finally, we get to the precise notion of a conditional probability distribution (cpd) 
in this more careful framework regarding measurability.
If $(\Omega, \mathcal F)$, and $(\Omega', \mathcal G)$ are two measurable spaces, a \emph{Markov Kernel} 
% $\kappa : \Omega \to \mathcal Y$, which we sometimes write as ``$\kappa(Y|X)$'', 
$\kappa$ from $(\Omega,\mathcal F)$ to $(\Omega', \mathcal G)$
is a function $\kappa : \Omega \times \mathcal G \to \mathbb R$, such that

\begin{enumerate}
    \item For every $\omega \in \Omega$, the map $\kappa(\omega, -) : \mathcal G \to [0,1]$ is a probability measure on $\Omega'$. 
        (So $\kappa$ is also a cpd.)
    \item For every $U \in \mathcal G$, the map $\kappa(-, U) : \Omega \to [0,1]$ is a measurable function from $\Omega$ to the Borel space $[0,1]$.
    Or more explicitly: for every open set $S \subseteq [0,1]$, and $U \in \mathcal G$, we have that
    $\{\omega \in \Omega : \kappa(\omega,U) \in S\} \in \mathcal F$. 
\end{enumerate}

The second condition is the new one, and it is necessary to ensure that we can compose them and treat them as models of randomized functions, just as in the finite case.
The analogue of viewing a cpd $p(Y|X)$ in the finite case 
    as a random variable, is to implicitly convert
    the Markov Kernel $\kappa(Y|X)$ to the measurable function 
\[
    (x,y) \mapsto \frac{\mathrm d\kappa(x,-)}{\mathrm d \lambda_Y} (y) 
        \quad : \V(X,Y) \to [0,\infty].
\]


\section{The Basic Theory of Graphs and Hypergraphs}
    \label{sec:prelim-graphs}

In this dissertation (as in much of mathematics and computer science),
the term \emph{graph} refers not to a \emph{graph}ical depiction of a function or data,
but rather to what is more commonly known as a \emph{network}. 

Graphs come in two flavors: directed and undirected. 

\begin{defn}
    A \emph{(directed) (multi)graph} $G = (N, A)$, or simply a \emph{graph}, is a set $N$ of nodes,
    and a collection $A$ of \emph{directed edges} (or \emph{arcs}), such that each $a \in A$ has a source node $\Src a \in N$ and a target node $\Tgt a \in N$.
    So, formally, the definition is
    $G = (N, A, \Src{}, \Tgt{})$, with $\Src{},\Tgt{}: A \to N$ often left implicit.
    \oldnewmaterial{%
    We write $\ed auv \in A$ to indicate that there is some $a \in A$
    with source $\Src a = u$ and target $\Tgt a = v$,
    and we call $a$ an arc ``from $u$ to $v$''. 
    }%
\end{defn}

\oldnewmaterial{%
% A graph that has at most one arc from $u$ to $v$
% Given a graph $G = (N,A)$ and a node $u \in N$,
In a graph $G = (N,A)$, the \emph{parents} of a node $u \in N$ 
are the nodes
\[
    \Pa_G(u) := \Big\{  v \in N : \exists \, \ed auv \in A \Big\}
        \subseteq N
\]
that are sources of arcs leading to $u$.
Dually, the \emph{children} of $u \in N$ are the nodes $\Ch_G(u)$ that are targets of some arc whose source is $u$. 
    % we write $$
There is no distinction between the two notions if the edges do not have direction.
}%


\begin{defn}
    % [Undirected (Multi) Graph]
    An \emph{undirected} (multi)graph $G = (N,E)$ is a set $N$ of vertices (or nodes)
    and a set $E$ of edges,
    each element $e \in E$ of which
    corresponds to an unordered pair of vertices $\{u,v\}$.
    More formally, there is a map
    \[
        \iota: E \to~ \faktor{ V \times V \setminus \{(v,v):v \in N\} }{ \{(u,v)\sim(v,u)\mid (u,v) \in N \times N\} }~.
    \]
    implicit in the definition of $G$, which we will write $G =
    % (V,E{,~ \color{gray}\iota})$ 
     (N,E, \iota)$ 
    only when being extra careful.
    \qedhere
\end{defn}


A \emph{clique} of an undirected graph $G = (N,E)$ is a subset $C \subseteq N$ of its vertices that are all connected by edges---that is, such that every distinct pair $\{x,y\}$ of vertices $x,y \in C$ is a member of $E$.  
We write $\mathrm{cliques}(G)$ for the set of all cliques in a graph. 
%
We often write $u{-}v \in E$ as a slightly more visual alternative to $\{u,v\} \in E$, and one that can be chained together. 
In an undirected graph, 
a \emph{path} from $u$ to $v$ is a sequence of edges
$u = u_1{-}u_2{-}\cdots{-}u_n = v$
that connect $u$ and $v$. 
A path in a directed graph has the additional requirement that every arc along the path must point from $u_i$ to $u_{i+1}$.
%
A \emph{cycle} is a path that starts and ends at the same node.
% (and passes through at least one other node).
A \emph{directed acyclic graph}, or simply \emph{dag} for short, is a directed graph without any cycles. 


It is common to identify a graph $H = (N,A)$ (or an undirected graph $G = (N,E)$) with its (symmetric) adjacency matrix
\[
    \mathbb A_H = \Big[ \#\Big\{ a \in A : {\singlespacing \begin{array}{c}\Src a = u,\\ \Tgt a = v \end{array}} \Big\} \Big]_{(u,v) \in N \times N}
    \quad
    \mathbb A_G = \Big[
    \#\{ e \in E : \iota(e) = \{u,v\}  \Big]_{(u,v) \in N\times N},
\]
in part because there is a natural bijection between
(undirected) multigraphs and (symmetric) square matrices
over the natural numbers.  
For example:
\[
    \begin{bmatrix}
        0 & 1 & 2 \\
        1 & 0 & 0 \\
        2 & 0 & 1
    \end{bmatrix}
    \qquad
    \leftrightsquigarrow
    \qquad
    \begin{tikzpicture}[center base]
        \begin{scope}
            % \node at (0:1) (a) {a};
            % \node at (120:1) (b) {b};
            % \node at (-120:1) (c) {c};
            \node at (0,0) (a) {a};
            \node at (1,0) (b) {b};
            \node at (-1,0) (c) {c};
        \end{scope}
        \draw[-] (a) to (b);
        % \draw (a) to[bend left] (b);
        % \draw (b) to[bend left] (a);
        
        \draw[-] (a) to[bend left] (c);
        \draw[-] (a) to[bend right] (c);
        
        \path[every loop/.style={}] (c) edge[loop above] (c);
    \end{tikzpicture}
\]


\commentout{%
A (directed) graph has more information than an undirected one.
There are some obvious ways to get a directed graph from an undirected one (add an arrow going in both directions) and to get an undirected graph from a directed one: forget the direction of the arc, making it an edge.
These choices are illustrated in the following diagram:
\[ 
\begin{tikzcd}
\text{(Directed) Graphs} \ar[r,bend left,"\mathit{forget}"]&
\text{Undirected Graphs}
\ar[l,bend left,"\mathit{annotate}"]
\end{tikzcd};
\]
$\mathit{forget} \circ \mathit{annotate}$
is the identity on undirected graphs, but
$\mathit{anotate} \circ \mathit{forget}$ is 
not the identity on (directed) graphs.
% Technically, this makes $\mathit{forget}$ a \emph{retraction}, and $\mathit{annotate}$ a \emph{section}.
These choices are essentially locked in if we want to preseve the validity of the representation in terms of square binary matrices. 
}%

\newmaterial{%
There is an important special class of (undirected) graphs that commonly arises in modeling the relationships between two distinct kinds of objects. 
% in which nodes can be partitioned into two separte parts, and every edge crosses that partition. 
}%

\begin{defn}
    A bipartite graph $G = (L, R, E)$ is an undirected graph $(L \sqcup R, E)$ whose
    vertices are partitioned into two components $V = L \sqcup R$ (the \emph{left part} and \emph{right part}) 
    % and whose edges $E \subset L \times R$ are only between $L$ and $R$. 
    with the property that every edge crosses the partition: $\forall e \in E. |\iota(e) \cap L| = |\iota(e) \cap R| = 1$. 
\end{defn}

% \fadeout{ \begin{defn}
%     A directed bipartite graph $G = (L, R, E)$ is a bipartite graph $(L, R, E)$ whose edges $E \subset (L \times R) \cup (R \times L)$ are directed. 
%     \end{defn} }

\newmaterial{%
% There is also an a
It seems that the notion of a graph only captures pairwise relationships between vertices. 
If we also want to model relationships between triples of vertices, it seems that we need a more general object.
}

\begin{defn}
    A \emph{hypergraph} $G = (V, \mathcal E)$ is a set $V$ of vertices,
    and a collection $\Ed$ of \emph{hyperedges}, which correspond to finite
    subsets of $V$. 
\end{defn}

An ordinary undirected graph is the special case in which every hyperedge contains two vertices.
In turn, a bipartite graph is a (very) special case of an ordinary undirected graph.
By transitivity, one might expect bipartite graphs to naturally be an extremely strict special case of hypergraphs---yet, perhaps counter-intuitively, they are in fact naturally isomorphic. 
% , which themselves are a strict (particularly easy to draw) special case of hypergraphs.  

\begin{prop} 
% There is  
% The following is
% natural bijection 
% an isomorphism
% between hypergraphs and bipartite graphs,
% given by
The functions
% \def\bitgr{\mathit{BG}}
% \def\bigr{\mathit{bipart}}
\def\bigr{\mathit{h2b}}
% \def\bitgr{\mathit{HG}}
% \def\hygr{\mathit{hyper}}
\def\hygr{\mathit{b2h}}
\begin{align*}
    \bigr(V, \mathcal E) &:=  (V, \mathcal E, \{(v, E) \in V \times \mathcal E : v \in E \} ) \\
    % (L, \{\{x \in L : \} \}) &\maspfrom (L, R, E)
    \bigr^{-1}(L,R,E) = \hygr(L, R, E) &:= (L, \{\{v \in L : (v,r) \in E\} : r \in R\})
\end{align*}   
define an isomorphism between hypergraphs and bipartite graphs.
    % \[
    %     \bigr \circ \hygr = \mathrm{id}_{BG}
    %     \quad\text{and}\quad
    %     \hygr \circ \bigr = \mathrm{id}_{HG}.
    % \]
\end{prop}

% The consequences of this can be unintuitive.
% This fact can be be unintuitive.
% Bipartite graphs are of course a strict (particularly nice) special case of ordinary undirected graphs, which themselves are a strict (particularly easy to draw) special case of hypergraphs.  
% By transitivity, one might expect bipartite graphs to naturally be an extremely strict special case of hypergraphs---yet in fact they are naturally isomorphic. 
% In fact, there are two isomorphisms. 
A second isomorphism can be obtained by swapping roles of the left and right parts of the bipartite graph. 
This is becasue there is an obvious symmetry in the definition of a bipartite graph: 
% one can swap the left and the right parts to 
by swapping $L$ and $R$, one
obtains another bipartite graph that is 
% isomorphic to the first one. 
    in many ways no different from the original.
The corresponding symmetry for hypergraphs, known as duality, is a bit less obvious.
% is known as duality, and is a bit less obvious.
%
The \emph{dual} of the hypergraph $G = (V, \mathcal E)$ is
the hypergraph
% \[
$
    \check G := (\mathcal E, \{\{e \in \mathcal E : v \in e\} : v \in V\}),
$
% \]
which in some sense, has swapped the roles of the vertices and the hyperedges.
These two views, and the symmetry between them, 
plays an important role in traditional graphical models, as we will see in \cref{sec:prelim-fgs}.

For the development of the ideas in this thesis, we are even more interested in the \emph{directed} analogue of a hypergraph.
% What about the directed analogue of a hypergraph? 
While these objects have been defined and studied before
    \citep{gallo-dirhypergraphs1993}, they are far less
    common in computer science than (ordinary) directed graphs, or (undirected) hypergraphs.
These directed hypergraphs will form the basis of the representations at the heart of this thesis (\cref{chap:pdg-repr,chap:QIM}).

% \begin{defn}
%     A \emph{directed} hypergraph $(N, \mathcal A)$ is a set $N$ of nodes,
%     and a collection $\Ar$ of hyperarcs, each of which has a set $S_a \subset N$ of
%     source variables and a set $T_a \subset N$ of target variables.        
% \end{defn}

\begin{defn}
        \label{defn:hypergraph}
    A \emph{directed hypergraph}
    consists of a set $\N$ of nodes
    and a set $\Ar$ of 
    directed hyperedges, or \emph{hyperarcs};
    each hyperarc  $a \in \Ar$ is associated with
        a set $\Src a \subseteq \N$ of
    source nodes and a set $\Tgt a \subseteq \N$ of target nodes.    %
    We write $\ed {\scriptstyle a}{S}{T} \in \Ar$ to specify a
    hyperarc $a \in \Ar$ together with its sources $S = \Src a$ and targets $T = \Tgt a$.
\end{defn}

Nodes that are neither a source nor a target of any hyperarc will
seldom have any effect on our constructions; the other nodes can
be recovered from the hyperarcs 
by selecting $\N_{\Ar} := \bigcup_{a \in \Ar} \Src a \cup \Tgt a$. 
If necessary, we can often add an ``identity'' arc 
    $N \to N$ for any node $N \in \N \setminus \N_{\Ar}$. 
% Thus, we often leave $\N$ implicit,
For these reasons, we often leave $\N$ implicit,
referring to the directed hypergraph simply as $\Ar$.

A directed hypergraph $(N, \mathcal A)$ can be equivalently defined
as an (ordinary) directed graph $(2^N, \Ar)$ whose set of nodes is the powerset of some set $N$. 
Just as hypergraphs and bipartite graphs are isomorphic,
    so too are directed hypergraphs and directed bipartite graphs:
\begin{align*}
    (N, \Ar) \mapsto \Big(N, \Ar, 
        \{ u{\to}a : a \in \Ar, u \in \Src a \} \cup
        \{ a{\to}v : a \in \Ar, v \in \Tgt a \}
         \Big) \\
    (L, R, A) \mapsto (L, \{ \Pa(r){\to}\Ch(r) : r \in R \}).
\end{align*}
% The analogue of that for (directed) graphs
%
%
% Interestingly, the concept of duality applies equally well to directed hypergraphs, and the definition is even more symmetric---although we are not aware of 
Interestingly, the corresponding concept of duality in directed hypergraphs is clearer than this isomorphism, and even more symmetric than its undirected counterpart.
Despite this, we are not aware of 
% any published work on subject.
% substantial work on the subject.
any thorough treatment of duality in directed hypergraphs. 
% \oldnewmaterial{%
% We mention it here because some representations of interest 
% }%

\begin{defn}
    The \emph{dual} of a directed hypergraph $\mathcal H = (N, \Ar)$ is 
    \(
        \check{\mathcal H} := (\mathcal A, N)
    \),
    where
    % \[
    $
    \check{\Src n} = \{ a \in \Ar : n \in \Tgt a\} \quad\text{and}\quad
    \check{\Tgt n} = \{ a \in \Ar : n \in \Src a\}.
    % \qedhere
    % \]
    $
\end{defn}

% It is instructive to check that
It is easy to verify that, as with undirected hypergraphs, the dual of the dual of a directed hypergraph is the original.%
% $\check{\check{\Ar}} = \Ar$.
% $\check{\check{\mathcal H}} = \mathcal H$.%
\footnote{
    $\check{\check{\Src a}}
    = \{ n \in N : a \in \check{\Tgt n} \} 
    = \{ n \in N : a \in \{ a' \in \Ar :  n \in \Src {a'} \} \} 
    = \{ n \in N : n \in \Src a\} 
    = \Src a$, and
    a symmetric argument etablishes that $\Tgt a = \check{\check{\Tgt a}}$.
}
%
\commentout{%
We now verify that $\check{\check{\mathcal H}} = \mathcal H$.
Observe that 
\begin{align*}
    \check{\check{\Src a}}
    &= \{ n \in N : a \in \check{\Tgt n} \} \\
    &= \{ n \in N : a \in \{ a' \in \Ar :  n \in \Src {a'} \} \} \\
    &= \{ n \in N : n \in \Src a\} 
    = \Src a.
\end{align*}
% \end{prop}
A symmetric argument etablishes that $\Tgt a = \check{\check{\Tgt a}}$. 
}%
% What does this look like?
% To visualize . 
% One might wonder why $\check\Src n$ is definde in terms of $\Tgt a$ instead of $\Src a$.
%
\begin{figure}%
    \centering
    \renewcommand\arraystretch{1.6}
    \setlength\tabcolsep{7pt}
    \begin{tabular}{cccc}
% \begin{tabular}{c}
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node[] (A) at (0,1) {A};
        \node[] (B) at (1,1) {B};
        \node[] (C) at (2,1) {C};
    \end{scope}
        \draw[arr1,<-] (A) to node[above]{\small 1} +(-0.9,0);
        \draw[arr1] (A) to node[above,pos=0.35]{\small 2} (B);
        \draw[arr1] (B) to node[above,pos=0.35]{\small 3} (C);
    \end{tikzpicture}
    &
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node (A) at (0,0) {A};
        \node (B) at (1,0.6) {B};
        \node (C) at (1,-0.6) {C};
        \node (D) at (2,0) {D};
    \end{scope}
        \draw[arr1,<-] (A) to node[above]{\small 1} +(-0.9,0);
        \draw[arr1] (A) to node[above,pos=0.35]{\small 2} (B);
        \draw[arr1] (A) to node[below,pos=0.35]{\small 3} (C);
        \mergearr[arr1] BCD
        \node[above=1pt of center-BCD]{\small 4};
    \end{tikzpicture}
    %
    &
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node (A) at (0,0) {A};
        \node (B) at (1.3,0) {B};
    \end{scope}
        \draw[arr1] (A) to[bend left] node[above]{\small 1} (B);
        \draw[arr1] (B) to[bend left] node[below]{\small 2} (A);
    \end{tikzpicture}
    &
    %
    \begin{tikzpicture}[xscale=1.2,center base]
    \begin{scope}[every node/.style={draw,rounded corners=5pt,outer sep=2pt}]
        \node[draw,outer sep=2pt] (A) at (0,0) {A};
        \node[draw,outer sep=2pt] (B) at (1,0) {B};
    \end{scope}
        \draw[arr1] (A) to node[above,pos=0.35]{\small 1} (B);
        \draw[arr1,<-] (B) to node[above]{\small 2} +(0.9,0);
        %
    \end{tikzpicture}
    \\[2ex]\\
    \begin{tikzpicture}[xscale=1.2,center base]
        \node[draw,outer sep=2pt] (1) at (-0.5,0) {1};
        \node[draw,outer sep=2pt] (2) at (0.5,0) {2};
        \node[draw,outer sep=2pt] (3) at (1.5,0) {3};
        \draw[->,arr1] (1) to node[below,pos=0.35]{\small A} (2);
        \draw[->,arr1] (2) to node[below,pos=0.35]{\small B} (3);
        \draw[arr1,] (3) to node[below,pos=0.35]{\small C} +(0.75,0);
    \end{tikzpicture}
    &
    \begin{tikzpicture}[xscale=1.2,center base]
    % \begin{scope}[shift={(-0.5,-2)}]
        \node[draw,outer sep=2pt] (1) at (0,0) {1};
        \node[draw,outer sep=2pt] (2) at (1,0.6) {2};
        \node[draw,outer sep=2pt] (3) at (1,-0.6) {3};
        \node[draw,outer sep=2pt] (4) at (2,0) {4};
        \unmergearr[arr1] 123
        \node[above left=1pt and 1pt of center-123]{\small A};
        \draw[arr1] (2) to node[above,pos=0.35]{\small B} (4);
        \draw[arr1] (3) to node[below,pos=0.35]{\small C} (4);
        \draw[arr1,] (4) to node[above]{\small D} +(0.9,0);
    % \end{scope}
    \end{tikzpicture}
    &
    \begin{tikzpicture}[xscale=1.2,center base]
        \node[draw,outer sep=2pt] (1) at (0.75, .65) {1};
        \node[draw,outer sep=2pt] (2) at (0.75,-.65) {2};
        \draw[arr1] (1) to[bend left] node[right]{\small B} (2);
        \draw[arr1] (2) to[bend left] node[left]{\small A} (1);
    \end{tikzpicture}
    &
    \begin{tikzpicture}[xscale=1.2,center base]
    % \begin{scope}[shift={(0.5,-1.5)}]
        \node[draw,outer sep=2pt] (1) at (0,0) {1};
        \node[draw,outer sep=2pt] (2) at (1,0) {2};
        \draw[arr1,<-] (1) to node[above]{\small A} +(-0.75,0);
        % \draw[arr1] ()
        \coordinate (a) at (0.5,-0.5);
        \mergearr[arr1] 12a
        \node at (0.5,0.2) {\small B};
    % \end{scope}
    \end{tikzpicture}
    \end{tabular}
    \caption[Examples of directed hypergraphs and their duals]
    {Examples of directed hypergraphs (one row) and their duals (the other row).}
        \label{fig:dhygraph-duals}
\end{figure}%
\commentout{%
% See \cref{fig:dhygraph-duals} for some visual illustrations of a few hypergraphs and their duals.
A few hypergraphs and their duals are depicted in \cref{fig:dhygraph-duals}. 
We remark that the left and center diagrams on the top can be viewed as (the hypergraphs corresponding to) qualitative Bayesian Networks, by regarding X,Y,Z and A,B,C,D as variables, and imagining that there is a (randomized) causal determination occuring along each arc. 
One can also imagine an analogue with cycles---resuling in perhaps a (randomized) causal model of the given shape.  But a causal model has one equation corresponding to each variable, and the corresponding hypergraphs thus has exactly one hyperarc leading to it. 
In the dual hypergraphs, one should view the nodes as processes and the arcs as wires. Such a hypergrah has precisely one hyperarc leading out of every node. When wires branch, one imagines a copy; when two arcs point to the same process (as in process 4, in the middle center), that process takes both of the wires as inputs.
In the duals of hypergraphs corresponding to causal models, there are no two-tailed arrows, which might be thought of as a ``merge''. 
Yet it is not clear how to merge the values of two variables, when they are not the same, in general---especially if we expect associativity and commutativity, as we do with \emph{copy}. 
}%
\newmaterial{%
The operation is worth visualizing. 
A few hypergraphs and their duals are depicted in \cref{fig:dhygraph-duals}. 
% If we view the hypergraphs on the top as representing processes, 
A hypergraph and its dual correspond to two different ways of notating a composite process: as a \emph{flowchart} or as a \emph{circuit} (see \citet{kasangian1990duality} for a discussion of this duality in a related context).
In a flowchart, nodes represent state and  arcs represent transformations between them.
In a circuit, nodes represent operations, and arcs, thought of as ``wires'' that connect them, carry state. 
So, if capital letters represent variables and carry state, while numbers represent (randomized) transitions between variables (i.e., cpds), 
then 
% the bottom row of 
the top row of \cref{fig:dhygraph-duals} is the flowchart representation and the bottom row is the circuit representation. 

In either representation, the two hypergraphs on the left of \cref{fig:dhygraph-duals} represent unambiguous composite processes.
The far left one is a chain: operation 1 (op$_1$) produes X, which is fed to op$_2$ which produces Y, and finally fed to op$_3$ to produce Z.
The second hypergraph from the left involves a split and a merge: op$_1$ produces A, which is copied and sent to op$_2$ and op_$_3$, which use it to produce B and C, repectively. Then, op$_4$ uses the values of B and C to produce D.
% The right one, on the other hand, is 
% Is the right one meaningful?  We 
The middle right hypergraph represents a cyclic process, in which op$_1$ produces B from A and op$_2$ produces A from B. 
% The right one, on the other hand, is less clear. 
In each of these three cases, every variable is produced by a single operation (on the top, one hyperarc goes into every node; on the bottom, one hyperarc comes out of every node). 
But what does the hypergraph on the far right represent? 
Op 1 produces B from A, but op$_2$ somehow also produces B. 
Is this meaningful?
% As we will see in \cref{chap:pdg-repr}
Previous approaches to modeling processes rule out situations like these, but in \cref{chap:pdg-repr}, we will use them to capture (possibly) conflicted beliefs.  
% \TODO

% Both kinds of diagram are used in AI. 
For representing processes like the ones on the left,
both circuit-style and flowchart-style diagrams are common in in computer science. 
% The neural network ``architecture diagrams'', are the most common way to visually communicate the architecture of
Focusing specifically on machine learning, architecture diagrams%
    \footnote{as produced by this library, for example \url{https://github.com/HarisIqbal88/PlotNeuralNet}} 
are circuit representations that have become the standard way to communicate the architecture of a neural network.
% In that context, the  PDG represents 
%
% The duals of these diagrams---the flowchart variants---have been studied at greater length, and are the subject of the next section. 
% Meanwhile, (some of) the flowchart variants are closely related to a standard representation called a Bayesian Network, which is one of two focuses of the next section. 
Meanwhile, the duals of these hypergrahs (i.e., flowchart variants) are closely related to a standard representation called a \emph{Bayesian Network}, which is a primary focus of the next section. 
}%

% As we will see th  
% If we view $X$,$Y$, and $Z$ as variables, 

% If the top row of hypergraphs represents 
% Imagine top row represents processes by having

% \TODO[ TODO: clean up previous paragraph; add network diagram ]
%
% \TODO[((What can be done with these objects? \\TODO: tie into the thesis elswehere or remove.))]
% \TODO[
%     Possible to have other identity morphisms? Require that, for each $X \in \N$, the identity morphism $\dg M(\ed {\mathrm{id}}XX)$ preserves the base measure $\lambda_X$, in addition to pre/post committing 
% ]

% \begin{fact}
%     All morphisms 
% \end{fact}

% \subsection{Independencies}


% \section{A Review of Graphical Models}
\section{Probabilistic Graphical Models}
    \label{sec:prelim-pgms}

% Broadly speaking, probabilistic graphical levels 
Like variables themselves, graphical models exist on two levels: a qualitative or structural level, and a quantitative or observational one.
%
A \emph{graphical model structure}, or a \emph{qualitative graphical model} is a (directed) (hyper)graph whose vertices $\X$ are variables, and whose (hyper)edges somehow indicate local influences between variables.
The quantitative form then annotates those concepts with data.
A \emph{probabilistic graphical model},
or simply  ``graphical model'', is a
graphical model structure together with  \emph{quantitative} or probabilistic information about these local influences.
%
The purpose of a graphical model, at least historically, is to be a compact representation of a probability distribution (by exploiding independencies). 
Thus, a graphical model $\cal M$
typically represents a joint probability distribution $\Pr_{\!\cal M}
 \in \Delta \V\!\X$ over the values its variables $\X$.
% \discard{
%     Although there is often more to the story,
%     it can typically be
%     expressed as a product
%     $\Pr_{\!\cal M}(\X) \propto \prod_{E \in \Ed} \phi_{E}(E)$
%     of factors $\boldsymbol   \phi = 
%     \{ \phi_E : \V E \to \mathbb R_{\ge 0} \}_{E \in \Ed}$
%     over a hypergraph $(\X, \Ed)$ closely related to the structure of $\cal M$.
%     For this reason, some authors use the term ``graphical model'' to refer to a tuple $(\X ,\Ed, \boldsymbol\phi)$,
%     i.e., a factor graph.
%     PDGs, however, do not represent probabilities this way.}

We now review the basics of the most important classes of graphical models used in practice:
Bayesian Networks, which are based on directed (acyclic) graphs, and
what we will call factor graphs, which are based on undirected (hyper)graphs. 
What follows is really a barebones sketch of the graphical models context for the ideas we present in this dissertation; for a proper treatment, we refer readers to 
    \citet{KF09}.
% These two models 
    
\subsection{Bayesian Networks and Variants}
    \label{sec:prelim-bns}

A \emph{qualitative Bayesian Network} (qualitative BN) is an acyclic directed graph $G = (\X, A)$ over a set of variables $\X$. 
An arc $X {\to} Y \in A$ intuitiely reflects a possibility that $X$ can influence $Y$. 
% The absence of an arc, then, reflects a kind of (conditional) independence. 
\newmaterial{%
    Intuitively, each variable should be thought of as being associated with a process
    that produces it in a way that depends jointly on all of its parents. 
}
The absence of an arc, then, in context, reflects a kind of independence. 
% One standard way to say this is that
% Here is one standard description of these independencies: 
% Here is n of these independencies: 
More precisely, a qualitative BN states that
\textit{every variable is conditionally independent of its non-descendents given its parents}.
Another famous (and more algorithmic) characterization of the same conditional independences is the \emph{d-separation} criterion  \citep{geiger-pearl-d-separation}. 
It is helpful to look at some examples.

\begin{center}
    % \tikzset{bnnode/.style={draw,circle}}
% \begin{tabular}{c}
    \def\XYZnodes{%
        % \node[bnnode] (X) at (-1,0) {$X$};
        % \node[bnnode] (Y) at (0,0) {$Y$};
        % \node[bnnode] (Z) at (1,0) {$Z$};
        \node[bnnode] (X) at (0,-1) {$X$};
        \node[bnnode] (Y) at (0,0) {$Y$};
        \node[bnnode] (Z) at (0,1) {$Z$};
    }
    \begin{tikzpicture}[scale=1.4,center base]
        \XYZnodes
        \draw[arr2] (X) to (Y);
        \draw[arr2] (Y) to (Z);
    \end{tikzpicture}
    ~~~
    % \\
    \begin{tikzpicture}[scale=1.4,center base]
        \XYZnodes
        \draw[arr2] (Z) to (Y);
        \draw[arr2] (Y) to (X);
    \end{tikzpicture}
    ~~~
    % \\
    \begin{tikzpicture}[scale=1.4,center base]
        \XYZnodes
        \draw[arr2] (Y) to (X);
        \draw[arr2] (Y) to (Z);
    \end{tikzpicture}
    % ;~~~
% \end{tabular}
    \hspace{2cm}
    \begin{tikzpicture}[scale=1.4,center base]
        \XYZnodes
        \draw[arr2] (X) to (Y);
        \draw[arr2] (Z) to (Y);
    \end{tikzpicture}
\end{center}

% The first, second, and third, all give the same independencies:
The three qualitative BNs on the left all encode the same independence information:
$X$ and $Z$ are conditionally independent given $Y$.
The last one, however, is different: it states that $X$ and $Y$ are independent (but $Z$ can depend arbitrarily on both $X$ and $Y$).
\newmaterial{%
    Note also that, while the left three qualitative BNs all are 
    (almost) flowchart representations, the one on the right is not;
        the two arrows do not represent two different processes, but 
        rather a single joint dependence. 
}%
This intuition can be further clarified once we add probabilities.

% A quantitative BN, or simply a BN, 
A \emph{(quantitative) BN}
    $\mathcal B = (\X, A, \mathbb P)$ 
    is a qualitative BN $(\X, A)$ together with a collection 
    of cpds $\mathbb P = \{ P_X( X \mid \Pa(X)) \}_{X \in \Pa(X)}$,
    one for each variable given its parents. 
A BN then induces a joint probability distribution 
\begin{equation}
    \Pr\nolimits_{\cal B}(\X) := \prod_{X \in \X} P_X( X \mid \Pa(X) ),
        \label{eq:bn-product}
\end{equation}
which indeed satisfies the independence properties specified its underlying qualitaitve BN. 
Furthermore, $\Pr_{\cal B}$ has every conditional marginal specified by $\mathbb P$. 
Indeed, the factorization \eqref{eq:bn-product} is yet another euquivalent characterization of the conditional independencies of this graph.  
% because the underlying graph is directed, this formula can be viewed as
% the chain rule of proability
% $\Pr(X_1, \ldots, X_n) = \prod_{i=1}^n \Pr(X_i \mid X_1, \ldots, X_{i-1})$, 
This quantitative view \eqref{eq:bn-product} is the primary way that the broader modern machine learning community views BNs.
It also suggests a way of generalizing BNs to handle cycles and more: simply use the same equation.
However, the resulting object will typically not be a probability distribution: its sum over all values of $\X$ may not equal 1. 
But, in principle, it could be normalized;
    this approach leads to the second major class of graphical models.  



\subsection{Markov Random Fields, Factor Graphs, and Variants}
    \label{sec:prelim-fgs}

Now, suppose $G = (\X, E)$ is an undirected graph over a set of variables.
By interpreting an edge $X{-}Y \in E$ as a possibility of (bi-directional) influence between $X$ and $Y$
% there are three different (but closely related), it also becomes a qualitative graphical model.
A distribution $\mu$ is said to be a \emph{Markov Random Field} for $G$ if it satisfies one of the following properties:

\begin{description}[nosep]
    \item [(pairwise)] 
        If $X,Y \in \X$ with $X \ne Y$ and $\{X, Y\} \notin E$, then 
        $
        \mu \models
            X \CI Y \mid \X \setminus \{X, Y\}$;
    \item [(local)] 
        For each $X \in \X$, we have $
            \mu \models
        X \CI \X - \{ X \} \mid \partial_G(X)$,
        where
        $\partial_G(X) := \{ Y \in \X : \{Y,X\} \in E \}$ is the set of variables neighboring $X$;
    \item [(global)]
        For all $\mat X, \mat Y, \mat Z \subseteq \X$,
        if every path from some $X \in \mat X$ to some $Y \in \mat Y$ 
            goes through some $Z \in \mat Z$,
        then $
        \mu \models 
        \mat X \CI \mat Y \mid \mat Z
        $;
    \item [(factorization)] 
        There exists a collection of functions 
        $\{ \phi_{\mat X} : \V \mat X \to \mathbb R_{\ge 0} \}_{\mat X \in \mathrm{cliques}(G)}$
        indexed by the cliques of the graph $G$,
        whose product is the distribution $\mu$.
        That is,
        $
            \displaystyle
            \mu(\X) = \prod_{\mat X \in \mathrm{cliques}(G)} \phi_K(\mat X).
        $
\end{description}

In general, the conditions are distinct, with (global) $\implies$ (local) $\implies$ (pairwise). 
However, if we restrict to \emph{positive} distributions (i.e., $\forall \omega \in \V\!\X.~\mu(\omega) > 0 $), then all four conditions are equivalent.
% The connection to the factorization property is a very deep result,
This result is
known as the \emph{Clifford-Hammersly Theorem}, or the \emph{fundemental theorem of graphical models}. 

If we care only about such distributions, this suggests another approach to the same idea: throw away the graph $G$, multiply together collection of functions $\{ \phi_{j} : \V \mat X_{j} \to \mathbb R_{\ge 0} \}_{j \in J}$ indexed by some arbitrary set $J$, and then renormalize;
    the subset $\mat X_j \subseteq \X$ is called the \emph{scope} of factor $j$. 
%
\newmaterial{%
Although the data we have described is an annotation of a hypergraph, it is easier to draw as a bipartite graph.}

The bipartite graph that corresponds naturally to this hypergraph is called a \emph{factor graph}. 
More precisely, a factor graph has two kinds of nodes: factors (drawn as squares), and variables (circles), with an edge between them if the factor depends on the variable.
\newmaterial{%
Some authors reserve the term ``factor graph'' just for the bipartite graph representation, but we will also refer to the hypergraph and collection of factors this way, as they are essentially the same object. 
}%
We will see more details in \cref{sec:factor-graphs}.


\subsection{Other Graphical Models}
    \label{sec:prelim-other-pgms}

Many other graphical models have been proposed as well, although
the vast majority are variants on the two we have already described. 
We now give a quick overview of some of these representations, although this list is far from complete. 
\newmaterial{%
These models make only minor appearances in the text, but knowing about them may nontheless be helpful for situating PDGs
    % (the subject of \cref{chap:pdg-repr})
    in the literature. 
}%
% There are other ways of describing 

The most important for our purposes, will be \emph{conditional} Bayesian Networks and \emph{causal} Bayesian Networks, both of which are called CBNs, and, fortuantely, both of which make the same modification: marking some variables as ``observed'' so that they do not require probability distributions. 
Writing $\U$ for the observed variables and $\V$ for the unobserved ones, a CBN $\mathcal B$ represents a cpd $\Pr_{\mathcal B}(\mathcal V \mid \mathcal U)$. 
%
\emph{Conditional random fields (CRFs)} are an analogue for factor graphs/MRFs.


Graphs with both directed and undirected edges have also been used as the basis for probabilistic models.
The most widely known graphical model of this kind is called a \emph{chain graph}, in part because it can be seen as a way of connecting conneted components (themselves factor graphs) along a directed acylcic chain.
People have also considered other ways of making precise the notion of a cyclic baysian network (aside from multiply together factors for a cyclic network), including unrolling it and looking at fixed points---such as dyanmic Bayesian Networks \citep{dean1989model}\citep[\S6.2]{KF09}, and a related cut-parameterized approaches to looking at fixed points \citep{Baier_2022}.
\oldnewmaterial{%
    \emph{Directed factor graphs} \citep{frey2012extending}
    combine the unify the qualitative expressiveness of directed graphical models with the (undirected) factor graph formalism,
    but are quantitatively no different from ordinary factor graphs.
}

% A related notion is that
A \emph{Dependency Network (DN)} is another
    (much less popular) kind of directed graphical model.
Syntactically it is like a Bayesian Network, but semantically, it is closer to an undirected graphical model. 
It is typically cyclic, and defines a probability distribuiton as a fixed point of a Markov Chain. 
% DNs can also be inconsistent in a sense, 
``Consistent'' DNs turn out to be an alternate way of parameterizing undirected graphical models.
There is also a notion of an \emph{inconsistent} DN, which turns out to be a very special case of a representation developed in this thesis (\cref{chap:relent-soup}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{A Primer on Information Theory}
\section{Information Theory}
    \label{sec:prelim-infotheory}

% Perhaps surprisingly
Information theory is the study of when computational tasks
    (such as communicating in a noisy environment, 
        or )
    are possible, and when they are not. 
%
The field is vast and heterogenous, 
    yet much of it is built using a small set of 
    particularly nice ways of quantifying (un)certainty in a probabilistic
    context, which we will refer to throughout as \emph{information-theoretic primatives}.
These quantities are all based on the concept of log probability, or \emph{surprisal} $\I_{\mu}[ X{=}x]$, which is
\begin{itemize}[nosep]
\item equal to zero if the event $X{=}x$ was certain to happen (i.e., totally unsurprising) but otherwise strictly positive, and
\item combines additively over independent events, just like most quantities in science and everyday life (meters, seconds, etc.).
\end{itemize}


Although not typically thought of in this way, 
    the basic information-theoretic quantities come in two flavors, mirroring the divide between qualitative and quantitative information that we saw in the presentation of graphical models. 
% We start by recounting some standard results and notation,
With the exception of these editorial comments on the theme of qualitative vs quantitative concepts, all of this material can be found in any introduction to information
theory (e.g., \citet[Chapter 1]{mackay2003information} or \citet{CoverThomas}).  

\subsection{Shannon Entropy and the Information Profile}
    \label{sec:qual-info-theory-primer}

Let $\mu$ be a probability distribution, and be $X,Y,Z$
    be (sets of) discrete (random) variables.
The \emph{information content} or \emph{surprisal} $\I_{\mu}[ X{=}x ] := \log \frac1{\mu(X{=}x)}$ of an event $X{=}x$ quantifies one's surprise to learn that $X{=}x$, knowing that $X \sim \mu(X)$.
    
The \emph{entropy} of $X$ is the uncertainty in $X$, when it is distributed according to $\mu$, as measured by the number of bits of information needed (in expectation) needed to determine it, if the distribution $\mu$ is known.  It is given by 
\[
    \H_\mu(X) := \sum_{x \in \V(X)} \mu(X{=}x) \log \frac{1}{\mu(X{=}x)} \qquad= -\Ex_{\mu}[\log \mu(X)],
\]
and has several very important properties. 
Chief among them, $\H_\mu(X)$ is non-negative, and equal to zero iff $X$ 
    is a constant according to $\mu$. 

The ``joint entropy'' $\H(X,Y)$ is just the entropy of the combined variable $(X,Y)$ whose values are pairs $(x,y)$ for $x \in \V\!X,y \in \V Y$; this is the same as the entropy of the variable $X \cup Y$ when $X$ and $Y$ are themselves sets of variables. 
    % takes as input not the value of a variable

The \emph{conditional entropy} of $Y$ given $X$
measures the uncertainty present in $Y$ if one knows the value of $X$
(think: the information in $Y$ but not $X$),
and is equivalently defined as any of the following three quantities:
\[
\H_\mu( Y | X) :=
        % \quad
    ~~
    \Ex_{\mu} [~\log \nicefrac1{\mu(Y | X)}~]
        % \quad
    ~~=~~\H_\mu(X,Y) - \H_\mu(X)
        % \quad
    ~~=\Ex_{x \sim \mu(X)} [~\H_{\mu \mid X{=}x}(Y)~]    
.
\]
The fact that these three quantities coincide is a remarkable fact,
% and one that makes entropy particularly special. 
% and just one of many that make entropy paticularly special.
and one reason that entropy is particularly special. 

\begin{fact}[Entropy Chain Rule]\label{fact:entropy-chain-rule}
    If $X$ and $Y$ are random variables, then the entropy of the joint
   variable $(X,Y)$ can be written as $\H_\mu(X,Y) = 
\H_\mu( Y \mid X) + \H_\mu(X)$.
It follows that if $\mu$ is a
       distribution over the $n$ variables $X_1, \ldots, X_n$,  then
	\[ \H(\mu) = \sum_{i = 1}^n \H_\mu(X_i \mid X_1, \ldots X_{i-1}). \]
\end{fact}
% \begin{defn}[Conditional Mutual Information]\label{defn:cmi}
%    The \emph{conditional mutual information} between two (sets of) random
%     variables is defined as  
%     \[ \I_\mu(X ; Y \mid Z) := \sum_{x,y,z \in \V(X,Y,Z)} \mu(x,y,z)
%         \log\frac{\mu(z) \mu(x,y,z)}{\mu(x,z)\mu(y,z)}. \] 
% \end{defn}

The \emph{mutual information} $\I(X;Y)$,
and its conditional variant $\I(X;Y|Z)$, 
are given, respectively, by
\[
    \I_\mu(X;Y) :=
        \Ex_{\mu} \Big[ \log \frac{\mu(X,Y)}{\mu(X) \mu(Y)}\Big],
    \quad\text{and}\quad
    \I(X;Y|Z):= 
        \Ex_{\mu} \Big[ \log \frac{\mu(X,Y,Z)\mu(Z)}{\mu(X,Z) \mu(Y,Z)}\Big].
\]
Both quantities are non-negative.
The former is equal to zero iff $\mu \models X \CI Y$, and the latter is equal to zero iff $\mu \models X \CI Y \mid Z$;
thus, they measure distance to independence. 


Just as conditional entropy can be written as a linear combination of unconditional entropies, so too can conditional mutual information be written as a linear combination of unconditional mutual informations: $\I(X;Y|Z) = \I(X;(Y,Z)) - \I(X;Z)$.  
Thus conditional quantities are easily derived from the unconditional ones. But at the same time, the unconditional versions are clearly special cases of the conditional ones; for example, $\H_\mu(X)$ is clearly the special case of $\H(X|Z)$ when $Z$ is a constant (e.g., $Z = \emptyset$). 
Furthermore, entropy and mutual information are also interdefinable and generated by linear combinations of one another. 
It is easy to verify that 
\begin{align*}
    \I_\mu(X;Y) 
    % = \H_\mu(X,Y) - \H(Y|X) - \H(X|Y)
    &= \H_\mu(X) + \H_\mu(Y) - \H_\mu(X,Y)
    \\&=  \H_\mu(X)  - \H_\mu(X|Y)
% $ and $
\\ \qquad\text{ and }\qquad
    \I_\mu(X;Y|Z)
    &= \H_\mu(X|Z) + \H_\mu(Y|Z) - \H_\mu(X,Y|Z)
    \\&= \H_\mu(X \mid Y) - \H_\mu(X \mid Y, Z);
\end{align*}
thus mutual information is derived from entropy. 
Yet on the other hand, $\I_\mu(Y;Y) = \H_\mu(Y)$ and $\I_\mu(Y;Y|X) = \H_\mu(Y|X)$, at least in the discrete case---thus entropy is also a special case of mutual information. 

% Sums and differences of any of these quantities
\paragraph{Information Diagrams and the Information Profile.}
Information theoretic quantities such as (conditional) entropy and (conditional) mutual information for different subsets of a set of variables $\X$ fit together with an inclusion-exclusion rule. 
% They are connected by an inclusion exclusion 
There leads to an easy graphical way to quickly read of identities such as the ones above from an \emph{information diagram}, 
% like the shown below for three variables. 
as depicted in \cref{fig:info-diag-prelim}.

% \begin{center}
\begin{figure}
    \centering
    \def\circsize{1.6}
    \def\radsize{0.8}
    % \edef\outerrad{{\circsize/2+\radsize}}
    \edef\outerrad{1.6}
    \def\innerrad{1.00}
    % \def\twosize{0.5}
    \begin{tikzpicture}[center base,scale=1.2]
        % \node at ({-\circsize-\radsize+0.1},{\circsize+\radsize-0.2}) {\underline{\textbf{(a)}}};

        \path[fill=white!90!black] (210:\radsize) circle (\circsize) ++(225:\circsize) node[label={[label distance=2pt,inner sep=0pt]left:\Large $X$}]{};
        \path[fill=white!90!black] (90:\radsize) circle (\circsize)
            % ++(125:\circsize) node[label={left:$Y$}]{};
            ++(65:\circsize) node[label={[label distance=1ex]right:\Large $Y$}]{};
        \path[fill=white!90!black] (-30:\radsize) circle (\circsize) ++(-45:\circsize) node[label={[label distance=1pt,inner sep=0pt]right:\Large$Z$\!}]{};


        \begin{scope}[gray]
            \draw[] (-30:\radsize) circle (\circsize);
            \draw[] (210:\radsize) circle (\circsize);
            \draw[] (90:\radsize) circle (\circsize);
        \end{scope}
        
        %oli9: replacing this mutual info circle with one more crescent.
        \begin{scope} % I(B;C)
            \clip (-30:\radsize) circle ({\circsize+0.03});
            \clip (90:\radsize) circle ({\circsize+0.03});
            \draw[ultra thick,blue,fill=blue,fill opacity=0.3] (-30:\radsize) circle (\circsize);
            \draw[ultra thick,blue] (90:\radsize) circle (\circsize);
        \end{scope}
        % \begin{pgfinterruptboundingbox} % H(Z|X)
        %     \begin{scope}[even odd rule]
        %         \clip (210:\radsize) circle ({\circsize-0.03}) (-3,-3) rectangle (3,3);
        %         \clip (-30:\radsize) circle ({\circsize+0.03});
        %         \draw[ultra thick,blue,fill=blue, fill opacity=0.3] (-30:\radsize) circle (\circsize);
        %         \draw[ultra thick,blue] (210:\radsize) circle (\circsize);
        %     \end{scope}
        % \end{pgfinterruptboundingbox}
        \begin{pgfinterruptboundingbox} % H(X|Y)
            \begin{scope}[even odd rule]
                \clip (90:\radsize) circle ({\circsize-0.03}) (-3,-3) rectangle (3,3);
                \clip (210:\radsize) circle ({\circsize+0.03});
                \draw[ultra thick,purple,fill=purple, fill opacity=0.3] (210:\radsize) circle (\circsize);
                \draw[ultra thick,purple] (90:\radsize) circle (\circsize);
            \end{scope}
        \end{pgfinterruptboundingbox}
        \begin{pgfinterruptboundingbox} % H(Y|Z)
            \begin{scope}[even odd rule]
                \clip (-30:\radsize) circle ({\circsize-0.03}) (-3,-3) rectangle (3,3);
                \clip (90:\radsize) circle ({\circsize+0.03});
                \draw[ultra thick,violet!50!black,fill=violet, fill opacity=0.2] (90:\radsize) circle (\circsize);
                \draw[ultra thick,violet!50!black] (-30:\radsize) circle (\circsize);
            \end{scope}
        \end{pgfinterruptboundingbox}
        \node at (-90:\innerrad) {\small$\I(X;Z|Y)$};
        \node[rotate=-60] at (30:\innerrad) {\small$\I(Y;Z|X)$};
        \node[rotate=60] at (150:\innerrad) {\small$\I(X;Y|Z)$};
        \node at (0:0){\small$\I(X;Y;Z)$};
        \node at (90:\outerrad) {\small$\H(Y|X,Z)$};
        \node[rotate=60] at (-30:\outerrad) {\small$\H(Z|Y,X)$};
        \node[rotate=-60] at (-150:\outerrad) {\small$\H(X|Y,Z)$};
        
        %oli9:
        % \node[purple!50!black,text opacity=1,fill=purple!15!white,fill opacity=1,
        %         % rotate=0] (HAB) at (164:2.7) 
        %         rotate=45] (HAB) at (164:2.3) 
        %         % rotate=0] (HAB) at (164:2.3) 
        %     {\small$\H(X|Y)$};
        \node[purple!50!black,text opacity=1,fill=purple!15!white,fill opacity=1,
                rotate=0] (HAB) at (-110:2.0) 
            {\small$\H(X|Y)$};
        % \node[violet!20!black,text opacity=1,fill=violet!15!white,fill opacity=1,
        %         % rotate=0] (HBC) at (148:2.60) 
        %         rotate=45] (HBC) at (125:2.40) 
        %         % rotate=0] (HBC) at (125:2.40) 
        %     {\small$\H(Y|Z)$};
        \node[violet!20!black,text opacity=1,fill=violet!15!white,fill opacity=1,
                rotate=0] (HBC) at (145:2.10) 
            {\small$\H(Y|Z)$};
        \node[blue!50!black,text opacity=1,fill=blue!15!white,fill opacity=1,
            % rotate=0] (IBC) at (28:2.55)
            % rotate=-50] (IBC) at (32:2.05)
            rotate=0] (IBC) at (32:2.05)
            {\small$\I(Y;Z)$};
        % \node[blue!50!black,text opacity=1,fill=blue!15!white,fill opacity=1,
        %     rotate=0] (IBC) at (15:2.1)
        %     {\small$\H(Z|X)$};
        % \draw[purple,ultra thick] (
    \end{tikzpicture}
    \caption[Illustration of the information profile $\mat I_\mu$]{%
        The components of the information profile $\mat I_\mu$
        for three variables $\X = \{X,Y,Z\}$. }
    \label{fig:info-diag-prelim}
% \end{center}
\end{figure}

\paragraph{Entropy for Continuous Variables.}
    \label{sec:prelim-gen-ent}
So far, everything we have said relies on intuitions that the variables are discrete.
% It is not hard to see that trying to take a limit 
\newmaterial{%
If we continous variables by discretizing them, it is not hard
    to see that the entropy as defined goes to $\infty$ as the discritization becomes fine.
Indeed, this is appropriate, because there is a sense in which a real number sampled from $[0,1]$ contains infinite information.
However, it is difficult to work with divergent limits in this way;
    instead, we define a much more useful quantity.
}%

% so that $\mu(X{=}x)$ = $\mu(x)$. 
In the general case, we need only replace instances of $\mu(X)$ used as a random variable with its derivative $\frac{\mathrm d\mu}{\mathrm d \lambda}$ with respect to $X$'s base measure $\lambda_X$.
So, for instance, the entropy of a variable $X$ in the general case is
\begin{equation}
    \H_\mu(X) 
    = \Ex_{\mu} \left[
        \log \frac{\mathrm d\lambda_X}{\mathrm d \mu(X)} \right]
    = \int_{\V\!\X} \log \frac{\mathrm d\lambda_X}{\mathrm d \mu(X)} \mathrm d \mu
    .
    \label{eq:gen-entropy}
\end{equation}
\newmaterial{%
In the finite case, this is just what we had before. 
However, it is important to keep in mind that this quantity can be negative in general (because, outside of the finite case, is possible that $\frac{\mathrm d \mu(X)}{\mathrm d \lambda_X}(x) > 1$). 
The reader should also be aware that, for real-valued variables, $\H(X) = 0$ no longer means that $X$ is a constant, but rather that $X$ has expected unit density (like the uniform distirbution over $[0,1]$). 
% Intuitively, the scale has been shifted by $$
Aside from these unwelcome developments, this analogue of entropy acts otherwise just like the original, but shifted by an additive constant to keep it in a more useful range.
It still satisfies all of the other properties (such as the chain rule), has the same minima and gradients as the limit of the discrete entropy at infinitely fine discretization.  
Fortunately, we do not have to lower our standards for 
    (conditional) mutual information, which works just as before:
    it is non-negative and zero iff there is a (conditional) independence. 
}
% Although not possible in the finite case, it is worth noting that this quantity can be negative in general (because it is possible that $\frac{\mathrm d \mu(X)}{\mathrm d \lambda_X}(x) > 1$), 


% More generally, when $X$ is a variable, we assume that $\V\!X$
%     not only comes equipped with $\sigma$-algebra $\mathcal F_X$, but also with a base measure $\lambda_X$.
% When we write $\mu(x)$, we mean the density $\frac{\mathrm d\mu}{d \lambda}$. 


\paragraph{The Principle of Maximum Entropy.}
    \label{sec:prelim-maxent}
%
% What should one do
Suppose that, with rigorous emperical investigation, we have determined some statistics about a variable $X$---perhaps its mean, and the proportion of the time it takes on a certain value---but have not (yet) collected enough information to uniquely determine the distribution of $X$. 
How should we proceed?
The most faithful representation of our knowledge, of course, is just the information we have---those statistics, and perhaps the set of probability distributions consistent with them. 
But what should we do if we are obligated to give a probability distribution 
    (e.g., to satisfy the axioms of rationality \citep{sep-dutch-book,halpern-RAU})?
% Here there is an important principle in the literature: 
% If forced to select a probability distribution, however, there is one principle
There is a well-established principle stating that among all possible distributions, the one with maximum entropy is most appropriate. 
%
% Here is a high-level reason for this: 
At a high level, this is because 
    entropy measures uncertainty in a distribution, and we do not want to assume things that we do not know.
% Since entropy measures uncertainty,
% \begin{quotation}
%     ``The maximum-entropy distribution may be asserted for the positive reason that it is uniquely determined as the one which is maximally noncomittal with regard to missing information, instead of the negative one that there is no reason to think otherwise''
% \end{quotation}
\begin{quotation}
    \it\singlespacingplus
    ``This is the only unbiased assignment we can make; to use any other would amount to arbirary assumption of informaiton which by hypothesis we do not have.'' 
        \citep[p.623]{jaynes1957information}
\end{quotation}
Imagine, for example, that our friend is undergoing a secret but important evaluation, 
and we know nothing about her score except that it will be a number between 1 and 5.
It would be inappropriate to assume that she will scored a 5 (or a 1) simply because it's consistent with what we know, because that does not adequately represent our ignorance. 
The principle of maximum entropy implies that we should assume a uniform distribution over scores until we learn more.  
% If we discover that the average score is a 4, then the maximum entropy distribution 

\newmaterial{%
% There are many other theoretical arguments in favor of the principle as well \citep{wallis}.
% Despite its successes, 
The principle of maximum entropy has also faced significant criticism, from a number of different angles \citep{seidenfeld1986entropy,critique-maxent,friedman1971jaynes}.
An recurring theme in these works is the dependence on representation.
For example, had we said only 
``our friend either will or will not score a 5'', 
then maximum entropy would have suggested that we should presume probability $\frac12$ that she scores a 5, which is incompatible with the probability $\frac15$ that we were supposed to presume before. 
%
Without wading too far into this debate, we point out that it may be problematic to compare our intuition, which has been primed with the context of all five possible scores, with a picture that has willfully ignored that extra context. 
Whether or not it is appropriate as a general rule, the principle of maximum entropy does not apply locally; if we expect it to work at all, we must represent the entirety of the relevant information. 
% If maximum entropy is to work at all, then it 
% Still, 
}


% Many years later, another strong argument can be made 
Many decades later, there is an empirical case in favor of the principle of maximum entropy: it validates many standard and intuitively natural choices made in the literature.
% The uniform distribution over a finite set of outcomes is the distribution with maximum entropy.
The uniform distribution over an interval $[a,b]$ is the maximum entropy distribution with that support. 
A gaussian with mean $m$ and variance $\sigma^2$ is the maximum entropy distribution with that mean and variance. 
Among distributions supported on $[0,\infty]$ with mean $\lambda$, the exponential distribution $\mu(X) = \lambda e^{-\lambda X}$ is the one that maximizes entropy. 
% Logitstic regression is the maximum entropy classifier consistent with \citep{}
Logitstic regression 
    % (i.e., a softmax layer)
    outputs the maximum entropy distribution over class labels,
    among those that share certain means with the training data.
More generally, \emph{exponential families}, important classes of models that subsume the examples above as well as undirected graphical models, also maximize entropy subject to an expectation matching constraint. 

But what about directed graphical models? Bayesian Networks, so far, have resisted such a clean characterization in terms of maximum entropy; maximum entropy approaches seem to require some sort of deference to a causal structure \citep{williamson2001foundations}. 
\newmaterial{%
A related concern with maximum entropy in the face of conditional information is addressed in \citep{Grove1997ProbabilityUC}.
}%
One contribution of this thesis is to expand the principle of maximum entropy to account for causal information and conditional constraints (\cref{sec:bn-maxent}). 
%
% It may be worth noting that these two arguments for maximum entropy---the high-level interpretation of the quantity as measuring the right thing, and the sociological argument to unity---directly parallel the two kinds of arguments we give for the semantics of PDGs, in \cref{chap:pdg-repr,chap:one-true-loss}, respectively.


\newmaterial{%
\paragraph{Entropy as a Structural Information Measure.} 
Throughout this thesis, we will argue that entropy and its ilk are purely ``qulitative'' or ``structural'' aspects of a probability distribution (although perhaps the latter term is clearer in this context, because we are still talking about real numbers, i.e., ``quantities''). 
% One reason for this is that we don't need to know any of the quantitative information to be able to talk about and use entropy intelligently. 
Here is one important reason why: 
    we do not need to know anything about $\V\!X$ in order make full use of these quantities.
Just like the qualitative notion of independence $X \CI Y$, the mutual information $\I(X;Y)$ is fully defined and makes complete sense without knowing what values the variables can take on.
Even the notation reflects this. 
Recall that, at least in the discrete case, $\mu(X), \mu(X,Y)$ and $\mu(Y|X)$ can all be viewed as random variables, taking as input values of $X$ and $Y$. 
% Unlike their counterparts, 
$\H_\mu(X)$, $\H_\mu(X,Y)$, and $\H_\mu(Y|X)$, however, 
    are not functions of the values of $X$ and $Y$, but rather functions of (the identities of) the variables themselves. 
Indeed, it makes no sense to feed values $(x,y) \in \V(X,Y)$ to these concepts.
In other words, the operate on the qualitative aspect of a variable, not its quantitative aspect. 

A second reason to view entropy as structural appears in its general definition \eqref{eq:gen-entropy}.
There, the derivative is taken with respect to structural information about the variable: the base measure, which describes only agreed-upon coordinates, not observational data about what is likely. 
But if we replace that base measure with a \emph{probability measure} of our choosing, we get a closely related (but quite different) kind of entropy that is appropriate for working with observational information. 
}


\subsection{Relative Entropy}
    \label{sec:relent-primer}

% We now move on to the quantitative 
We now move on to a second kind of information-theoretic primitve, of a more quantitative flavor.
If $\mu, \nu \in \Delta \Omega$, then the \emph{relative entroy} of $\nu$ with respect to $\mu$ is given by:
%
\begin{align*}
    \kldiv {\mu} {\nu} 
    &:= \sum_{ \omega \in \Omega } \mu(\omega) \log \frac{ \mu(\omega) }{ \nu(\omega) }
    % \\&
    = \Ex_{\omega \sim \mu} \Big[ \log \frac{\mathrm d \mu}{\mathrm d \nu}(\omega) \Big].
\end{align*}

One critical property of relative entorpy, known as \emph{Gibbs' inequality}, is that
$\kldiv{\mu}{\nu} \ge 0$,
%  and $\kldiv{\mu}{\nu} = 0$ 
with equality if and only if $\mu = \nu$ (with probability 1). 
This is why $\thickD$ is called a ``divergence''. 
It is not, however, a ``distance measure'' in the standard sense, because it is not symmetric: typically $\kldiv{\mu}{\nu} \ne \kldiv{\nu}{\mu}$. 
%
But this is a good thing, because
 % the concept of probabilistic discernability itself
the task of discerning one probability distribution from another
is inherrently asymmetric. 

\begin{example}
    Suppose that there are two coins: a fair one, and a double headed one. 
    One of them (although you do not know which) has been selected, and flipped; you observe the result. 

    First, suppose you believe the coin is fair, but in fact it is double-headed. 
    You observe the only possible outcome: heads. This is totally within the realm of possibility. It would take many flips to convince yourself the coin was not fair, and even then you could not be completely sure. 
    %
    Now reverse the roles: suppose that in reality, the coin is fair, but you believe it is double-headed. With probability 1/2, you observe a tails and are immediately persuaded (with certainty) that your belief is incorrect. 

    There is an inherrent asymmetry: it is far easier to ``distinguish the double-headed coin from the fair one'', than it is to ``distinguish the fair coin from the double-headed one''.
\end{example}

Relative entropy is arguably exactly the appropriate measure of discrepancy between belief and reality. 
The most obvious argument for this is the empirical one:
relative entropy has become the overwhelmingly standard measure of discrepancy between distributions used to train machine learning systems, where it is standardly known as \emph{Kullback Leibler (KL) divergence}.
\Cref{chap:one-true-loss} can be viewed as a significant strengthening of this argument.


Here is a more theoretical argument, that allows us to interpret relative entorpy in terms of coding theory. 
The length of an optimal code for a sample $\omega \sim \nu$ is proportional to its surprisal $\log \nf1{\nu(\omega)}$.
Imagine that you have belief $\nu$ and the codes you are using reflect this, while in fact reality is distributed according to $\mu$.
Upon seeing $\omega$, the length of your code is $\log\frac{1}{\nu(\omega)}$, while the length of an optimal one would be $\log\frac{1}{\mu(\omega)}$. 
In expectation, the difference between these quantities,
    which is the overhead of using your mis-specified codes, is 
     $\Ex_{\omega \sim \mu}[ \log \frac{\mu(\omega)}{\nu(\omega)}] = \kldiv\mu\nu$.


% Relative entropy is extremely special, and arguably exactly the
% appropriate measure of discrepancy between belief and reality.  It has
% a clear information theoretic interpretation as 
There are a number of other results showing that relative entropy is the unique quantity with important properties of interest; treating them all is beyond the scope of this thesis.% 
\footnote{%
    There are many nice axiomatizations of relative entropy;
    some important ones include Renyi's \citep{renyi1961measures}, Fadeev's \citep{fadeev1957begriff}, Leinster's result showing
    that it is the only function satisfying three trivial regularity
    properties and the chain rule \citep{leinster2017shortcharacterizationrelativeentropy}, and a characterization as the unique functor from statistical maps to the additive monoid $([0,\infty],+)$ \citep{baez2014bayesiancharacterizationrelativeentropy}.
    It is the only statistical divergence that is both a Bregman divergence and an f-divergence. 
    The Hessian of relative entropy is the Fisher metric, which is the unique metric tensor that is invariant under sufficient statistics. 
    In that geometry, we will see (\cref{chap:relent-soup}) that the gradient flow of relative entropy with respect to its first (``belief'') argument amounts to multiplicative interpolation of probability measures, while gradient flow of relative entropy with respect to its second (``reality'') argument amounts to additive interpolation. 
}
%
Suffice it to say that relative entropy is an extremely special,
    and a natural measure of quantitative discrepancy between belief. 
A great deal of this thesis can be viewed as further evidence of this. 
    
\commentout{%
We conclude this section by drawing a connection with the previous one. 
% While the entropy $\H(X)$ implicitly depends on $\mu$
Let $\mu(X)$ be the distribution of reality (on the variable $X$),
    $\nu(X)$ be a belief distribution over $X$.
Let $\lambda(X)$ be the base measure over $X$, which is typically not a probability measure; 
Observe the similarity:
\[
    \H( \mu ) = \Ex_{\mu}\Big[ \log \frac{\lambda(X)}{\mu(X)} \Big]
    \qquad
    \kldiv{\mu}{\nu} = \Ex_{\mu} \Big[ \log \frac{\mu(X)}(\nu(X)) \Big]
\]
}%

% $
%     \H_\mu(X) = \log |\V\!X| - \kldiv{\mu(X)}{ \Unif(X) }
% $
\newmaterial{%
\paragraph{Cross Entropy.}
Finally, to head off confusion, we mention that the \emph{cross entropy}
\[
    \XEnt(p,q) := \Ex_{p} \left[\log\frac{1}{q}\right]
         = \Ex_{\omega \sim p}\left[\log\frac{\mathrm d\lambda_{\Omega}}{\mathrm d q} \right]
         \qquad = \kldiv pq + \H(p),
\]
the standard function used to train a statistical model $q$ using data $p$, is not the same as relative entropy $\kldiv pq$.
However, as can be seen on the right-hand side of the equation above, minimizing it is equivalent to minimizing $\kldiv pq$, because the entropy $\H(p)$ of the data is a constant and does not affect the optimization process. 
We view use cross entropy as a small shortcut that simplifies computing with relative entropy in some contexts; we will only use it when comparing with standard learning objectives (in \cref{chap:one-true-loss,chap:LIR}).
}%

\temphide{
\begin{subappendices}    
\section{Primer on Advanded and Auxiliary Concepts}
The main body of this chapter has focused on material that is directly in the critical path to understanding every important result in this dissertation. 
 % 
 % the state of the field and building up a formal mathematical foundation on which the rest of the dissertation s.
%
At the other extreme, there is also some mathematical background needed only to understand a single chapter; that material is presented before it is needed. 
%
Here, we review some standard mathematical concepts that lie somewhere in between: these concepts play more minor roles than the ones already covered, and are common to a handful of chapters. 
%
%
We encourage a reader to quickly skim this section,
    and return to it if necessary. 


\subsection{Algebra}
\cref{chap:conf,chap:PDG-cat}

\begin{defn}[Monoid]%
    A \emph{monoid} is a tuple $(S, *, e)$, where $S$ is a set, $* : S \times S \to S$ is a binary operation, and $e \in S$ is a distinguished identity element, such that:
    \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt,]
        \item (associativity) $\forall a, b,c \in S.~~(a*b)*c = a*(b*c)$;
        \item (identity) $\forall a \in S.~~a * e = a = e * a$.
    \end{itemize}
    A monoid is called \emph{commutative} if it also satisfies
    \begin{itemize}[nosep]
        \item (commutativity): $\forall a, b \in S.~~ a * b = b * a$,
    \end{itemize}
    and \emph{idempotent} if it satisfies
    \begin{itemize}[nosep]
        \item (idempotence): $\forall a \in S.~~ a + a = a$. 
    \end{itemize}
    An idempotent semiring defines partial order by $a \le b \iff a + b = b$. 
\end{defn}

\subsection{Relations}

Let $\mathcal X = \{ X_1, \ldots, X_n \}$ be variables, traditionally called atributes. 
A \emph{relation} $R(\mathcal A) = R(X_1, \ldots, X_n) \subseteq 
\V(X_1) \times \cdots \times \V(X_n)$, or equivalently, $R : \prod_{i=1}^n \V\!X_i \to \{0,1\}$, is a subset of joint values of attributes. 
The natural number $n$ is called the \emph{arity} of $R$. 


The \emph{natural join} of two relations $R(A,B)$ and $S(B,C)$ combines them in a particularly obvious way: $(a,b,c) \in R \bowtie S$ iff $(a,b) \in R$ and $(b,c) \in S$. 
More generally, we have: 

% \begin{defn}[natural join]
%     \[
\begin{equation}
    R(\mathcal X) \bowtie S(\mathcal Y)
        := 
        \Big\{
            \boldsymbol\omega \in  \V(\mathcal X \cup \mathcal Y)
            ~\Big|~
            \mathcal X(\boldsymbol\omega) \in R \land
            \mathcal Y(\boldsymbol\omega) \in S
        \Big\}
        % \qedhere
%     \]
% \end{defn}
\end{equation}

At one extreme, if $\mathcal X$ and $\mathcal Y$ are disjoint sets of attributes, then $R(\mathcal X) \bowtie S(\mathcal Y)$ coincides with the cartesian product of $R \subseteq \V \mathcal X$ and $S \subseteq \V\mathcal Y$.
At the opposite extreme, if $\mathcal X = \mathcal Y$ are the same set of variables, then $R(\mathcal X) \bowtie S(\mathcal X)$ coincides with the intersection of the subsets $R$ and $S$. 

Even when $A_1, \ldots, A_n \subseteq \X$ are not disjoint, we give a convenient extended syntax by defining the quantity $R(a_1, \ldots, a_n)$, where $a_i \in \V(A_i)$. 
Concretely, define $R(a_1,\ldots, a_n) := 0$ if when $\{a_1, \ldots, a_n\}$ do not agree on the value of some shared attribute (i.e., if $\exists X \in \X, \exists i,j \in [n]. ~X \in A_i \cap A_j \land X(a_i) \ne X(a_j)$).
When $\{a,b,c\}$ do agree on all values of shared attributes, let $\mat x$ denote the joint value of $A \cup B \cup C$ obtained from $(a,b,c)$ by removing redundant copies of variable values.
In this case, define $R(a,b,c) := R(\mat x)$. 


\subsection{Geometry}

    
\end{subappendices}
}
