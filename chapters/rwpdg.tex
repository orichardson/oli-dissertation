    \label{chap:reason}

% How should you 
% We have just
In the last two chapters, we have come to understand a great deal about how to reason \emph{about} PDGs.
Yet, through a handful of simple rules for manipulating PDGs, a great deal of probabilistic reasoning can be done \emph{internally} within a PDG.  
% In this chapter, we will  
We have already seen some important examples.
In \cref{chap:one-true-loss} we saw that many important inequalities are instances of what we called \emph{monotonicity of inconsistency}: believing more things can only make you more inconsistent, not less.
% This kind of reasoning 
When reasoning this way, the PDG is analogous to a logical expression, and monotonicity is a kind of inference rule. 

In this chapter, we deepen our exploration of this kind of reasoning technique, developing an analogous principle for qualitative information in a directed hypergraph (\cref{sec:qual-monotone}).
It turns out that both the qualitative scoring function $\SDef$ (\eqref{eqn:sdef}) and QIM-compatibility (the subject of \cref{chap:QIM})) are monotonic in this way. 
We then develop a notion of qualitative equivalence (\cref{sec:QIM-equivalence}).
Finally, in \cref{sec:logic}, we combine our qualitative and quantitative reasoning principles to define a natural pre-order on PDGs, which shows promise as the basis of an inconsistency-tolerant logic. 

% Related monotonicity properties hold of many natural epistemic representations.
% One classical representation of knowledge is a list of formulas
%     $[ \phi_1, \phi_2, \ldots, \phi_n]$
%     that one knows to be true.
% This representation has an analogous property:
%     learning an additional formula $\phi_{n+1}$
%     can only narrow the set of worlds one considers possible.
    
%
    % after including an additional formula $\phi_{n+1}$
    % % has the effect of strengthening knowledge:
    % one can still prove anything that one could prove before.  
    % (as do many standard representations of knowledge \citep{rak}).
%oli1: although I like this flourish, I believe I agreed
% to remove it in order to prevent the story from seeming to undercut
% itself if one doesn't read carefully.
%
% Qualitative Bayesian Networks (and MRFs) are monotonic,
% 	but in the opposite direction.
% 	One might be tempted to think that an edge $A{\to}B$
% 	in a BN represents knowledge (that $A$ influences $B$).
% 	But in fact it is the \emph{lack} of an edge that caries knowledge:
% 	$A{\not\to}B$ means that $A$ does not influence $B$ directly.
%joe1: I don't see why this is exactly right, but in any case, it's a
%distracting.  I cut it.
% 	Thus, a BN is more like a list of formulas that one considers possible,
% 	than a list of formulas one knows.
%oli1: to help you see why the above is true: an edge in a BN indicates the possibility of dependence, not the necessity of dependence; if all variables are independent, it is consistent with every BN structure. 
%   \Scibility, however, is monotonic in the direction
%   we expect of a representation of knowledge.
%
%oli1: replacing it with the following, condensed version:
% The same is true of both kinds of information in a PDG, and also of \scibility. 
%joe2*: this does not seem consistent with your English.  It does
%*not* say that anything you can prove from A' can also be proved from
%A.  It says that if all the formulas in A are true according a
%probability distribution \mu, then so are the formulas in a subset.
%This is quite a different statement.  
%oli2: Good point. The English I had and the proposition are (classically)
% equivalent, but not directly so; they're logical duals. I'll rewrite the
% English to try to make it more direct.  
% 
% In this analogy, \Ar' corresponds to [\phi_1, ... \phi_n], not 
% to the extension to \phi_{n+1}.  Also, one should not think of \Ar
% as a collection of formulas, because mechanisms interact. Just
% because \mu \models { a } for all a \in \Ar (which is always true),
% does not allow us to conclude that  \mu \models \Ar.
%


% \section{Observational (Quantitative) Monotonicity and Equivalence}
%     \label{sec:quant-monotone}

% Monotonicity of PDG inconsistency \citep{one-true-loss}
% %joe16: all these inequalities need a reference
% is a powerful reasoning principle. Many important inequalities (e.g., the data processing inequality, relationships between statistical distances, the evidence lower bound, \ldots) can be proved using only a simple inference rule: ``more beliefs can only increase inconsistency''.


\section{Structural/Qualitative Monotonicity and Equivalence}

One classical representation of knowledge is a list of formulas
    $[ \phi_1, \phi_2, \ldots, \phi_n]$
    that one knows to be true.
This representation has an analogous property:
    learning an additional formula $\phi_{n+1}$
    can only narrow the set of worlds one considers possible,
    strengthening knowledge. 
Qualitative Bayesian Networks (and MRFs) are monotonic,
	but in the opposite direction:
        adding edges \emph{weakens} knowledge.
One might imagine that an edge $A{\to}B$
	in a BN represents knowledge that $A$ influences $B$, 
	but in fact it is the \emph{lack} of an edge that caries knowledge.
	% $A{\not\to}B$ means that $A$ does not influence $B$ directly.
%joe1: I don't see why this is exactly right, but in any case, it's a
%distracting.  I cut it.
	% Thus, the edges of a qualitative BN give the dependencies that one considers possible,
	% rather than a list of dependencies that one knows for certain are present.
%oli1: to help you see why the above is true: an edge in a BN indicates the possibility of dependence, not the necessity of dependence; if all variables are independent, it is consistent with every BN structure. 

Hypergraphs, both with semantics in terms of QIM-compatibility (the subject of \cref{chap:QIM}) and in terms of $\SDef$, however, are monotonic in the directions we expect of knowledge.
But it turns out that more is true. 
In \cref{sec:qual-monotone}, we establish a stronger notion of qualitative monotonicity that makes use of the directed nature of a hypregraph. 
We then provide a complementary notion of qualitative equivalence (\cref{sec:QIM-equivalence}). 
These two qualitative ``inference rules'' can be combined to form a formal system for reasoning about qualitative aspects of probability. 

%oli22*: moved full section to the appendix
% \subsection{Monotonicity of Structural Compatibility}
%oli3: rebranding section
% \section{Structural Equivalence and Monotonicity}
%oli6:
% \section{Reasoning about \SCibility}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%oli22: rebranding, for appendix
% \section{Monotonic Reasoning about \SCibility}
\subsection{Qualitative Monotonicity}
    \label{sec:qual-monotone}


%oli4: should I add eeven more motivation? I feel it distracts from the current story, but it still might help to motivate. Maybe the below should go elsewhere...
%joe5*: You should definitely add motivation.  But motivating this
%section by an appeal to cyclic models, which you haven't spent any
%time discussing in this paper, is not good at all.  The fact that
%they're complicated might say more about our approach than about
%cyclic models.  In any case, this definitely does not count as good
%motivation 
\commentout{
    There are simple rules for manipulating
    %oli6: "quantiative" is not what I want to emphasize here
     % quantiative
    PDGs,
    %joe5: What is a proof language?  
    %oli6: maybe you'd be more comfortable calling it an axiom system? 
    % that allow them to be used as a proof language. 
    % that allow them to be used as an axiomatic proof system. 
    which can be used as an axiomatic proof system. 
}
%oli6*: wow, you were right! Now that equivalence material has been cut, the story becomes easier to state and sell. 
%oli6:* Why do I struggle so much to prune connections? Why is everything so strongly connected in my head? 
\commentout{
    %joe5: confidence is coming out of the blue here.
    %oli6: I was trying to be extra accurate, but I'll just suppress that thought.
    % Holding beliefs $p(X)$ and $p(Y|X)$ (with the same confidence),
    Holding beliefs $p(X)$ and $p(Y|X)$, 
    for example, is observationally equivalent to holding a single belief $p(X,Y)$ or beliefs $p(Y)$ and $p(Y|X)$.
}
%joe5: The reader will have no clue of what "nmonotocity of inconsistency" is.
%oli6: That's why I'm describing what it is.  And then I give a reference. You definitely aren't expected to know what "monotonicity of inconsistency" is here. To clarify, I am moving the reference to the beginning of the sentence, from the end.
% Monotonicity of inconsistency is a more powerful reasoning principle: many important statistical relationships (e.g., the data processing inequality and the evidence lower bound) can be proved directly, using only the fact that more beliefs cannot decrease inconsistency, and equivalences like the one above \citep{one-true-loss}.
%
%oli21: instead:
%joe17
%Monotonicity of (quantitative) PDG inconsistency
% The fact that (quantitative) PDG inconsistency  is monotonic
% is a powerful reasoning principle that can be used to prove many important inequalities \citep{one-true-loss}. 
Recall from \cref{chap:one-true-loss} that, by simple applications of a rule called \emph{monotonicity of (observational) inconsistency} (\cref{lemma:!}), we were able to derive things such as variational bounds, inequalities between statistical divergences, and properties such as the data-processing inequality. 
In this section, we develop a related principle for qualitative information in a hypergraph, that applies to both \scibility\ and the function $\SDef$.
%joe4*: I have no idea what's going on in this section.  At one point I
%sent you an email explaining why I thought nonpartitional models
%weren't good models of causality.  I'm not convinced by the
%connection to models with constraints at all.   It might be better to
%restrict this section to partional models, where causality is well
%understood, and cut this subsection altogether.
%oli5: You are right not to be convinced by this subsection, which isn't very precise right now. Your suggestion to remove this section has been noted, and I'm not a priori opposed to it, but after reading your paper on causal models with constraints more carefully last week, I'm even more certain than I was before, that there's a deep connection here. (But I wasn't able to formulate "the one appropriate thing to prove", in part because your paper isn't clear about how the concept interacts with probabilities.) I think it's still worth saying a few words if we have space.
%
%oli6*: moved SIM equivalence to appendix for UAI paper
%
%
% Here is an alternate, equivalent characterization of this definition
% in terms of de-randomization.
%
% \begin{theorem}
%     $\mu(\X)$ is s-compatible with $\Ar$ if an only if
%     there exists a variable $U$, an indexed set of functions
%     $\mathcal F = \{ f_a : \V\Src a \times \V U \times \V \Tgt a\}_{a \in \Ar}$,
%     and an extended distribution $\nu(\X, U)$ such that
% \begin{enumerate}[label={\rm(\alph*')}]
%     \item $\mu(\X) = \nu(\X)$;
%     \item $\nu$ has support only on ``consistent pairs''
%         $(\omega, u) \in \V(\X \times U)$
%         such that
%         $f_a(\Src a (\omega), u) = \Tgt a(\omega)$
%         for every \arc\ $a \in \Ar$;
%
%     \item For every $a \in \Ar$, the pair $(f_a, \nu(U|\Src a))$
%         is a derandomization of $\mu(\Tgt a | \Src a)$.
%
%     \item
%         for every subset $A \subseteq \Ar$ of \arc s,
%         and every fixed choice of settings
%         $\mat{st} = \{ (s_a, t_a) \in \V(\Src a, \Tgt a) \}_{a \in A}$,
%         the set of events
%         \[
%             \Big\{
%                 \{ u \in \V U : f_a(s_a, u) = t_a \}
%                 ~\Big|~a \in \Ar \Big\}
%         \]
%         are mutually independent according to $\nu(U)$.
% \end{enumerate}
% \end{theorem}
%
%oli6: no longer need subsection here
% \subsection{Monotonicity}
%joe1*: I know that you won't like to hear this, but I found this
%section extremely weak.  At the end of the day, what have I learned?
%That \scibility\ is monotonic in one sense (adding
%certain edges) and anti-monotonic in another )Theorem 4) and that you
%can't have monotonicity and there's a strange, unmotivated sense in
%which you can't capture BNs and MRFs with a monotonic definition of
%|=.  While monotonicity seems like a nice property, it's not that
%compelling (particularly since you consider in some sense "positive"
%and "negative" version of it.  I see no reason why I should care
%about Theorem 5, which seems to be the punchline of this section.
%Unless you can come up with a much better story and motivation, I
%think we should cut this too.  To repeat, the paper is not a place to
%do a core dump of all the things you thought about.  I think this
%section weakens the paper.  I put in some comments in the rest of the
%section, but my strong preference would be  to cut it.
%
%oli1: After our discussion, we had a plan for how to proceed here.
% At a high level, though, let me respond to your concerns above
% in writing.
%
%    First, there's only one sense of monotonicity. The fact that it's
% covariant in outputs and contravariant in inputs is extremely
% good news given that functions and mappings work.  Inputs and 
% outputs play opposite roles. 
%    Second, this will be useful for a running example of a 3-cycle, 
% which is one of the things we're buliding towards, as described in the
% introduction. 
%    Finally, the last theorem (5/6) is more important than it looks
% at first. The "positive MRF capture" and "negative BN captue" are
% weaker than "capturing MRFs" and "capturing BNs", so these words
% actually strengthen the result, not weaken it. 
% The high-level of the theorem is: there's no "natural" (in particular,
% monotonic) definition of \scibility\ that captures both
% BNs and MRFs.
%
%
%oli7: breaks the flow; a good place to cut 

We begin with a direct analogue of \cref{lemma:!}.
% that does not turn out to be terribly useful: 
% if $\Ar \subseteq \Ar'$ and $\mu \models \Diamond\Ar'$, conclude $\mu \models \Diamond\Ar$. 


%oli21: cutting proposition from condensed version
% \vfull{%
\begin{prop}
        % [monotonicity]
        \label{prop:mono}
    % If $\Ar' \subseteq \Ar$, then
    %     $\bbra{\Ar} \subseteq \bbra{\Ar'}$.
    %oli2: changing the order so that \Ar
    % If $\Ar' \subseteq \Ar$ and $\mu \models \Ar$, then $\mu \models \Ar'$.
    % If $\Ar \subseteq \Ar'$ and $\mu \models \Ar'$, then $\mu \models \Ar$.
    If $\Ar' \subseteq \Ar$,
    then 
    \begin{itemize}[nosep]
    \item 
    $\mu \models \Diamond\Ar$ implies $\mu \models \Diamond\Ar'$.
    \item 
    $\SDef_{\Ar}(\mu) \ge \SDef_{\Ar'}(\mu)$.
    \end{itemize}
\end{prop}
% }%
%oli21: instead, a short inline discussion. 
%
% This is an important requirement for any definition of mechanism independence.  If
% $\mu$ is consistent with a causal picture consisting of some set of independent mechanisms, then surely it must be consistent with a picture in which only a subset of those mechanisms are independent.  
%oli20: many cuts to shave a line
% This should be intuitive: if
%oli21: 
After all, if 
% This is intuitive: if
$\mu$ is consistent with a set of independent causal mechanisms, then surely 
%oli20: 
% it must be
it is
consistent with a causal picture
%oli20:
in which
% where
only a subset of those mechanisms 
%oli20:
% must be
are
present and independent.  
%oli21: bringing up from below. (material in brackets). 
{%
There is a sense in which 
%oli21: compress by using the acronyms
    % Bayesian Networks and Markov Random Fields are also monotonic,
    BNs and MRFs are also monotonic,
%oli21:
    % but in the direction opposite to \cref{prop:mono}:
    but in the opposite direction:
    adding edges to a graph results in a weaker
    %oli21:
    % statement about independence.
    independence statement.
%oli21: adding
    We will soon see why.
}%

% When we translate a DAG to a hypergraph,
% In compiling a DAG to a \hgraph\ (see \cref{sec:bns}),
%     adding more edges in the DAG corresponds to adding more tails to an \arc\ in the corresponding hypergraph.

%oli1:
% For directed edges, there is a finer notion of monotonicity at play.
%oli2:
% Since a hypergraph has directed \arc s,
%oli5: compressing with wrapfigure
\begin{wrapfigure}[4]{o}{1.8cm}
    % \fbox{
    \vspace{-3ex}
    % \hspace{-1.2em}
    \begin{tikzcd}[row sep=3ex,column sep=0.2em]
        A \ar[rr,"f"] &\ar[d,squiggly]& B\ar[d,hook,gray]  \\
        A'  \ar[u,hook,gray]\ar[rr, dashed]&{\vphantom{a}}& B'
    \end{tikzcd}
    % }
\end{wrapfigure}
%joe5: this is somewhat circular
%oli6: not anymore, since I changed \hgraph back to "hypergraph"---but your version is still better, I think.
% Since our notion of a \hgraph\ has \emph{directed} \arc s,
Since we use \emph{directed} hypergraphs,
     there is actually a finer notion of monotonicity at play. 
%oli1: added
Inputs and outputs play opposite roles, 
    and they are naturally monotonic in opposite directions. 
%oli3: out of order; this should come later.
% Consider a function $f : A \to B$.
%joe1*: I have no idea what subtype means, and I'm sure that a typical
%reader won't either.  We can't read your mind!
%oli1: it means a value of B can be implicitly converted to a value of B'. I'll change the language.
% If $B$ is a subtype of $B'$ (written $B \sqsubseteq B'$), then $f$ can be used as a function from $A$ to $B'$.
%oli3: "implicitly convert" -> "regard" for brevity
% If there is an obvious way to implicitly convert an element $B$ to an element of $B'$ (abbreviated $B \,{\color{gray}\hookrightarrow}\, B'$)
If there is an obvious way to regard an element of $B$ as an element of $B'$ (abbreviated $B \,{\color{gray}\hookrightarrow}\, B'$),
and $A' \,{\color{gray}\hookrightarrow}\, A$, 
then
%oli3: heres's where it should be
% $f$ can be used as a function from $A'$ to $B'$.
a function $f : A \to B$ can be regarded as one of type $A'\to B'$.
%joe1*: you've totally lost me here.  I think that this hurts far more
%than it helps.  I just cut it.
%oli1*: I think the fact that inputs work the opposite ways is actually
% the punchline. I've shortened it and moved it above.
% (Anti)symmetrically, if $A' \sqsubseteq A$, then $f$ can be used as a funciton from $A'$ to $B$
%oli1: shortening
% (This may be easier to see in the diagrammatic construction below, where ${\color{gray}\hookrightarrow}$ indicates the subtype inclusion.)
%oli5:
% This is depicted below.
%joe5: I don't find the figure particularly helpful.  I would cut it.  
%oli6: I like figures and commutative diagrams, and I find things like this very helpful. I know for a fact that there are a lot of people like me who pay more attention to diagrams and figures and equations, than they do to the text.   That said, I agree it was using a lot of space, so I found another fix for that. 
This is depicted to the right.
%oli5: moved the diagram above
% \[
% %oli1: don't need the first part of the equation; the commutative diagram is enough.
% % \begin{array}{l}
% %     A' \sqsubseteq A, \quad B \sqsubseteq B' \\
% %     \Rightarrow \quad (A \to B) \sqsubseteq (A' \to B').
% % \end{array}
% % \qquad
% \begin{tikzcd}[row sep=2ex,column sep=1em]
%     A \ar[rr,"f"description] &\ar[d,squiggly]& B\ar[d,hook,gray]  \\
%     A'  \ar[u,hook,gray]\ar[rr, dashed]&{\vphantom{a}}& B'
% \end{tikzcd}
% \]
%oli5: merging paragraphs
%
% These principiles of function variance also apply in our setting.
The same principle applies in our setting.
%oli1:
% have a natural analogue  in our setting, which we call structural weakening.
If $\mat X$ and $\mat Z$ are sets of variables and $\mat X \subseteq \mat Z$, then 
%oli3:
% there is an obvious way to implicitly convert a $\mat z \in \V(\mat Z)$ to 
%oli5:
% there is an obvious way to implicitly regard $\mat z \in \V(\mat Z)$ to 
% as an element of $\mat x = \mat z[\mat X] \in \V(\mat X)$: restrict $\mat z$ to the coordinates of $X$.
$\V(\mat Z) {\color{gray}\hookrightarrow} \V(\mat X)$, by restriction. 
%oli5: shorter
% It follows from the reasoning above that 
It follows, for example, that
    any mechanism by which $X$ determines $(Y,Y')$ can be viewed
        as a mechanism by which $(X,X')$ determines $Y$. 
%oli3: too verbose
% This allows us to conclude that some sets of
%    (independent) mechanisms imply the existence of others.
The general phenomenon is captured by the following
%oli5: save a line
    \unskip formal definition.
    % \unskip.
% Here, the relevant notion of subtype is not set inclusion, but
%     rather variable superset.
% Thus, the behavior of our definition follows the appropriate variance
% of function typing: a function $f : A \to B$ can be regarded as a function
%     from $A'$ to $B'$ if $A' \subseteq A$ and $B' \supseteq B$.

%joe5*: This is a definition of "weakening", not structural
%weakening.  Why do you keep insisting on adding the word "structural"
%everywhere.  It's certainly not an improvement
%oli6: ok, I'm willing to take "structural" out of the name
% \begin{defn}[structural weakening]
\begin{defn}
    % [weakening]
        \label{defn:weakening}
    % Consider two sets of \arc s, $\Ar = \{ \Src a \to \Tgt a\}_a$
    %     and $\Ar' = \{ \Src a{\!}' \to \Tgt a{\!}'\}_a$.
    % If $\Ar' = \{ \Src a{\!}' \to \Tgt a{\!}' \}_{a \in \Ar}$
    % Consider a collection of \hyperarc s
    %oli21: cleaning up this definition + cutting some lines. 
    \commentout{%
        Consider a directed hypergraph
        $\Ar = \{ \Src a \to \Tgt a \}_{a \in \Ar}$.
        If another graph $\Ar'$ can be obtained by adding sources to and removing targets from the \arc s of $\Ar$---that is, if $\Ar' = \{ \Src a{\!}' \to \Tgt a{\!}' \}_{a \in \Ar}$, with $\Tgt a{\!}' \subseteq \Tgt a$ and
         $\Src a{\!}' \supseteq \Src a$ for all $a \in \Ar$---then we
         say $\Ar'$ is a \emph{weakening} of $\Ar$ and
         write $\Ar \rightsquigarrow \Ar'$.
     }%
     % If $\Ar = \{ \Src a \to \Tgt a \}_{a \in \Ar}$,
     % $\Ar' = \{ \Src a{\!}' \to \Tgt a{\!}' \}_{a \in \Ar'}$,
     If $\Ar = \{ \ed aST  \}_{a}$, 
     $\Ar' = \{ \ed{a'}{S'}{T'} \}_{a'}$,
     % are hypergraphs,
     and there is an injective map $\iota : \Ar' \!\to\! \Ar$
     such that
     $\Tgt {a}{\!}' \subseteq \Tgt {\iota(a)}$ and
     $\Src {a}{\!}' \supseteq \Src {\iota(a)}$
     for all $a \in \Ar'$,
     % (that is, $\Ar$' is adding sources removing targets from the \arc s of $\Ar$), 
     then $\Ar'$ is a \emph{weakening} of $\Ar$
     % and we write $\Ar {\rightsquigarrow} \Ar'$.
     % ?and we write 
     (written $\Ar {\rightsquigarrow} \Ar'$).
\end{defn}

%oli21: this whole thing is unnecessary, and too expensive.
% \vfull{
% %joe16: no need
%Indeed, 
Both the function $\SDef$ and the notion of
\scibility\ are monotonic with respect to weakening of the underlying hypergraph structure ($\rightsquigarrow$).
% 
\begin{theorem}
    % [strong monotonicity]
        \label{theorem:strong-mono}
    % If $\Ar' = \{ \Src a{\!}' \to \Tgt a{\!}' \}_{a \in \Ar}$
    % is obtained by adding heads and removing tails
    % from each \arc\ of $\Ar$ (i.e., $\Tgt a{\!}' \supseteq \Tgt a$ and $\Src a{\!}' \subseteq \Tgt a$ for all $a \in \Ar$),
    % then
    % $\bbra{\Ar'} \subseteq \bbra{\Ar} $.
    If
    % $\mu$ is \scible\ with $\Ar$
    % $\mu \models \Ar$ and $\Ar \rightsquigarrow \Ar'$, then
    $\Ar \rightsquigarrow \Ar'$, then
    \begin{itemize}[nosep]
    \item 
     $\mu \models \Diamond \Ar$ implies $\mu \models \Diamond \Ar'$;
    \commentout{% the text version 
        and $\mu$ is 
        % (E)
        \scible\ with $\Ar$,
        then $\mu$ is also 
        % (E)
        \scible\ with $\Ar'$. 
        % $\mu$ is \scible\ with $\Ar'$
    }%
    \item 
    $\SDef_{\Ar}(\mu) \ge \SDef_{\Ar'}(\mu)$. 
    \end{itemize} 
\end{theorem}

%oli21: not critical; streamlining. 
% \vfull{%
\Cref{theorem:strong-mono} is strictly stronger than \cref{prop:mono}
because a \arc\ with no targets is vacuous, so
removing all targets of a \arc\ is equivalent to deleting it.
% }%
%
%oli3: relocating this material here, from above the theorem. 
%oli21: moving this material above
% There is a sense in which 
%     BNs and MRFs are also monotonic,
%     but in the direction opposite to \cref{prop:mono}:
%     adding edges to a graph results in a weaker statement about independence.
% When we view a graph $G$ as specifying a directed hypergraph
%     $\Ar_G$ (as described in \cref{sec:bns}),
%     adding an edge to $G$ amounts to adding a tail to
%     a \hyperarc\ already present in $\Ar_G$.
% Thus \cref{defn:weakening} corresponds to 
% As mentioned before, this is structural weakening captures
% the way in which Bayesian Networks and Markov Networks
% are (anti)monotonic: if $G = (\X, E)$ and $G' = (\X, E')$
% are graphs with the same set of vertices and $E \subseteq E'$,
% then $\mathcal I(G) \supseteq \mathcal I(G')$,
% and so $\mu \models \mathcal I(G)$ implies $\mu \models \mathcal I(G')$.
%oli3:
% \cref{defn:weakening,defn:graph-to-hypergraph}
% captures the sense in which Bayesian Networks are (anti)monotonic.
%oli6:
% \Cref{defn:weakening} provides an explanation: the effect of adding an edge $X \to Y$ to a directed graph $G$ on $\Ar_G$, is to add $X$ to the sources the \arc\ that determines $Y$.
%oli21:
% \Cref{theorem:strong-mono} provides an explanation: adding $X \to Y$ to a graph $G$ means adding $X$ to the \emph{sources} the \arc\ whose target is $Y$, in the \hgraph\ $\Ar_G$.
\Cref{theorem:strong-mono} also explains why BNs and MRFs are in a sense \emph{anti}-monotonic: adding $X \to Y$ to a graph $G$ means adding $X$ to the \emph{sources} the \arc\ whose target is $Y$, 
%oli21: line shave
% in the \hgraph\ $\Ar_G$.
in $\Ar_G$.
%oli3: if I did my job before definition 5 properly, I will have already made this point.
% This can only weaken the corresponding statement of (in)dependence, because one can always throw out this new source information and use the old mechanism.
%oli6: merging paragraphs, combining subjects
%
%oli1: overly verbose.
%   Now that we know that
%     \scibility\ 
%     (\cref{defn:qim})
%     is structurally monotonic, we have the tools
%     we need to at least partially understand more examples,
%     and cyclic graphs in particular.
% Structural montonicity is a helpful tool for understanding cyclic graphs.
%oli6:
% Structural montonicity is a helpful tool for understanding \scibility\ with respect to cyclic graphs.
%oli6.2: May as well make coreference easier, since it doesn't cost us a line
% It 
%oli21: 
% \Cref{theorem:strong-mono}
Yet by far the most important consequence of this result is that it
%oli16:
% is also helpful for understanding \scibility\ with cyclic \hgraph s.
helps us begin to understand what \scibility\ means for cyclic \hgraph s.
%oli22:
For the reader's convenience, we now restate the examples in the main text,
which are really about monotonicity.

\medskip
\textbf{\cref{example:xy-cycle}.}
% \begin{example}
        % \label{example:xy-cycle}
    Every $\mu(X,Y)$ is
    %oli21: removing a QIM
    % \scible\ with
    \cible\ with
    \begin{tikzpicture}[center base]
        \node[dpadinline] (X) {$X$};
        \node[dpadinline,right=1.2em of X] (Y) {$Y$};
        \draw[arr1] (X) to[bend left=13] (Y);
        \draw[arr1] (Y) to[bend left=13] (X);
    \end{tikzpicture}.
    This is because this cycle is weaker than
    % a set of \arc s
    a \hgraph\ 
    that can already represent any distribution, i.e.,
    %oli4: no more displaymode, to save a line
    % \[
    \begin{tikzpicture}[center base]
        \node[dpadinline] (X) {$X$};
        \node[dpadinline,right=1.0em of X] (Y) {$Y$};
        \draw[arr1] (X) to (Y);
        \draw[arr1,<-] (X) to +(-0.68,0);
    \end{tikzpicture}
    % ~~\rightsquigarrow~~
    $~\rightsquigarrow~$
    \begin{tikzpicture}[center base]
        \node[dpadinline] (X) {$X$};
        \node[dpadinline,right=1.2em of X] (Y) {$Y$};
        \draw[arr1] (X) to[bend left=13] (Y);
        \draw[arr1] (Y) to[bend left=13] (X);
    \end{tikzpicture}~. \qedhere
    % \]
    % $\to X \to Y$.
% \end{example}
\hfill$\triangle$.
\medskip

% \newlength{\cycleboxlen}
\settowidth{\cycleboxlen}{\usebox{\cyclebox}}
\begin{wrapfigure}[4]{o}{1.2\cycleboxlen}
    % \vspace{-0.8em}
    % \usebox\cyclebox
    \begin{tikzpicture}[center base, scale=1.2]
        \node[dpad1] (X) at (0:.8) {$X$};
        \node[dpad1] (Y) at (120:.8) {$Y$};
        \node[dpad1] (Z) at (-120:.8) {$Z$};
        \draw[arr2] (X) to 
            % node[above right]{\small 1}
            (Y);
        \draw[arr2] (Y) to
            % node[left]{\small 2}
            (Z);
        \draw[arr2] (Z) to 
            % node[below]{\small 3}
            (X);
    \end{tikzpicture}
\end{wrapfigure}
% \begin{example}
% \refstepcounter{example}
% \label{example:xyz-cycle-1}
\textbf{\cref{example:xyz-cycle-1}.~} 
    % \newlength{\cycleboxlen}
    % \settowidth{\cycleboxlen}{\usebox{\cyclebox}}
    % \begin{wrapfigure}{o}{\cycleboxlen}
    %     \usebox\cyclebox
    % \end{wrapfigure}
    % Consider the set of \hyperarc s
    % to the right.
    What
    %oli21:
    %  distributions
    $\mu(X,Y,Z)$
    %oli5:
    % $\mu(A,B,C)$
    %oli21: removing a QIM 
    % are \scible\ 
    are \cible\ 
    with 
    %oli5: compressing, by putting to the right
    % \[
    % \Ar_{\text{3-cycle}} := 
    %     % \Ar_{3\bigcirc} :=
    %     \usebox\cyclebox\qquad ?
    % \]
    %oli21: 
    % the hypergraph $\Ar_{\text{3-cycle}}$, on the right? 
    % By monotonicity, we know that 
    the 3-cycle shown, on the right?
    By monotonicity,
    % \[
    %
    % \]
    %oli21: fixing several issues, and compressing
    % the chain $\to X \to Y \to Z$, and all permutations of it,  
    % are \scible\ with this graph. Thus,
    among them must be all distributions consistent with a linear chain ${\to}X{\to}Y{\to}Z$. Thus,  
    %oli21:
    % any distribution $\mu(X,Y,Z)$
    %joe17
        %    any distribution
        %spencer3: I think ``any distribution'' is what we want to say here. Reverting.
    % a distribution
        any distribution
    in which two variables are conditionally independent given the third
    is compatible with 
    the 3-cycle.
    %oli1: replacing long attempt to finish example at a high level, with
    % this sentence:
    Are there any distributions that are \emph{not} compatible with 
    %oli20:
    % this graph? It is not obvious.
    this hypergraph? 
    It is not obvious.
%     We return to this
%    %oli21: line shave
%     %  question
%       in \cref{sec:pdgs}. 
    %
    %
    % By randomly selecting distributions $\Pr(U_1),
    % \Pr(U_2)$, and $\Pr(U_3)$ (see \cref{sec:null}), one finds that the set of distributions that are consistent with this 3-cycle has larger dimension than the set of distributions that factorize according to $\Pr(X,Y,Z) \propto \phi_1(X,Y) \phi_2(Y,Z) \phi_3(Z,X)$.
    % Thus, our definition does not coincide with the corresponding factor graph.
    %
    %oli1: removing the final paragraph, which is an unnecessarily verbose
    % forward pointer. 
    %
    % But what about negative examples? Are there any distributions that
    % are \emph{not} compatible with the cycle?
    % It seems intuitively clear that the XOR distribution $\muxor$ (in which $X,Y$ are independent random coin flips, and $Z = X \oplus Y$) cannot arise from mechanisms of this shape.
    % But it is not so obvious how to prove this.
    % We will return to this example once we have a few more tools available.
% \end{example}
\hfill$\triangle$

\begin{wrapfigure}[4]{i}
        % {1.7cm}
        % {1.9cm}
        {2.2cm}
    % \vspace{-1em}
    \centering
    \begin{tikzpicture}[center base,scale=1.3]
        \node[dpad1] (A) at (-0.6,0) {$A$};
        \node[dpad1] (B) at (0,1) {$B$};
        \node[dpad1] (C) at (0.6,0) {$C$};

        \draw[arr2,<-] (A) -- (B);
        \draw[arr2,<-] (C) -- (B);
        \mergearr[arr2] ACB
    \end{tikzpicture}
\end{wrapfigure}
% \section{BNs, MRFs, and Structural Compatibility}
% Initially, our
% We were quite surprised to discover that this definition,
% which is fundamentally a much better match to directed models,
% goes a long ways for undirected models as well.
%joe1*: In what sense can we "handle" cycles.
%oli1: I just mean that our definition applies for cylcic graphs.  I'll try to change the wording to clarify.
% Now that we can handle cycles, it is natural to wonder whether our definition can capture undirected models.
% Again by monotonicity, it is easy to see that the complete graph 
%oli21: compressing
% Because our definition of \scibility\ applies to cyclic models, it is natural to wonder whether 
Because \scibility\ applies to cyclic structures,  one might wonder if
    it also captures the independencies of undirected models 
    %oli6: not helpful here, and breaks the flow
    % (i.e., Markov Random Fields).
    \unskip.
%joe5: You have to explain what cyclic models have to do with MRFs
%oli6: one moment...
%oli1
% In many contexts, it is standard to regard 
%oli2:
% It is common to identify an undirected edge $A {-} B$
Undirected edges $A {-} B$ are commonly identified
%oli6: answering your question above:
% with a pair of directed edges $\{ A{\to}B, B{\to}A\}$,
with a (cylic) pair of directed edges $\{ A{\to}B, B{\to}A\}$,
%joe5: ?? Where did you do this in Equation (2):
%oli6: Equation (2) is the conversion of graphs to directed hypergraphs; looking at it closely should reveal why it's implicitly identifying an unorded edge with a pair of directed edges in each direction.
as we have implicitly done in
%oli21: 
% \cref{defn:graph-to-hypergraph}.
defining $\Ar_G$. 
%joe5
%In this way, undirected graphs, too, naturally generate to directed hypergraph s.
In this way, undirected graphs, too, naturally correspond to directed hypergraph s.
For example, 
%oli21: shortening
% the undirected graph
$G = A{-}B{-}C$
%joe5
%results in the directed hypergraph\
%oli21: 
% corresponds to the directed hypergraph\
corresponds to the \hgraph\ 
%oli6: compressing to save space
\commentout{
\[
    \Ar_{G} = \Bigg\{
            \begin{array}{r@{}l} \{B\}&{\to}\{A\},\\ \{A,C\}&{\to}\{B\},\\ \{B\}&{\to}\{C\}
            \end{array}\Bigg\}
    =     \begin{tikzpicture}[center base]
        \node[dpad1] (A) at (-0.6,0) {$A$};
        \node[dpad1] (B) at (0,1) {$B$};
        \node[dpad1] (C) at (0.6,0) {$C$};

        \draw[arr2,<-] (A) -- (B);
        \draw[arr2,<-] (C) -- (B);
        \mergearr[arr2] ACB
    \end{tikzpicture}
    ~.
\]
}
%oli6:
$\Ar_G$ shown on the left.
% 
%joe5: I don't know what "match" means.  This needs to be explained
%better.  If you want to say something about the set of distributions
%compatible with the resulting hypergraph, say that. 
%oli6: ok
    % Applying \cref{defn:qim} to the resulting \hgraph\ $\Ar_G$, however,
    % does not match any of the three standard Markov properties
    %     which define semantics for MRFS \citep{lauritzen}.
%oli21: cutting QIM
% \Scibility\
Compatibility
with $\Ar_G$, however, does not coincide with any of the standard Markov properties
%oli6: can be simplified
 % for the MRF corresponding to $G$ \citep{lauritzen-mrf-indeps}.
corresponding to $G$
    \citep{lauritzen-mrf-indeps}.
    % \citep{KF09}.
%oli15: adding, for clarity
%oli21: not helpful; removing. 
% as we will soon see.
%
%oli1:
% There is a sense in which this is unavoidable.  
%oli21:
% This may seem to be a flaw in our particular definition of \scibility\ (\cref{defn:qim}), but it is unavoidable.  
This may appear to be a flaw in \cref{defn:qim} (\scibility), but it is unavoidable.  
%oli3:
% While BNs and MRFs are both monotonic, 
%     it is impossible to monotonically capture
%     both Bayesian Networks (directed graphical models) and Markov Random Fields (undirected graphical models) with a single monotonic definition of \scibility.
%oli21: making use of abbreviations we've had all along to save some space
% While both Bayesian Networks (directed graphical models)
%     and Markov Random Fields (undirected graphical models)
%     are both monotonic on their own,
%     it is impossible to capure both BNs and MRFs monotoncially together.
While both BNs and MRFs are monotonic, it is impossible to capture both classes with a monotonic definition.
% The proof is a straightforward application of monotonicity.
% and one instance of a graphical proof technique.

% \begin{linked}{theorem}{mrf-bn-monotone-impossible}
\begin{theorem}\label{theorem:mrf-bn-monotone-impossible}
    % No monotonic definition $\bbr{-}$ of structural
    % compatibility that captures BN structures can also capture
     % with dag $D$ as $\bbr{\Ar_D}$,
     % also compatibility with an undirected graph $G$ as $\bbr{\Ar_G}$.
     %oli1: added \prime to \models in this definition throughout
    % No definition $\models$ of \scibility\ with a set of \hyperarc s can satisfy all three of the following:
    %oli2:
    % It is possible to give a definition  $\models'$ of structural compatibility 
    %oli9:
    % \def\newmodels{\models^{\mathllap{\cdot~~~}}}
    % \def\newmodels{\models^{\mathllap{\bullet~}}}
%joe5*: You need to discuss and motivate what you mean by a
%"compatibility" relation before you state the theorem.  This is
%coming out of the blue.
%oli6: I don't mean anything technical. I will remove the quotes and the word, for the theorem statement.
    % It is possible to define a ``compatibility'' relation  $\models'$ 
    It is possible to define a  relation  $\dotmodels$ 
    %oli5: saving a line, part 1:
        % between probability distributions $\mu$ and directed hypergraph s $\Ar$
        between distributions $\mu$ and directed hypergraph s $\Ar$
        % between a distribution and directed hypergraph
    satisfying 
    any two, but not all three, of the
    %oli5: ... and part 2.
    % following properties.
    following.
    \begin{description}[itemsep=0pt,parsep=0.3ex,topsep=0pt]
        \item [\rm(monotonicity)]
            % If $\Ar \Rightarrow \Ar'$ then $\bbr{\Ar} \supseteq \bbr{\Ar'}$.
            % If $\mu$ is \scible\ with $\Ar$
            If $\mu \dotmodels \Ar$
            and $\Ar \rightsquigarrow \Ar'$,
            then
             % $\mu$ is \scible\ with $\Ar'$.
            $\mu \dotmodels \Ar'$.
        \item [\rm(positive BN capture)]
            % For directed graphs $G$,
            % $\mu \in \bbr{\Ar_G}$ if and only if $\mu \models \mathcal I(G)$.
            %oli1:
            % If $\mu \models \mathcal I(G)$ for a directed graph $G$, then
            If $\mu$ satisfies the independencies
            %oli3:
            % of a BN $G$,
            %oli15: saving a line; also I don't like "implied"
            % implied by a directed acyclic graph $G$,
            $\mathcal I(G)$ of a dag $G$,
            then
                % $\mu$ is \scible\ with $\Ar_G$.
            $\mu \dotmodels \Ar_G$.
        \item [\rm(negative MRF capture)]
            % For undirected graphs $H$,
            % if $\mu \in \bbr{\Ar_G}$ then $\mu \models \mathcal I(G)$.
            % If $\mu$ is \scible\ with $\Ar_G$ for an undirected
            If $\mu \dotmodels \Ar_G$ for an undirected directed graph $G$,
            %oli2: oops, the undefined notation didn't get fixed
            % then $\mu \models \mathcal I(G)$.
            %oli3: missing citation
            % then $\mu$ has a Markov property with respect to $G$.
            then $\mu$ has one of the Markov properties 
                % \citep{lauritzen-mrf-indeps} 
                with respect to $G$.
    \end{description}
\end{theorem}


\label{proof:mrf-bn-monotone-impossible}
    %oli1: added to make it explicit
%joe5: is this what you meant
% a notion of
%oli6: yes, but I now removed it; see below.
%oli5: removed because I've now said it better above
% In particular, there's no way to define of \scibility\ 
%     that fully captures BNs and MRFs in a monotonic way. 
The proof is
a direct and easy-to-visualize application
% an easy-to-visualize application
of monotonicity (\cref{theorem:strong-mono}).
% {\renewcommand*{\proofname}{Proof Sketch}
% \begin{proof}
%oli21: expanding
Assume montonicity and positive BN capture. 
Let $\muxor(A,B,C)$ be the joint distribution in which 
%oli3: two variables
$A$ and $C$ are independent fair coins, and
$B = A \oplus C$ is their parity.
% Finally, let $G$ be the undirected graph $A{-}B{-}C$. 
%joe16
% Then, we have:
We then have:
%oli20: saving a line:
\[
% \\$
\muxor
\models
\begin{tikzpicture}[center base,scale=1]
    \node[dpadinline] (A) at (-0.6,0) {$A$};
    \node[dpadinline] (B) at (0,1) {$B$};
    \node[dpadinline] (C) at (0.6,0) {$C$};

    \draw[arr2,<-] (A) -- +(0,1);
    \draw[arr2,<-] (C) -- +(0,1);
    \mergearr[arr2] ACB
\end{tikzpicture}
~~\rightsquigarrow~~
\begin{tikzpicture}[center base,scale=1]
    \node[dpadinline] (A) at (-0.6,0) {$A$};
    \node[dpadinline] (B) at (0,1) {$B$};
    \node[dpadinline] (C) at (0.6,0) {$C$};

    \draw[arr2,<-] (A) -- (B);
    \draw[arr2,<-] (C) -- (B);
    \mergearr[arr2] ACB
\end{tikzpicture}
~=~ \Ar_{A{-}B{-}C}.
%oli20:
\]
% $
% ~~
But
$\muxor \not\models A \CI C \mid B$.
% \qedhere
% and so
% $\muxor$ does not satisfy any of the three
% markov properties with respect to the graph $A{-}B{-}C$.
% \end{proof}}
\hfill\qedsymbol
% \end{lproof}


%oli21*: moved the proof that was previously here, to the appendix. 
%
%joe3*: Perhaps you found it surprising, but I suspect that most
%readers won't appreciate the points you made.  I have no clue what
%the connection is between the first paragfraph and the result, let
%alone why it should make me feel the result is surprising.  The same
%goes for the second paragraph (although at least I understand better
%what it's saying).  If all readers were clones of you, this might
%interest them.  I strongly suspect that for most readers, these two
%paragraphs detract from the paper.
%oli3: ok, I'll try to distill these paragraphs to something that others can find more useful. 
% We found this result surprising, for two reasons. First, for an undirected graph, $\Ar_G$ is a consistent dependency network stucture \citep{heckerman2000dependency} that corresponds exactly to that Markov Network (for positive distributions), intuitively by placing independent mechanisms along each \arc. This is because dependency networks with a fixed sampling order only capture those BNs consistent with that order, and if the order changes depending on the graph, then the definition is not monotonic. 
%joe5
%We found this result surprising for a few reasons.
%First, if $G$ is an undirected graph then $\Ar_G$ is the structure of
%a consistent dependency network \citep{heckerman2000dependency}, and
%
%joe5*: this requires more explanation.  The "represents precisely" 
%presumably means "according to the semantics of DNs".  If that's
%what it means, you have to say so; if that's not what it means, then
%I'm confused and yet more explanation is needed.
%oli6: that's right; I've modified the below as you requested.
%joe16*: I feel very strongly that this should be cut.  It will
%confuse lots of readers.  You're basically saying to the (very small)
%set of readers who may be confused "here's why you shouldn't be
%confused", at the price of confusing many other readers, who didn't
%caref about this at all.  But not that, at the end of this section,
%you basically have one result, that is no where close to the results
%of other sections in terms of conceptual imporance.   I would cut this
%section altogether.  It makes the paper worse.
%oli21*: I see where you're coming from. I agree that this next part is nowhere near valuable enough to keep in the main paper at this point. I still think that the section itself is valuable though. It's not as valuable, but it's also shorter.  It will ultimately take up less than one page. 
%joe17*: I'm not talking about "valuable".  I'm saying that adding this
%to the paper makes it seem worse (i.e., less good from the point of
%view of the reader/reviewer).   We can debate its value (I don't
%think it's that high), but that's %not what I'm talking about.  I a
%giving you my judgment on what will make the paper more likely to be
%accepted at NeurIPS.  As is often the case, you think you know
%better, and I don't know how to convince you.  Please talk to Spencer.
%oli21*: adding replacement and link to appendix, where the technical discussion to un-confuse the very small subset of graphical-model  readers, will be at home. 
%joe17*: I feel strongly that this should be cut.  I don't even know
%what "qualitative smeantics of any graphical model" even means.  Do
%all graphical models have qualitative semantics?  On top of that, we
%should talk about "lesser-known models".  After all the work you've
%gone through to cut material, this is an easy cut that should
%definitely be made.  (You should also cut Appendix B.  Less, in this
%case, is much more.  The appendix is not a dumping ground.) 
%We emphasize that \cref{theorem:mrf-bn-monotone-impossible} has
%implications for the qualitative semantics of \emph{any} graphical
%model (even rejecting \scibility).  
%It may be especially surprising to experts in lesser-known models
%such as dependency networks \cite{heckerman2000dependency}. 
%For a full discussion, see \cref{appendix:undirected PGMs}. 
% To readers familiar with \emph{dependency networks (DNs)} ... 
%spencer3: I agree; this at least needs more explanation, and probably
% should be cut, at least for NeurIPS.
We emphasize that \cref{theorem:mrf-bn-monotone-impossible} has implications for the qualitative semantics of \emph{any} graphical model (even if one were to reject the definition \scibility). 
%oli22: bridging; this is now appendix B.
% It may be especially surprising to experts in lesser-known models.
% such as dependency networks \cite{heckerman2000dependency}.
% For a full discussion, see \cref{appendix:undirected PGMs}.
% It may be especially surprising to experts 
%
% Some classes of graphical models appear to defy the theorem, and so it is worth looking into this carefully.
% We now address some conceptual implications of \cref{theorem:mrf-bn-monotone-impossible} 
% We reconcile \cref{theorem:mrf-bn-monotone-impossible} for two classes of graphical models: PDGs and Dependency Networks (DNs), which may at first appear to defy the theorem.
% We reconcile \cref{theorem:mrf-bn-monotone-impossible} with PDGs and Dependency Networks (DNs), which may at first appear to contradict the theorem.
\newmaterial{%
Yet some graphical models may appear to defy the theorem,
    seemingly exhibiting all three properties. 
We now reconcile \cref{theorem:mrf-bn-monotone-impossible} with 
    two such models: PDGs, and Dependency Networks (DNs). 
}%

\paragraph{Probabilistic Dependency Graphs}
PDGs capture BNs and MRFs
% \citep[Theorems 4.4 \& 4.5]{pdg-aaai} 
\cref{theorem:bns-are-pdgs,theorem:fg-is-pdg},
with a monotonic scoring function.
But keep in mind that, while the independencies and observational information can be easily separated for a BN, the same is not true of a factor graph.
% Perhaps PDGs do not capture MRFs on a purely qualitative level? 
Capturing a quantiative MRF is not the same as capturing its independencies.
The independence property exploited by the PDG inference procedure \cref{theorem:markov-property} is the same as that of a MRF with the same structure. 
The subtlety here is that the independencies exploited for PDG inference only need to be sound, not complete. 
That is, a PDG's underlying \hgraph\ $\Ar$ may well imply more independencies than the inference procedure exploits. 
    

\paragraph{Dependency Networks}
To readers familiar with \emph{dependency networks (DNs)} \citep{heckerman2000dependency},
\cref{theorem:mrf-bn-monotone-impossible} may raise some conceptual issues. 
When $G$ is an undirected graph, $\Ar_G$ is the structure of a consistent DN.
The semantics of such a DN,
which intuitively describe an independent mechanism on each \arc,
coincide with the MRFs for $G$ (at least for positive distributions). 
In more detail, DN semantics are given by the fixed point of a Markov chain that repeatedly generates independent samples along the \arc s of $\Ar_G$ for some (typically cyclic) directed graph $G$. The precise definition requires an order in which to do sampling. Although this choice doesn't matter for the ``consistent DNs'' that represent MRFs, it does in general. With a fixed sampling order, the DN is monotonic and captures MRFs, but can represent only BNs for which that order is a topological sort.



%joe7*: You're doing a core dump.  You have made no attempt to connect
%this material to the rest of the paper.  Either cut this, or 
%write some paragraphs explaining the connection.  
\subsection{QIM Equivalence}
% \subsection{SIM-Equivalence}
    \label{sec:QIM-equivalence}

We now begin to explore what it means for two hypergraphs to be \emph{equivalent},
    at least as far as QIM-compatibility (\cref{defn:qim}) is concerned.
One obvious idea is to consider two hypergraphs $\Ar_1$ and $\Ar_2$ equivalent iff they are compatible with same distributions ($\mu \models \Diamond\Ar_1$ iff $\mu \models \Diamond \Ar_2$),
    but this isn't quite right. 
We have to deal with context. 
It is easy to see that
    every \hgraph\ consisting of just one \arc\ is QIM-compatible with all probability measures $\mu \in \Delta \V\!\X$,
    % even though each \arc\ intuitively means something different.
    even though different \arc s intuitively mean different things.
Moreover, every \hgraph\ is a sum of one-arc \hgraph s,
    and we have already seen that not all \hgraph s are equivalent.
% It follows that notion of \scibility\ cannot be
%     defined inductively over unions of independent mechanisms,
%     and also that equivalence must mean something stronger. 
%
%oli2:
% In order to capture the meanings of \hgraph s more precisely, it seems
% we need a stronger notion of equivalence. 
%joe3: "honor" is the wrong word here
%To honor the the causal meanings of \hgraph s more precisely,
%joe3: I also don't know what you mean by the "causal meanings" of
%hypergraphs.  You've already said that \nu captures the causal
%meaning in some sense.  How does this relate.
%oli3: Again it seems to me that your question is confusing the quantitative and qualitative levels.  The witness \nu has causal information, but that information is not present in the hypergraph.  
% To capture the the causal meanings of \hgraph s more precisely, 
To distinguish between \hgraph s that are not interchangeable,
    we clearly need a stronger notion of equivalence.
    
%joe2*: moved here from above.  However, given that hypergraph union
%is not terribly interesting in the structural case, I don't find this
%material particularly interesting or well-motivated.  Why do we care
%about equivalence?
Given \hgraph s $\Ar_1$ and $\Ar_2$, 
we can form the combined \hgraph\ $\Ar_1 + \Ar_2$
%%joe2
%%that has disjoint union of the two sets of arcs,
that consists of the disjoint union of the two sets of \hyperarc s,
    and the union of their nodes.
%joe1*: You need to define (and discuss) hypergraph union, and the
    %notation.  (What you did above and I commented out was not enough.)
We say that $\Ar$ and $\Ar'$ are \emph{(QIM-)equivalent}
%joe1:
%I wouldn't call a hypergraph \Ar'' a context; just call it a hypergraph
%oli1: I don't understand the objection;
% what's wrong with "context"? It's clearly a hypergraph by type,
% but for the present discussion what's more important is that it represents
% a context of other independent mechanisms. We don't always refer
% to real numbers as "reals" but also use words like "offset", "position",
% "confidence", etc., to describe their role. Here, "context" is appropriate.
($\Ar \cong \Ar'$) if for every context $\Ar''$ and distribution $\mu$,  we have that
$\mu \models \Ar + \Ar''$ iff $\mu \models \Ar' + \Ar''$.
%joe1*: why does "+" mean "disjoint union".  As I said, you need to
%discuss + much more carefully.
%oli1:
% It follows immediately that this definition  is stable under disjoint union ($+$).
%
By construction, structural equivalence ($\cong$) is itself
%joe3: You haven't define "stable", and I wouldn't call the property
%that you're referring to stability.  (I think there is a standard
%name for it, although I can't recall it offhand.)
%oli3: I am looking for an intuitive word, or an appropriate technical one that doesn't need to be defined. Let me know if you think of the standard term! Might it be... invariant? 
% stable if one adds context:
invariant to additional context:
%oli1: now given earlier after definition of the hypergraph. The union is disjoint because combining -> X and -> X this way means two independent mechanisms that determine X, i.e., -> X <-. 
     if $\Ar \cong \Ar'$ then
    $\Ar + \Ar'' \cong \Ar' + \Ar''$.
%oli1:
Our next result is a simple, intuitive, and particularly useful equivalence.
%joe2*: why is it useful?  What's it useful for?
%oli2: having done many such proofs, I can say with some authority that this result is one of the most convenient tools we have for proving that various distributions are or are not \scible\ with various \hgraph s. It works especially well in tandem with monotonicity to prove looser bounds.  With some effort I can give you examples, but those examples are probably not important enough to merit space in the main body of the paper.  This proposition itself, though, I think is more than worth the space it takes up, and I suspect that by the time the paper is finished, it will be used many times in various proofs and proof sketches. 
%joe3*: While you may be right, we don't have such proofs in this
%paper.  To say it's useful, we have to show usefulness, and doing so
%would take us far beyond what this paper should be about, as far as
%I'm concerned.  I would still cut it.  It distracts from the story
%we should be trying to tell.
%oli3: you may well be right, and it may turn out to be a distraction---but I don't think so. I  think everything will tie together very nicely, if we write it properly.  If it still seems like an unimportant sideshow in a few iterations, we can cut it --- but I have plans that involve this thread. 
%joe4*: When do you expect to write it up "properly", whatever that
%means.  As it stands, it is an unimportant sideshow.     

%oli3*: gave a stronger version of the proposition, which is necessary to prove the conjecture.
\begin{prop}
        \label{prop:equiv-factorizations-cnd}
        The following \hgraph s are equivalent:
    \[
        % \vspace{-3ex}
        \begin{tikzpicture}[center base]
            \node[dpad0] (X) {$X$};
            \node[dpad0,above right=0.8em and -0.4em of X] (Z) {$Z$};
            \node[dpad0,right=1.2em of X] (Y) {$Y$};
            % \cunmergearr[arr1] {Z}{X}{Y}{0.5,.5}
            % \draw[arr1] (X) to (Y);
            \mergearr[arr1] {Z}{X}{Y};
            \draw[arr1] (Z) to (X);
        \end{tikzpicture}
        ~~\cong~~
        \begin{tikzpicture}[center base]
            \node[dpad0] (X) {$X$};
            \node[dpad0,right=0.6 of X] (Y) {$Y$};
            \node[dpad0,anchor=center] (Z) at ($(X.east)!0.5!(Y.west) + (0,0.75)$){$Z$};
            % \draw[red] (Z.center) circle (0.1);
            % \draw[blue] ($(X.east)!0.5!(Y.west) + (0,0.75)$) circle (0.12);
            % \cunmergearr[arr1] {Z}{X}{Y}{0.5,.5}
            % \draw[arr1] (X) to (Y);
            \unmergearr[arr1] {Z}{X}{Y};
        \end{tikzpicture}
        ~~\cong~~
        \begin{tikzpicture}[center base]
            \node[dpad0] (X) {$X$};
            \node[dpad0,above left=0.8em and -0.4em of Y] (Z) {$Z$};
            \node[dpad0,right=1.2em of X] (Y) {$Y$};
            % \cunmergearr[arr1] {Z}{X}{Y}{0.5,.5}
            % \draw[arr1] (X) to (Y);
            \mergearr[arr1] {Z}{Y}{X};
            \draw[arr1] (Z) to (Y);
        \end{tikzpicture}
    %    $
    % \end{enumerate}
    .
    \]
\end{prop}
These three \hgraph s correspond, respectively, to equivalent factorizations 
of a conditional probability measure
\[ 
    \def\Pr{P}
    % \def\gr#1{{\color{gray}#1}}
    % \Pr(X\gr{|Z})\Pr(Y|X\gr{,Z}) 
    %     = \Pr(X,Y\gr{|Z}) 
    %     = \Pr(X|Y\gr{,Z})\Pr(Y\gr{|Z}). 
    \Pr(X|Z)\Pr(Y|X,Z) 
        = \Pr(X,Y|Z) 
        = \Pr(X|Y,Z)\Pr(Y|Z).
\]
%\oli3: removing old proposition, for now
\commentout{
    \begin{prop}
            \label{prop:equiv-factorizations}
            The following \hgraph s are equivalent:
        \[
            % \vspace{-3ex}
            \begin{tikzpicture}[center base]
                \node[dpad0] (X) {$X$};
                \node[dpad0,right=1.1em of X] (Y) {$Y$};
                \draw[arr1] (X) to (Y);
                \draw[arr1,<-] (X) to +(-0.68,0);
            \end{tikzpicture}
            ~~\cong~~
            \begin{tikzpicture}[center base]
                \node[dpad0] (X) at (0,0) {$X$};
                \node[dpad0] (Y) at (1,0) {$Y$};
                \cunmergearr[arr1] {0.5,.8}{X}{Y}{0.5,.5}
            \end{tikzpicture}
            ~~\cong~~
            \begin{tikzpicture}[center base]
                \node[dpad0] (X) {$X$};
                \node[dpad0,right=1.1em of X] (Y) {$Y$};
                \draw[arr1] (Y) to (X);
                \draw[arr1,<-] (Y) to +(0.68,0);
            \end{tikzpicture}
        %    $
        % \end{enumerate}
        .
        \]
    \end{prop}
    %joe1: The "These" needs names, and "correspond" needs to be explained.
    %oli1: Ok. I played with a version that expanded on "correspond",
    % but I think it made things muddier rather than cleaner. Can you help
    % me see where the ambiguity is, or why it might be hard to follow?
    %
    % These are equivalent BN structures as well, and correspond
    These three \hgraph s also arise from equivalent BN structures.
    As such, they correspond, respectively, to equivalent factorizations 
    %oli1: moved up from above
    of a probability measure
    \[ \Pr(X)\Pr(Y|X) ~=~ \Pr(X,Y) ~=~ \Pr(X|Y)\Pr(Y). \]
}
%
%joe2*: downplaying this.  I'm really concerned this material and the
%section on monotonicity is out of place.  It's preventing us from
%getting to more interesting material on causal models.
%oli2: now the causal models are first, so it's no longer standing
% in the way of that material. It may still need to be downplayed, 
% but I want to keep it for now.  I've put your replacement text
% commented out below. 
%
\commentout{
%joe2
We conjecture that, in a sense, \cref{prop:equiv-factorizations}
characterizes when two qualitative BNs satisfy the same set of
independencies.  More specifically, we conjecture that if 
two qualitative Bayesian Networks describe the same set of
independencies, then they can 
be provd equivalent using only instances of \cref{prop:equiv-factorizations}.
%oli2: I'm happy to use this text if the conjecture remains a conjecture,
% but I also think it should be provable, independently interesting, and
% immediately justify the claims above that the Proposition is useful.
% I don't yet see the proof for the general case, but I'd like to show 
% you why it's true for some subclasses of BNs, and see if you have any
% ideas for how to prove the general case!
}
%oli2: reinstated old material, for now...
%joe3*: I continue to stand by what I said above, but I don't know how
%to convince you that what you wrote makes things worse, not better
%(unless you can prove the theorem).
%oli3: I proved the theorem! Or, more accurately, I figured out that somebody else had proved a slight variant of it. 
%oli3: to make proposition correct...
% \cref{prop:equiv-factorizations}
\cref{prop:equiv-factorizations-cnd}
%oli3: shortening and combining
% provides a simple yet useful axiom system for for deriving relationships between the structural meanings of different \hgraph s. 
provides a simple and useful way to relate \scibility\ of different \hgraph s. If we restrict to acyclic structures, for instance, we find: 

%oli3: promoting to theorem, and adding attribution b/c apparently the result is known
% \begin{conj}[completeness for BN structures]
%joe5*: If Max's statement is different from this, you have to say
%something more about the difference, and why our current approach
%makes things easier to state.  Just saying that it does is not good enough.
\begin{theorem}[\citealt{chickering-equiv-bns}]
% \begin{theorem}[alternate statement; proof due to \citealt{chickering-equiv-bns}]
        \label{theorem:bn-completeness}
    Any two qualitative Bayesian Networks that 
    %oli3:
    % describe the same set of independencies
    represent the same independencies
    can be proven equivalent using only instances of \cref{prop:equiv-factorizations-cnd}
    %oli3: to be precise, adding:
    (in which $X$, $Y$, $Z$ may be sets of variables).
% \end{conj}
\end{theorem}

\Cref{theorem:bn-completeness} 
is essentially a restatement of
% the one given by \citet{chickering-equiv-bns}, 
%joe4: titular?  That seems like the wrong word.
%oli5: because I'm referring to the result that's referenced in the title of that work. But I can be less precise and just write "main"
% titular result of \citet{chickering-equiv-bns}, 
main result of \citet{chickering-equiv-bns}, 
% but it can be stated more simply in the language of \hgraph s.
but it is simpler to state in terms of directed hypergraph equivalences.
%oli9:
To state the result in its original form, one has to first
define an edge $X \to Y$ to be \emph{covered} in a graph $G$ iff $\Pa_G(Y) = \Pa_G(X) \cup \{X\}$;
then, the result states that all equivalent BN structures are related by a chain of reversed covered edges. 
Observe that this notion of covering is implicit in \cref{theorem:bn-completeness}.
% \begin{enumerate}
%     \item 
%     \item 
% \end{enumerate}
% This characterization of BN equivalence, 
\Cref{theorem:bn-completeness} is one demonstration of the usefulness of 
\cref{prop:equiv-factorizations-cnd},
but the latter applies far more broadly, to cyclic structures
and beyond. 
It becomes even more useful in tandem
    %oli6:
    % an analogue of implication, which we describe next. 
    with the definition of monotonicity presented in the 
    previous subsection (\cref{sec:qual-monotone}).
    
%oli2: note that the material below, which you previously grouped with the material above and commented out, is a segue to the material on monotonicity.  Whether or not we include it is independent of whose approach we take to describing the conjecture above. 
%oli3: ... and I think it's not all that valuable. I wrote a shorter transition above.
\commentout{
    Suppose we want
        to know that \scibility\ with $\Ar$ suffices to guarantee compatibility with $\Ar'$, but are not interested in the converse.
    % Is there a one-sided analogue of structural equivalence?
    As we will see in the next section, there is such a notion, and \cref{prop:equiv-factorizations-cnd} becomes even more useful in tandem with it.
}


% \section{QIM }


\section{A Logic Based on PDGs}
    \label{sec:pdg-logic}

% So far in this chapter, we have treated the two aspects of PDGs separately, and developed separate reasoning principles for each aspect. 
\newmaterial{%
In the previous section (\cref{sec:qual-monotone}) we developed a 
    monotonicity principle that applies to directed structural information.
Recall this was inspired by an analogous reasoning principle for observational information (\cref{lemma:!}).
}%
    % that quantitative inconsistency 
    % the proof language based on monotonicity that powered 
    % seen that there is a natural notion of monotonicity
    % in the observation
% analogue of monotonicity for structural information in a hypergraph, 
In this section, we put the two together. 
The result is the beginning of an inconsistency-tolerant logic. 


% \subsection{PDG Entailment: A Natural Preorder on PDGs}
\subsection{A Natural Preorder on PDGs}

Some logical statements are stronger than others. If $\varphi$ says strictly more about the world than $\psi$, then we say that $\varphi$ implies $\psi$, and write $\varphi \Rightarrow \psi$.  
% An important aspect of implication is that it is \emph{context-free}: any implication can be turned into 
We are interested in developing a similar notion of implication for PDGs.
It might seem that \cref{lemma:!} provides us with one---but simply because $\aar{\dg M_1} \ge \aar{\dg M_2}$ doesn't seem to be strong enough. 
%
A key feature implication (or equivalence) is that it allows for in-context substitution.
For instance, if $\varphi \Rightarrow \psi$, then 
$\varphi \land \varphi' \Rightarrow \psi \land \varphi'$
and
$\varphi \lor \varphi' \Rightarrow \psi \lor \varphi'$, no matter what we take $\psi'$ to be.
But the analogue is not true for PDGs. 
This motivates the following definition, which corrects for this by explicitly taking into account context. 

\begin{defn}
    \label{defn:entail}
    We say that $\dg M_1$ \emph{entails} $\dg M_2$
    and write $\dg M_1 \models \dg M_2$
    % $\dg M_1$,
    % to indic
    % to indicate that a
    iff
    \[
        \forall \gamma \ge 0.~
        \forall \dg M'.\quad \aar{\dg M_1 + \dg M'}_{\gamma}
            \ge \aar{\dg M_2 + \dg M'}_{\gamma}.
        \qedhere
    \]
\end{defn}

    % Treating both PDGs as black boxes, so that we can add extra stuff to them, but all we can measure is how inconsistent they are, 
    Let's imagine that we can interact with a PDG only by altering it, and by measuring its degree of inconsistency.
    % We will later justify this perspective by showing that 
    $\dg M_1 \models \dg M_2$ means $\dg M_1$ is a ``stronger statement'': in every context (so long as it's the same context added to $\dg M_1$ and $\dg M_2$), $\dg M_1$ has is at least as inconsistent as $\dg M_2$. 
    It may be helpful to think of the extremes: 
        % the empty PDG, which we will denote $0$, 
    an outright contradiction, which has infinite inconsistency no matter the context, entails everything.  Here are a few more examples. 
    
    
    % 
    % \bigskip
    % Are these definitions appropriate? Some examples:
    % 
    \begin{linked}{prop}{entailment}
    \begin{enumerate}[parsep=0pt,itemsep=0.2ex]
        \item Marginals are entailed by joint distributions, e.g., $p(X,Y) \models p(X)$.
        \item Weaker constraints are entailed by larger ones, e.g., $(XY {=}xy) \models (X{=}x)$. 
    \end{enumerate}   
    \end{linked}
    % I believe the following are true (although I need to come back and verify this):
    
    % Some more properties of the entailment relation, for general PDGs.
     % properties of the entailment relation, for general PDGs.
    % The entailment relation has  for general PDGs. 
    More generally, applied to arbitrary PDGs, our entailment relation has some intuitive properties. 
    
    \begin{linked}{prop}{entail-properties}
        \begin{enumerate}[parsep=0pt,topsep=0pt]
            \item Reflexivity: $\dg M \models \dg M$.
            \item Transitivity: $\dg M_1 \models \dg M_2$ and $\dg M_2 \models \dg M_3$ imply that $\dg M_1 \models \dg M_3$. 
            \item If 
                % $\dg M_1 \supseteq \dg M_2$
                $\Ar_1 \supseteq \Ar_2$
                 or if $\bbeta_1 \ge \bbeta_2$ but otherwise the two are identical, then $\dg M_1 \models \dg M_2$. 
            \item The same is true if $\dg M_1$ and $\dg M_2$ are identical except for
            $\balpha_1 \ge \balpha_2$.%
                % \footnote{
                %     % {\color{gray}\small (
                % So far, this result only holds for PDGs with discrete variables;
                % It is not true true for the approach I have been using to
                % formalize $\SDef$ for continuous variables.
                %     % )}
                % }
                % The way this works with $\balpha$ depends on which
                % way formalizations go, but using the default one with entropy, $\balpha_1 \ge \balpha_2$ also implies $\dg M_1 \models \dg M_2$. 
        \end{enumerate}
    \end{linked}
    
    
    If both $\dg M_1 \models \dg M_2$ and $\dg M_2 \models \dg M_1$, 
        then we say the two PDGs $\dg M_1$ and $\dg M_2$ are \emph{equivalent}, and write $\dg M_1 \equiv \dg M_2$.
    Two PDGs are eqivalent if they respond the same way to all probes $\dg M'$ and all choices of $\gamma$.
    Intuitively, $\dg M_1 \equiv \dg M_2$ iff $\dg M_1$ and $\dg M_2$ are equally (in)consistent no matter how one alters them, so long as both are altered in the same way. 
    
\TODO[ TODO: introduce $(2) \dg M$ if not defined]
    
\begin{linked}{prop}{sem-equiv-properties}
    \begin{enumerate}[parsep=0pt]
        \item For all distributions $p(X)$, $p(X) + p(Y|X) \equiv p(X,Y)$. 
        \item If $\dg M_1 \equiv \dg M_2$, then their mixtures are also equivalent:
            $\dg M_1 \equiv (1-\alpha) \dg M_1 + (\alpha) \dg M_2 \equiv \dg M_2$. 
        \item $\dg M + \dg M \equiv (2) \dg M$. 
            Compared to $\dg M$, the PDG $\dg M + \dg M$ contains two copies of each arc, while $(2) \dg M$ has all confidences doubled. 
    \end{enumerate}
\end{linked}

% Taking only the binary aspect of this logic into account, we might want
This last property is rather interesting, because it shows that the set-of-distribution semantics $\SD{-}$ of a PDG does not capture important distinctions between PDGs. 
If $\SD{\dg M_1} \subseteq \SD{\dg M_2}$, then one might hope to find that
$\dg M_1 \models \dg M_2$---but this is not true.%
    \footnote{For instance, $\SD{\dg M} = \SD{(2) \dg M}$, but $\dg M \not\models (2) \dg M$.}
% but it is the case that
% but it is true, in this case, that
% $(\infty) \dg M_1 \models (\infty) \dg M_2$. 
    

From its definition, it is clear that $\equiv$ is symmetric.
Thus transitivity and reflexivity of $\models$ (above), imply that
$\equiv$ is an equivalence relation. Moreover, it is the same equivalence relation as equality of scoring function semantics!

\begin{linked}{theorem}{sem-equiv}
    $\dg M_1 \equiv \dg M_2$ iff $\bbr{\dg M_1} = \bbr{\dg M_2}$.
    % \footnote{More explicitly, these two objects are functions of 
    %     $\gamma$, and equal iff $\bbr{\dg M_1}_\gamma = \bbr{\dg M_2}_\gamma$
    %     for all $\gamma \ge 0$.}
    That is, $\dg M_1 \equiv \dg M_2$ if and only if 
    $\dg M_1$ and $\dg M_2$ have the same set of variables $\X$, and
    $\bbr{\dg M_1}_\gamma(\mu) = \bbr{\dg M_2}_\gamma(\mu)$
    for all $\gamma \ge 0$ and $\mu \in \Delta\V\!\X$.
\end{linked}    


It is worth stressing an implication of this theorem: equivalent PDGs always have the same set of variables.
In some ways this makes perfect sense: postulating the existence of $X$ (or forgetting about $X$) does in fact distinguish your mental state, even if you know nothing about $X$. 
However, it may also be too strict a requirement.
% Here are some reasons why, and some alternative weaker notions of congruence that may address them.
% \begin{itemize}
    % \item 
    When $\dg M_1 + \X_2 \equiv \dg M_2 + \X_1$,
    we call $\dg M_1$ and $\dg M_2$ \emph{congruent modulo variables}, and write $\dg M_1 \cong_\X \dg M_2$.
    This definition 
    % places the two PDGs on the same set of variables before comparison.
    effectively ensures that the two PDGs have the same set of variables before comparing them. 
    All PDGs with $\balpha = \bbeta = 0$ are congruent to the empty PDG, modulo variables. 
    % 
    % \item 
    % A somewhat orthogonal concern is that variable identities. 
    % If $\dg M_1$ and $\dg M_2$ are identical except that their variables have different names,
    % then we don't really want them to be distinct. 
    % We call $\dg M_1$ and $\dg M_2$ \emph{isomorphic} iff
    % there exists a bijection $\iota : \X_1 \to \X_2$ and an indexed family of bijections $\{\psi_{X} : \V(X) \to \V(\iota(X)) \}_{X \in \X_1}$ such that 
    % 
% \end{itemize}



We can also add entailments together.

\begin{linked}{prop}{entail-sum}
    If $\dg M_1 \models \dg M_3$ and $\dg M_2 \models \dg M_4$, then
    $\dg M_1 + \dg M_2 \models \dg M_3 + \dg M_4$.
\end{linked}
\begin{lproof}\label{proof:entail-sum}
    % Suppose  $c_1, c_2 > 0$, $\dg M_1 \models 0$, and $\dg M_2 \models 0$.
    % Then $\forall \dg M''$, we have $\aar{\dg M_1 + \dg M''}_\gamma \ge \aar{\dg M''}$. 
    % In particular, for any $\dg M'$, we can set $\dg M'' := \dg M' + \dg M_2$, 
    % and thus
    % \[
    % \aar[\big]{\dg M_1 + \dg M_2 + \dg M'}_\gamma ~~\ge~~ \aar[\big]{\dg M_2 + \dg M'}_\gamma
    %     ~~\ge~~ \aar[\big]{\dg M'}_\gamma
    % .3
    % \]
    Suppose $\dg M_1 \models \dg M_3$, and $\dg M_2 \models \dg M_4$.
    Then $\forall \dg M''$, we have $\aar{\dg M_1 + \dg M''}_\gamma \ge \aar{\dg M_3 + \dg M''}$. 
    In particular, for any $\dg M'$, we can set $\dg M'' := \dg M' + \dg M_2$, 
    and thus
    \[
    \aar[\big]{\dg M_1 + \dg M_2 + \dg M'}_\gamma ~~\ge~~ \aar[\big]{\dg M_3 + \dg M_2 + \dg M'}_\gamma
        ~~\ge~~ \aar[\big]{\dg M_3 + \dg M_4 + \dg M'}_\gamma
    .
    \qedhere
    \]
\end{lproof}

% \temphide{%
%     One way of defining subtraction is to add the negated second argument.
%     The biggest unintuitive aspect of this is that $\dg M - \dg M$ is not equivalent to the empty PDG; the variables are still there. 
%     (But they are congruent modulo variables.)
%     One might also consider a variant that deleted variables, but it seems
%     impossible to do this continuously and invertably.
%     Do we want $\dg M - \X$ to be the trivial PDG?

%     \begin{coro}
%         If $\dg M_1 \models \dg M_2$, then $\dg M_1 - \dg M_2 \models \X_2$.
%     \end{coro}
%     % To further specialize, choosing $\dg M_1 = \dg M_2$, the premise is always true,
%     % and so we find that $\X \models
% }%

\begin{coro}
    If $\dg M_1 \models 0$ and $\dg M_2 \models 0$, then $\dg M_1 + \dg M_2 \models 0$. 
    This is because $0$ is an idempotent PDG:  $0 \equiv 0+ 0$. 
\end{coro}

% 
% \paragraph{\color{red} Trivial PDG.}
Thus the PDGs that entail the trivial PDG are closed under addition. 
Do all PDGs have this property? 
As a point of comparison, in propositional logic, every proposition entails true. 
So, in this setting, is it the case that $\dg M \models 0$ for all $\dg M$---that is, do we have
% \[
$
\aar{\dg M + \dg M'}_\gamma \ge \aar{\dg M'}_\gamma
$
for all $\dg M$ and $\dg M'$?
% This is certaintly true if $\dg M$ has $\balpha,\bbeta \ge 0$.  
It might seem that this has to be the case, because it is so closely related to 
    \cref{lemma!}. Yet,  in general, the answer is no.
Nevertheless, it does hold for PDGs $\dg M$ that have $\bbeta \ge 0$ and are in a sense ``qualitatively complete'' 
% (definition \cref{defn:qual-complete} below),
which includes many PDGs of interest. 
% But it may not be true for PDGs $\dg M$ that have insufficient qualitative edges
%     (or those that have negative confidences).
    
\begin{defn}\label{defn:qual-complete}
    A weighted hypergraph $(\Ar, \balpha)$ is \emph{qualitatively complete}
    iff its structural deficiency is non-negative.
    More precisely: $(\Ar, \balpha)$ is qualitatively complete iff
    for all interpretations $\X$ of its nodes as variables,
    all and joint distributions $\mu \in \Delta\V\!\X$, 
    the structural deficiency $\SDef_{(\Ar, \balpha)}(\mu) \ge 0$
    of $\mu$ with respect to $(\Ar, \balpha)$ is non-negative. 
\end{defn}


\temphide{%

\begin{linked}{prop}{qual-complete-criterea}
Let $\dg M$ be a PDG with structure $(\Ar,\balpha)$,
    and observational confidences $\bbeta \ge \mathbf 0$.
Then the following are equivalent:
\begin{enumerate}[parsep=0ex]
    \item $(\Ar, \balpha)$ is qualitatively complete. 
    \item $\dg M \models 0$.
    \item 
    $\balpha$
    can be written as a mixture of BNs plus a positive residual.
    That is, there exist BN structures $B_1, \ldots B_n$ 
    (which correspond to vectors $\balpha_1, \ldots, \balpha_n$), mixture coefficients
    $w_1, \ldots w_n \in [0,1]$ with $\sum_{i=1}^n w_i = 1$, and a vector
    $\balpha' \ge \mathbf 0$ such that $\balpha = \balpha' + \sum_{i=1}^n w_i \,\balpha_i$. 
\end{enumerate}
\end{linked}


% I suspect that such PDGs form a cone with vertex $0$ somehow.
% If $c_1, c_2 > 0$, $\dg M_1 \models 0$, and $\dg M_2 \models 0$, then
% is it true that $c_1 \dg M_1 + c_2 \dg M_2 \models 0$?  To investigate,
% we must look into scalar multiplication. 


We can scale a PDG $\dg M$ by $r \in \mathbb R$, by multiplying
all of its constituent confidences $\balpha$ and $\bbeta$ by $r$. 
Concretely, this means $\dg M' = (r) \dg M$
has confidences $\beta_a' = r \cdot \beta_a$ and $\alpha_a' = r \cdot \alpha_a$,
where $\beta_a$ and $\alpha_a$ are the confidences of arc $a$ in $\dg M$. 
Can we get a result similar to \cref{prop:entail-sum}
    for scalar multiplication? 
No, we cannot.

\begin{falsity}
    If $\dg M_1 \models \dg M_2$ then $(r) \dg M_1 \models (r) \dg M_2$, 
    for all $r \ge 0$. 
\end{falsity}

For when $r=0$, this means 
But here are some variants that might be true:

\begin{conj}
    \begin{enumerate}
        \item If $\X_1 \subseteq \X_2$ and $\dg M_1 \models \dg M_2$, then
        $(r) \dg M_1 \models (r) \dg M_2$ for all $r \ge 0$. 
        \item If $\dg M_1 \models \dg M_2$ then
        $(r) \dg M_1 \models (r) \dg M_2$ for all $r \ge 1$. 
    \end{enumerate}
\end{conj}
  
}%
% Why is this? 
% Adding arcs or confidences 
% Part of the reason 
% Introducing variables has the opposite effect.
While new beliefs are like constraints and cannot make you any less inconsistent, 
new \emph{variables} do just the opposite: they give additional degrees of freedom, and
cannot make you any more inconsistent. 
\begin{prop}
    If $\X_1 \subseteq \X_2$, then $\X_1 \models \X_2$. 
\end{prop}

It follows, for example, that the empty PDG entails any PDG that only contains variables, 
    i.e., $0 \models \X$ for any set of variables $\X$. 
(It might be worth noting that, as PDGs, $\X_1 + \X_2 = \X_1 - \X_2  = \X_1 \cup \X_2$ are all the same.)
% If $\X_1 \subseteq \X_2$, it is also perhaps interesting that
% $\X_1 - \X_2$ is 


The bottom (or initial) element(s) of the entailment order are easily identified;
like in propositional logic, they are the (most extreme) contradictions.
Every such PDG is equivalent to
$\dg{\False} := \begin{tikzpicture}
    \node[dpadinline] (X) {$X$};
    \draw[arr1, <<-] (X) to node[pos=0.7,inner sep=1pt, above]{$\delta_{0}$} + (-1,0);
    \draw[arr1, <<-] (X) to node[pos=0.7,inner sep=1pt, above]{$\delta_{1}$} +(1,0);
\end{tikzpicture}$,
which has $\aar{\dg{\False}}= \infty$, and satisfies $\False \models \dg M$ for all $\dg M$. 

Are there largest PDGs in this order?  What is the analogue of ``true''? Is it the empty PDG? 
Clearly not, because $\dg M \models 0$ only if $\dg M$ is qualitatively complete. 
It turns out there is such a ``most anodyne'' PDG if and only if
the collection of all possible variables is a PDG.
Concretely, let $\mathcal U$ be a ``universe of all possible variables''.  
Then every PDG $\dg M$ with variables $\X \subseteq \mathcal U$ satisfies $\dg M \models \mathcal U$. 



% \subsection{``Propositional'' PDG Logic}


% We now use the preorder defined in the previous section to build up an inconsistency-tolerant analogue of propositional logic. 
Starting with the entailment operation and introducing usual connectives, we 
    can obtain a propositional logic whose primitive formulas are PDGs.
We adopt the usual convention of writing 
``$\models \varphi$'' if $\dg M \models \varphi$ for all PDGs $\dg M$. 
% Observe that something nice happens: 
% \begin{align*}
%     0 \models \varphi \iff \models \varphi
% \end{align*}

\commentout{%
\textbf{Exploring the Set-of-Distribution Semantics.}
% In a sense, PDGs themselves already encode a set of probabilistic models: the 
% collection $\SD{\dg M}$ of distributions consistent with them. In many
% ways, this seems like the intension of a formula formula.  
Because $\SD{-}$ is a feature of just the observational/quantitative
half of a PDG, let's ignore $\SDef$ for now, and assume $\gamma = 0$. 

Taking only the binary aspect of this logic into account, we might want
it to be the case that if $\SD{\dg M_1} \subseteq \SD{\dg M_2}$, then
$\dg M_1 \models \dg M_2$. This is not true,%
    \footnote{For instance, $\SD{\dg M} = \SD{(2) \dg M}$, but $\dg M \not\models (2) \dg M$.}
% but it is the case that
but it is true, in this case, that
$(\infty) \dg M_1 \models (\infty) \dg M_2$. 
Similarly, if 
}%


% \subsubsection{Defining Classical Connectives}
\paragraph{Adding Classical Connectives.}
% \textbf{Approach 1.}
As usual, we can introduce conjunction by
    defining $\dg M \models \varphi \land \varphi'$ iff
    $\dg M \models \varphi$ and $\dg M \models \varphi'$.

\TODO
    
So, for example, 
    $\dg M_1 \models (\dg M_2 \Rightarrow \dg M_3)$
    states that if $\dg M_1 \models \dg M_2$, then $\dg M_1 \models \dg M_3$. 
    %
    % The equivalent of ``no assumptions'' or true, as discussed above,
    % is the universe $\mathcal U$ of all variables.
    % These definitions let us 
    This lets us prove things like
    \[
        \models p(X,Y) \Leftrightarrow p(X) + p(Y|X)
        \qquad
        \text{and}
        \qquad
        \models (p(X) + q(X)) \Rightarrow p(X).
    \]
We can also continue with $\lnot$ and then define other connectives
    ($\lor, \Rightarrow, \Leftrightarrow$) in terms of ($\land, \lnot$).    
But the usual definition of $\dg M \models \lnot \varphi$ iff 
    $\dg M \not\models \varphi$,
    is not so well-behaved.
% For instance, ...
We know for example that $ \dg M \not \models (2) \dg M$ (at least, when $\balpha,\bbeta > \mat 0$), 
yet it seems bizarre to say that $\dg M \models \lnot (2) \dg M$: the two PDGs say the same thing, just with different strengths! 

    
% \textbf{Approach 2.}
\def\entimp{\overset{\smash{\!\!\bullet}}\Rightarrow}
Often outside of classical logic, it is more natural to start with 
    implication instead of negation.
Here is an alternate conception of implication that more closely
    follows the intuition we had for entailment.
% We might to say something along the lines of
Define
\[ \dg M \models \varphi \entimp \varphi' \qquad\text{if and only if}\qquad
    \dg M + \varphi \models \dg M + \varphi'. \] 
In words: a formula $\varphi$ implies another formula $\varphi'$ in context $\dg M$ 
    iff $\varphi$ with context $\dg M$ entails $\varphi'$ with context $\dg M$. 
One nice proprerty of this definition: $\dg M_1 \models \dg M_2$ becomes
    equivalent to $\models \dg M_1 \entimp \dg M_2$.
%
This definition has a very significant drawback, however:
it only makes sense if $\varphi$ and $\varphi'$ are themselves PDGs (or at least,
    can be added to $\dg M$ to form a new PDG). 
This would rule out
    ``synthetic'' logical connectives such as $\land$ and $\lnot$
    inside an implication.
Thus, the logic inside a connective $\entimp$ collapses to ordinary constructions on PDGs themselves.        

\temphide{%
    
    PDGs also have their own operations, most notably $+$ and $\sqcup$.
    One interesting question is how these interact with the logical connectives we have just defined. 
    Here are some interactions between connectives $\land, \lor, \lnot$, and PDG operations:
    \begin{linked}{conj}{pdg-connective-interactions}
    \begin{enumerate}[parsep=0pt]
        \item If $\dg M \models 0$, and $\dg M \models \dg M_1 + \dg M_2$,
        then $\dg M \models \dg M_1 \land \dg M_2$. 
        \item $\dg M_1 + \dg M_2 \models \dg M_1 \sqcup \dg M_2$ 
        \item   
        % A consequence of \cref{prop:entail-sum}:
            For all $\alpha\in [0,1]$, we have:
            $\models (\dg M_1 \land \dg M_2) \Rightarrow ((1-\alpha)\dg M_1 + (\alpha)\dg M_2)$. 
    \end{enumerate}
    \end{linked}

    \temphide{%
    \begin{lproof}[Exploration]\label{proof:pdg-connective-interactions}
    \begin{enumerate}[parsep=0pt]
        \item 
        \item 
        \item Suppose $\dg M \models \dg M_1 \land \dg M_2$. 
        Then $\dg M \models \dg M_1$ and $\dg M \models \dg M_2$.
        % So by \cref{prop:entail-sum}, 
        Thus
        % \[ \dg M \equiv \dg M + \dg M \models \dg M_1 + \dg M_2 \]
    \end{enumerate}
    \end{lproof}
    }%

\TODO
    
\subsection{Epistemic Logic}
            
    Given a PPDG $\dg M(\Theta)$, 
    and a set $I$ of agents, each of which has an attention mask
        $\mathit{Attn}_i(\theta) : 2\Ar \to \mathbb R$, 
        we can define the local state of agent $i$ at $\theta$ to be $\dg M_i(\theta) := \mathit{Attn}_i(\theta) \odot \dg M(\theta)$. 
    If it's always the case that $\dg M_i(\theta) \equiv \dg M(\theta)$ (which happens iff $\mathit{Attn}_i(\theta)(a) = 1$) we say that $i$ has uniform attention over $\dg M$.   
    We can form a Kripke structure
    with states $\Theta$, and accessibility relation 
    % $\mathcal K_i \subseteq \Theta^2$
    $\theta  \sim_i \theta'$ iff $\dg M_i(\theta) \equiv \dg M_i(\theta')$.
    Because $\equiv$ is an equivalence relation, the resulting logic satisfies S5. 
    We can also take primitive propositions $\Phi$ to be the set of PDGs, using the entailment relation.

% In the service of clarity about the types, we distinguish the PPDG $\dg M(\Theta)$ from $\dg M\& \Theta$, its translation to a PDG by include each $\Theta$ as a variable.    
% {\color{gray}Equivalently, rather than thinking in terms of PPDGs, 
% we can think in terms of a set $\mathbf W$ of ``world variables'',
% by converting $\dg M(\Theta)$ to a PDG with the inclusion of variables $\{\Theta_a \}_{a \in \Ar} =: \mathbf W$.
% }
% Either way, we can then build an epistemic logic. 
% If $\dg M(\Theta)$,
% which we sometimes abbreviate
% \TODO

Some basic properties of this logic:
\begin{prop}
    \begin{enumerate}[parsep=0pt]
    \item $ (\dg M, \theta, I) \models \dg M(\theta)$.
    \item For all agents $i \in I$, 
        $(\dg M, I) \models K_i ( \dg M_i(\theta) )$, i.e., each agent
            knows its local state. 
    \item In particular, if agent $i$ has uniform global attention over $\dg M$, then 
        $(\dg M, \theta, I) \models K_i (\dg M(\theta))$.
    \item Introducing variables,  have $(\dg M, I) \models K_i (\dg M \& \Theta)$.
    \item Moreover, the PPDG is common knowledge: $(\dg M, I) \models C_{I} (\dg M \& \Theta)$.
    \end{enumerate}
\end{prop}


Now some other examples. 

\begin{example}
    There are two agents, $I = \{1,2\}$. Each has its own belief
    about a variable $X$, and can only see that belief. 
    We can't say anything interesting here, because the beliefs
    of the agents don't interact.
    
    Now, suppose both agents observe $X=x$. That is, $\dg M_1 = \{p, x\}$
    and $\dg M_2 = \{q, x\}$. Then  $(\dg M, I) \models C_{\{1,2\}}(X=x)$. 
    
    Also, $(\dg M, I) \models C_{\{1,2\}}( (\dg M_1 - \dg M_2) \lor (\dg M_2 -\dg M_1))$.
    In English: it's common knowledge that one of the two is more inconsistent than the other.
    % Now, suppose the value of $x$ is parameterized, 
\end{example}

}%

\section{Discussion}

We have seen natural qualitative analogues of monotonicity and equivalence (\cref{sec:qual-monotone,sec:QIM-equivalence}). 
Our key contributions in this section are a pair of reasoning principles that can prove phenomena such as BN equivalence (\cref{theorem:bn-completeness}), and help us begin to make sense of cyclic structures from the opposite direction as studied in \cref{sec:qim-info}. 
We also derive an impossibility result (\cref{theorem:mrf-bn-monotone-impossible}) showing that, while undireted and directed models are both monotonic in a way, there is no way to monotonically capture both at the same time.
We then developed general notions of PDG entailment and equivalence (\cref{sec:pdg-logic}), and began to use them as the basis of a logic.


We still have a great deal of work ahead of us. 
One particularly exciting direction is that of epistemic logic, which appears to be implementable through parametric PDGs and focus/attention masks as laid out in \cref{chap:LIR}. 
%
Another is that of linear logic. Since this logic is sensitive to quantities, and there seem to be multiple kinds of conjunctions, it would make sense if there were a deep relationship there as well.
While we have only skimmed the surface of what can be done with these reasoning principles, it is clear that PDGs can be a useful visual proof language, and have promise as the basis of an inconsistency-tolerant logic. 

\begin{subappendices}
    
    \section{Proofs}

    \begin{lemma}
        $\bbr{\dg M_1 + \dg M_2}_\gamma =
            \bbr{\dg M_1}_\gamma(\X_1) + \bbr{\dg M_2}_\gamma(\X_2) 
                + \gamma \I_\mu(\X_1; \X_2| \X_1 \cap \X_2).$
    \end{lemma}
    This was an intermediate result in \hyperref[proof:theorem:markov-property]{the proof of} \cref{theorem:markov-property}.

    % \begin{defn}
    %     If $\dg M_1 + \dg M_2 \equiv \dg M_1 \sqcup \dg M_2$, then 
    %     we say $\dg M_1$ and $\dg M_2$ are independent. 
    % \end{defn}
    % \begin{lemma}
    %     If $\dg M_1$ and $\dg M_2$ have disjoint variables $\X_1 \cap \X_2 = \emptyset$, 
    %     then $\aar{\dg M_1 + \dg M_2} = \aar{\dg M_1} + \aar{\dg M_2}$. 
    % \end{lemma}

    \begin{lemma}
            \label{lem:commonvar-dominance}
        If $\dg M_1$ and $\dg M_2$ are such that,
            for all pairs of distributions 
            $\mu_1 \in \Delta\V\!\X_1$ and $\mu_2 \in \Delta\V\!\X_2$
            that have the same marginal on their common variables $\X_1 \cap \X_2$, it is the case that
        \begin{align*}
            \OInc_{\dg M_1}(\mu_1) \ge \OInc_{\dg M_2}(\mu_2)
                \qquad\text{and}\qquad
            \SDef_{\dg M_1}(\mu_1) \ge \SDef_{\dg M_2}(\mu_2),
        \end{align*}
        then $\dg M_1 \models \dg M_2$. 
    \end{lemma}
    \begin{lproof}
        Suppose that $\dg M_1$ and $\dg M_2$ satisfy the property of the lemma statement.
        In this case, we find that $\bbr{\dg M_1}_\gamma(\mu_1) \ge \bbr{\dg M_2}_\gamma(\mu_2)$,
        because
        \begin{align*}
            \bbr{\dg M_1}_\gamma(\mu_1)
                &= \OInc_{\dg M_1}(\mu_1) + \SDef_{\dg M_1}(\mu_1) \\
                &\ge \OInc_{\dg M_2}(\mu_2) + \SDef_{\dg M_2}(\mu_2) = \bbr{\dg M_2}_\gamma(\mu_2).
        \end{align*}
        % This shows us that
        Let $\dg M'$ be an arbitrary PDG, with variables $\X'$.
        With some calculation, we find that 
        \begin{align*}
            &\aar[\big]{\,\dg M_1 + \dg M'\,}_\gamma \\
                &= \inf_{\mu \in \Delta\V(\X_1 \cup\X')}\, \Big( 
                    \bbr{\dg M}_\gamma(\mu(\X)) + \bbr{\dg M'}_\gamma(\mu(\X'))
                    + \gamma \I_\mu(\X_1 ; \X' | \X_1 \cap \X')
                \Big) \\
                &= \inf_{\mu \in \Delta\V(\X_1 \cup \X' \cup \X_2)}\, \Big( 
                    \bbr{\dg M_1}_\gamma(\mu(\X_1)) + \bbr{\dg M'}_\gamma(\mu(\X'))
                \Big) \\
                &\ge \inf_{\mu \in \Delta\V(\X_1  \cup \X'  \cup \X_2)}\, \Big(
                    \bbr{\dg M_2}_\gamma(\mu(\X_2)) + 
                    \bbr{\dg M'}_\gamma(\mu(\X'))
                \Big) \\
                &= \inf_{\mu \in \Delta\V(\X'  \cup \X_2)}\, \Big(
                    \bbr{\dg M_2}_\gamma(\mu(\X_2))  + 
                    \bbr{\dg M'}_\gamma(\mu(\X'))
                    + \gamma \I_\mu(\X_2 ; \X' | \X_2 \cap \X')
                \Big) \\
                &= \inf_{ \mu \in \Delta\V(\X_2  \cup \X')} \bbr{ \dg M_2 + \dg M' }_\gamma(\mu) \\
                &= \aar[\big]{\,\dg M_2 + \dg M'\,}_{\gamma}.
        \end{align*}
        We can freely remove the term $\I_\mu(\X;\X'|\X\cap \X')$ inside the infemum (line 1), because this quantity
        is minimized when $\X$ and $\X'$ are conditionally independent given their common variables, and we can always find a distribution with this property that has the same marginals on $\X$ and $\X'$.
    \end{lproof}

    \recall{prop:entailment}
    \begin{lproof}\label{proof:entailment}
    \textbf{Marginals.}
       Implicitly, we have converted $p(X,Y)$ and $p(X)$ to PDGs 
        by assigning them a default value of $\alpha=1$. 
    One can easily calculate:
    \begin{align*}
    &\OInc_{p(X,Y)}(\mu(X,Y))
            &  &\SDef_{\to XY}(\mu(X,Y)) 
            \\
    &= \kldiv{\mu(X,Y)}{p(X,Y)} 
            & &= 0
            \\
    &= \kldiv{\mu(X)}{p(X)} + \thickD( \mu(Y|X) \Vert p(Y|X) \mid \mu(X))
            & &= \SDef_{ \to X }(\mu(X))\\
    &\ge \kldiv{\mu(X)}{p(X)} = \OInc_{p(X)}(\mu(X))
    \end{align*} 
    So, by \cref{lem:commonvar-dominance}, we have $p(X,Y) \models p(X)$.


    \paragraph{ Hard Constraints.}
    First, observe that
     % If $X=x$ implies $Y=y$, we can 
     % If $\bbr{\dg M_1}_
     \[
        \OInc_{XY{=}xy}(\mu(XY)) = \infty \mathbbm1[\mu(XY{=}xy) < 1]
        \ge
        \infty \mathbbm1[\mu(X{=}x) < 1] = \OInc_{X{=}x}(\mu(X))
    \]
    % Since $\SDef$ of both PDGs is the negative entropy of the
    In other words, the only distribution attaining a finite $\OInc$
    for $XY = xy$ is the point mass $\delta_{xy}$; thus this
    is the only candidate distribution $\mu$ which 
    $\bbr{X=x}_\gamma(\mu)$ could be larger. But this distribution
    has no entropy, thus $\bbr{XY=xy}_\gamma(\delta_{xy}) = 0 
        = \bbr{XY=xy}_\gamma(\delta_{xy})$. 
    Thus, 
        $\bbr{XY=xy}_\gamma(\mu) \ge \bbr{X=x}_\gamma(\mu)$ for \emph{all}
        distributions $\mu$, showing $XY=xy \models X=x$. 
    % 
    % \begin{align*}
    %     \bbr{XY=xy}_\gamma(\mu) &= -\gamma \log \#\big| w : XY(w) = xy \big|
    %         \\
    %         &\le - \gamma \log \# \big| w : X(w) = x \big|
    %         = \bbr{X=x}_\gamma(\mu),
    % \end{align*}        
    % which is backwards. While OInc ensures the set of distributions
    % is smaller for the joint constraint (and hence a larger score),
    % the freedom to choose the other variable makes the qualitative part



    \end{lproof}




    \recall{prop:entail-properties}
    \begin{lproof}
            \label{proof:entail-properties}
        \textbf{Reflexivity and Transitivity.~~}
        Reflexivity is immediate.
         % is only slightly less trivial.
        For transitivity, we have assumed $\dg M_1 \models \dg M_2$ and $\dg M_2 \models \dg M_3$. 
        By definition, we have that for all $\dg M'$, we have
        \[
            \aar{\dg M_1 + \dg M'} \ge \aar{\dg M_2 + \dg M'} \ge \aar{\dg M_3 + \dg M'}. 
            % \qedhere
        \]
        
    \textbf{Domination.} If $\dg M_1$ and $\dg M_2$ share
    variables but $\dg M_1$ has more edges or higher weights,
    the condition of \cref{lem:commonvar-dominance} is satisfied, and so $\dg M_1 \models \dg M_2$. 
    \end{lproof}

    \recall{prop:sem-equiv-properties}
\begin{lproof}\label{proof:sem-equiv-properties}
    \textbf{Equivalence of conditional/marginal and Joint.}\\
    % Both of the following are well-known:
    % Clearly
    \begin{minipage}{0.5\linewidth}
    \begin{align*}
        &\OInc_{[p(X) + p(Y|X)]}(\mu) \\
        &= \Ex_{\mu} \Big[\log \frac{\mu(X)}{p(X)} + \log \frac{\mu(Y|X)}{p(Y|X)}\Big] 
                    \\
         &= \Ex_{\mu} \Big[\log \frac{\mu(X)\mu(Y|X)}{p(X)p(Y|X)}\Big] 
                    &  \\
         &= \OInc_{p(X,Y)}(\mu);
    \end{align*}
    \end{minipage}
    \begin{minipage}{0.5\linewidth}
        \begin{align*}
            &\SDef_{\to X \to Y}(\mu)\\
            &= \H_\mu(X) + \H_\mu(Y|X) - \H(\mu)\\
            &= \H_\mu(X,Y) - \H(\mu) \\
            &= \SDef_{\to XY}(\mu).
        \end{align*}
    \end{minipage}
    Thus, by applying \cref{lem:commonvar-dominance} once in each direction, or by appeal to \cref{theorem:sem-equiv}, 
    $p(X) + p(Y|X) \equiv p(X,Y)$. 
    
    
    
    \paragraph{Mixtures.}
    Let $\H_\mu(\dg M) := \sum_{a \in \Ar^{\dg M}} \alpha_a^{\dg M} \H_\mu(\Tgt a | \Src a)$,
    so that $\SDef_{\dg M}(\mu) = \H_\mu(\dg M) - \H_\mu(\X)$. 
    Suppose $\dg M_1 \equiv \dg M_2$. 
    Then, the theorem above tells us
    \begin{align*}
        \bbr{\dg M_1}_\gamma(\mu) &= \bbr{\dg M_2}_\gamma(\mu)
        \\\iff\quad%%%
        \OInc_{\dg M_1}(\mu) + \gamma \SDef_{\dg M_1}(\mu)   
            &= \OInc_{\dg M_2}(\mu) + \gamma \SDef_{\dg M_2}(\mu)
        \\\iff\quad%%%
        \OInc_{\dg M_1}(\mu) + \gamma \H(\dg M_1) - \gamma \H(\mu)
            &= \OInc_{\dg M_2}(\mu) + \gamma \H(\dg M_2) - \gamma \H(\mu)
        \\\iff\quad%%%
        \OInc_{\dg M_1}(\mu) + \gamma \H(\dg M_1)
            &= \OInc_{\dg M_2}(\mu) + \gamma \H(\dg M_2)
    \end{align*}
    But $\dg M \mapsto \OInc_{\dg M}(\mu)$ and $\dg M \mapsto \H_\mu(\dg M)$
    are both linear in $(\balpha,\bbeta)$. 
    Thus, for $\alpha \in [0,1]$, letting $\dg M_{\alpha} := (1-\alpha) \dg M_1 + (\alpha) \dg M_2$, we have:
    \begin{align*}
        % &\bbr{(1-\alpha) \dg M_1 + (\alpha) \dg M_2}_\gamma(\mu) + \gamma \H(\mu) \\
        % &\bbr{\dg M_\alpha}_\gamma(\mu) + \gamma \H(\mu) \\
            % &= 
    & \OInc_{\dg M_\alpha} (\mu)   + \gamma \H_\mu({\dg M_\alpha}) \\
        &= (1-\alpha) \OInc_{\dg M_1} + \alpha \OInc_{\dg M_2} +
            \gamma (1-\alpha) \H_\mu({\dg M_1}) + \gamma\alpha \H_\mu({\dg M_2})\\
        &= (1-\alpha) \Big( \OInc_{\dg M_1} + \gamma \H_\mu({\dg M_1}) \Big)
            + \alpha \Big( \OInc_{\dg M_2} + \gamma \H_\mu({\dg M_2}) \Big)\\
        &= \OInc_{\dg M_1} + \gamma \H_\mu({\dg M_1}).
    \end{align*}
    The last line follows from the equality of the two parenthesized expressions
        (as shown in the previous block of algebra). Thus following the first argument
        in reverse, we learn that $\bbr{\dg M_\alpha} = \bbr{\dg M_1} = \bbr{\dg M_2}$, 
        and so by the previous Theorem, $\dg M_\alpha \equiv \dg M_1 \equiv \dg M_2$.
    \end{lproof}


    \recall{theorem:sem-equiv}
    \begin{lproof}\label{proof:sem-equiv}    
        ($\implies$). Suppose $\dg M_1 \equiv \dg M_2$. 
        Let $\X_1$ be the variables of $\dg M_1$, and $\X_2$ be the variables of $\dg M_2$. 
        % First, we show that $\dg M_1$ and $\dg M_2$ have the same set of variables.
        We begin by showing that $\X_1 = \X_2$ (up to constant variables, that can take only one value). 
        In search of a contradiction,
        suppose there is a variable $X$ be a variable present in one PDG but not the other;
        without loss of generality, suppose $X \in \X_1 \setminus \X_2$. 
        Then in particular, we must have $\aar{\dg M_1} = \aar{\dg M_2}$,
        and also $\aar{\dg M_1 + X} = \aar{\dg M_2 + X}$. But $X \in \X_1$,
        so $\dg M_1 + X = \dg M_1$. Therefore,
        \[ \aar{\dg M_2} = \aar{\dg M_1} =   \aar{\dg M_1 + X} = \aar{\dg{M_2}+X}. \]
        Since $X \notin \X_2$, the minimizer of 
        $\bbr{\dg M_2 + X}_\gamma(\mu(X,\X_2))$ is realized when $X$ and $\X_2$ are independent \cite[Theorem 5]{pdg-infer}.
        This means that $\aar{\dg M_2 + X}_\gamma = \aar{\dg M_2}_\gamma - \gamma \H(\mathrm{Unif}(X))$.
        By selecting $\gamma > 0$, we discover that
        find a contradiction, unless $X$ can only take one value. 
            
        So far we have shown that $\dg M_1$ and $\dg M_2$ have the same set of (nontrivial) variables; we now show that they have the same scoring function. For any $\gamma \ge 0$ and $\mu \in \Delta\V\!\X$,
            the definition of $\equiv$ with a choice of $\dg M' = \mu!$ gives
            us the middle equality in the following.
        \begin{align*}
            \bbr{\dg M_1}_\gamma(\mu) = 
            \aar{\dg M_1 + \mu!}_\gamma = \aar{\dg M_2 + \mu!}_\gamma = \bbr{\dg M_2}_{\gamma}(\mu).
        \end{align*}
        
        ($\impliedby$). Suppose $\bbr{\dg M_1} = \bbr{\dg M_2}$.
        In particular, for the two objects to even have the same type, this means $\dg M_1$ and $\dg M_2$ have the same set $\X$ of variables.  
        For every $\dg M'$ and $\gamma \ge 0$, we have
        \begin{align*}
            \aar{\dg M_1 + \dg M'}_\gamma
                &= \inf_{\mu \in \Delta \V(\X + \X')} 
                    \Big( \bbr{\dg M_1}_\gamma(\X)
                        + \bbr{\dg M'}_\gamma(\X') \Big) \\
                &= \inf_{\mu \in \Delta \V(\X + \X')} 
                    \Big( \bbr{\dg M_2}_\gamma(\X)
                        + \bbr{\dg M'}_\gamma(\X') \Big) 
                = \aar{\dg M_2 + \dg M'}_\gamma.
                \qedhere
        \end{align*}
    \end{lproof}


    \section{Negative Results and Anti-Conjectures}
    \begin{falsity}\label{more-examples}\begin{enumerate}
        \item If $\dg B$ is the PDG form of a BN, which determines distribution
            $\Pr_{\dg B}$, then $\dg B \models \Pr_{\dg B}$. 
        Moreover, $\Pr_{\dg B} \models \dg B$, and thus $\dg B \equiv \Pr_{\dg B}$. 
        
        % \item {\it (conjecture)}
        % \item 
    \end{enumerate}   
    \end{falsity}

    1. cannot be true because by selecting $\Pr_{\dg B}$ we have lost
    the ability to control the relative weight of independence and cpds;
    it's all mushed together, and this will only hold at $\gamma = 1$.

    \begin{falsity}
        If $\dg M \models 0$, then $(\alpha) \dg M \models 0$ for all 
        $\alpha \ge 0$. 
    \end{falsity}
    % I think this might be true, actually.
    This can't be true; for $\alpha=0$, it reduces to $\X^{\dg M} \models 0$, which is false. 
    % This is false, because if $\dg M = ( {\to} X)$, then $\bbr{\dg M} = 0$

    Corollary: the following is false. 
    \begin{falsity}
        If $\dg M_1 \models \dg M_2$, then $(r) \dg M_1 \models (r) \dg M_2$, 
        for all $r \ge 0$. 
    \end{falsity}
    Chosing $\dg M_2 = 0$ would imply the previous false result.


    We can show that ${\to} X \models 0$, but not necessarily the reverse.
    If $\dg M'$ does not include $X$, then $\aar{\dg M' + {\to} X}_\gamma = \aar{\dg M'}$.
    But what if $\dg M'$ includes $X$? 
    Then $\aar{\dg M' + {\to} X} \ge \aar{\dg M'}$ by domination, and the
    inequality is strict for ``most'' choices of $\dg M'$.
    So either way $\aar{{\to}X + \dg M'} \ge \aar{0 + \dg M'}$, so ${\to X} \models 0$.


\end{subappendices}
