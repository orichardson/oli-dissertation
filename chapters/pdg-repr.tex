\label{chap:pdg-repr}

% In this chapter, based on a 2021
We introduce Probabilistic Dependency Graphs (PDGs), a new class of
directed graphical models. PDGs can capture inconsistent beliefs in a
natural way and are more modular than Bayesian Networks (BNs), in that
they make it easier to incorporate new information and restructure the  
representation. We show by example how PDGs are an especially natural
modeling tool.
We provide three semantics for PDGs, each of which can be derived from a
scoring function (on joint distributions over the
variables in the network) that can be viewed as representing a
distribution's incompatibility with the PDG.
For the PDG corresponding
to a BN, this function is uniquely minimized by the distribution the
BN represents, showing that PDG semantics extend BN semantics.  
We show further that factor graphs
and their exponential families
can also be faithfully represented as PDGs,
while there are significant barriers to modeling a PDG with a factor graph.
% \end{abstract}

\section{Introduction and Examples}
	\label{sec:pdg-intro-examples}

In this chapter we introduce yet another graphical tool 
    for modeling beliefs, called \emph{Probabilistic Dependency Graphs} (PDGs). 
There are already many such models in the literature, including Bayesian networks (BNs) and factor graphs. 
    % (For an overview, see \citet{KF09}.)
(For an overview, see \cref{sec:prelim-pgms}, or better yet, \citet{KF09}.)
Why does the world need one more?  

Our original motivation for introducing PDGs was to be able capture
inconsistency. We want to be able to model the process of resolving
inconsistency; to do so, we have to model the inconsistency itself. But our
approach to modeling inconsistency has many other advantages. 
In particular, PDGs are significantly more modular than other directed graphical models: operations like restriction and 
combination (\cref{sec:pdg-combine}) that are easily done with PDGs are difficult or impossible to do with other representations.
The following examples motivate PDGs and illustrate some of
their advantages.

\begin{example} \label{ex:guns-and-floomps}
Grok is visiting a neighboring district. From prior reading, she thinks it
likely (probability .95) that guns are illegal here. Some brief conversations
with locals lead her to believe that, with probability .1, the law
prohibits floomps.

The obvious way to represent this as a BN is to use two variables
$\var F$ and $\var G$ (respectively taking values $\{f, \lnot f\}$ and
$\{ g, \lnot g\}$),
indicating whether  floomps and guns are prohibited.
The semantics of a BN offer her two choices: either assume that $\var F$ and $\var G$
to be independent and give (unconditional) probabilities of $\var F$ and $\var G$, or
choose a direction of dependency, and give one of the two unconditional
probabilities and a conditional probability distribution. 
As there is no reason to choose either direction of dependence, the
natural choice is to 
assume independence, giving her the 
BN on the left of \Cref{fig:gun-floomp-diagram}.

\begin{figure}[htb]
  \centering
\ifprecompiledfigs
	\raisebox{-0.5\height}{\includegraphics[scale=0.8]{figure-pdfs/fg-BN.pdf}}
~\vrule~
	\raisebox{-0.5\height}{\includegraphics[scale=0.8]{figure-pdfs/fg-PDG.pdf}}
\else
	\scalebox{1.0}{
\begin{tikzpicture}[center base, scale=0.7, AmpRep]
        \def\figtabledist{0.2}
        \def\fignodedist{1.4}
        \def\figtableheight{0.45} 

        \matrix [table with head, column 1/.style={leftrule}, anchor=south east,
             column 2/.style={rightrule}, row 2/.style={bottomrule}] at (-\figtabledist,\figtableheight) {
            % \vphantom{$\overline fg$} $f$ \& \vphantom{$\overline fg$}$\overline f$\\
            % $f$ \& $\lnot f$\\
            $\vphantom{fg}f$ \& $\vphantom{fg}\lnot f$\\
            .9 \& .1\\
        };
        \matrix [table with head, column 1/.style={leftrule}, anchor=south west,
             column 2/.style={rightrule}, row 2/.style={bottomrule}] at (\figtabledist,\figtableheight) {
             % \vphantom{$\overline fg$}$g$ \& \vphantom{$\overline fg$}$\overline g$\\
            $\vphantom{fg}g$ \& $\vphantom{fg}\lnot g$\\
             .05 \& .95\\
        };
        \node[dpadded, circle, fill=black!08, fill opacity=1] (floomp) at (-\fignodedist,0) {$\var F$};
        \node[dpadded, circle, fill=black!08, fill opacity=1] (gun) at (\fignodedist,0) {$\var G$};
    \end{tikzpicture}
    ~~\vrule~~
	\begin{tikzpicture}[center base]

        \def\fignodedist{2.1}
        \def\fignodeheight{1.1}
        \def\newcptX{-0.2}
        \def\newcptY{-0.5}
                     
		\node[dpadded, fill=white, draw=gray] (true)  at (0,2) {$\var\pdgunit$};
		\node[dpadded] (floomp) at (-\fignodedist,\fignodeheight) {$\var F$};
		\node[dpadded] (gun) at (\fignodedist,\fignodeheight) {$\var G$};			
		
		\draw[arr] (true) to[bend left=0] coordinate(A) (floomp);
		\draw[arr] (true) to[bend right=0] coordinate(B) (gun);

		\node[above left=2.0em and 1.5em of A, anchor=center] {
			\begin{idxmat}[\color{black}\smalltext]{\!\!\!$\star$\;\;\;}{$f$, $\lnot f$}
				.90 & .10 \\
			\end{idxmat}
		};
		\node[above right=2.0em and 1.3em of B, anchor=center] {
			\begin{idxmat}[\color{black}\smalltext]{\!\!\!$\star$}{$g$, $\lnot g$}
				.05 & .95 \\
			\end{idxmat}
		};
		\definecolor{heldout}{rgb}{0.6, 0.6, .6}	
		\draw[heldout, dashed, arr] (floomp.-30) to[bend right=7] node[pos=0.65, fill=white, inner sep=2pt] (C) {$\smash{p}\vphantom{v}$} (gun.210);
        \draw[heldout, dashed, arr] (gun.190) to[bend left=5] node[pos=0.668, fill=white, inner sep=2pt] {$\smash{p'}\vphantom{v}$} (floomp.-10);
		\node[anchor=center] (newcpd) at (\newcptX,\newcptY) {
			\color{heldout}
		$p
			=\!\!\!$\begin{idxmat}[\color{heldout}\smalltext]
                % {$f$,$\overline f$}{$g$, $\overline g$}
                {$\smash f$,$\smash{\lnot f}$}{$\smash g$, $\smash{\lnot g}$}
			  .92 & .08 \\ .08 & .92 \\
          \end{idxmat}$~=~
		    {(p')^{\textsf T}}$
		};
	\end{tikzpicture}
	}
\fi
        \caption[
        A BN without arcs, its corresponding PDG, 
            and how the latter can be augmented
            (losslessly) with additional cpds.
        ]
        {A BN (left) and corresponding PDG (right), which can
        be augmented with additional cpds. The cpds $p$ and/or $p'$
        make it inconsistent.} 
    \label{fig:gun-floomp-diagram}
\end{figure}


A traumatic experience a few hours later leaves Grok believing that
``floomp'' is likely (probability .92) to be another word for gun.
Let $p(G \mid F)$ be the \emph conditional \emph probability \emph
distribution (cpd) that describes 
the belief that if floomps are legal (resp., illegal),
then with probability .92, guns are as well, and $p'(F \mid G)$ be
the reverse. 
Starting with $p$, Grok's first instinct is to
simply incorporate the conditional information by adding $F$ as a parent of
$G$, and then associating
the cpd
$p$ with $G$. But then what should she do
with the original probability she had for $G$?  Should she just discard it?
It is easy to check that there is no 
joint distribution
that is consistent with
both
the two original priors on $F$ and $G$ and also 
$p$.  So if she
is to represent the information with a BN, which always represents a consistent
distribution, she must resolve the inconsistency. 





However, sorting this out immediately may not be ideal. For instance, if the inconsistency arises from a conflation between two definitions of ``gun'', a resolution will have destroyed the original cpds. A better use of computation may be to notice the inconsistency and look up the actual law. 
%
By way of contrast, consider the corresponding PDG. In a PDG, the cpds are
attached to arcs, rather than nodes of the graph.
In order to represent unconditional probabilities, we introduce
a \emph{unit variable} $\pdgunit$ which 
takes only one value, denoted
$\star$. 
This leads Grok to 
the PDG depicted in \Cref{fig:gun-floomp-diagram},
where the arcs from $\pdgunit$ to $F$ and $G$ are associated with the
unconditional probabilities of $F$ and $G$, and the 
arcs between $F$ and $G$ are associated with $p$ and $p'$.



The original state of knowledge consists of all three nodes and the two solid arcs from $\pdgunit$. This is like Bayes Net that we considered above, except that we no longer explicitly take  $F$ and $G$ to be independent; we merely record the constraints imposed by the given probabilities.  
	
The key point is that we can incorporate the new information into our original
representation (the graph in \Cref{fig:gun-floomp-diagram} without the arc from
$F$ to $G$) simply  by adding the arc from $F$ to $G$ and the associated cpd
$p$ (the new information is shown in blue).
% Doing so does not change the meaning of the original arcs.
Doing so does not change the meaning of the original arcs.
\unskip\footnote{%
    While the meaning of the original arcs does not change, one might want something different at a qualitative level, as we will explore in \cref{sec:pdg-combine}.
}
Unlike a Bayesian update, the operation is even reversible: all we need
to do recover our original belief state is delete the new arc, 
making it possible to mull over and then reject an observation.
\end{example}


The ability of PDGs to model inconsistency, as illustrated in
\Cref{ex:guns-and-floomps}, appears to have come at a significant cost. We seem
to have lost a key benefit of BNs: the ease with which they can
capture
(conditional) independencies, which, as Pearl (\citeyear{pearl}) has
argued forcefully, are omnipresent.




\begin{example}[emulating a BN]\label{ex:smoking}

We now consider the classic (quantitative) Bayesian network $\cal B$, which has
four binary variables indicating whether a person ($C$) develops cancer, ($S$)
smokes, ($\mathit{SH}$) is exposed to second-hand smoke, and ($\mathit{PS}$) has
parents who smoke, presented graphically in \Cref{subfig:smoking-bn}. We now
walk through what is required to represent $\cal B$ as a PDG, which we call
$\PDGof{{\mathcal B}}$, shown as the solid nodes and arcs in
\Cref{subfig:smoking-pdg}. 


\begin{figure}[ht!]
\addtocounter{figure}{1}
\centering
\hfill
\ifprecompiledfigs
\raisebox{-0.5\height}{\includegraphics{figure-pdfs/smoking-BN.pdf}}
\else
\begin{tikzpicture}[center base, scale=1.2]
	\begin{scope}[every node/.style={dpadded, fill opacity=1,fill=black!08, circle, inner sep=2pt, minimum size=2em, draw=gray}]
		\node (PS) at (0,1.1) {$\mathit{PS}$};
		\node (SH) at (-0.6,0) {$\mathit{SH}$};
		\node (S) at (0.6,0) {$\mathit{S}$};
		\node (C) at (0,-1.1) {$\mathit{C}$};
	\end{scope}
	\draw[->] (PS) to (S);
	\draw[->] (PS) to (SH);
	\draw[->] (SH) to (C);
	\draw[->] (S) to (C);
\end{tikzpicture}
\fi
\refstepcounter{subfigure}
\label{subfig:smoking-bn}
~~\vline~~
\ifprecompiledfigs
\raisebox{-0.5\height}{\includegraphics{figure-pdfs/smoking-PDG.pdf}}
\else
\begin{tikzpicture}[center base, scale=1.2]
	\fill[fill opacity=0.1, blue!80!black, draw, draw opacity=0.5] (2.73,1.35) rectangle (6.7, -1.35);
	
	\node[dpadded] (1) at (1.65,1) {$\pdgunit$};
	\node[dpadded] (PS) at (1.65,-0.4) {$\mathit{PS}$};
	\node[dpadded, fill=black!.16, fill opacity=0.9] (S) at (3.2, 0.8) {$S$};
	\node[dpadded, fill=black!.16, fill opacity=0.9] (SH) at (3.35, -0.8) {$\mathit{SH}$};
	\node[dpadded, fill=black!.16, fill opacity=0.9] (C) at (4.8,0) {$C$};
	
	\draw[arr1] (1) -- (PS);
	\draw[arr2] (PS) -- (S);
	\draw[arr2] (PS) -- (SH);
	\mergearr{SH}{S}{C}
	
	\node[dpadded, fill=black!.16, fill opacity=0.35, dashed] (T) at (6.15,0) {$T$};
	\draw[arr1,dashed] (T) -- (C);	

	\draw[very thick, |-|, color=blue!50!black,text=black] (2.7, 1.35) --coordinate(Q) (6.73,1.35);%
	\fill[white] (2.6, 1.36) rectangle (6.9,1.55);
	\node[above=0.05em of Q]{\small Restricted PDG in \cref{ex:grok-ablate,ex:grok-union}};
\end{tikzpicture}
\fi
	\hfill~
	\refstepcounter{subfigure}
	\label{subfig:smoking-pdg}
\addtocounter{figure}{-1}
\caption[The classic ``smoking'' BN, its corresponding PDG, and a restriction of
    the latter, which can be augmented with other information about cancer.
]{ (a) The Bayesian Network $\mathcal B$ in \cref{ex:smoking} (left), and
(b) $\PDGof{\mathcal B}$, its corresponding PDG (right). The shaded box
indicates a restriction of $\PDGof{\mathcal B}$ to only the nodes and arcs it
contains, and the dashed node $T$ and its arrow to $C$ can be added in the PDG,
without taking into account $S$ and $SH$.}
\label{fig:smoking-bn+pdg}
\end{figure}

We start with the nodes corresponding to the variables in $\cal B$, together
with the special node $\pdgunit$ from \Cref{ex:guns-and-floomps}; we add an arc
from ${\pdgunit}$ to $\mathit{PS}$, to which we associate the unconditional
probability given by the cpd for $\mathit{PS}$ in $\cal B$. We can also re-use
the cpds for $S$ and $\mathit{SH}$, assigning them, respectively, to the arcs
$PS \to S$ and $PS \to SH$ in $\PDGof{{\mathcal B}}$.
There are two remaining problems: (1) modeling the remaining table in $\cal B$,
which corresponds to the conditional probability of $C$ given $S$ and $SH$; and
(2) recovering the additional
conditional
independence assumptions in the BN. 

For (1), we cannot just add the arcs $S \to C$ and $SH \to C$ that are present
in $\cal B$. As we saw in \Cref{ex:guns-and-floomps}, this would mean
supplying two \emph{separate} tables, one indicating the probability of $C$
given $S$, and the other indicating the probability of $C$ given
$\mathit{SH}$.  We would lose significant information that is
present in $\cal B$  about 
how $C$ depends jointly on $S$ and $SH$. To distinguish the joint dependence on
$S$ and $\mathit{SH}$, for now, we draw an arc with two tails---a
(directed)
\emph{hyperarc}---that completes the diagram in \Cref{subfig:smoking-pdg}. 
With regard to (2), there are many distributions consistent with the conditional
marginal probabilities in the cpds, and the independencies presumed by $\cal B$
need not hold for them. 
Rather than trying to distinguish between them with additional constraints,
we develop a a scoring-function semantics for PDGs
which 
is in this case uniquely minimized by the distribution 
specified by ${\mathcal B}$ (\Cref{theorem:bns-are-pdgs}).
This allows us to recover the semantics of Bayesian networks without requiring the independencies that they assume.

Next suppose that we get information beyond that captured by the original BN.
Specifically, we read a thorough empirical study demonstrating that people who
use tanning beds have a 10\% incidence of cancer, compared with 1\% in the
control group 
(call the cpd for this $p$); we would like to add this information to
$\cal B$. The first step is clearly to add a new node labeled $T$, for ``tanning
bed use''.  But simply making $T$ a parent of $C$ (as clearly seems appropriate,
given that the incidence of cancer depends on tanning bed use) requires a
substantial expansion of the cpd; in particular, it requires us to make
assumptions about the interactions between tanning beds and smoking.  
The corresponding PDG, $\PDGof{{\mathcal B}}$, on the other hand, has no
trouble: We can simply add the node $T$ with an arc to $C$ that is associated
with $p$.  But note that doing this makes it possible for our knowledge to
be inconsistent. To take a simple example, if the distribution on $C$ given $S$
and $H$ encoded in the original cpd was always deterministically ``has cancer''
for every possible value of $S$ and $H$, but the distribution according to the
new cpd from $T$ was deterministically ``no cancer'', the resulting PDG would be
inconsistent.  
\end{example}


We have seen that we can easily add information to PDGs; removing information is
equally painless.   

\begin{example}[restriction]\label{ex:grok-ablate}
  After the Communist party came to power,
  children were raised communally, and so parents' smoking habits no longer had any impact on them. Grok is reading her favorite book on graphical models, and she realizes that while the node $\mathit{PS}$ in \Cref{subfig:smoking-bn} has lost its usefulness, and nodes $S$ and $\mathit{SH}$ no longer ought to have $\mathit{PS}$ as a parent, the other half of the diagram---that is, the node $C$ and its dependence on $S$ and $\mathit{SH}$---should apply as before.
Grok has identified two obstacles to modeling deletion of information from a BN
by simply deleting nodes and their associated cpds. First, this restricted model
is technically no longer a BN (which in this case would require unconditional
distributions on $S$ and $\mathit{SH}$), but rather a \emph{conditional} BN
\citep{KF09}, which allows for these nodes to be marked as observations;
observation nodes do not have associated beliefs. Second, even regarded as a
conditional BN, the result of deleting a node may introduce \emph{new}
independence information, incompatible with the original BN. For instance, by
deleting the node $B$ in a chain $A \rightarrow B \rightarrow C$, one concludes
that $A$ and $C$ are independent, a conclusion incompatible with the original BN
containing all three nodes.   
PDGs do not suffer from either problem.  We can easily delete the
nodes labeled 1 and $PS$ in \Cref{subfig:smoking-pdg} to get the
restricted PDG shown in the figure, which captures Grok's updated information.
The resulting PDG has no arcs leading to $S$ or $\mathit{SH}$, and hence no
distributions specified on them; no special modeling distinction between
observation nodes and other nodes are required. Because PDGs do not directly
make independence assumptions, the information in this fragment is truly a
subset of the information in the whole PDG. 	
\end{example}

The ability to form restrict to arbitrary subsets of information is useful,
and closely related to an even more compelling reason to use PDGs:
they make it easy to aggregate probabilistic information. 
	
\begin{example}\label{ex:grok-union}
Grok dreams of becoming Supreme Leader ($\it SL$), and has come up with a plan.
She has noticed that people who use tanning beds have significantly more power
than those who don't. Unfortunately, her mom has always told her that tanning
beds cause cancer; specifically, that 15\% of people who use tanning beds get
it, compared to the baseline of 2\%. Call this cpd $q$. Grok thinks people will
make fun of her if she uses a tanning bed and gets cancer, making becoming
Supreme Leader impossible. This mental state is depicted as  a PDG on the left
of \Cref{fig:grok-combine}.

Grok is reading about graphical models because she vaguely remembers that the
variables in \Cref{ex:smoking} match the ones she already knows about. When she
finishes reading the statistics on smoking and the original study on tanning
beds (associated to a cpd $p$ in \Cref{ex:smoking}), but before she has
time to reflect, we can represent her (conflicted) knowledge state 
% as the sum of the two graphs,
with the 
% depicted graphically 
PDG depicted
on the right of \Cref{fig:grok-combine}.
%
% \forjoe{I believe the text above is still accurate (although the conflicted knowledge state may be conflicted not just about the cpds, but also about the causal mechanisms.) We haven't said anything about qualitative information or mechanisms at this point, and I don't think it would be a good idea to do so here.}


\begin{figure}
	\hfill
	\ifprecompiledfigs
\raisebox{-0.5\height}{\includegraphics{figure-pdfs/grok-pre.pdf}}
\hspace{1.2em}\vline\hspace{1.2em}
\raisebox{-0.5\height}{\includegraphics{figure-pdfs/grok-post.pdf}}
	\else
	\colorlet{colorsmoking}{blue!50!black}
	\colorlet{colororiginal}{orange!80!black}
	\tikzset{hybrid/.style={postaction={draw,colorsmoking,dash pattern= on 5pt off 8pt,dash phase=6.5pt,thick},
		draw=colororiginal,dash pattern= on 5pt off 8pt,thick}}
	\centering
	\begin{tikzpicture}[paperfig, thick, draw=colororiginal, text=black]
		\node[dpadded] (C) at (0,0) {$C$};
		\node[dpadded] (T) at (2,0){$T$};
		\node[dpadded] (SL) at (1,-1.5){$\it SL$};
		
		\draw[arr] (T) to[bend right] node[above]{$q$} (C);
		\mergearr{C}{T}{SL}
	\end{tikzpicture}
	\hspace{1.6em}\vline\hspace{1.6em}
	\begin{tikzpicture}[paperfig]
		\begin{scope}[postaction={draw,colorsmoking,dash pattern= on 3pt off 5pt,dash phase=4pt,thick}]
			
			\node[dpadded,hybrid] (C) at (0,0) {$C$};
			\node[dpadded,hybrid] (T) at (2,0){$T$};
		\end{scope}
		
		\begin{scope}[thick, draw=colororiginal, text=black]
			\node[dpadded] (SL) at (1,-1.5){$\it SL$};
			\draw[arr] (T) to[bend right] node[above]{$q$} (C);
			\mergearr{C}{T}{SL}
		\end{scope}


		\begin{scope}[thick, draw=colorsmoking, text=black]
			\node[dpadded] (S) at (-1.4, 0.8) {$S$};
			\node[dpadded] (SH) at (-1.45, -0.8) {$\mathit{SH}$};
			\draw[arr] (T) to node[fill=white, fill opacity=1,text opacity=1,inner sep=1pt]{$p$} (C);
			\mergearr{S}{SH}{C}
		\end{scope}
	\end{tikzpicture}
	\fi
	\hfill~
	\caption{Grok's prior (left) and combined (right) knowledge.}
	\label{fig:grok-combine}
\end{figure}

% The union of the two PDGs, even with overlapping nodes, is still a PDG.
% This is not the case in general for BNs.
% 
By joining the two PDGs together, we get another PDG---which is not the case in general for BNs.
% Note that the PDG that Grok used to represent her two different sources of information (the mother's wisdom and the study) regarding the distribution of $C$ is a \emph{multigraph}: there are two
% arcs from $T$ to $C$, with inconsistent information.
% Had we not allowed multigraphs, we would have needed to choose between the two arcs, or represent the information some other (arguably less natural) way. 
% As we are already allowing
% inconsistency, merely recording both is much more in keeping with the way we
% have handled other types of uncertainty. 
Note that the resulting PDG has two distinct two arcs from $T$ to $C$
 (corresponding to the mother's wisdom $q$ and the study $p$), 
    that are inconsistent.
Specifically, there is no distribution $\mu(C,T)$ for which $\mu(T|C)$ equals both $p$ and $q$. 
Observe once again that this inconsistency is possible because we now have two distinct beliefs about $C$.
% Had we not allowed multigraphs, we would have needed to choose between the two arcs, or represent the information some other (arguably less natural) way. 
% As we are already allowing
% inconsistency, merely recording both is much more in keeping with the way we
% have handled other types of uncertainty. 
\end{example}

Not all inconsistencies are equally egregious.
For example, even though the cpds
$p$ and $q$ are different, they are numerically close, so, intuitively, the PDG on the right in
\Cref{fig:grok-combine} is not very inconsistent.
Making this precise 
is
the focus of \Cref{sec:scoring-semantics}.

These examples give a taste of the power of PDGs.  
In the coming sections, we formalize PDGs and relate them to other approaches.
	
	
\section{Syntax}\label{sec:formal+syntax}
% We now provide formal definitions for PDGs.        
% We now provide our first formal definition of a PDG.
We can now provide the formal definition of a 
% PDG. 
% \emph{%
Probabilistic Dependency Graph
% (PDG)
% }%
\unskip. 


% Although it is possible to formalize PDGs with hyperarcs directly, we opt for a different approach here, in which PDGs have only regular arcs, and hyperarcs are captured using a simple construction that involves adding an extra node.
%
% \vfull{\unskip\footnote{
%     In a large part of the graphical models community,
%     it is common to  call a collection of marginals that are not
%       necessarily all compatible with a distribution
%       \emph{pseudomarginals} \citep{wainwright2008graphical}.
%     Syntactically, a PDG is therefore 
%     collection of (weighted) \emph{conditional} pseudomarginals. 
%     This gives an alternate expansion of ``PDG'' 
%      as ``Pseudomarginal Dependency Graph'',
%      with nomenclature rooted in the literature.}}

\begin{defn}
    % \label{\OInc}
    \label{def:model}
% A \emph{Probabilistic Dependency Graph}
A \emph{PDG}
is a tuple $\dg M = (\N,\Ar,\V,\mathbb P, \balpha, \bbeta)$, where 
%
\begin{description}[topsep=0pt]%
	\item[$\N$] 
		% \notation{$:\Set$}%
		is a finite set of nodes, corresponding to the identities of variables;
	\item[$\Ar$] 
		% \notation{$\subseteq \N \times \N \times \mathit{Label}$}%
		% \notation{$: \sum_A 2^\N \times 2^\N$}%
		is a collection of (hyper)arcs each $a\in \Ar$ of which has source(s) $\Src a$ and target(s) $\Tgt a$ in $\N$;
	\item[$\V$]
		% \notation{$: \N \to \Set$}%
		associates each node $N \in \N$ with a set $\V(N)$ of possible values, allowing us to view it as a variable;
  	\item[$\mathbb P$]
        % \notation{$:\big(\!({A,B,\ell})\colon\!\Ar \big) \to \V(A) \to \Delta\V(B)$}%
        % \notation{$:\big(\ed aXY \in \Ar \big) \to \V(X) \to \Delta\V(Y)$}%
		associates to each arc $\ed aXY \in \Ar$
		% a distribution $\p_a(x)$ on $Y$ for each $x \in \V(X)$; 
		a cpd $\p_a(Y|X)$;

\item[$\balpha$] 
	% \notation{$:\Ar \to \mathbb R$}
associates to each arc $\ed aXY$ a real number $\alpha_a$ which represents the modeler's confidence in the functional
dependence of $Y$ on $X$ implicit in $a$; 
\item[$\bbeta$] 
    % \notation{$:\Ar \to \Rext$}
	associates to each arc $a \in \Ar$ a (possibly infinite) real number $\beta_a$,
	the modeler's 
	subjective confidence in the reliability of
	$\mathbb P$. 
\end{description}

% Note that we allow multiple arcs in $\Ar$ with the same source and
% target; thus $(\N,\Ar)$ is a multigraph.
The definition is ambiguous about whether $(\N, \Ar)$ is a directed graph, multi-graph, or a directed hypergraph. This is because, perhaps counter-intuitively, all of these variants will turn out to be equivalent.
% We occasionally write a PDG
% as $\dg M = (\Ar,\mathbb P, \alpha,\beta)$, where $\Ar = (\N,\Ar,\V)$, 
% and abuse terminology by referring to $\Ar$ as a multigraph.
Since $\Ar$ can be extended with identity arcs $\{ X \xrightarrow{id} X \}_{X\in \N}$, we make our usual assumption that $\N$ is implicit in $\Ar$.
Furthermore, in this case, both $\Ar$ and $\V$ are then implicit in 
% the type of $\mathbb P$ 
$\mathbb P = \{ \p_a : \V \Src a \to \Delta \V \Tgt a \}_{a \in \Ar}$.
Thus, we refer to $\mathbb P$ as an \emph{unweighted PDG};
	we give it semantics as though it were the weighted PDG that has $\bbeta = \balpha = \mat 1$. 
% We refer to 
% ${\dg N} = (\Ar, \mathbb P)$ as an \emph{unweighted} PDG,
% and give it semantics as though it were the (weighted) PDG $(\Ar, \mathbb P, \mat 1, \mat 1)$, where
% $\bf 1$ is the constant function (i.e., so that $\alpha_a = \beta_a = 1$ for all $a$). 
% In this chapter, with the exception of \cref{sec:expfam}, we implicitly take $\balpha = {\bf 1}$
% and omit $\alpha$, writing $\dg M = (\Ar, \mathbb P, \beta)$.
% \footnote{The appendix gives results for arbitrary $\alpha$.} 
\end{defn}

If $\dg M$ is a PDG, we reserve the names 
$\N^{\dg M}, \Ar^{\dg M}, \ldots$,
for the components of $\dg M$, so that we may reference one without naming them
all explicitly.
% We write $\V(S)$ for the set of possible joint settings of a set $S$ of variables.
The pair $(\N, \V)$ describes a set of nodes $\N$ and a set of values $\V\!X$ for each node $X \in \N$;
    thus $(\N,\V)$ is equivalent to specifying a set $\X$ of variables.
For this reason, we write 
% $\V\!\X$
% \[ \V\!\X := \prod_{X \in \N} \V(X) \]
$ \V\!\X := \prod_{X \in \N} \V\!X $
for the set of all joint settings of the variables
    (which will be our outcome space $\Omega$);
    we refer to $\omega \in \V\!\X$ as ``worlds''.
Rather than specifying $\N$ and $\V$ separately, it is common practice in the graphical models community to instead specify a set $\X$ of variables directly \citep{KF09}.
We have kept them them separate in \cref{def:model} to emphasize the separation between the qualitative information $(\N, \Ar, \balpha)$ and the quantitative information $(\V, \mathbb P, \bbeta)$ that annotates it. 
% This separation between qualitative and quantitative information will
    % play a large role in our semantics in the next section, 
	% play a large role in the theory of PDGs.
% and later on when we revisit the point in \cref{chap:QIM,chap:PDG-cat}.

We now present some shorthand to clarify the presentation.
We typically conflate a cpd's symbol with its edge label,
thus drawing
the PDG with a single edge attached to $p(Y|X)$ as
\raisebox{-.2ex}{%
\begin{tikzpicture}[center base]
	\node[dpadinline] (X) at (0,0) {$X$};
	\node[dpadinline] (Y) at (1.2,0){$Y$};
 	\draw[arr1,->] (X) -- node[fill=white, fill opacity=1, pos=0.35, inner sep=0pt]{$p$} (Y);
\end{tikzpicture}}\,.
% \Cref{defn:pdg} is equivalent to one in which edge sources and targets are both \emph{sets} of variables.
Regardless of the underlying definition of a PDG, we will use hypergraph notation, which is typically more compact and clearer. 
Thus, we will indicate joint dependence with multi-tailed arcs, joint distributions with multi-headed arcs,
and unconditional distributions with nothing at the tail.
For instance, we draw
% \\[-0.5ex]
\[
\phantom{aa}
	p(Y|X,Z) \text{~as~~}
	\begin{tikzpicture}[center base,scale=1.2]
		\begin{scope}[inner sep=3pt]
		\node[dpad0] (Y) at (0.9,0) {$Y$};
		\node[dpad0] (X) at (0,-0.4) {$X$};
		\node[dpad0] (Z) at (0, 0.4) {$Z$};
		\end{scope}
		\mergearr[arr2] XZY
		\node[above right=-1pt and -3pt of center-XZY] {$p$};
	\end{tikzpicture}\,,
   ~~\text{ and }~~ q(A,B) \text{~as~~}
	\begin{tikzpicture}[center base,scale=1.2]
		\begin{scope}[inner sep=3pt]
		\node[dpad0] (A) at (0,0) {$A$};
        \node[dpad0] (B) at (1,0) {$B$};
		\end{scope}
        \coordinate (above) at (0.5,0.8);
        \coordinate (center) at (0.5,0.4);
        \cunmergearr[arr2] {above}{A}{B}{center}
        \node[above left=0pt and 0pt of center,inner sep=2pt]{$q$};
	\end{tikzpicture}\,.
\]
% To emphasize that a cpd $f(Y|X)$ is deterministic (i.e., a function $f:X\to Y$),
To emphasize that a cpd is deterministic (i.e., $\delta\!f(Y|X)$ for a function $f: \V\!X\to \V\mskip-2muY$),
we will draw its arc with two heads, as in:
\begin{tikzpicture}[center base,scale=1.2]
	\node[dpad0, inner sep=2.5pt, rounded corners=2.5pt] (X) at (0,0) {$X$};
	\node[dpad0, inner sep=2.5pt, rounded corners=2.5pt] (Y) at (1.2,0){$Y$};
 	\draw[arr1,->>] (X) -- node[fill=white, fill opacity=1, pos=0.35, inner sep=-1pt]{$f$} (Y);
\end{tikzpicture}\,.
We identify an event $X\!\!=\!x$ with
the degenerate unconditional distribution $\delta_x(X)$ that places all mass on $x$;
hence it may be associated to an edge and drawn simply as
\begin{tikzpicture}[center base,scale=1.0]
	\node[dpad0] (X) {$X$};
 	\draw[arr1,<<-] (X) --
		node[above, pos=0.60, inner sep=1pt]{$x$} +(-1,0);
\end{tikzpicture}\,.
To specify a confidence $\beta \ne 1$,
we place the value near the edge, lightly colored and parenthesized, as in:
\!\!\!\!
\begin{tikzpicture}[center base,scale=1.2]
	\node[dpad0, inner sep=2.5pt, rounded corners=2.5pt](X){$X$};
    \draw[arr, <-]  (X) --
        node[above,pos=0.62, inner sep=1pt, outer sep=0pt, fill=white]{$p\!$}
        node[below,pos=0.55, inner sep=0, outer sep=1.5pt]{${\scriptscriptstyle\color{gray}(\beta)}$}
         ++(-1,0);
\end{tikzpicture}\,,
and we write ${\scriptstyle\color{gray}(\infty)}$ for the limit of high confidence ($\beta\!\to\! \infty$). %



% While the definition above is sufficient to represent the class of all legal
% PDGs, we often use two additional bits of syntax
% to indicate common constraints:  
% the special variable $\pdgunit$ such that $\V(\pdgunit)=\{\star\}$
% from \Cref{ex:guns-and-floomps,ex:smoking},
%  and
% double-headed arrows, $A \tto B$, which visually indicate 
% that the corresponding cpd represents a deterministic
% function $f : \V\!A \to \V B$. 


\subsection{Alternate Equivalent Definitions of PDGs}
    \label{sec:alt-pdgs}

\Cref{def:model} is not the only way to define a PDG. 
% In fact, there are many 
Indeed, there are many definitions of PDGs, and seeming generalizations
or restrictions of them, all of which turn out to be essentially equivalent. 
We now give an overview of important alternative variants.

\begin{enumerate}[wide, label={\textbf{PDG Variant \arabic{*}:}}]
	\item
	\textbf{Separated PDGs.}
	% A PDG can also taken to contain arcs of two different kinds
	% In \cref{def:model}, each  % the
	According to \cref{def:model}, each
	(hyper)arc $\ed aXY \in \Ar$ of PDG plays two distinct roles: 
	qualitatively, it says something about causality and functional dependence through the parameter $\alpha_a$;
	quantitatively, it specifies a cpd $\p_a$ and a confidence $\beta_a$ in it. 
	% it says something qualitative about causality through a degree of functional dependence
	Conceptually, however, these two aspects of an arc are rather separate. 
	Indeed, a PDG can be equivalently defined with two distinct kinds of arcs, each of which only play one role. An arc that plays both roles can be separated into two separate arcs, one for each kind of information. Conversely, an arc with only one kind of information can be specified by spacing $\alpha = 0$ or $\beta = 0$ to nullify the other kind of information, as appropriate. 
	%
	We have opted for combined edges because certain semantic and inference properties of PDGs are easier to describe in this presentation (e.g., \cref{theorem:limit-uniq,theorem:wfg-is-pdg,theorem:approx-infer}).
    However,
        in some cases it may be wise to separate PDGs before combining them, 
        as discussed in \cref{sec:pdg-combine}. 


    \item 
	\textbf{Probabilistic Dependency Multigraphs. }
	As mentioned previously,
    the structure $(\N, \Ar)$ of a PDG can be taken to directly be an (ordinary) directed multigraph rather than the (seemingly more expressive) directed \emph{hyper}graph. 
	% With this definition, we circumvent the need for \cref{constr:hyperedge-reducton}, and get a 
    % This is the approach taken in later chapters of the thesis. 
	This approach is an instance of the
    approach to probabilistic modeling that makes variables primitive and adds constraints,
    as first described in \cref{sec:var-randomvar,ssec:fin-prob}.

	\begin{constr}\label{constr:hyperedge-reducton}
		% We can now explain how we capture the multi-tailed arcs that 
		% were used in 
		% \Crefrange{ex:smoking}{ex:grok-union}. 
		We can capture multi-tailed arcs and joint dependence, even with ordinary graphs, at the cost of an extra node and a few extra arcs.
		% That notation can be viewed as shorthand for the graph that results by adding a new node at the junction representing the joint value of the nodes at the tails, with projections going back.  
		For instance,
		the diagram displaying Grok's prior knowledge in \Cref{ex:grok-union}, on the left of \Cref{fig:grok-combine}
		can be viewed shorthand for the following PDG, where
		where we insert a node labeled $C \times T$ at the junction:
		\smallskip
			\begin{center}
			\begin{tikzpicture}[paperfig]
				\node[dpadded] (SL) at (1.0,0) {$\mathit{SL}$};
				%
				\node[dpadded] (C) at (-1.0, -0.6) {$C$};
				\node[dpadded] (T) at (-1.0, 0.6) {$T$};
				%
				\cmergearr{C}{T}{SL}{0.0,0}
				%
				\draw[arr] (T) to [bend right=90, looseness=2] (C);
			\end{tikzpicture}
			$\qquad\rightsquigarrow\qquad$
			\ifprecompiledfigs
		\raisebox{-0.5\height}{\includegraphics[scale=0.9]{figure-pdfs/widget.pdf}}
			\else
				\begin{tikzpicture}[paperfig]
					\node[dpadded] (SL) at (-1.0,0) {$\mathit{SL}$};
					
					\node[dpadded,light pad] (CT) at (-2.9, 0){$\scriptstyle C \times T$};
					\node[dpadded] (C) at (-4.8, -0.6) {$C$};
					\node[dpadded] (T) at (-4.8, 0.6) {$T$};
					
					\draw[arr, ->>] (CT) -- (C);
					\draw[arr, ->>] (CT) -- (T);
					\draw[arr] (CT) -- (SL);
					\draw[arr] (T) to [bend right=90, looseness=2] (C);
			\end{tikzpicture}
			\fi
			\end{center}
			\smallskip
		
		As the notation suggests, $\V( C \times T) = \V(C) \times \V(T)$.
		For all $(c,t) \in \V(C \times T)$ the cpd for the arc from $C \times T$ to $C$ gives probability 1 to $c$; similarly, the cpd for the arc from $ C \times T$ to $T$ gives probability 1 to $t$.
		
		More generally, a hyperarc $S \to T$, where $S, T \subseteq \N$, 
			can be compiled to a fragment of an ordinary graph, containing:
		\begin{enumerate}[nosep]
			\item two additional ``super-nodes'' that are not elements of $\N$, corresponding to $S$ and $T$ respectively, whose possible values are joint settings of their constituent variables;
			\item projections $S \to X$ for each $X \in S$, and projections $T \to Y$ for each $Y \in T$, all with high confidence ($\alpha = \beta = \infty$);
			\item an arc $S \to T$, associated with the original cpd and confidences. 
		\end{enumerate}
		The special case of a hyperarc with no tails leads to the the special variable $\pdgunit$ such that $\V(\pdgunit)=\{\star\}$, 
		as introduced in \Cref{ex:guns-and-floomps,ex:smoking}.
		\end{constr}
		
		




    \item 
    \textbf{Strict PDGs.}
    Going even further, the restriction to PDGs in which $(\N, \Ar)$ is not even a multi-graph, but rather an ordinary directed graph, is not a restriction at all. It is not hard to see that a multi-graph could be simulated in that formalism with the following widget:
    \[
        \begin{tikzpicture}[center base]
            \node[dpadded](X) at (0,0) {$X$};
            \node[dpadded](Y) at (3,0) {$Y$};
            \draw[arr2] (X) to[bend left] (Y);
            \draw[arr2] (X) to[bend right] (Y);
        \end{tikzpicture}
        \qquad\rightsquigarrow\qquad
        \begin{tikzpicture}[center base]
            \node[dpadded](X1) at (0,0) {$X$};
            \node[dpadded](Y1) at (3,0) {$Y$};
            \node[dpadded](X2) at (0,2) {$X'$};
            \node[dpadded](Y2) at (3,2) {$Y'$};
            \draw[arr2] (X1) to (Y1);
            \draw[arr2] (X2) to (Y2);
            \draw[arr2,->>] (X1) to[bend left] (X2);
            \draw[arr2,->>] (Y1) to[bend left] (Y2);
            \draw[arr2,->>] (X2) to[bend left] (X1);
            \draw[arr2,->>] (Y2) to[bend left] (Y1);
        \end{tikzpicture}
    \]
    where the vertical arcs enforce equality. 

    \item 
    % \textbf{Lax PDGs.}
    % \textbf{Adaptive and Semi-PDGs.}
    \textbf{Adaptive confidences, missing values, and probability ranges.}
    The confidence $\beta_a$ in the arc $\ed aXY$ may be replaced with a separate confidence $\beta_{a,x}$ for each possible input $x \in \V(X)$. 
	%
    Relatedly, one might imagine a variant in which one needn't specify a complete conditional probability table, but could leave some entries blank.
    More generally still, instead of a single conditional probability, one might want to specify a range of possible values. 
	%  Relatedly, one could imagine a 
    We will see in \cref{sec:prob-widget} that these seeming generalizations of a PDG are equivalent to the definition we have already given. 

    \item 
    \textbf{Parametric PDGs.}
    % The conditional probabilities needn't be 
    One might want to also consider a \emph{parametric} variant of a PDG,
		 in which each cpd is not fixed, but dependent on some hyper\-parameters---like a neural network. 
    This approach, which we take in \cref{chap:LIR}, 
		may be especially appropriate if the cpds represent parametric models.
    Like the others, this parametric variant appears to be a more general object---%
	yet, once again, it turns out to be equivalent to all of the other variants of PDGs we have considered so far. 
\end{enumerate}

\subsection{Combining PDGs}
    \label{sec:pdg-combine}

How should we combine independent sources of information? 
%
As illustrated by \cref{ex:grok-union}, a significant advantage of PDGs is their modularity:
% we can combine the information in $\dg M_1$ and $\dg M_2$ 
there is a simple way to combine the information in $\dg M_1$ and $\dg M_2$: take the union of their variables and the disjoint union of their arcs (and associated data) to get a new PDG, denoted $\dg M_1 + \dg M_2$.
We call this the \emph{sum} or \emph{join} of $\dg M_1$ and $\dg M_2$, 
    and it is appropriate when the information in the two PDGs is independent. 
% (As we will son see, )
% We first give the formal details of this operation, 
%     and then caution a modeler to think carefully about whether or not
%     this is 

% \paragraph{Formal Details of PDG Sums.}
To be completely precise, it's helpful to first make a preliminary definition:	
% \begin{defn}
%     % [$\V$-compatibility]
%         \label{defn:v-compat}
% 	% Two PDGs 
% 	$\dg M_1$ and $\dg M_2$ are $\V$-\emph{compatible}
% 	iff the variables they have in common take the same sets of values%
%     ---that is, if the nodes and values of $\dg M_1$ and $\dg M_2$,
% 	are $\N_1, \V_1$ and $\N_2, \V_2$, respectively, then
%     $
% 	\forall X \in \N_1 \cap \N_2.~~
% 		\V_1( X) = \V_2 (X). \qedhere
% 	$
% \end{defn}
$\dg M_1$ and $\dg M_2$ are $\V$-\emph{compatible}
iff the variables they have in common take the same sets of values%
---that is, if the nodes and values of $\dg M_1$ and $\dg M_2$,
are $\N_1, \V_1$ and $\N_2, \V_2$, respectively, then
$
\forall X \in \N_1 \cap \N_2.~~
	\V_1( X) = \V_2 (X). $
%
%
\commentout{%
	This definition is the same as the one given by \citet{halpern-combining-causal-models}, in combining causal models (structural equations models, or SEMs, to be precise).
	It is worth noting that SEMs are another formalism in which the variables and their possible values are given separately. 
	But the definition is unnecessary if we take the traditional approach in graphical models and rely on implicit variable equality.}
% Indeed, if
%
We can only combine PDGs that are $\V$-compatible. 
But this is not a very significant restriction;
% If in place of the pair $(\N, \V)$, PDGs are specified directly using a set $\X$ of variables, then this definition is unnecessary, as its sole purpose is to ensure that we do not accidentally identify variables in different PDGs that do not take on the same values. 
its sole purpose is to ensure that we do not accidentally identify variables in different PDGs that do not take on the same values. 
 % $N \in \N$ in $\dg M_1$ and $\dg M_2$ if their interpretations can be identified as one variable. With that, we can make
%
% In any case, the point is the following formal definition for how to combine independent information:

\begin{defn}
		\label{defn:pdg-sum}
	The \emph{sum} or \emph{join} of two $\V$-compatible PDGs is given by
		taking the disjoint union of their arcs and associated data.
	Explicitly, if
	$\dg M_1 = (\N_1, \V_1, \Ar_1, \mathbb P_1, \balpha_1,\bbeta_1)$ and
	$\dg M_2 = (\N_2, \V_2, \Ar_2, \mathbb P_2, \balpha_2,\bbeta_2)$
	are $\V$-compatible, then
	\[
		\dg M_1 + \dg M_2 :=
			\Big(
				\N_1 \cup \N_2,~~
				\V_1 \cup \V_2,~~
				\Ar_1 \sqcup \Ar_2,~~
				\mathbb P_1 \sqcup \mathbb P_2,~~
				\balpha_1 \sqcup \balpha_1,~~
				\bbeta_2 \sqcup \bbeta_2,
			\Big),
	\]
	% where, for $f,g : X \to Y$, the function $f \sqcup g : X \sqcup X \to Y$
	% is given by
	% \[
	% 	f \sqcup g := \left\{\singlespacing\begin{array}{c}
	% 		\mathrm{inl}(x) \mapsto f(x) \\
	% 		\mathrm{inr}(x) \mapsto g(x)
	% 	\end{array} \right.
	% 	.
	% 	% f \sqcup g ( \mathrm{inr}(x)) := g(x).
	% 	\qedhere
	% \]
	where, for $f : A \to X$ and $g : B \to X$, the function 
        $f \sqcup g : A \sqcup B \to X$
	is given by
	\[
		f \sqcup g := \left\{\singlespacingmath\begin{array}{c}
			(A,a) \mapsto f(a) \\
			(B,b) \mapsto g(b)
		\end{array} \right.
		.
		% f \sqcup g ( \mathrm{inr}(x)) := g(x).
		\qedhere
	\]
\end{defn}

\newmaterial{%
%%%%% WHAT'S MISSING IS THE FACT THAT THERE IS NO INVERSE TO ADDING A VARIABLE.
This operation is called a \emph{sum}
    in part because PDGs 
    over a fixed set of variables $\X$
    effectively form a real (infinite-dimensional)
    vector space with this operation.
We will make this precise in \cref{sec:pdg-vecspace}. 
    % and scala multiplication for     
    % in which addition is given by a sum of PDGs,
    % scalar multiplication scales the confidence vectors
    % vectors $\balpha$ and $\bbeta$, 
    % and the identity element is the PDG containing only the variables $\X$. 
}%
% In addition to this \emp



\commentout{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As mentioned above, combining PDGs is simpler (and circumvents $\V$-compatibility) if PDGs are defined through a set of variables $\X$. 
Given
$\dg M_1 = (\X_1, \Ar_1, \mathbb P_1, \balpha_1,\bbeta_1)$ and
$\dg M_2 = (\X_2, \Ar_2, \mathbb P_2, \balpha_2,\bbeta_2)$,
we can form
\[
	\dg M_1 + \dg M_2 =
		\Big(
			\X_1 \cup \X_2,~~
			\Ar_1 \sqcup \Ar_2,~~
			\mathbb P_1 \sqcup \mathbb P_2,~~
			\balpha \sqcup \balpha,~~
			\bbeta \sqcup \bbeta
		\Big);
\]
in this case, $\V$-compatibility is handled automatically by
the meaning of variable equality. 
While the formalism is simpler, this approach really just sweeps the issue under the rug. 
For instance, one needs to take care that two variables are not implicitly considered unequal simply because because they come from different contexts.
}%%%%%%%%%%%%%%%%%%%%%%%%
% and nor are they considered equal simply because they happen to have been given the same generic name ($X$) by two people modeling different things.  

% Quantitatively, the resulting PDG is 
% In many cases, this operation does exactly what one would want.

\paragraph{An Important Caveat For Qualitative (Structural) Information.}
% It is important to keep in mind, however, that this means treating the information in the PDGs 
For the quantitative data (i.e., the parameters $\mathbb P$ and $\bbeta$), this is precisely how one would want to combine independent observations. 
% Independently combining
Merely joining
the qualitative information, however, may not always 
    have the intended effect.
 % if there is any overlap. 
To illustrate, suppose that $\dg M_1$ and $\dg M_2$ both contain a single arc from $X$ to $Y$,
	each specifying a single causal mechanism by which $X$ gives rise to $Y$.
The 
% combined
PDG $\dg M_1 + \dg M_2$ contains two \emph{distinct} mechanisms by which $X$ gives rise to $Y$ (see \cref{chap:QIM}).
% To be precise: taking the sum is appropriate if the two models describe underlying mechanims that are in fact independent of one another. 
% This is especially appropriate when the two hypergraphs
This is appropriate if the  underlying mechanisms are in fact independent---
for instance, if the presence of a compound (whose concentration is the variable $X$) is known to inhibit an enzyme (whose activity is the variable $Y$) in two unrelated ways.
Mechanism independence (again, see \cref{chap:QIM})
    is also typically a reasonable assumption if the two models
    have sufficiently different structures (e.g., if
\raisebox{-.2ex}{%
\begin{tikzpicture}[center base]
	\node[dpadinline] (X) at (0,0) {$X$};
	\node[dpadinline] (Y) at (1.0,0){$Y$};
 	\draw[arr1,->] (X) to (Y);
\end{tikzpicture}}
and
\raisebox{-.2ex}{%
\begin{tikzpicture}[center base]
	\node[dpadinline] (Y) at (0,0) {$Y$};
	\node[dpadinline] (Z) at (1.0,0){$Z$};
 	\draw[arr1,->] (Y) to (Z);
\end{tikzpicture}}).
% It can also be appropriate in settings where 
% The sum of qualitative information may also be appropriate 
% But more generally
% Yet one may also want to model independent parallel mechanisms---
%     whose meaning we discuss in \cref{chap:QIM}. 
% It may also be appropriate to assume that mechanisms 


% However, one might well have intended to 
% Rather than adopting two distinct mechanisms,    one might well have intended to merge them, 
However, 
if one views the qualitative information in two different PDGs not as two distinct mechanisms, but as as independent accounts of the \emph{same} mechanism, 
then the sum of the PDGs alone is not appropriate---in
this case, it would be better to also merge the two mechanisms into a single one.
% one might well have intended to merge the two mechanisms,
% viewing the two arcs as independent accounts that there is a \emph{single} such causal mechanism. 
% This inability to  with factor graphs, which we discuss in \cref{ex:overdet}. 
% Yet PDG union performs no such merge.
The sum of PDGs performs no such merge.
%
%
% Indeed, the ``appropriate thing'' to do seems to depend on context, and so we urge the modeler to think about which (if any) qualitative mechanisms ought to be merged together, after this operation. 
Unfortunately, the ``appropriate thing'' to do seems to depend on context, and so we urge the modeler to think about which (if any) qualitative mechanisms ought to be merged together, after this operation. 
We will return to this point in \cref{ex:overdet},
    and again in \cref{ex:overdet2}.
% ; indeed, we have not found an algorithm we endorse to implement that intuition, and so we urge the modeler to think carefully about the appropriate causal picture in the combined PDG.
% This inability to  with factor graphs, which we discuss in \cref{ex:overdet}. 
% We mention that there is an even bigger problem in combining factor graphs, because the parameters 
In that context, we will also see that the
	analogue of this problem is even bigger 
    for factor graphs, because 
    $\balpha$ and $\bbeta$ are essentially ``fused together'' in these models.
	% , as we will see in \cref{sec:factor-graphs}.
% We will explore this in \cref{ex:overdet}. 

%TODO: Convex combination of 
% Recall that we may identify a graph $G$ with its adjacency matrix, 
%     i.e., the vector of weights over all possible arcs that places 
It is worth mentioning that \emph{convex combinations} of qualitative parameters $\balpha$ actually \emph{do} work as one might hope. 
If you believe that there's a 60\% chance that the causal graph is $G_1$, and a 40\% chance that the causal graph is $G_2$,  
% there is no problem with taking  $\balpha$ to be
it is appropriate to form a new PDG with $\balpha :=$
$.6 \balpha_1 + .4 \balpha_2$, where $\balpha_1$ and $\balpha_2$ are vectors representing the causal graphs $G_1$ and $G_2$, respectively.%
    \footnote{%
        % Recall that, 
        More precisely, since $\balpha_1 : \Ar_1 \to \mathbb R$ and
        $\balpha_2 : \Ar_2 \to \mathbb R$ have different domains, 
        by $.6 \balpha_1 + .4 \balpha_2$ we really mean 
        $.6 (\balpha_1 \sqcup \mat 0) + .4 (\mat 0 \sqcup \balpha_2)$. 
        % Just like in an adjacency matrix,
        % $\alpha=1$ indicates the presence of an arc
        %     (one independent mechanism),
        % and $\alpha = 0$ indicates no arcs 
        %     (no independent mechanism).
        This is no different from taking the convex combination
        of two weighted graphs, which coincides with the convex combination of their adjacency matrices. 
    }
If $G_1$ and $G_2$ are distinct causal graphs that represent the same independencies, for instance, 
then $\balpha$ will still encode those shared independencies. 

% Still, despite the caution, there are also some cases where independent combination is appropriate even for causal information. 
% We now proceed with the formal details of PDG sums. 
% There are also cases where PDG sums are appropriate even for causal information. 
% To be precise: taking the sum is appropriate if the two models describe underlying mechanims that are in fact independent of one another. 
% This is especially appropriate when the two hypergraphs
%     describe very different things (e.g., 
% \raisebox{-.2ex}{%
% \begin{tikzpicture}[center base]
% 	\node[dpadinline] (X) at (0,0) {$X$};
% 	\node[dpadinline] (Y) at (1.0,0){$Y$};
%  	\draw[arr1,->] (X) to (Y);
% \end{tikzpicture}}
% and
% \raisebox{-.2ex}{%
% \begin{tikzpicture}[center base]
% 	\node[dpadinline] (Y) at (0,0) {$Y$};
% 	\node[dpadinline] (Z) at (1.0,0){$Z$};
%  	\draw[arr1,->] (Y) to (Z);
% \end{tikzpicture}}).
% % It can also be appropriate in settings where 
% % The sum of qualitative information may also be appropriate 
% % But more generally
% % Yet one may also want to model independent parallel mechanisms---
% %     whose meaning we discuss in \cref{chap:QIM}. 
% % It may also be appropriate to assume that mechanisms 
% One may want to keep two mechanisms distinct even if they are parallel, if they are known to be different and simultaneously present. 
% For instance, if the presence of a compound (whose concentration is the variable $X$) is known to inhibit an enzyme (whose activity is the variable $Y$) in two unrelated ways, then a combined model should include both mechanisms. 
% We discuss the meaning of parallel mechanisms in more detail in \cref{chap:QIM}. 



\section{The Semantics of PDGs}
	\label{sec:semantics}
Although the meaning of an individual cpd is clear, we have not yet given 
PDGs a ``global'' semantics. We discuss three related approaches to doing so.
The first is the simplest: we associate with a PDG the set of distributions that
are consistent with it. This set will be empty if the PDG is inconsistent.
The second approach associates a PDG with a scoring function, indicating the fit
of an arbitrary distribution $\mu$, and can be thought of as a \emph{weighted}
set of distributions \citep{HL12}. This approach allows us to distinguish
inconsistent PDGs, while the first approach does not. The third approach chooses
the distributions with the best score, typically associating with a PDG a unique
distribution.

\subsection{PDGs As Sets Of Distributions}\label{sec:set-of-distribution-semantics} 
We have been thinking of a PDG as a collection of constraints on distributions,
specified by matching cpds. From this perspective, it is natural to consider the
set of all distributions that are consistent with the constraints.

\begin{defn} \label{def:set-semantics} 
If $\dg M$ is a PDG (weighted or unweighted) with arcs $\Ar$ and
cpds $\mathbb P$, 
let $\SD{\dg M}$ be the \emph{s}et of
\emph{d}istributions over the variables in $\dg M$ 
whose conditional marginals are exactly those given by $\mathbb P$.
That is, $\mu \in \SD{\dg M}$ iff, for all arcs $a \in \Ar$ from $X$
to $Y$,  $x \in 
\V(X)$,  and $y \in \V(Y)$, we have that $\mu(Y{=}y\mid X{=}x) = \p_a(Y{=}y\mid X{=}x)$.
% \notation{
Formally, define
    \[ \SD[\big]{\dg M} :=
    \left\{\mu \in \Delta \V\!\X ~\middle|~
        % {\singlespacing\begin{array}{l}
        % \forall \ed aXY \in \Ar, x \!\in\!\V\!X, y \!\in\! \V Y.\\[0.1ex] 
        % ~~\mu(Y{=}y \mid X{=}x) \geq \p_a(y \mid x) 
        % \end{array}}
        \forall \ed aXY \in \Ar.~\mu(Y|X) = \p_a(Y|X)
        \right\}.
    \]
    % }
We say that
$\dg M$ is \emph{inconsistent} if $\SD{\dg M} = \emptyset$, and \emph{consistent} otherwise.
\end{defn}
% We remarks that $\SD{\dg M}$ 
% is really an observational notion---it takes into account 
Note that $\SD{\dg M}$
% is independent of 
does not depend on the weights $\balpha$ or $\bbeta$.
Furthermore, it reflects only the quantitative probabilistic information in the PDG; it does not say anything about independencies, for example.
Developing an analogous definition for a \emph{qualitative} information in a PDG is far more subtle, and is the subject of \cref{chap:QIM}. 

This semantics is useful, and we will make frequent use of it, 
but it has one other extremely important shortcoming: viewed through the lens of this semantics, all inconsistent PDGs are equivalent, no matter how small the inconsistency.
In the next subsection, we develop another semantics, which, among other things, gives us a modification of this semantics that does not have this problem.


\subsection{PDGs As Scoring Functions over Joint Distributions}
    \label{sec:scoring-semantics}   

% We now explain the primary way that PDGs 
We now present the full PDG semantics. 
% We now give the full semantics . 
Specifically, we will interpret a PDG $\dg M$ 
not as a (convex) set of distributions, but rather as a
(convex) \emph{scoring function} on distributions. 
% given an arbitrary distribution 
Given $\mu \in \Delta \V\!\X$, the scoring function for $\dg M$
returns a real-valued score,
% indicating the degree of \emph{incompatibility} between $\mu$ and $\dg M$.
indicating how well (or, rather, how poorly) $\mu$ matches the information in $\dg M$. 
 % returns a real-valued score indicating how well $\mu$ fits $\dg M$.
Distributions with the lowest (best) scores are those that most closely match the cpds in $\dg M$, and contain the fewest unspecified correlations.

% This scoring function has two components: 
%     one for the qualitative (observational) information in the PDG, 
%     and one for the qualitative (structural information). 


These two aspects of the scoring function correspond 
to the two kinds of information present in a PDG:
``structural'' information, in the (hyper)graph $\Ar$ and
weights $\balpha$, and ``observational'' data,
in the cpds  $\mathbb P$ and weights $\bbeta$.
PDG semantics are based on two scoring functions
that quantify discrepancy between 
each type of information and a distribution
$\mu \in \Delta \V \!\X$ over its variables.

\paragraph{Quantitative/Observational Scoring.}
We need a function that measures ``distance'' between $\mu$ and the cpds of $\dg M$.
The key desideratum for such a function is that it must assign higher scores to distributions $\mu$ when $\dg M$ requires larger changes to be consistent with $\dg M$. 
In principle, there are many functions with this property. 
Yet, because it is a discrepancy between beliefs ($\dg M$) and reality ($\mu$)
    there is an especially natural way of measuring its magnitude: relative entropy.

% remind reader about which argument is which and refresh on the formula.   
Recall (from \cref{sec:relent-primer}) that relative entropy 
% $\thickD(\mu\Vert p)$
$\kldiv{\mu}{p} = \Ex_{\mu}[\log \frac\mu p]$
measures divergence of a belief $p$ with respect to reality $\mu$.
It can be viewed, for example, as the overhead (in extra bits per sample)
of using representations optimized for $p$, when in fact samples are distributed according to $\mu$ \citep{mackay2003information}.
This suggests scoring distributions with a weighted sum of relative entropies
    of the appropriate types. 

% We measure the magnitude of this perturbation with relative entropy.
\begin{defn}
    \label{def:inc}
The \emph{observational incompatibility} of $\mu(\X)$ with 
$\dg M 
= (\X, \Ar, \mathbb P, \balpha, \bbeta)
$
is given by the weighted sum of relative entropies:
\begin{equation}
    \OInc_{\dg M}(\mu) :=
        \sum_{\ed aST \mathrlap{\,\in \Ar}} \subafalse
        \beta_a\, \kldiv[\Big]
            {\mu(\Tgt a,\Src a)}
            {\p_a(\Tgt a | \Src a)\, \mu(\Src a)}.
        % \qedhere
        \mathrlap{\qquad~~\popQED}
        % \popQED
            \label{eq:oinc1}
\end{equation}
\end{defn}

As we have argued, not all inconsistencies are equally problematic. 
% If one uses cpds in proportion to the confidence one has in them, then
Discrepancies with low confidence cpds are not as problematic as  discrepancies with trusted, high-confidence ones.
The confidences $\bbeta$ operationalize this relative importance. 
% Discrepancies for low-confidence cpds are less costly for
% Inefficiencies for of high-confidence cpds are compounded, and hence more costly
So overall,
% Under the standard interpretation of the relative entropy,
$\OInc_{\dg M}$ measures the excess cost of using codes optimized for the cpds of $\dg M$ (weighted by their confidences $\bbeta$), when reality is
distributed according to $\mu$.
%%%%%%%%%%%%%%%%%%%%%%
% If one uses cpds in proportion to the confidence one has in them, then inefficiencies for of high-confidence cpds are compounded,
% and hence more costly.
% So $\Inc_{\dg M}(\mu)$ measures the total excess cost of
% using $\dg M$'s cpds in proportion to their confidences $\bbeta$,
% when worlds are distributed according to $\mu$.


% There is actually more than one way of making this precise. 

In \eqref{eq:oinc1}, 
% for each $a \in A$, 
we have one ``belief'' distribution per hyperarc $\ed aXY \in \Ar$, 
namely, the one induced by drawing $x \sim \mu(X)$ and applying 
the cpd $\p_a(Y|X{=}x)$. 
%
At the same time, it also makes sense to model things at a finer level.
A case could also be made that $\p_a$ describes not one context-dependent belief, but rather a separate belief $\p_a(Y|x)$ each 
% value of $X$. 
$x \in \V\!X$.
This suggests a different approach:
for each $\ed aXY$ and $x \in \V\!X$, start by measuring
the relative entropy from $\p_a(Y|x)$ to $\mu(Y|X{=}x)$.
% Just as different cpds are weighted differently,
We still want to combine these into a single number, 
and not all of these beliefs have equal importance--those
% that correspond to $x$ that are  $\mu(X{=}x) \approx 0$
that correspond to the likeliest values of $X$ should have the most weight---%
which suggests taking an expectation of these quantities with respect to the marginal distribution $\mu(X)$.
After taking a weighted sum over arcs as before, we would get the quantity
% on the left of \eqref{eq:oinc2}. 
\begin{equation}
    \OInc_{\dg M }( \mu ) = 
        \sum_{\ed aXY \mathrlap{\in \Ar}}~ \beta_a \cdot \Ex_{x \sim \mu(X)} 
        \left[\kldiv[\Big]{ \mu(Y \mid X {=} x) }{\p_a(Y|\,x) } \right]
    % \OInc_{\dg M}(\mu)  = \sum_{\ed aXY}\!\! \beta\ssub a  \Ex_{{x \sim \mu(X)}} \kldiv[\Big]{\mu(Y|\, x)}{p_a(Y |\, x)}
    .
        \label{eq:oinc2}
\end{equation}
% It is easy to show that this definition of $\OInc$ is the same
% as the previous one.
It is not hard to show that our two formulas for $\OInc$, 
\eqref{eq:oinc1} and \eqref{eq:oinc2}, are in fact equal---%
reflecting a(nother) virtue of relative entropy.
%
% This is just one more 
% The equality between our two expressions for $\OInc$, 
% \eqref{eq:oinc1} and \eqref{eq:oinc2}, is
%
% This is one more virtue of relative entropy
% not a given.
(It would not have been the case, for instance, had we taken $\kldiv {q}{p}$ to be the Euclidean distance between $q$ and $p$ as vectors.)
% is rather one of the many virtues of relative entropy.
Over the course of this thesis, we will discover a great many more. 
% The following just makes precise that 
% But for the 
Still, any measure of discrepancy $\thickD$ would have sufficed to get
an analogue of the next result, which makes precise that
    % the scoring semantics generalizes the first semantics.
    the (observational) scoring function semantics of a PDG
    generalizes the set-of-distributions semantics from 
    the previous section (\ref{sec:set-of-distribution-semantics}).

\begin{linked}{prop}{sd-is-zeroset}
    If $\dg M$ is a PDG with $\bbeta > 0$, then
	% $\SD{\dg M} = \{ \mu : \OInc_{\dg M}(\mu) = 0\}$.
	$\mu \in \SD{\dg M}$ iff  $\OInc_{\dg M}(\mu) = 0$.
	% $\SD{\dg M} \!= \{ \mu : \bbr{\dg M}_0(\mu) \!=\! 0\}$ for 
	% all $\dg M$ that satisfy $\bbeta^{\dg M} > 0$. 
\end{linked}

% Indeed, $\SD{\dg M} = \OInc_{\dg M}^{\matbbm 1}$
It is worth pausing here to reflect on the meaning of $\bbeta$,
which is not a collection of probabilities, but of confidences in a different sense 
\iffoundations (which we develop at full generality in \cref{chap:conf}) \fi
\unskip.
%
A choice of $\beta_a = 0$ means that the cpd $\p_a$ is effectively ignored, in the sense that such a PDG is equivalent to one in which $a$ is attached to a different cpd $q \ne \p_a$ (or missing altogether, modulo the qualitative effect of removing the arc $a$).
This is why \cref{prop:sd-is-zeroset} requires $\bbeta$ to be nonzero.
At the the upper extreme, a large (or even infinite) value of $\beta_a$ indicates high (or absolute) confidence in $\p_a$. 
By default, we assume an intermediate value of confidence $\beta = 1$, which is just a convenient choice of units---whats important are the magnitudes of $\beta$ relative to one another.

Here's an concrete way to think about it.
In your head, fix a reference person---someone who is worth listening to a little bit, but far from infallible---and call that degree of trust ``$\beta = 1$''. 
Let's call this person Bob. 
To determine your degree of confidence in information
    % more trustworthy than Bob, 
ask yourself: how many times would I have to hear this independently from a person like Bob, before I got to this level of trust? The answer to that question is the value of $\beta$. 
Thus, $\beta = 2$ on an arc $X \to Y$ is (quantitatively) indistinguishable from two distinct arcs from $X \to Y$. 
% For information less trustworthy than Bob, ask yourself the reciprocal question: 

\newmaterial{%
Finally, although we typically focus on positive confidence, 
	our formalism also allows for negative confidences.
A confidence $\beta < 0$ on a cpd $p$ encodes an \emph{active distrust} of $p$, and semantically, means distributions are more consistent with this belief if they are further from satisfying $p$. 
In our previous analogy, if you currently distrust $p$ but would feel neutral towards it if someone like Bob were to tell you that it is correct, then you should select  $\beta=-1$.
}%

\paragraph{Qualitative/Structural Scoring.}
	\label{ssec:qual-scoring}
% $\SD{\dg M}$ and $\OInc_{\dg M}$ distinguish between distributions based on their compatibility with $\dg M$, but even among distributions that match the marginals, some more closely match the qualitative structure of the graph than others. 
So far, we have said nothing about the semantics of the qualitative information in a PDG.  
% Getting something of an analogue of 
For traditional graphical models---that is, Bayesian Networks (BNs) and Markov Random Fields (MRFs)---the qualitative information is a conjunction of independencies implied by the graphical structure. 
% But for PDGs, it's not 
But what are the independencies implied by a directed hypergraph? Or even an ordinary directed graph with cycles?
% But it's not clear what the independence implied even for cyclic directed graphs---let alone directed hypergraphs. 
Despite decades of interest in this simple question, no clear consensus has emerged.
% In \cref{chap:QIM}, will develop a convincing answer to these questions, that suggest 
% We will see in \cref{chap:QIM} that there is reason for this:
%     the question is posed too narrowly:
% Resolving these questions is the subject \cref{chap:QIM},
%     in which we will see why they have remained unanswered: 
% When we develop a qualitative set-of-distributions semantics for directed hypergraphs 
As we shall see in \cref{chap:QIM}, there is a good reason why: qualitative information in an arbitrary directed (hyper)graph is not just about independence; in general, it can be far more subtle. 
% Instead, we will develop this theory the other way around, starting with the scoring function. 
% So in starting with the st
% That development relies on the scoring function.
Indeed, for the qualitative information, we have found it easier to motivate the scoring function directly.


Intuitively, 
    each arc $\ed aXY$ represents 
    % the qualitative that there is an (independent), 
    an independent randomized mechanism by which $X$ leads to $Y$. 
Or, perhaps slightly more precisely, that $Y$ can be computed from $X$ alone (and independent random noise). 
 % an \emph{independent mechanism}:
% Recall frthat the degree of noise .
So, given a (hyper)graph $\Ar$ whose nodes correspond to variables,
    and distribution $\mu(\X)$ over (the values of) those variables,
% contrast the amount of information required to 
% contrast the following two quantities:
contrast the number of bits of randomness needed to do each of the following (in expectation):
%
\begin{enumerate}[label=(\alph*),nosep]
\item 
% directly describe a joint outcome 
% $\mat w$ drawn from $\mu$, and 
% $\omega \sim \mu$, and 
% the number of bits of randomness needed to 
directly specify the values of all variables $\X$, and 
 \label{item:globalinfo}
\item 
% separately specify, for each arc $\ed aXY$, the value $\mat w_Y$ (of $Y$ in world $\mat w$) given the value $\mat w_X$, in expectation.
separately specify, for each arc $\ed aXY$, the value
% $\omega[Y]$ (of $Y$ in world $\omega$) given the value $\omega[X]$, in expectation.
of $Y$ given the value of $X$.
	\label{item:localinfo}
\end{enumerate}
% Perhaps unsurprisingly, 
As one might have expected, we measure the amount of randomness using entropy.
In a sense, 
(b) is the total amount of randomness we need in order to generate components of an outcome of $\mu$ along the structure of the hypergraph $\Ar$,
while
(a) is the amount of randomness needed to generate an outcome of $\mu = \mu(\X|\varnothing)$ along its own simple dependency structure ($\varnothing \to \X$). 
Clearly, the two quantities must be equal in order for $\Ar$ to be a complete and accurate causal explanation for the origin of $\mu$.

% We think of each arc $\ed aXY$ as representing a qualitative claim 
% (with confidence $\alpha_a$) 
% that the value of $Y$ can be computed from $X$ alone. 
% To formalize this, we require only the multigraph $\Ar^{\dg M}$.

% What if they differ?
What if they differ?
% If \ref{item:globalinfo} $=$ \ref{item:localinfo},
% a specification of (b) has
%  exactly the same length as a full description of the world. 
If \ref{item:localinfo} $>$ \ref{item:globalinfo}, then there are
correlations in $\mu$ that allow for a more compact representation
than $\Ar$ represents---that is, $\mu$ is \emph{information deficient} for the structure $\Ar$. 
The larger the difference, 
% the more additional information is needed to determine
% targets $Y$ beyond the conditional probabilities associated with the
% arcs $X \rightarrow Y$ leading to $Y$
% (which according to $G$ should be sufficient to compute them), 
the larger the overhead of using the structure $\Ar$ to compute probabilities, compared to what is necessary,
    and so the poorer the structural fit of $\mu$ to $\Ar$.
% In a way, this is a direct qualitative analogue of the information-theoretic interpretation of $\OInc$.
Notice how this parallels the overhead of erroneous quantitative beliefs discussed above.
If \ref{item:globalinfo} $>$ \ref{item:localinfo}, 
on the other hand (e.g., if $\Ar$ is empty), then $\Ar$ must be an incomplete qualitative picture---because to explain everything in $\mu$, at least $\H(\mu)$ bits are required.
Although an incomplete picture may not be a good thing, 
	it fits data better than a complete one. 
So, in both cases, the fit is worst when (b) is large and (a) is small.
This observation motivates our qualitative scoring function.
%
% 
% $\mu$ additional information to specify, beyond
% what is necessary to determine $\mu$ along $\Ar$.

\begin{defn}\label{def:info-deficiency}
% For a multigraph $G = (\N, \Ar, \V)$ over a set $\N$ of variables,
% Given a directed (multi)(hyper)graph $\Ar$, 
The \emph{structural (information) deficiency}
of distribution $\mu(\X)$, with respect to a directed (hyper)graph $(\X,\Ar)$ is given by:
% denoted $\SDef_{\!\Ar}(\mu)$,
% is the difference between (a) and (b), 
% where we measure the amount of information needed for a description
% using entropy: 
\begin{equation}
	\SDef_{\!\Ar}(\mu) := \sum_{\ed aXY \in \Ar}
        \alpha_a
        \H_\mu(Y\mid X) ~- \H_\mu(\X). 
	\label{eqn:sdef}
\end{equation}
% (Recall that $H_\mu(Y\mid X)$, the \emph{conditional entropy of $Y$ given $X$}, is defined as $- \sum_{x,y \in \V(X,Y)} \mu(x,y) \log \mu(y\mid x)$.)
For a PDG ${\dg M}$, we take $\SDef_{\dg M} := \SDef_{\!\Ar^{\dg M}}$
to be shorthand for the information deficiency with respect to the  structure of $\dg M$.
\end{defn}


%%%%%%%%%%%%%%%%%%%%% FROM LIR %%%%%%%%%%%%%%%%%%%%%%
\commentout{
%%%%
% UPDATE:
% This material does not need to be done separately so long as we do entropy properly in the preliminaries section, including its continuous variant. 
%
\begin{align*}
    % \SInc_{\dg M}(\mu) := - \H(\mu) + \sum_{L \in \Ed} \alpha\ssub L\, \H_\mu(\Tgt L | \Src L),
    \SDef_{\!\dg M}(\mu) &:=
    % \SDef_{\!(\Ar,\balpha)}(\mu) &:=
        % - \H(\mu) + \sum_{a \in \Ar} \alpha_a\, \H_\mu(\Tgt a | \Src a).
        % \pqty[\Big]{\; \sum_{\ed aST \mathrlap{\,\in \Ar}}\subafalse \alpha_a\, \H_\mu(\Tgt a | \Src a) } - \H(\mu).
        %%%%
        % \kldiv{\mu}{\lambda_{\X}} -
        % \sum_{\ed aST \mathrlap{\,\in \Ar}}\subafalse \alpha_a\,
        %     \kldiv{\mu (\Tgt a\Src a)} { \lambda_{\Tgt a} \mu(\Src a) }
        \numberthis
         \label{eq:sdef}
        %%%%
        % \\ &=
            \Ex_{\mu}  \bigg[ \log\; \frac{\mu(\X)}{\lambda(\X)}
            % \prod_{\vphantom{\big|}\ed aST \mathrlap{\,\in \Ar}}\subafalse
            \prod_{\ed aST }\subafalse
                \left(\frac{\lambda(\Tgt a|\Src a)}{\mu(\Tgt a | \Src a)}\right)^{\!\alpha_a}
            \bigg]
    ,
    % \vspace{-1ex}
\end{align*}
%UPDATE: 
and, roughly, measures $\mu$'s failure to arise as a result of
    independent causal mechanisms along each edge.
If $\Ar$ is a qualitative Bayesian Network, for instance,
    then $\SDef_{\!\Ar}(\mu) \ge 0$ with equality
    iff $\mu$ has the independencies of $\Ar$.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We illustrate $\SDef$ with some simple examples.
	% the reader can follow along with graphical visualizations
	% 	in \cref{fig:sdef-example-panel}.
% in which $\Ar$ has just two variables, $X$ and $Y$. 
First, when $\Ar$ has no arcs 
% (e.g., subfigures \ref{subfig:justX-0} and \ref{subfig:justXY})
\unskip, then $\SDef_{\!\Ar}(\mu) = - \H(\mu)$. 
% There is no information required to specify, for
% each arc in ${\dg M}$ from $X$ to $Y$, the value ${\mat w}_Y$ given ${\mat
% w}_X$, since there are no arcs.
Since smaller numbers represent a better fit, 
	$\SDef_{\dg M}$ tells us to maximize entropy---%
which is precisely what the literature tells us to do, at least, to break ties between distributions consistent with our observations in the absence of no structural or causal information \citep{jaynes1957information}. 
% If $\dg M$ has one arc from $X$ to $Y$, then since $\H(\mu) =
% \H_{\mu}(Y \mid X) + \H_\mu(X)$
% by the well known \emph{entropy chain rule} \citep{mackay2003information},
% $\SDef_{\dg   M}(\mu) = -\H_{\mu}(X)$.
Next, suppose $\Ar = 
\begin{tikzpicture}[center base]
	\node[dpadinline] (X) at (0,0) {$X$};
	\node[dpadinline] (Y) at (1,0) {$Y$};
	\draw[arr2] (X) to (Y);
\end{tikzpicture}
$
% (\ref{subfig:XtoY})
\unskip.
% Intuitively, while knowing the conditional probability $\mu(Y \mid X)$ is helpful, to
% completely specify $\mu$ we also need $\mu(X)$. Thus, in this case, $\SDef_{\dg
% M}$ prefers distributions that maximize the entropy of the marginal on $X$. 
In general, specifying $X$ and $Y$ together takes the same amount of information as first specifying $X$, and then using that value to specify $Y$ (recall the chain rule, from \cref{sec:qual-info-theory-primer}).
So $\Ar$ is incomplete, and missing precisely an explanation for the origin of $X$. 
By the same reasoning as before, the best bet is to maximize entropy (but now just of $X$); indeed, $\SDef_{\!\Ar} = - \H_\mu(X)$. 
%
If $\Ar$ has sufficiently many parallel arcs
 from $X$ to $Y$
 and $\H_{\mu}(Y \mid X) > 0$ 
(so that $Y$ is not totally determined by $X$)
then we have $\SDef_{\!\Ar}(\mu) > 0$, because the redundant arcs add no
information, but there is still a cost to specifying them.
In this case, $\SDef_{\!\Ar}$ prefers distributions that make $Y$ a
deterministic function of $X$ (while still maximizing the entropy of $X$).
%
Finally, if $\Ar = \begin{tikzpicture}[center base]
	\node[dpadinline] (X) at (0,0) {$X$};
	\node[dpadinline] (Y) at (1.3,0) {$Y$};
	\draw[arr2] (X) to[bend left] (Y);
	\draw[arr2] (Y) to[bend left] (X);
\end{tikzpicture}$,
%  then a distribution $\mu$ minimizes $\SDef_{\!\Ar}$ when 
% $X$ and $Y$  vary together (so that $\H_\mu(Y \mid X) = \H_\mu(X \mid Y) = 0$)
% while maximizing $\H(\mu)$, 
then $\SDef_{\!\Ar}(\mu) = - \I_\mu(X ; Y)$ encourages mutual information between $X$ and $Y$
(and so it is optimal for $X$ and $Y$ to be functions of each other).
% for example, by taking $\mu(0,0)
% = \mu(1,1) = 1/2$. 

\input{fragments/idef-intuition}

% To build intuition for $\SDef$, which characterizes our bounds in \cref{sec:info},
% we now visualize the vector $\mat v_{\!\Ar}$ for various example hypergraphs. 
Since $\SDef_{\!\Ar}$ is a linear combination of (conditional) entropies, it is also a linear function of $\mu$'s information profile vector.
This means $\SDef_{\!\Ar}$ is a dual vector in information profile space, and thus can be illustrated with the same diagram as used to show information profiles.
A number of small examples have been visualized this way in \cref{fig:sdef-example-panel}.
We now make a few observations.
% We now make a few observations about that figure. 

\begin{itemize}
\item
In the definition of $\SDef$, each arc $X\to Y$ is compiled to a ``cost'' $H(Y|X)$ for uncertainty in $Y$ given $X$.
One can see this visually in \cref{fig:sdef-example-panel}
in the form of a red crescent that's added to the information profile as we move 
from \ref{subfig:justXY} to \ref{subfig:XtoY} to \ref{subfig:XY-cycle}.
In the unconditional case,
	Subfigures \ref{subfig:justX-0}, \ref{subfig:justX-1}, 
    and \ref{subfig:justX-2} show
    how adding arcs increases the incentive for determinism.

\item 
Some hypergraphs (see \Cref{subfig:justX-1,subfig:1XY}) are \emph{indiscriminate}, in the sense that every distribution gets the same score.
% (so that score must be zero, because this is the score a degenerate distribution gets).
Specifically, a score of zero, because a point mass $\delta$ always has $\SDef_{\!\Ar}(\delta) = 0$:
% For such a distribution, no randomness is required to generate anything at all.
for such a distribution, no randomness is required to generate a joint sample over all variables, and nor is any randomness required to generate samples along $\Ar$.

\item
As we will soon see, 
 $\SDef$ can indicate independencies and conditional independencies, illustrated respectively in Subfigures \ref{subfig:XYindep} and \ref{subfig:1XYZ}.

\item
 For more complex structures, the
    structural information deficiency $\SDef$ 
    can represent more than independence and dependence. 
The cyclic structures in \cref{example:xy-cycle,example:xyz-cycle-1}, 
    correspond to the structural deficiencies pictured
    in Subfigures \ref{subfig:XY-cycle} and \ref{subfig:XYZ-cycle}, respectively,
    which are functions that encourage shared information between the three variables. 
    
\item 
    The structures in blue boxes correspond to Bayesian Network (BN) structures, and their corresponding $\SDef$s 
    quantify discrepancy with a set of (conditional) independencies.
    % Only the boxed structures in blue,
    %  % whose $\SDef$ can be seen as measuring distance to a
    %  particular set of (conditional) independencies, 
    %  % are expressible as BNs.
    Conversely, only those structural deficiencies can be expressed with a BN. 
      
\end{itemize}

\paragraph{The Combined Scoring Funcion.}
$\OInc_{\dg M}(\mu)$ and $\SDef_{\dg M}(\mu)$ give us two measures
of compatibility between ${\dg M}$ and a distribution $\mu$.
The right way to combine them into a single score, as we must 
	to handle cases in which the observational structural information conflict with one another, 
depends on the importance of structure relative to observation. 
% If the observational and structural information conflict, 
% then the distribution(s) that best represent a PDG 
% will depend
% on the importance of structure relative to observation.
%
% \discard{ This is captured by a trade-off 
%     parameter $\gamma \ge 0$, which 
%     can be used to define the scoring function
%     $\bbr{\dg M}_\gamma: \Delta \V\!\X \to \Rext$, as follows:}%
This could be captured by a trade-off parameter $\zogamma \in [0,1]$
that controls the convex combination
$(1-\zogamma) \cdot \OInc + \zogamma \cdot \SDef$. 
%
% For several reasons---%
% 	most importantly
%     % , to simplify the math, and
%     to bring out relationships with other graphical models and the thermodynamics literature, %
%     % which we will see below
% 	% and so as not to create the impression that $\OInc$ and $\SDef$ 
% 	% are playing the same role
% 	---%
However, for several reasons (discussed below),
% So as to simplify the math
% and match the notation in
% previous work (\citeyear{pdg-aaai,one-true-loss}),
our primary definition will instead be a rescaled variant with a different parameterization.
%
Using
$\gamma := \nicefrac{\zogamma}{(1-\zogamma)} \in [0,\infty]$,
define the overall scoring function:
\begin{align*}
    \bbr{\dg M}_\gamma&(\mu) 
        := \OInc_{\dg M}(\mu) + \gamma \, \SDef_{\!\dg M}(\mu)
            \numberthis\label{eqn:scoring-fn}  \\
        &= \scalebox{0.85}{$\displaystyle\frac{1}{1-\zogamma}$} \Big(\, (1-\zogamma) \OInc_{\dg M}(\mu) + \zogamma \, \SDef_{\!\dg M}(\mu)\, \Big) 
		\\
        &= \Ex\nolimits_{\mu}\bigg[
            \,
            \sum_{\ed aST \mathrlap{\, \in \Ar}} \subafalse
            \log \frac
            {\mu(\Tgt a| \Src a)^{\beta_a - \gamma \alpha_a}}
            {\p_a(\Tgt a | \Src a)^{\beta_a}}
        \bigg] - \gamma \H(\mu)
        .
\end{align*}

% We would like to point out that whil
While $\gamma$ and $\zogamma$ do indeed parameterize a trade-off between observational and structural information, the two quantities are not really symmetric.
% At an intuitive level, the quantitative information 
% The rthan the qualitative information, 
For instance, the structural information such as independencies could in principle be captured quantitatively (e.g., with a distribution happens to have those independencies), but not vice versa. 
At a lower level, $\OInc$ is unbounded and non-negative, while $\SDef$ is bounded and but can be negative. 
% This asymmetry is clearer 
% Thus, we should not particularly 
Because of the inherent asymmetry, 
    % it is not particularly desirable 
    % to use the symmetrical trade-off parameter $\zogamma$.
    there is no inherent reason to prefer
    a symmetrical formalism
    % (e.g., with $\zogamma$).
    % (like the one in terms of $\zogamma$). 

In addition, there are benefits to using $\gamma$ and the scoring function as we have defined them. 
% For instance, $\gamma$ may be seen as analogous to a \emph{temperature}.
The first is to draw an analogy with thermodynamics, that is 
    common in some sub-communities that study graphical models.
% Thermodynamically, 
A basic principle of thermodynamics is that
    macro-states (i.e., distributions) with higher entropy are more stable at higher temperatures, while macro-states with with lower entropy are more stable at lower temperatures.
In the case of no qualitative information (i.e., $\balpha = \mat 0$),
    the scoring function \eqref{eqn:scoring-fn}
    is a direct analogue of the Gibbs free energy $G = U - T \H$,
    where $T = \gamma$ is temperature and $\H$ is entropy. 
Furthermore, when $\bbeta = \gamma \balpha$,
    we will see soon see that \eqref{eqn:scoring-fn} is the free energy
    of a factor graph with weights $\bbeta$ and temperature $\gamma$
    (\cref{theorem:wfg-is-pdg}).
% Recall some basic chemistry:
%     the stability of a system is quantified by its \emph{Gibbs Free Energy} $G = H - TS$, where $H$ represents the 
% In the case of no qualitative information (i.e., $\balpa = \mat 0$),
% the scoring equation \cref{} 
Alternatively, $\gamma$ can be thought of as one final degree of confidence, in the same scale as the values of $\beta$, in the qualitative information overall.

For these reasons, and also to simplify the math, we have chosen to use the asymmetric variant in \eqref{eqn:scoring-fn} that uses the parameter $\gamma \in [0,\infty]$ to control the strength of $\SDef$, rather than the symmetric trade-off parameter $\zogamma \in [0,1]$.

% $\OInc_{\dg M}(\mu)$ and $\SDef_{\dg M}(\mu)$ give us two measures
% of compatibility between ${\dg M}$ and a distribution $\mu$.
% We take the score of interest to be their sum, with the tradeoff
% controlled by a parameter $\gamma \ge 0$:

% \begin{equation}
%   	  \bbr{\dg M}_\gamma(\mu)
% 	 := \OInc_{\dg M}(\mu) + \gamma \SDef_{\dg M}(\mu)  \label{eqn:full-score}
% \end{equation}
% 
%           
% While we focus on this particular scoring function in the paper, 
% in part because
% it has deep connections to the free energy of a factor graph \citep{KF09},
% other scoring functions may well end up being of interest. 
        


\subsection{PDGs As Unique Distributions}\label{sec:uniq-dist-semantics}

Finally, we provide an interpretation of a PDG as a probability distribution.
Before we provide this semantics, we stress that this distribution does
\emph{not} capture all of the important information in the PDG---for example, a
PDG can represent inconsistent knowledge states.  Still, by giving a
distribution, we enable comparisons with other graphical models%
, and show that PDGs are
a surprisingly flexible tool for specifying distributions.  
The idea is to select the distributions with the best score. 
We thus define 
\begin{equation}
	\bbr{\dg M}_\gamma^* = \argmin_{\mu \in \Delta\V\!\X} \bbr{\dg M}_\gamma(\mu).
\end{equation}   

In general, $\bbr{\dg M}_\gamma^*$ does not yield a unique distribution.  But if
$\gamma$ is sufficiently small, then it does:

\begin{linked}{prop}{sem3}
	% If $\dg M$ is a PDG and $0 < \gamma \leq \min_{a \in \Ar} 
	If $0 < \gamma \leq 
        {\displaystyle \min_{a \in \Ar} \frac {\beta_a^{\dg M}}{\alpha_a^{\dg M}} }$
    \unskip, then $\bbr{\dg M}_\gamma^*$ is a singleton. 
\end{linked}

This is because the scoring function $\bbr{\dg M}_\gamma$ is 
strictly convex for these (small) values of $\gamma$. 
%
% We will soon see that the 
In much of this dissertation, we will be
    particularly interested in the case where $\gamma$ is small,
    which means emphasizing the accuracy of the probability
    distribution as a description of probabilistic information,
    over the graphical structure of the PDG.  
This motivates us to consider
what happens as $\gamma$ goes to 0. If $S_\gamma$ is a set of
probability distributions for all $\gamma > 0$,
we define $\lim_{\gamma \rightarrow 0} S_\gamma$ to consist of all distributions $\mu$ such that there
is a sequence $(\gamma_i, \mu_i)_{i \in \mathbb N}$ with $\gamma_i \to 0$ and
$\mu_i \to \mu$ such that $\mu_i \in S_{\gamma_i}$ for all $i$. 
It can be further shown that 

\begin{linked}{theorem}{limit-uniq}
    % For all $\dg M$
    % with $\bbeta \gg \balpha$
    %     (and, in particular, if $\bbeta > \mat 0$)
    For all proper PDGs (such as when $\bbeta > 0$),
    $\lim\limits_{\gamma\to0}\bbr{\dg M}_\gamma^*$ is a singleton.
\end{linked}
Let $\bbr{{\dg M}}_{0^+}^*$ be the unique element of $\smash{\lim\limits_{\gamma
	\rightarrow 0}} \bbr{{\dg M}}_\gamma^*$. 
This limiting semantics intuitively represents the ``quantitative'' or ``observational'' extreme, in which the quantitative probabilities and their confidences $(\mathbb P, \bbeta)$ dominate, and the qualitative information $(\Ar, \balpha)$ is used only to break ties. 
%
\oldnewmaterial{%
This makes the semantics $\bbr{ - }_{0^+}^*$ a direct analogue of (and, indeed, a generalization of) the principle of maximum entropy. 
}%
This semantics has an important property: 

\begin{linked}{prop}{consist}
	$\bbr{\dg M}^*_{0^+} \in \bbr{\dg M}_0^*$, so if $\dg M$ is consistent,
	then $\bbr{\dg M}^*_{0^+} \in \SD{\dg  M}$.
\end{linked}

% Thus, all of our semantics are compatible 
% In this sense, all of our semantics that focus on observation are compatible with one another.
% Thus, all of our quantitatively focused semantics capture
Thus, $\bbr{\dg M}_{0^+}^*$ is the unique distribution among those consistent with the cpds of a consistent PDG, that has minimal structural deficiency. 

% \subsection{Further Semantics: PDGs as Degrees of Inconsistency 
%     \texorpdfstring{\\}{}and as Transformations on Distributions}
\subsection{Inconsistency Semantics: PDGs as Real Numbers}

The three semantics given in the previous section are the ones presented in the original PDG paper \citep{pdg-aaai}.
However, there are other semantics that will play an important role later on. 
Far and away the most important semantics, which we have talked about at length but still not explicitly defined,  
is a PDG's \emph{(degree of) inconsistency}:
% 
% The (observational) \emph{inconsistency of PDG $\dg M = 
%     (\N, \Ar, \mathbb P, \balpha, \bbeta)
%     $}, 
% \[ 
%     % \OInc(\dg M) 
%     \aar{\dg M}_0
%     := 
%     % \inf_{ \mu \in \Delta [W_{\cal V}]} 
%     \inf_{ \mu \in \Delta \V\!\X} 
%     \OInc_{\dg M}(\mu) . 
% \]
the smallest possible incompatibility of $\dg M$ with any distribution.
More precisely, for each $\gamma$, we can define
\begin{equation}
    \aar[\big]{\dg M}_{\gamma} := \inf_{\mu \in \Delta \V\!\X} \bbr{\dg M}_\gamma(\mu) 
    \quad \in \Rext.
        \label{eqn:inconsistency-defn}
\end{equation}
Thus, it interprets a PDG as a single number.  This quantity may at first seem less important than the others, but we will see in \cref{chap:one-true-loss} that it is a ``universal loss function''.
Furthermore, we will show in \cref{chap:inc-infer-connection} that all of the other semantics (including the full scoring function) can be recovered from PDG's inconsistency semantics. 

\iffoundations
In \cref{chap:relent-soup}, we will see how PDGs also naturally arise from a second perspective, where they have a semantics in terms of belief updates, so that we can interpret a PDG $\dg M$ as a function $\bbr{\dg M}^\top : \Delta \V\!\X \to \Delta \V\!\X$
that updates distributions.
%
Both of these alternate semantics turn out to to be equivalent to the scoring function semantics developped in \cref{sec:scoring-semantics}. 
\fi

\section{Relationships to Other Graphical Models}
    \label{sec:other-graphical-models} 

% We start by relating
In this section, we relate
PDGs to two of the most popular graphical models: 
BNs and factor graphs.
PDGs are strictly more general than BNs, and emulate factor graphs for the particular value $\gamma = 1$. 
More precisely, the distribution specified by a BN $\dg B$
is the unique one that minimizes both $\OInc_{\dg B}$ and $\SDef_{\dg B}$ (and hence every positive linear combination of the two), while the
distribution specfied by a factor graph $\Phi$ uniquely minimizes the
sum $\OInc_{\Phi} + \SDef_{\Phi}$
in which the observational and structural information are weighted equally.
Now for the details.

\subsection{Bayesian Networks} 
    \label{sec:bn-pdg}

\Cref{constr:hyperedge-reducton} can be generalized to convert arbitrary Bayesian Networks into PDGs.
Given a BN $\mathcal B$ and a positive confidence $\beta_X$ for
the cpd of each variable $X$ of $\cal B$,
let $\PDGof{\mathcal B, \beta}$
be the PDG comprising the cpds of $\cal B$
in this way.
% ; we defer the straightforward formal details to the appendix. 

	
\begin{linked}{theorem}{bns-are-pdgs}
 	  If $\cal B$ is a Bayesian network
          and $\Pr_{\cal B}$ is the distribution it specifies, then
        for all $\gamma > 0$ and all vectors $\beta$ such
        that $\beta_a > 0$ for all arcs $a$,
        $\bbr{\PDGof{\mathcal B, \beta}}_\gamma^* = \{ \Pr_{\cal B}\}$, 
and thus $\bbr{\PDGof{\mathcal B, \beta}}^* = \Pr_{\cal B}$.    
\end{linked}
\Cref{theorem:bns-are-pdgs} is quite robust to parameter choices: it holds for every
weight vector $\beta$ and all $\gamma > 0$.
However, it does lean heavily on our assumption that $\alpha = \mathbf 1$, intuitively because that is what encodes the BN's independencies. 

An intuitive proof sketch explains why: a distribution $\mu$ minimizes $\OInc_{\cal B}$ iff it has the right cpds (no matter the value of $\bbeta$), and (as we show in the appendix) it minimizes $\SDef_{\cal B}$ iff it has the appropriate independencies. 
We know that the BN's distribution $\Pr_{\cal B}$ is the unique distribution with both of these properties. So, no matter how the two quantities are weighted (i.e., for all $\gamma > 0$), it is the unique optimal distribution.

% making it our only result that does not have a natural analog for general $\alpha$.

\paragraph{BNs and Maximum Entropy.}
	\label{sec:bn-maxent}
One welcome side effect of this result is to shine light on an old question about the role of maximum entropy in directed graphical models.
% As mentioned in \cref{sec:prelim-maxent}, 
In \cref{sec:prelim-maxent}, we mentioned that BNs 
are among the few standard ways of specifying probabilistic information that are not maximum-entropy distributions subject to the observational constraints (the specified cpds). 
To get any result of the kind, prior work has resorted to building up the distribution ``in causal order'', applying the principle of maximum entropy many times \citep{williamson2001foundations}.
% Why is this? 
This makes some sense, but why should it be necessary?

First, we note that maximum entropy does not always miss the mark:
if the BN happens to have the structure of a Markov chain, or more generally be absent of joint dependence, then it is not hard to show that the maximum entropy distribution is in fact the correct one. 
Here's a broader but more subtle class of BNs for which maximum entropy produces the right answer: 
if for every node $X$ with multiple parents, the entropy of the probability $\mathbb P_X(X \mid \Pa(X){=}\mat z) \in \Delta \V \! X$ does not depend on $\mat z$. 
To see why this would help, it is helpful to see
    an example of a BN whose distribution does not maximize entropy
        subject to the observational constraints. 
    % a case where it isn't true. 

\begin{example}\label{ex:counterexample}
	Consider the BN
	% \begin{tikzcd}[cramped, sep=small]
	% 	A \ar[r] & C & B \ar[l]
 	% \end{tikzcd}
    \begin{center}
    \begin{tikzpicture}
        \node[bnnode] (A) at (-1.5,0){$A$};
        \node[bnnode] (C) at (0,-0.5){$C$};
        \node[bnnode] (B) at (1.5,0){$B$};
        \draw[arr2] (A) to (C);
        \draw[arr2] (B) to (C);
    \end{tikzpicture}
    \end{center}
	where $A$ and $B$ are binary, and $C$ can take $2^k$ values, including $c_0$. 
    For the probability tables, suppose that both $A$ and $B$ get uniform unconditional probabilities ($\nf12$ apiece), and set $C$'s cpt to be
	\[
        \p_C(C \mid A,B) = 
		\begin{idxmat}{{$a$, $b$},{$\lnot a$, $b$},{$a$, $\lnot b$},{$\lnot a$, $\lnot b$}}{$\Delta C$}
			\mathit{Unif}(C) \\ \delta_{c_0}(C)\\ \delta_{c_0(C)} \\ \mathit{Unif}(C) \\
		\end{idxmat}
	\]
	where $\delta_{c_0}$ places all probability on $c_0$.
    It has zero entropy, while the uniform distribution on $C$ has $k$ bits of entropy, and $A$ and $B$ both have one bit of entropy. 
	The semantics of a BN require that $A$ and $B$ are independent.
    Thus, the BN's distribution has  
    $
    2 + \nf k2$ bits of entropy (one bit for each of $A$ and $B$ since they are independent, plus an expected $\nf k2$ bits from getting the uniform distribution on $C$ with probability $\nf12$).
    However, a distribution in which $A$ and $B$ are correlated so as to always be equal has $1 + k$ bits (one bit from $A$ and $B$, and $k$ bits from $C | A,B$). 
    % The maximum entropy distribution becomes closer and c
    For $k > 2$, the latter distribution has larger entropy.
    % For large $k$, this is still not the maximum entropy distribution, but it is much higher entropy than the one the BN suggests.	
	Therefore, the maximum entropy distribution consistent with the tables does not encode the independece assumption that a BN does. 
\end{example}

The key observation is that the cpds
    (because they encode \emph{conditional} information), 
can carry different amounts of entropy depending on the realized values of the variables. 
But intuitively, the principle of maximum entropy was trying to maximize entropy \emph{beyond} the constraints---not within them. 
Adjusting the principle of maximum entropy so as not to account for the entropy implied by the given cpds leads us directly back to the formula for $\SDef$; indeed, this is how we initially discovered it.
%
We argue that the princple of \emph{minimizing structural deficiency} ($\SDef$) subject to observational constraints (i.e., the $0^+$ semantics of a PDG) is the appropriate generalization of the principle of maximum entropy, when there is conditional (and especially causal) information involved. 
%
\Cref{theorem:bns-are-pdgs} is a strong argument in favor of this;
    we will see others examples later on.


\subsection{Factor Graphs} 
    \label{sec:factor-graphs}
Factor graphs 
\citep{kschischang2001sumproduct}
% like PDGs, generalize BNs.
are another class of graphical model, which are often thought of as generalizing BNs.%
    \footnote{This claim is true at a quantiative level, but it is worth noting that factor graphs do not capture the same independencies unless augmented with extra infromation \citep{frey2012extending}, and lack some properties that makes BNs useful in causality.}
In this section, we explore the relationship between PDGs and factor graphs.

\begin{defn}
 A \emph{factor graph} $\Phi$ is a set of random variables
        $\X$ and
    and indexed collection of \emph{factors}
       $\{\phi_J : \V(X_J) \to \mathbb R_{\geq0}\}_{J \in
\mathcal J }$,
where $\mathcal J$ is an arbitrary index set, 
and each $X_J \subseteq \X$.  
In other words, each factor $\phi_J$ is associated with a subset
$X_J\subseteq \mathcal{X}$ of variables, and maps
joint settings of $X_J$ to non-negative real numbers.
The factor graph $\Phi$ specifies a distribution
\[ {\Pr}_{\Phi}(\vec x) := \frac{1}{Z_{\Phi}}
 	\prod_{J \in \cal J} \phi_J(\vec x_J), \]
where $\vec{x} \in \V\!\X$ is a joint setting of all of the variables,
 $\vec{x}_J$ is the restriction of $\vec{x}$ to only the
 variables $X_J$, and $Z_{\Phi}$ is the constant required to
 normalize the distribution.  
\end{defn}

% The cpds of a PDG naturally constitute a collection of factors,
% so it natural to wonder how the semantics of a PDG compares to 
% simply treating the cpds as factors in a factor graph. 
To view a BN as a factor graph, one essentially views each cpd as a factor.
From this perspective, a PDG also seems to specify a collection of factors. 
How do PDG semantics as we have already defined them compare to this 
``multiply-and-renormalize'' semantics of a factor graph?
To answer this, we start by making the translation precise.

\begin{defn}[unweighted PDG to factor graph]\label{def:PDG2fg}
% When $\dg N = (\Ar, \mathbb P)$ is an unweighted PDG with variables $\X$, 
Given an unweighted PDG $\mathbb P = \{ \p_a(\Tgt a|\Src a) \}_{a \in \Ar}$ 
% over the variables $\X$,
define the associated FG $\FGof{\dg N}$ as follows:
take $\mathcal J := \Ar$, 
and for each $\ed aZY \in \mathcal J$, let $X_{a} := \{ Z,Y\}$
with factor $\phi_a(z,y) := \p_a(y \mid z)$ 
% (i.e., $(\p_a^{\dg M}(z))(y)$)
\unskip.
\end{defn}


It turns out we can also do the reverse. 
Using essentially the same idea as in \cref{constr:hyperedge-reducton},
we can encode a factor graph as an assertion about the unconditional
probability distribution over the variables associated to each
factor.  

\begin{defn}[factor graph to unweighted PDG] \label{def:fg2PDG}
For a FG $\Phi = \{ \phi_J : \mat X_J \to \mathbb R \}_{J \in \mathcal J}$ over variables $\X$, let $\UPDGof{\Phi}$ be
% the unweighted PDG consisting of
% \begin{itemize}[nosep]
% 	\item the variables in $\Phi$ together
%    with $\pdgunit$ and a variable $X_{\!J} := \prod_{j \in J} X_j$ for every factor $J \in \mathcal J$%
%    , and
%    \item arcs ${\pdgunit} \!\!\to\! X_{\!J}$ for each $J$ and $X_{\!J} \!\!\tto\! X_j$ for each $X_j \in \mat X_J$,
% \end{itemize}
% where the arcs $ X_{\!J} \!\tto\! X_j$ are associated with the appropriate projections, and each ${\pdgunit} \!\to\! X_{\!J}$ is associated with the unconditional joint distribution on $X_J$ obtained by normalizing $\phi_J$. 
the unweighted PDG over the same variables $\X$,
with hypergraph $\Ar = \{ \varnothing \to \mat X_J \}_{J \in \mathcal J} $,
such that each arc $J$ is associated with the renormalized distribution $\p_J(\mat X_J) :\propto \phi_J(\mat X_J) ~=~ \phi_J(\mat X_J) / \sum_{{\mat x \in \mat X_J}} \phi_J(\mat x)$.
The process is illustrated in \cref{fig:fg2PDG}.
\end{defn}

\begin{figure*}[htb]
	\centering
	\begin{tikzpicture}[center base, xscale=1.4, yscale=1.2,
		fgnode/.append style={minimum width=2.4em, inner sep=0.2em}]
		\node[factor] (prior) at (1.65,-1) {};
		\node[factor] (center) at (3.85, 0.5){};

		\node[fgnode] (PS) at (1.65,0.5) {$\mathit{PS}$};
		\node[fgnode] (S) at (3.1, 0.8) {$\mathit S$};
		\node[fgnode] (SH) at (3.0, -0.8) {$\mathit{SH}$};
		\node[fgnode] (C) at (4.8,0.5) {$\mathit C$};

		\draw[thick] (prior) -- (PS);
		\draw[thick] (PS) --node[factor](pss){} (S);
		\draw[thick] (PS) --node[factor](pssh){} (SH);
		\draw[thick] (S) -- (center) (center) -- (SH) (C) -- (center);


		\node[fgnode] (T) at (4.8, -1.3) {$\mathit T$};
		\draw[thick] (T) -- node[factor]{}  (C);
		\end{tikzpicture}
	~{\Huge$~\boldsymbol\rightsquigarrow~$}~
	\begin{tikzpicture}[center base, xscale=1.5]
		\coordinate (prior) at (1.65,-1) ;
		\coordinate (center) at (4.1, 0.5);

		\node[dpadded] (PS) at (1.65,0.5) {$\mathit{PS}$};
		\node[dpadded] (S) at (3.3, 0.8) {$\mathit S$};
		\node[dpadded] (SH) at (3.3, -0.6) {$\mathit{SH}$};
		\node[dpadded] (C) at (4.9,0.5) {$\mathit C$};

		\node[inner sep=1em, circle] (1) at (2.7,-1.8) {};

		% \coordinate (pss) at (barycentric cs:PS=1,S=1.2);
		% \coordinate (pssh) at (barycentric cs:PS=1.2,SH=1);
		% \coordinate (tc) at (barycentric cs:T=1.2,C=1);
        \coordinate (pss-origin) at (2.5,1.35);
		\coordinate (pssh-origin) at (2.35,-0.6);
		\coordinate (tc-origin) at (4.7, -0.65);

		\draw[arr2, <-] (PS) to +(0,-1.5);
		% \draw[arr,shorten <=0pt] (pss) to[out=95, in=20] (PS);
		% \draw[arr,shorten <=0pt] (pss) to[out=95, in=180] (S);
        \unmergearr[arr1]{pss-origin}{PS}{S}

		% \draw[arr,shorten <=0pt] (pssh) to[out=85, in=-20] (PS);
		% \draw[arr,shorten <=0pt] (pssh) to[out=85, in=150] (SH);
        \unmergearr[arr1]{pssh-origin}{PS}{SH}

		% \draw[blue!50,arr, <->] (PS) -- (S);
		% \draw[blue!50,arr, <->] (PS) --coordinate(pssh) (SH);
		\draw[arr2, <-, shorten >=0pt] (S) to[out=-20, in=-110] (center);
		\draw[arr2, <-, shorten >=0pt, shorten <=0pt] (SH) to[out=45, in=-110] (center);
		\draw[arr2, <-, shorten >=0pt] (C) to[out=180, in=-110] (center);


		% \draw[blue!50, thick, shorten <=3pt] (1) -- (prior);
		\draw[thick, shorten >=3pt] (center) to[out=70] +(0.1,0.4);
		% \draw[thick, shorten <=3pt] (1) to[bend right = 5] (pss);
		% \draw[thick, shorten <=3pt] (1) to[bend left = 10] (pssh);


		\node[dpadded] (T) at (5.1, -1.5) {$T$};
		% \draw[arr, shorten <=0pt] (tc) to[out=40,in=-110]  (C);
		% \draw[arr, shorten <=0pt] (tc) to[out=30,in=115]  (T);
		% \draw[ thick, shorten <=3pt] (1.east) to[bend right = 15] (tc);
        \unmergearr[arr2]{tc-origin}{T}{C}
		\end{tikzpicture}
    \caption[Illustration of how a factor graph is converted to a PDG]{
        Conversion of the PDG in \cref{ex:smoking} to a factor graph
        according to \cref{def:PDG2fg} (left), and from that factor graph back
        to a PDG by \cref{def:fg2PDG} (right). 
        The factors are associated with
        the unconditional distribution obtained by normalizing the appropriate factor.
    } 
	\label{fig:fg2PDG}
\end{figure*}
\begin{figure*}[htb]
	\centering
	\ifprecompiledfigs
\raisebox{-0.5\height}{\includegraphics{figure-pdfs/smoking-convert.pdf}}
	\else
	\begin{tikzpicture}[center base, xscale=1.7, yscale=1.2,
        newnode/.style={rectangle, inner sep=5pt, fill=gray!30, rounded corners=3, thick,draw}]
		\node[newnode] (prior) at (1.65,-1) {};
		\node[newnode] (center) at (4.1, 0.25){};
		
		\node[dpadded] (PS) at (1.65,0.5) {$\mathit{PS}$};
		\node[dpadded] (S) at (3.3, 0.8) {$S$};
		\node[dpadded] (SH) at (3.3, -0.6) {$\mathit{SH}$};
		\node[dpadded] (C) at (4.9,0.5) {$C$};
		
		\draw[arr1, ->>, shorten <=0pt] (prior) -- (PS);
		\draw[arr1, <<->>] (PS) --node[newnode](pss){} (S);
		\draw[arr1, <<->>] (PS) --node[newnode](pssh){} (SH);
		\draw[arr1, <<-, shorten >=0pt] (S) -- (center); 
		\draw[arr1, <<-, shorten >=0pt] (SH)-- (center); 
		\draw[arr1, <<-, shorten >=0pt] (C) -- (center);
		
		\node[dpadded, fill=blue] (1) at (2.7,-1.8) {$\pdgunit$};
		
		\draw[blue!50, arr] (1) -- (prior);
		\draw[blue!50, arr] (1) to[bend right=30] (center);
		\draw[blue!50, arr] (1) to[bend right = 5] (pss);
		\draw[blue!50, arr] (1) to[bend left = 10] (pssh);

		
		\node[dpadded] (T) at (4.8, -1.7) {$T$};
		\draw[arr1, <<->>] (T) -- node[newnode](tc){}  (C);	

		\draw[blue!50, arr] (1) to[bend right = 10] (tc);
	\end{tikzpicture}
	\fi
	\caption[Converting a PDG to a \emph{strict} PDG]{
The conversion from a factor graph to 
to a \emph{strict} PDG, as done in the original paper \citep{pdg-aaai}
In this case, for each factor $\phi_J$ we introduce a new variable
    $X_J$ (displayed as a smaller darker rectangle), whose values are joint settings of the
variables connected it, and also an arc $1 \to X_J$ (shown in blue),
to which we associate the unconditional distribution given by normalizing $\phi_J$.
} 
	\label{fig:fg2PDG-strict}
\end{figure*}


PDGs are directed graphs, while factors graphs are undirected. The
map from PDGs to factor graphs thus loses this structure, 
as shown in \cref{fig:fg2PDG}.
The resulting PDG is also typically inconsistent.
The structural changes are even more significant if we convert it to a \emph{strict} PDG that does not have hyperedges, as done in the original paper \citep{pdg-aaai} and illustrated in \cref{fig:fg2PDG-strict}. 
Nevertheless, when $\gamma = 1$, so that the quantitative and qualitative terms are weighted equally, the semantics coincide. 


\begin{linked}{theorem}{fg-is-pdg}
    \label{theorem:pdg-is-fg}
\begin{enumerate}[label={(\alph*)}]
    \item
% $\Pr_{\Phi} = \bbr{\UPDGof{\Phi}}_{1}^*\;$ for all factor graphs
$\bbr{\UPDGof{\Phi}}_{1}^* = \{ \Pr_\Phi \} \;$ for all factor graphs
$\Phi$.%
\onlyfirsttime{\footnote{Recall that we identify the unweighted PDG $(\Ar,\mat
p)$ with the weighted PDG $(\Ar,\mathbb P,  \mat 1, \mat 1)$.}}
% \end{linked}
% \begin{linked}{theorem}{pdg-is-fg}
\item
$\bbr{\dg N}_{1}^* = \Pr_{\FGof{\dg N}}\;$ for all unweighted
	PDGs $\dg N$.  
\end{enumerate}
\end{linked}

The correspondence hinges on the fact that we take $\gamma=1$, so that $\OInc$ and
$\SDef$ are weighted equally.
% Because $\gamma$ is not part of the data of a PDG but rather chosen
%     by the end user, 
% Because the user of a PDG gets to choose $\gamma$, 
% the fact that the translation from factor graphs to PDGs preserves 
%     semantics only for $\gamma=1$ can be viewed 
Because $\gamma$ is not part of the data of a PDG but rather a parameter of the semantics, this means
    % the translation from a factor graph to a PDG, 
    the $\gamma=1$ semantics of the PDG $\dg N_\Phi$ exactly captures
    the traditional semantics of $\Phi$.
%
%
However, the fact that the reverse correspondence requires
$\gamma=1$ suggests that factor graphs are less flexible than unweighted PDGs: factor graphs only describe one particular aspect of their semantics.

What about 
% weighted PDGs  $(\Ar, \mathbb P, \bbeta)$ where $\bbeta \ne {\mat 1}$?
PDGs for which $\bbeta \ne {\mat 1}$?
There is also a standard notion of weighted factor graph,
    but to relate them to PDGs with weights, 
    we must stray from this chapter's 
    convention of taking $\balpha = {\mat 1}$. 
% As we are about to see,
% nce we drop this convention, we can do much more.


\subsection{Factored Exponential Families}\label{sec:expfam}

A \emph{weighted factor graph (WFG)} $\Psi$ is a pair
$(\Phi,\theta)$ consisting of a factor graph $\Phi$ 
together with a vector of non-negative weights
$\{ \theta_J \}_{J \in \mathcal J}$.
$\Psi$ specifies a canonical scoring function 
\begin{equation}
\VFE_{\Psi}(\mu)
	 := \!\Ex_{\vec x\sim\mu}\left[  \sum_{J \in
           \cal J} \theta_J \log\frac1{\phi_J(\vec
               x_J)}\right] - \H(\mu)  , 
			   \label{eqn:free-energy}
\end{equation}
called the \emph{variational
Gibbs free energy} \citep{mezard2009information}. 
$\VFE_{\Psi}$ is uniquely minimized by the distribution
${\Pr}_{\Psi}(\vec x) = \frac{1}{Z_{\Psi}}
 	\prod_{J \in \cal J} \phi_J(\vec x_J)^{\theta_J}$, 
which matches the unweighted case when every $\theta_J = 1$.
The mapping $\theta \mapsto \Pr_{(\Phi,\theta)}$ is known as 
$\Phi$'s \emph{exponential family}, and is central to the analysis
and development of many algorithms for undirected 
graphical models \citep{wainwright2008graphical}.
% new
Indeed, this is an alternate and in principle equally expressive 
appraoch to modeling with factor graphs: instead of choosing the factors $\Phi$
based on data and assuming weight $\mat 1$ on all factors,
it is possible to instead choose $\Phi$ beforehand to be a suitibly expressive basis, 
and then use data to inform the choice of \emph{weights} $\theta$. 


PDGs can capture the full exponential family of a factor graph, 
    % but only by allowing values of $\alpha$ other than ${\mat 1}$. 
    but it requires value of $\balpha$ other than $\mat 1$. 
\commentout{%
    In this case, the
    only definition  
    that requires alteration is $\SDef$, which now depends on the \emph{weighted multigraph}
    $(\Ar^{\dg M}, \alpha^{\dg M})$, and is given by
    \begin{equation}
    	\SDef_{\dg M}(\mu) := \sum_{\ed aXY \in \Ar} \alpha_a \H_\mu(Y\mid X) - \H(\mu). 
    	\label{eqn:sdef2}
    \end{equation}
    Thus, the conditional entropy $\H_\mu(Y\mid X)$ associated with the
    arc $\ed aXY$ is multiplied by the weight $\alpha_a$ of that arc.
}%
% One important use case for $\alpha$ is 
%     it enables us to capture arbitrary weighted factor graphs
% not just ones with a constant weight vector.
It turns out that the key is to ensure that the ratio $\alpha_a/\beta_a$ is constant across arcs $a$.  
% (Of course, if we allow arbitrary weights, we cannot hope to do this with $\balpha = \mat 1$.) 
We therefore define a family of translations, parameterized by the
ratio of $\alpha_a$ to $\beta_a$.
\begin{defn}[WFG to PDG]\label{def:wfg2pdg}
Given a WFG
$\Psi=(\Phi, \theta)$,
and postive number $k$, 
we define the corresponding PDG $\PDGof{\Psi,k} = (\UPDGof{\Phi},\alpha_{\theta}, \beta_{\theta})$ 
by taking $\beta_J = k \theta_J$ and $\alpha_J = \theta_J$ for the arc
 $\pdgunit  \rightarrow X_J$.
% and taking $\beta_a = k$ and $\alpha_a = 1$ for the projections $X_J \!\tto\! X_j$.
\end{defn}

We now extend Definitions~\ref{def:PDG2fg} and \ref{def:fg2PDG} to
(weighted) PDGs and WFGs.  
In translating a PDG to a WFG, 
there will necessarily be some loss of information: PDGs have two sets, while WFGs have 
only have one. Here we throw out $\alpha$ and keep $\beta$, 
though in its role here as a left inverse of \cref{def:wfg2pdg},
either choice would suffice. 


\begin{defn}[PDG to WFG]
Given a (weighted) PDG $\dg M =
(\dg N, \beta)$, we take its corresponding WFG to be $\WFGof{\dg M} :=
(\FGof{\dg N}, \beta)$; that is, $\theta_a := \beta_a$ for all arcs $a$.
\end{defn}
We now show that we can capture the entire exponential family of a factor graph,
and even its associated free energy, 
but only for $\gamma$ equal to the constant $k$ used in
the translation.  


\begin{linked}{theorem}{wfg-is-pdg}
For all WFGs $\Psi = (\Phi,\theta)$ and all $\gamma > 0$,
we have that
$\VFE_\Psi
= \nicefrac1{\gamma} \bbr{{\dg M}_{\Psi,\gamma}}_{\gamma} 
+ C$   
for some constant $C$, so
$\Pr_{\Psi}$ is the unique element of
$\bbr{{\dg M}_{\Psi,\gamma}}_{\gamma}^*$.
\end{linked}

In particular, for $k\!=\!1$, so that $\theta$ is used for both the functions
$\alpha$ and $\beta$ of the resulting PDG,
\cref{theorem:wfg-is-pdg} strictly generalizes \cref{theorem:fg-is-pdg}.
\begin{coro}
	For all WFGs $(\Phi, \theta)$,
	we have that
	$\Pr_{(\Phi,\theta)} = \bbr{(\UPDGof{\Phi}, \theta,\theta)}_1^*$.
\end{coro}

Conversely, as long as the ratio of $\alpha_a$ to $\beta_a$ is constant, the
reverse translation also preserves semantics.
Recall that $(\mathbb P, \balpha, \bbeta)$ is the PDG obtained by augmenting the unweighted PDG $\mathbb P$ with weights over every arc. 

\begin{linked}{theorem}{pdg-is-wfg}
% For all unweighted PDGs $\dg{N}$ and non-negative vectors $\mat v$
% over $\Ar^{\dg N}$, and all $\gamma > 0$, we have that 
% $\bbr{(\dg N, \mat v, \gamma \mat v)}_{\gamma}
% = \gamma\,\VFE_{(\Phi_{\dg N}, \mat v)} $; consequently,
% $\bbr{(\dg N,  \mat v,  \gamma\mat v)}_{\gamma}^*
% 		= \{\Pr_{(\Phi_{\dg N}, \mat v)} \}$. 
For all unweighted PDGs $(\Ar, \mathbb P)$, non-negative vectors $\mat v$
of shape $\Ar$, and all $\gamma > 0$, we have that 
$\bbr{(\mathbb P, \mat v, \gamma \mat v)}_{\gamma}
= \gamma\cdot\VFE_{(\Phi_{\mathbb P}, \mat v)} $; consequently,
$\bbr{(\mathbb P,  \mat v,  \gamma\mat v)}_{\gamma}^*
		= \{\Pr_{(\Phi_{\mathbb P}, \mat v)} \}$. 
In other words, if $\dg M$ is a PDG with $\bbeta \propto \balpha$,
and $\gamma$ is the constant of proportionality,
then $\bbr{ \dg M }^*_\gamma$ is a singleton containing the distribuion of the factor graph $\Phi_{\mathbb P}$ weighted by $\balpha$.  
\end{linked}




In particular, when $\balpha = \bbeta$, then $\bbr{\dg M}_1$ is the semantics of the factor graph that regards the cpds $\mathbb P$ as factors with these weights. 
The key step in proving \Cref{theorem:wfg-is-pdg,theorem:pdg-is-wfg}
(and in the proofs of a number of other results) involves 
rewriting  
the scoring function 
% $\bbr{\dg M}_\gamma$ in the following way.
as follows.

\begin{linked}{prop}{nice-score}%
    % \def\xinw{x^{\mat w}}
    % \def\yinw{y^{\mat w}}
    % \def\xinw{X(\omega)}
    % \def\yinw{Y(\omega)}
    \def\xinw{X}
    \def\yinw{Y}
% Letting $\xinw$ and $\yinw$ denote the values of
% $X$ and $Y$, respectively, in
% % $\mat w \in \V\!\X$, 
% a joint setting $\omega \in \V\!\X$, we have 
For all PDGs $\dg M$, 
$\bbr{\dg M}_\gamma(\mu) = $
\begin{equation}\label{eq:semantics-breakdown}
% \begin{split}
% \bbr{\dg M}_\gamma(\mu) =  
    % \Ex_{\omega \sim \mu}\!
    % \Bigg[
    %  \sum_{ X \xrightarrow{\!\!a} Y  }
    % \bigg[\,
    %     \color{gray}
    %     \underbrace{\color{black}
    %       \!\beta_a \log \frac{1}{\p_a(\yinw | \xinw)}
    % 	}_{\color{gray}\smash{\mathclap{\text{log likelihood / cross entropy}}}} +
    %     % \onlyfirsttime{ \qquad\\[-0.7em] }
    %     {\color{gray}\underbrace{\color{black} 
    % ({\alpha_a}\gamma - \beta_a ) \log \frac{1}{\mu(\yinw |\xinw)} 
    % 	}_{\color{gray}\smash{\mathclap{\text{local regularization (if $\beta_a > 
    % 	\alpha_a
    % 	\gamma$)}}}}}\bigg] - \underbrace{\color{black}
    % \gamma \log \frac{1}{\mu(\omega)}
    % 	}_{\color{gray}\smash{\mathclap{\text{global
    %         regularization}}}}\color{black} \Bigg].
    \Ex_{\mu}
    \Bigg[
     \sum_{ \ed aXY \in \Ar }
     \!
    \bigg(
        {\color{gray}
        \underbrace{\color{black}
          \beta_a \log \frac{1}{\p_a(\yinw | \xinw)}
    	}_{\color{gray}\smash{\mathclap{\text{log likelihood / cross entropy}}}}} \,+\,
        % \onlyfirsttime{ \qquad\\[-0.7em] }
        {\color{gray}\underbrace{\color{black} 
    ({\alpha_a}\gamma - \beta_a ) \log \frac{1}{\mu(\yinw |\xinw)} 
    	}_{\color{gray}\smash{\mathclap{\text{local regularization (if $\beta_a > 
    	\alpha_a
    	\gamma$)}}}}}\,\bigg) - \underbrace{\color{black}
    \gamma \log \frac{1}{\mu(\X)}
    	}_{\color{gray}\smash{\mathclap{\text{global
            regularization}}}}\color{black} \Bigg].
% \end{split}
\end{equation}
\end{linked}
For a fixed $\gamma$, the first and last terms
of \eqref{eq:semantics-breakdown} are equal to a scaled
version of the free energy, $\gamma\VFE_\Phi$, 
if we set $\phi_J := \p_a$ and $\theta_J := \nicefrac{\beta_a}{\gamma}$.  
If, in addition, $\beta_a = {\alpha_a}\gamma$ for all
arcs $a$, then
the local regularization term disappears, giving us
the desired correspondence. 
%
%
\Cref{eq:semantics-breakdown} also makes it clear that 
taking $\beta_a = {\alpha_a} \gamma$ for all arcs $a$ is
essentially necessary to get the result of \cref{theorem:pdg-is-fg,theorem:fg-is-pdg}.
Of course, fixed $\gamma$ precludes 
    taking the limit as $\gamma$ goes to 0, so 
    \cref{prop:consist} does apply to factor graphs. 
% This is reflected in some strange
This is reflected in some strange
behavior in factor graphs trying to capture the same phenomena as
PDGs, as the following example shows.

\begin{example}\label{ex:overdet}
Consider the PDG $\dg M$ containing just one variable
    $X$, and two arcs.
(Such a PDG can arise if we get different information about the probability of $X$ from two different sources; this is a situation we certainly want to be able to capture!)
% Consider the simplest situation,
Suppose further that $p$ and $q$ are both associated with the same distribution on $X$.
% ; further suppose that the agent is certain about the distribution, so
% $\beta_p = \beta_q = 1$.
For definiteness, suppose that
$\V\!X = \{x_1,x_2\}$, and
that the distribution associated with both arcs is $\mu_{.7}$, which ascribes
probability $.7$ to $x_1$. Then, as we would hope  $\bbr{\dg M}^*_{0^+} =
\{\mu_{.7}\}$; after all, both sources agree on the information.
However, it can be shown that 
$\Pr_{\WFGof{\dg M}} = \mu_{.85}$, so  $\bbr{\dg M}_1^* = \{\mu_{.85}\}$.
In this way, factor graphs seem to be \emph{uncalibrated}. 

The reason for this can be viewed as an instance of the caveat in combining qualitative information, mentioned in \cref{sec:pdg-combine}.
% By default, we took all weights 
Our default assumption that $\alpha = 1$ makes sense for each individual arc, but having two independent randomized mechanisms by which $X$ is determined intuitively means $X$ ought to be a constant (a point we will expand on in \cref{sec:qim}).
Arguably, this is not what was intended in combining the two models.
By taking the $0^+$ semantics, the problem with the qualitative information disappears. 
Alternatively, we could have merged the qualitative picture so that there is a total of $\alpha_p + \alpha_q = 1$ between the two arcs.
In that case, we would have had no conflict between the two types of information, and indeed $\bbr{\dg M'}_\gamma^* = \{ \mu_{.7} \}$ for all values of $\gamma \ge 0$.
\end{example}


Although both $\theta$ and $\beta$ are measures of confidence, 
the way that the Gibbs free energy varies with $\theta$ 
is quite different from the way that the score of a PDG
varies with $\beta$. 
The scoring function that we use for PDGs can be viewed as
extending ${\VFE}_{\Phi,\theta}$ by including
the local regularization term.
As $\gamma$ approaches zero,
the importance of the global regularization terms decreases relative
to that of the local regularization term, so the PDG scoring function
becomes quite different from Gibbs free energy.

\section{Discussion}
We have introduced PDGs, a powerful tool for representing
% probabilistic information. 
beliefs in the language of probabilities and confidences.
They have a number of advantages over other
    probablisitic graphical models. 
Most notably:
\begin{itemize}
\item PDGs allow us to capture inconsistent beliefs, including conflicting information from multiple sources with varying degrees of reliability.
% (and measure its magnitude as a matter of degree
    Moreover, they provide a natural way of measuring the degree of this inconsistency. 
\item 
	PDGs are much more modular 
	than other representations; for example, we can combine information from two sources by simply taking the union of two
	PDGs, and it is easy to add new information (arcs)
	and features (nodes) without affecting 
    (the meaning of)
    previously-received information.
\item They allow for a clean separation between quantitiatve information (the
	cpds $\mathbb P$ and weights $\bbeta$) and the qualitative information (carried by
	the graph structure $\Ar$ and the weights $\balpha$); 
	this separatoin is captured semantically by the
	terms $\OInc$ and $\SDef$ in our scoring function.
\item PDGs have (several) natural semantics; one of them allows us to
pick out a unique distribution.  Using this distrbution, PDGs
	can capture BNs and factor graphs.
    In the latter case, a simple parameter shift in the corresponding PDG eliminates arguably problematic behavior of a factor graph.
\end{itemize}

So far, we have only scratched the surface of what can be done with PDGs.
Two major issues that need to be tackled are inference and dynamics.
How should we query a PDG for probabilistic information? 
How should we modify a PDG in light of new information or to make it more consistent?
These issues turn out to be closely related, as we will see in 
    \cref{chap:inc-infer-connection}.
% Due to space limitations, 
% we just briefly give some intuitions and examples here.

% Suppose that we want to compute the probability of $Y$ given $X$ in a PDG $\dg
% M$. For a cpd $p(Y|X)$, let $\dg M^{+p}$ be the PDG obtained by associating $p$
% with a new arc in $\dg M$ from $X$ to $Y$, with $\alpha_p \!=\! 0$. We judge
% the quality of a candidate answer $p$ by the best possible score that $\dg
% M^{+p}$ gives to any distribution (which we call the \emph{degree of
% inconsistency} of $\dg M^{+p}$). It can be shown that the deegree of
% inconsistency is minimized by $\bbr{\dg M}^*(Y\!\mid\! X)$. 

% To take another example,
For now, we content ourselves with a simple example:
conditioning can be understood in terms of resolving inconsistencies
in a PDG.  To condition on an observation $Y\!=\!y$, given a situation
described by a PDG $\dg M$, we can add an arc 
from $\pdgunit$ to $Y$ in $\dg M$, annoted with the cpd that gives
probability 1 to $y$, to get the (likely inconsistent) PDG
% $\vphantom{\raisebox{0.05em}{$\big|$}}$
${\dg M}^{+(\!Y\!=y)}$.   
% The distribution $\bbr{{\dg M}^{+(\!Y\!=y)}}^*$ turns out
% to be the result of conditioning $\bbr{\dg M}^*$ on $Y\!=\!y$.
In the special case where $\dg M$ represents or is equivalent to a joint distribution,
    % (which will be made precise in \cref{sec:prob-as-pdg,sec:equivalence}), 
    $\bbr{{\dg M}^{+(\!Y\!=y)}}^*$ turns out
    to be the result of conditioning that distribution on the event $Y{=}y$. 
This account of conditioning generalizes
without modification to give Jeffrey's Rule \citep{Jeffrey68}, a more
general approach to belief updating. 
This will be made precise in \cref{sec:prob-as-pdg}.

% Issues of updating and inconsistency also arise in variational inference. A  
% variational autoencoder \citep{kingma2013autoencoding}, for instance, 
% is essentially three cpds: a prior $p(Z)$, a decoder $p(X \!\mid\! Z)$, and 
% an encoder $q(Z \!\mid\! X)$. Because two cpds target $Z$ (and the cpds are 
% inconsistent until fully trained), this situation
% can be represented by PDGs but not
% by other graphical models.
Properly modeling inconsistency also turns out to be a recurrent theme in machine learning, as we will see in \cref{chap:one-true-loss}. 
A variational autoencoder \citep{kingma2013autoencoding}, for instance, 
is essentially three cpds: a prior $p(Z)$, a decoder $p(X \!\mid\! Z)$, and 
an encoder $q(Z \!\mid\! X)$. Because two cpds target $Z$ (and the cpds are 
inconsistent until fully trained), this situation
can be represented by PDGs but not by other graphical models.

% In the coming chapters, we will explore 
% the deep connection between inference, updating, 
% and the resolution of inconsistency in PDGs.
In the coming chapters, we will explore these connections, and the broader theory of probabilistic modeling that is enabled by the PDG representation.
	
% \section*{Ethics Statement}
% 
% Because PDGs are a recent theoretical development, there is a lot of guesswork in evaluating the impact. Here are two views of opposite polarity.
% 
% \subsection*{Positive Impacts}
% One can imagine many applications of enabling simple and coherent aggregation of (possibly inconsistent) information. In particular we can imagine using PDGs to build and interpret a communal and global database of statistical models, in a way that may not only enable more accurate predictions, but also highlights conflicts between information.
% 
% This could have many benefits. Suppose, for instance, that two researchers train models, but use datasets with different racial makeups. Rather than trying to get an uninterpretable model to ``get it right'' the first time, we could simply highlight any such clashes and flag them for review.
% 
% Rather than trying to ensure fairness by design, which is both tricky and costly, we envision an alternative: simply aggregate (conflicting) statistically optimal results, and allow existing social structure to resolve conflicts, rather than sending researchers to fiddle with loss functions until they look fair.
% 
% \subsection*{Negative Impacts}
% We can also imagine less rosy outcomes. To the extent that PDGs can model and reason with inconsistency, if we adopt the attitude that a PDG need not wait until it is consistent to be used, it is not hard to imagine a world where a PDG gives biased and poorly-thought out conclusions. It is clear that PDGs need a great deal more vetting before they can be used for such important purposes as aggregating the world's statistical knowledge.
% 
% PDGs are powerful statistical models, but are by necessity semantically more complicated than many existing methods. This will likely restrict their accessibility. To mitigate this, we commit to making sure our work is widely accessible to researchers of different backgrounds.



\begin{subappendices}

\section{Proofs} \label{sec:proofs}
For brevity, when the appropriate variables are clear from context,
we write $\mu(x, y)$
in place of $\mu(X \!=\! x, Y \!=\! y)$, $\mu(x \mid y)$ in place of
$\mu(X \!=\! x\mid Y \!=\! y)$, and so forth.

	
\subsection{Properties of the Scoring Semantics}

In this section, we prove the properties of scoring functions that we
mentioned in the main text,
Propositions~\ref{prop:sd-is-zeroset}, \ref{prop:sem3}, and
\ref{prop:consist}.  We repeat the statements for the reader's convenience.

% \restate{prop:sd-is-zeroset}{
% $\SD{\dg M} \!= \{ \mu : \bbr{\dg M}_0(\mu) \!=\! 0\}$ for all $\dg M$.
% }
\recall{prop:sd-is-zeroset}
\begin{lproof}\label{proof:sd-is-zeroset}
	 By taking $\gamma = 0$, the score is just $\OInc$. By
			 definition, a distribution $\mu \in \SD{\dg M}$ satisfies
	  all the
			 constraints, so $\mu(Y = \cdot \mid X=x) =
			 \p_a(x)$ for all arcs $X \rightarrow Y \in \Ar^{\dg
			   M}$ and $x$ with 
			 $\mu(X=x)>0$. By Gibbs' inequality
			 \citep{mackay2003information}, 
			 $\kldiv{\mu(Y|x)}{\p_a(x)} = 0$. Since this is true
			 for all arcs, we must have $\OInc_{\dg M}( \mu) =
			 0$. Conversely, if $\mu \notin \SD{\dg M}$, then it
			 fails to marginalize to the cpd $\p_a$ on some arc
							  $a$, and so again by Gibbs inequality,
			 $\kldiv{\mu(Y|x)}{\p_a(x)} > 0$. As relative entropy
			 is non-negative, the sum of these terms over all
			 arcs must be positive as well, (because we have assumed $\bbeta > 0$) and so $\OInc_{\dg M}(
			 \mu) \neq 0$. %
\end{lproof}


Before proving the remaining results, we prove a lemma that will be useful
in other contexts as well. 

%%%%%%%%%%% BAD PROOF; EASIER TO APPEAL TO LATER RESULTS %%%%%%%%%%%%
\commentout{%
\begin{lemma}
	\label{theorem:inc-convex}
	$\OInc_{\dg M}( \mu)$ is a convex function of $\mu$, provided $\bbeta^{\dg M} \ge 0$.
\end{lemma}
\begin{lproof}\label{proof:inc-convex}
It is well known that $\thickD$ is convex \citep[Theorem
        2.7.2]{CoverThomas}, in the sense that  
\[
    \kldiv{\lambda p_1 + (1-\lambda) p_2 }
          { \lambda q_1  + (1-\lambda) q_2}
    \leq \lambda     \kldiv{p_1}{q_1} 
       + (1-\lambda) \kldiv{p_2}{q_2} 
\] 
for all tuples $(p_1, p_2, q_1,q_2)$ of distributions over the same space.
% Given an arc $\ell \in \Ar$ from $A$ to $B$ and $a \in \mathcal V(A)$,

Fix two joint distributions $\mu_1, \mu_2 \in \Delta \V\!\X$;
our goal is to show that $\OInc_{\dg M}$ is convex, i.e.,
$\OInc_{\dg M}( (1-\lambda) \mu_1 + \lambda \mu_2) \le (1-\lambda) \OInc_{\dg M}(\mu_1) + \lambda \OInc_{\dg M}(\mu_2)$ for $\lambda \in [0,1]$.

Given an arc $\ed aXY \in \Ar$ and a value $x \in \V X$,
apply the convexity of relative entropy  with $q_1 = q_2 := \p_a(Y|x)$, 
and $p_1 := \mu(Y|x)$, and $p_2 := \mu_2(Y|x)$, to find:
% for all $p_1, p_2$
\[ 
    % \thickD(\p_a(x) \ ||\ \lambda p_1 + (1-\lambda) p_2)
	% 		\leq \lambda \thickD (\p_\ell(a) \ ||\ p_1) + (1-\lambda)
	% 						\thickD(\p_\ell(a)\ ||\ p_2). 
    \kldiv[\big]{  \lambda p_1 + (1-\lambda) p_2 }{ \p_a(x) }
        \leq
     \lambda \kldiv{ p_1 }{ \p_a(x) } + (1-\lambda) \kldiv { p_2 }{ \p_a(x) }
\] 
Since this is true for all $x \in \V\!X$, we can take a sum of 
these inequalities weighted by $\lambda \mu_1(X{=}x) + (1-\lambda)\mu_2(X{=}x)$, 
giving us
\begin{align*}
	&\Ex_{x \sim \lambda \mu_1 + (1-\lambda)\mu_2}
        \kldiv[\Big]{\lambda \mu_1(Y|x) +
		(1-\lambda) \mu_2(Y|x)}{\p_a(x)} 
    \\&\qquad\leq 
		 \Ex_{x \sim \lambda \mu_1 + (1-\lambda)\mu_2}
               \lambda  \kldiv {\mu_1(Y|x)}{\p_a(x)} +
			(1-\lambda) \kldiv {\mu_2(Y|x)}{\p_a(x)}
    \\&\qquad=
        \Ex_{x \sim \lambda \mu_1 + (1-\lambda)\mu_2}
              \lambda  \kldiv {\mu_1(Y|x)}{\p_a(x)} +
           (1-\lambda) \kldiv {\mu_2(Y|x)}{\p_a(x)}
\end{align*}
Taking a sum over all arcs $a \in \ed XY$, weighted by the non-negative
number $\beta_a$, we find that
\begin{align*}
	&\sum_{\ed aXY \in \Ar} \Ex_{x\sim \mu(X)} 
        \kldiv[\Big]{\lambda \mu_1(Y|x) + (1-\lambda) \mu_2(Y|x) } { \p_a(Y|x) }
	\\&\qquad
        \leq \sum_{\ed aXY \in \Ar}
            \Ex_{a\sim \mu(A)}
                     \lambda  \kldiv[\big]{\p_a(Y|x)}{ \mu_1(Y|x) } 
                + (1-\lambda) \kldiv[\big]{\p_a(Y|x)}{ \mu_2(Y|x) },
\end{align*}
which means by definition that
\[
	\OInc_{\dg M}( \lambda \mu_1 + (1-\lambda) \mu_2)
		\leq \lambda \OInc_{\dg M}(\mu_1) + (1-\lambda)
				\OInc_{\dg M}(\mu_2). 
\]
and so $\OInc_{\dg M}( \mu )$ is a convex function of $\mu$.
\end{lproof}
}%%%%

The next proposition gives us a useful representation of $\bbr{\dg M}_\gamma$,
and letting us decompose into three more interpretable pieces.
% \restate{prop:nice-score}{
% Letting $x^{\mat w}$ and $y^{\mat w}$ denote the values of
%  $X$ and $Y$, respectively, in $\mat w \in \V\!\X$, 
% we have 
% \begin{equation*}
% \begin{split}
% \bbr{\dg M}(\mu) =  \Ex_{\mat w \sim \mu}\! \Bigg\{
% \sum_{ X \xrightarrow{\!\!a} Y  }
% \bigg[\,
% 	 \!\beta_a \log \frac{1}{\p_a(y^{\mat w} |x^{\mat w})}
%    +
% ({\alpha_a}\gamma - \beta_a ) \log \frac{1}{\mu(y^{\mat w} |x^{\mat w})} 
%  \bigg] - 
% 	\gamma \log \frac{1}{\mu(\mat w)}
%    \Bigg\} .
% \end{split}
% \end{equation*}
% }
\recall{prop:nice-score}
\begin{lproof}\label{proof:nice-score}
\allowdisplaybreaks
We use the more general formulation of $\SDef$ given
in \Cref{sec:expfam}, in which each arc $a$'s conditional  
information is weighted by $\alpha_a$.
  \begin{align*}
	&\bbr{\dg M}_\gamma(\mu) := \OInc_{\dg M}( \mu) + \gamma \SDef_{\dg M}(\mu) \\
		&= \left[\sum\alle \beta_a \Ex_{x\sim \mu_X}\kldiv[\Big]{ \mu(Y | X {=} x) }{\p_a(x) } \right]  + \gamma \left[\sum\alle \alpha_a \H_\mu(Y\mid X) ~-\H(\mu)\right]\\
		&= \sum\alle 
			\Ex_{x \sim \mu_{\!_X}}  \left[ \beta_a\; \kldiv[\Big]{ \mu(Y \mid x) }{\p_a(Y \mid x) } + \gamma \; \alpha_a \H(Y \mid X{=} x) \right]  - \gamma \H(\mu) \\ 
		&= \sum\alle 
			\Ex_{x \sim \mu_{\!_X}}  \bigg[ \beta_a\; \bigg(\sum_{y \in \V(Y)} \mu(y \mid x) \log\frac{\mu(y\mid x)}{\p_a(y\mid x)}\bigg) 
                \\&\qquad\qquad
                + \alpha_a\gamma \; \bigg(\sum_{y \in \V(Y)} \mu(y\mid x) \log \frac{1}{\mu(y\mid x)} \bigg) \bigg]  - \gamma  \H(\mu) \\ 
		&= \sum\alle 
			\Ex_{x \sim \mu_{\!_X}}  \left[ \sum_{y \in \V(Y)} \mu(y \mid x) \left(  \beta_a\; \log\frac{\mu(y\mid x)}{\p_a(y\mid x)} + \alpha_a \gamma \; \log \frac{1}{\mu(y\mid x)} \right) \right]  - \gamma  \H(\mu) \\
		&= \sum\alle 
			\Ex_{x \sim \mu_{\!_X}}  \left[ \Ex_{y \sim \mu(Y \mid X=x)} \left(  \beta_a\; \log\frac{\mu(y\mid x)}{\p_a(y\mid x)} + \alpha_a \gamma \; \log \frac{1}{\mu(y\mid x)} \right) \right]  - \gamma \sum_{\mathclap{\mat w \in \V\!\X}} \mu(\mat w) \log \frac{1}{\mu(\mat w)} \\  
		&= \sum\alle 
			\Ex_{x,y \sim \mu_{\!_{XY}}}  \left[ \beta_a\; \log\frac{\mu(y\mid x)}{\p_a(y\mid x)} + \alpha_a\gamma \; \log \frac{1}{\mu(y\mid x)}  \right]  - \gamma  \Ex_{\mat w \sim \mu} \left[ \log \frac{1}{\mu(\mat w)}\right] \\
		&= \Ex_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!a} Y  } \left[
			\beta_a \log \frac{1}{\p_a(y | x)}   - \beta_a  \log \frac{1}{\mu(y | x)}+ \alpha_a\gamma \log \frac{1}{\mu(y | x)} \right]
            % \Bigg\}  -  \gamma  \Ex_{\mat w \sim \mu} \left[\log \frac{1}{\mu(\mat w)}\right] 
              -  \gamma  \log \frac{1}{\mu(\mat w)}\Bigg\}
            \\
		&=  \Ex_{\mat w \sim \mu} \Bigg\{ \sum_{ X \xrightarrow{\!\!a} Y  } \left[
			\beta_a \log \frac{1}{\p_a(y\mid x)} +
	                        (\alpha_a\gamma - \beta_a ) \log
	                        \frac{1}{\mu(y \mid x)} \right] -
	                        \gamma \log \frac{1}{\mu(\mat w)}  \Bigg\}.  
                \qedhere
	\end{align*}
\end{lproof}

We can now prove         Proposition~\ref{prop:sem3}.
% \restate{prop:sem3}{
% If $\dg M$ is a PDG and $0 < \gamma \leq \min_a \nicefrac{\beta_a^{\dg M}}{\alpha_a^{\dg M}}$, then
% $\bbr{\dg M}_\gamma^*$ is a singleton. 
% }
\recall{prop:sem3}
\begin{lproof}\label{proof:sem3}
It suffices to show that $\bbr{\dg
			  M}_\gamma$ is a strictly convex function of $\mu$,
since every strictly convex function has a unique minimum.
Note that
\begin{align*}
&   \bbr{\dg M}_\gamma(\mu) 
\\	&= \Ex_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!a} Y  } \left[
		\beta_a \log \frac{1}{\p_a(y\mid x)} + ({\alpha_a}\gamma - \beta_a ) \log \frac{1}{\mu(y \mid x)} \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\} \\
	&= \Ex_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!a} Y  } \bigg[ \gamma {\alpha_a} \log \frac{1}{\p_a(y\mid x)} + 
		(\beta_a - {\alpha_a} \gamma) \log \frac{1}{\p_a(y\mid x)} \\[-2ex]
        &\hspace{6.5cm}- (\beta_a - {\alpha_a} \gamma) \log \frac{1}{\mu(y \mid x)} \bigg] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\}  \\
	&= \Ex_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!a} Y  } \left[ \gamma {\alpha_a} \log \frac{1}{\p_a(y\mid x)} + 
		(\beta_a - {\alpha_a} \gamma) \log \frac{\mu(y\mid x)}{\p_a(y\mid x)} \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\} \\
	&=  \sum_{ X \xrightarrow{\!\!a} Y  } \left[ \gamma {\alpha_a} 
        % \Ex_{\vphantom{|}(x,y) \sim \mathrlap{\mu(X,Y)}}
        \Ex_{\vphantom{|}(x,y) \sim \mu_{XY}}
        % \Ex_{\substack{\phantom{aaaaaa}\\\mathclap{(x,y) \sim \mu(X,Y)}}}
            \log \frac{1}{\p_a(y| x)} + 
		(\beta_a - {\alpha_a} \gamma) \Ex_{x\sim\mu_X}
          \kldiv[\big]{\mu(Y | x)}{\p_a(x)} \right] - \gamma \H(\mu). 
\end{align*}
The remainder of the argument analyzes the convexity of
    the three terms of this final formula. 
The first term, 
\( \Ex_{x,y \sim \mu_{\!_{XY}}} \left[-\log {\p_a(y\mid x)}\right] \) 
is linear in $\mu$ (since $\p_a(y\mid x)$ does not depend on $\mu$), and hence convex. %
As for the second term, it is well-known that relative entropy is convex, in the sense that 
\begin{align*}
    &\kldiv{\lambda q_1 + (1-\lambda) q_2 }{ \lambda p_1 + (1-\lambda) p_2}
    \\&\qquad\qquad \leq \lambda \kldiv {q_1}{ p_1} +
                (1-\lambda) \kldiv{q_2}{p_2}.
\end{align*}
Instantiate this for distributions over values of $Y$, setting $p_1 =
 p_2 := \p_a(x)$, 
    and  $q_1 := \mu_1(Y \mid X=x)$ and
			$q_2 := \mu_2(Y\mid X=x)$, to get:
\begin{align*}
    &\kldiv{\lambda \mu_1(Y\mid x) + (1-\lambda)
		  \mu_2(Y\mid x) }{ \p_a(x) } 
    \\&\qquad\qquad \leq \lambda \kldiv
		   {\mu_1(Y\mid x)}{\p_a(x)} + (1-\lambda) \kldiv{\mu_2(Y\mid x)}{\p_a(x)}. 
\end{align*}
So $\kldiv*{\mu(Y\mid x)}{\p_a(Y|x)}$ is convex as a function of $\mu$. 
As convex combinations of convex functions are convex,
the second term, $\Ex_{x\sim\mu(X)} \kldiv*{\mu(Y\mid x)}{\p_a(x)}$, is convex.
Finally, negative entropy (the third term) is well known to be strictly convex.                

Any non-negative linear combinations of the three
terms is convex, and if this combination applies a
positive coefficient to the (strictly convex) negative entropy,
it must be strictly convex. Therefore, as
long as $\beta_a \geq \gamma$ for all arcs $a \in
\Ar^{\dg M}$, $\bbr{\dg M}_\gamma$ is
strictly convex.  The result follows.
\end{lproof}


We next prove \Cref{theorem:limit-uniq}.  The first step is provided by the
following lemma, which shows that any limiting optimal dsitributions as $\gamma \to 0$ must also be optimal distributions for $\gamma = 0$.  

\begin{lemma}\label{lem:gamma2zero}
 $\lim\limits_{\gamma\to0}\bbr{\dg M}_\gamma^* \subseteq \bbr{\dg M}_0^*$. 
\end{lemma}
\begin{lproof}\label{proof:gamma2zero}
\def\lb{k}
\def\ub{K}  

Since $\SDef_{\dg M}$ is a finite weighted sum of entropies
and conditional entropies over the variables $\N^{\dg M}$, which have
finite support%
, it is bounded.
Thus, there exist bounds $k$ and $K$ depending only on $\N^{\dg M}$ and
$\V^{\dg M}$, such that $\lb \leq \SDef_{\dg M}(\mu) \leq \ub$ for all $\mu$.
Since $\bbr{\dg M}_\gamma = \OInc_{\dg M} + \gamma \SDef_{\dg M}$,
it follows that, for all $\mu \in \V\!\X$, we have
\[ \OInc_{\dg M}( \mu) + \gamma\lb \leq~ \bbr{\dg M }_\gamma(\mu) 
\leq~  \OInc_{\dg M}( \mu) + \gamma\ub. \]
For a fixed $\gamma$, since this inequality holds for all $\mu$, and
both $\OInc$ and $\SDef$ are bounded below, it must be the case that  
\[
\min_{\mu \in \Delta\V\!\X} \Big[ \OInc_{\dg M}( \mu) + \gamma\lb \Big]
~\leq~ \min_{\mu \in \Delta\V\!\X}\bbr{\dg M }_\gamma(\mu) ~\leq~
\min_{\mu \in \Delta\V\!\X} \Big[ \OInc_{\dg M}( \mu) + \gamma\ub
    \Big], \] 
even though the distributions that minimize each expression will in general be different.
Let $\OInc(\dg M) = \min_{\mu} \OInc_{\dg M}(\mu)$.
Since $\Delta\V\!\X$ is compact, the minimum of the middle term is
achieved.  
Therefore, for $\mu_\gamma \in \bbr{\dg M}^*_\gamma(\mu)$ that
minimizes it, we have 
$$\OInc(\dg M) +\gamma \lb \le \bbr{\dg M }_\gamma(\mu_\gamma) \le
		 \OInc(\dg M) +\gamma \ub$$ for all $\gamma \ge 0.$
Now taking the limit as $\gamma\rightarrow 0$ from above, we get that
$\OInc(\dg M) = \bbr{\dg M }_0(\mu^*)$.
Thus, $\mu^* \in \bbr{\dg M}_0^*$, as desired.
\end{lproof}

We now apply Lemma~\ref{lem:gamma2zero} to show that the limit as
$\gamma \to 
0$ is unique, as stated in \Cref{theorem:limit-uniq}. 
% \restate{theorem:limit-uniq}{
% 	For all $\dg M$, $\lim_{\gamma\to0}\bbr{\dg M}_\gamma^*$ is a singleton.
% }
\recall{theorem:limit-uniq}
\begin{lproof}\label{proof:limit-uniq}
First we show that $\lim_{\gamma \to 0}\bbr{\dg M}_\gamma^*$ cannot be empty.
Let $(\gamma_n) = \gamma_1, \gamma_2, \ldots$ be a sequence of
positive reals 
converging to zero.  For all $n$, choose some $\mu_n \in \bbr{\dg
M}_{\gamma_n}^*$. Because $\Delta\V\!\X$ is a compact metric
space, it is sequentially compact, and so, by the
BolzanoWeierstrass Theorem, the sequence $(\mu_n)$ has at least one
accumulation point, say $\nu$. By our definition of the limit, $\nu \in
\lim_{\gamma\to0}\bbr{\dg M}_\gamma^*$, as witnessed by the sequence
$(\gamma_n, \mu_n)_n$.  It follows that $\lim_{\gamma\to0}\bbr{\dg
  M}_\gamma^* \ne \emptyset$.

Now, choose $\nu_1, \nu_2  \in  \lim_{\gamma\to0}\bbr{\dg
  M}_\gamma^*$. 
Thus, there are subsequences $(\mu_{i})$ and $(\mu_{j})$ of
$(\mu_n)$ converging
to $\nu_1$ and $\nu_2$, respectively.
By \Cref{lem:gamma2zero}, $\nu_1, \nu_2 \in \bbr{\dg M}_0^*$, so
$\OInc_{\dg M}(\nu_1) = \OInc_{\dg M}(\nu_2)$.  
Because  $(\mu_{j_n}) \to \nu_1$, $(\mu_{k_n}) \to \nu_2$, and
$\SDef_{\dg M}$ is
continuous on $\Delta\V\!\X$,
we conclude that  
$(\SDef_{\dg M}(\mu_{i}))\to \SDef_{\dg M}(\nu_1)$ and
$(\SDef_{\dg M}(\mu_{j}))\to \SDef_{\dg M}(\nu_2)$.

Suppose that $\SDef_{\dg
M}(\nu_1) \neq \SDef_{\dg M}(\nu_2)$. Without loss of generality,
suppose that $\SDef_{\dg M}(\nu_1) > \SDef_{\dg M}(\nu_2)$. 
Since $(\SDef_{\dg M}(\mu_{i})) \to \SDef_{\dg M}(\nu_1)$, there exists some $i^*
\in \mathbb N$ such that for all $i > i^*$,  
$ \SDef_{\dg M}(\mu_{i}) >  \SDef_{\dg M}(\nu_2) $.
But then for all $\gamma$ and $i > i^*$, we have 
\[ \bbr{\dg M}_\gamma(\mu_i) = \OInc(\mu_i) + \gamma\SDef_{\dg M}(\mu_i)
> \OInc(\nu_2)  
+ \gamma \SDef_{\dg M}(\nu_2) = \bbr{\dg M}_\gamma(\nu_2),\]
contradicting the assumption that $\mu_{i}$ minimizes
$\bbr{\dg M}_{\gamma_{i}}$. We thus conclude that we
cannot have $\SDef_{\dg M}(\nu_1) > \SDef_{\dg M}(\nu_2)$.  By the same
argument, we also cannot have $\SDef_{\dg M}(\nu_1) < \SDef_{\dg
  M}(\nu_2)$, so $\SDef_{\dg M}(\nu_1) =\SDef_{\dg M}(\nu_2)$.  
  
Now, suppose that $\nu_1$ and $\nu_2$ distinct. Since $\bbr{\dg M}_\gamma$
is strictly convex for $\gamma > 0$, among the possible convex
combinations of $\nu_1$ and $\nu_2$, the distribution $\nu_3 = \lambda
\nu_1 + (1-\lambda) \nu_2$ that minimizes $\bbr{\dg M}_\gamma$ must
lie strictly between $\nu_1$ and $\nu_2$. 
Because $\OInc$ itself is convex and $\OInc_{\dg M}(\nu_1) = \OInc_{\dg
  M}(\nu_2) =: v$, we must have $\OInc_{\dg M}(\nu_3) \le v$. 
But since
$\nu_1,\nu_2 \in \bbr{\dg M}_0^*$ minimize $\OInc$,
we must have $\OInc_{\dg M}(\nu_3) \ge v$.
Thus, $\OInc_{\dg M}(\nu_3) = v$. 
Now, because, for all  $\gamma > 0$,
\[ \bbr{\dg M}_\gamma(\nu_3) = v + \gamma \SDef_{\dg M}(\nu_3) 
 	< v + \gamma \SDef_{\dg M}(\nu_1) = \bbr{\dg M}_\gamma(\nu_1), \] 
it must be the case that $\SDef_{\dg M}(\nu_3) < \SDef_{\dg M}(\nu_1)$. 
        
We can now get a contradiction by applying the same argument as that used to show
that $\SDef_{\dg M}(\nu_1) =\SDef_{\dg M}(\nu_2)$.  
    Because $(\mu_{i}) \to \nu_1$, there exists some
    $i^*$ such that for all $i > i^*$, we have $\SDef_{\dg M}(\mu_{i}) >
    \SDef_{\dg M}(\nu_3)$. Thus, for all $i > i^*$ and all
    $\gamma > 0$, 
    \[ \bbr{\dg M}_\gamma(\mu_{i}) = \OInc(\mu_{i}) + \gamma\SDef_{\dg M}(\mu_{i}) > \OInc(\nu_3) 
    + \gamma \SDef_{\dg M}(\nu_3) = \bbr{\dg M}_\gamma(\nu_3),\]
again contradicting the assumption that $\mu_{i}$ minimizes
$\bbr{\dg M}_{\gamma_{i}}$.
Thus, our supposition that $\nu_1$ was distinct from $\nu_2$ cannot hold, and so
$\lim_{\gamma \to 0}\bbr{\dg M}_\gamma^*$ must be a singleton, as desired.
\end{lproof}

Finally, \Cref{prop:consist} is a simple corollary of \Cref{lem:gamma2zero} and \Cref{theorem:limit-uniq}, as we now show. 
% \restate{prop:consist}{
% $\bbr{\dg M}^* \in \bbr{\dg M}_0^*$, so if $\dg M$ is consistent,
% then $\bbr{\dg M}^* \in \SD{\dg  M}$.
% }
\recall{prop:consist}
\begin{lproof}\label{proof:consist}
By \Cref{theorem:limit-uniq}, $\lim_{\gamma \to 0}\bbr{\dg M}_\gamma^*$
is a singleton. As in the body of the chapter, we refer to its unique element by $\bbr{\dg M}^*_{0^{\!+}}$.
\Cref{lem:gamma2zero} therefore immediately gives us $\bbr{\dg M}^*_{0^{\!+}} \in \bbr{\dg M}_0^*$.  
%
If $\dg M$ is consistent, then by \Cref{prop:sd-is-zeroset},
$\OInc({\dg M}) = 0$, so $\bbr{\dg M}_0(\bbr{\dg M}^*_{0^{\!+}}) = 0$, and thus
$\bbr{\dg M}^*_{0^{\!+}} 
\in \SD{\dg M}$. 
\end{lproof}


% \subsection{PDGs as Bayesian Networks}
\subsection{Bayesian Networks as PDGs}
In this section, we prove Theorem~\ref{theorem:bns-are-pdgs},
%
% We start by recounting some standard results and notation, all of
% which can be found in a standard introduction to information
% theory (e.g., \cite[Chapter 1]{mackay2003information}).  
% 
% First, note that just as we introduced new variables to model joint dependence
% in PDGs, we can view a finite collection $\mathcal X=X_1, \ldots, X_n$ of random
% variables, where each $X_i$ has the same sample space, as itself a random
% variable%
% , taking the value $(x_1, \ldots, x_n)$ iff each $X_i$ takes the value $x_i$.
% Doing so allows us to avoid cumbersome and ultimately irrelevant notation which treats sets of raomd variables differently, and requires lots of unnecessary braces, bold face, and uniqueness issues. 
% Note the notational convention that the joint variable $X,Y$ may be indicated by a comma.
%
    starting with the detals of how to transform a BN into a PDG.
%
The transformation to a PDG with a directed hypergraph structure is immediate.
Here, we give the formal details of how to compile it to a strict PDG, with only ordinary directed edges. 

\begin{defn}[BN to strict PDG]\label{def:bn2PDG}
Recall that a (quantitative) Bayesian Network $(G, \mat P)$ consists of two
parts: its qualitative graphical structure $G$, 
described by a dag,
and quantitative data $\mat P$ which consists of 
a cpd $P_i(X_i \mid \Pa(X_i))$ for each variable $X_i$.
Given a Bayesian network $\cal B$ over variables
$X_1, \ldots, X_n$, we construct the corresponding \emph{strict} unweighted PDG
%
$\PDGof{{\mathcal B}}$
			as follows: we take $\N := \{X_1, \ldots, X_n \} \cup
			\{ \Pa(X_1), \ldots, \Pa(X_n)\}$.  
That is, the variables of 
	  $\PDGof{{\mathcal B}}$
consist of all the variables in
${\cal B}$ together with a variable corresponding to the parents
of $X_i$.
These additional variables are used to emulate the hyperarcs.  
The values $\V\!X_i$ for the original variables
			$X_i$ are unchanged, 
% (i.e., $\V^{\PDGof{{\mathcal B}}}(\{X_i\}) := \V\! X_i$)
and we set $\V(\Pa(X_i)) := \prod_{Y \in \Pa(X_i)} \V(Y)$.
(If $\Pa(X_i) = \varnothing$, so that $X_i$ has no parents, then we 
then we identify $\Pa(X_i)$ with $\pdgunit$ and
take $\V(\Pa(X_i)) = \{\star\}$, as discussed in \cref{ssec:basic-vars}.).

We take the set of arcs $\Ar := \{ (\Pa(X_i){\to} X_i) : 
i = 1, \ldots, n \} \cup \{ (\Pa(X_i){\to}Y) : Y \in
			\Pa(X_i)\}$ to be the set of arcs to a variable $X_i$
	  from its parents, together with an arc from
	  from $\Pa(X_i)$ to each of the elements of $\Pa(X_i)$, for
	  $i = 1, \ldots, n$.  
For the cpds, we set $\p_{(\Pa(X_i){\to}X_i)}$ to be the cpd associated
			with $X_i$ in $\cal B$.
Finally, we give the standard projection
    $X_j \in \Pa(X_i)$ defined in \cref{constr:hyperedge-reducton}
    to the remaining arcs.
So, if $X_j \in \Pa(X_i)$, we set
\[ \p_{(\Pa(X_i){\to}X_j)}(\ldots, x_j, \ldots) = \delta_{x_j};\]
that is,
$\mathbb P_{(\Pa(X_i), X_j)}^{\PDGof{\mathcal B}}$ is the the cpd 
on $X_j$ that, given a setting $(\ldots, x_j, \ldots)$ of $\Pa(X_i)$, yields the distribution that puts all mass on $x_j$. 
\end{defn}

Consider a BN $\mathcal B$  with variables $\X X = \{X_1,\ldots, X_n\}$, and let $\bbeta > 0$ be a positive vector over $\X$.
% Let $\mathcal X$ be the variables of some BN $\mathcal B$, and
Let $(\N, \V, \Ar, \mathbb P) := \PDGof{\mathcal B}$ 
be the corresponding strict (unweighted) PDG.
$\N$ contains $\X$, but also contains variables of the form $\Pa(X_i)$,
    so it is a strict superset.
However, there is a unique way to determine all of these other variables from $\X$, in a way that is compatible with the extra structural cpds in $\PDGof{\cal B}$.
Specifically, given a joint setting $\omega \in \V\!\X$, the variable $\Pa(X_i)$ must take on the restriction $\omega[\Pa(X_i)]$ of $\omega$ to those variables. Let $f: \V\X \to \V\N$ denote this function.
Thus, it makes sense to identify distributions
    $\mu \in \Delta \V\N$ of the form $\mu(\X) \delta\!f(\N \mid \X)$,
    and their marginals $\mu(\X) \in \Delta \V\X$.
It is easy to see that any extended distribution $\nu(\N)$ that is not of this form will have infinite score, and so will not be relevant for the scoring function.

\commentout{%
Because the set $\N$ of variables in $\PDGof{{\mathcal
    B},\beta}$ includes 
    variables of the form $\Pa(X_i)$, it is a strict superset of
    $\mathcal X = \{X_1,\ldots, X_n\}$, the set of variables of $\mathcal B$.
% For the purposes of this theorem,
As usual, 
there is a unique distribution 
we identify a distribution
$\mu \in \Delta \V\!\X$ 
with the unique distribution $\Pr_{\cal B}$ whose marginal on the
variables in $\mathcal X$ is $\mu_{\mathcal X}$ such that if $X_j \in
\Pa(X_i)$, then 
$\mu_{\mathcal N}(X_j = x_j' \mid \Pa(X_i) = (\ldots, x_j,\ldots)) =
1$ iff $x_j = x_j'$.  In the argument below, we abuse notation,
dropping the the subscripts $\mathcal X$ and $\mathcal N$ on a
distribution $\mu$.
}%

% \restate{theorem:bns-are-pdgs}{
% If $\cal B$ is a Bayesian network
% and $\Pr_{\cal B}$ is the distribution it specifies, then
%   for all $\gamma > 0$ and all vectors $\beta$ such
%   that $\beta_a > 0$ for all arcs $a$,
%   $\bbr{\PDGof{\mathcal B, \beta}}_\gamma^* = \{ \Pr_{\cal B}\}$, 
% and thus $\bbr{\PDGof{\mathcal B, \beta}}^* = \Pr_{\cal B}$.    
% }
\recall{theorem:bns-are-pdgs}
\begin{lproof}\label{proof:bns-are-pdgs}
  For the cpd $p(X_i \mid \Pa(X_i))$ associated to a node $X_i$ in 
$\cal B$, we have that $\Pr_{\cal B}(X_i
\mid \Pa(X_i)) = p(X_i \mid \Pa(X_i))$.  
For all nodes $X_i$ in $\mathcal B$ and $X_j \in \Pa(X_i)$, 
by construcction, $\Pr_{\cal B}$, when viewed as a distribution on
$\mathcal N$, is also with the cpd on the arc from $\Pa(X_i)$ to
$X_j$.
Thus, $\Pr_{\cal B}$ is consistent with all the cpds in
$\PDGof{\mathcal B, \beta}$;
so$\OInc_{\PDGof{\mathcal B,\beta}}(\Pr_{\cal B}) = 0$.

We next want to show  that $\SDef_{\PDGof{\mathcal B,\beta}}(\mu) \ge 0$ for all
distributions $\mu$.  To do this, we first need some definitions.
Let $\rho$ be a permutation of $1, \ldots,  n$.  Define an order
$\prec_{\rho}$ by taking $j \prec_{\rho} i$ if $j$ precedes $i$ in the
permutation; that is, if 
$\rho^{-1}(j)$ < $\rho^{-1}(i)$. Say that a permutation is \emph{compatible with
  $\mathcal B$} if $X_j \in \Pa(X_i)$ implies $j \prec_{\rho} i$.   There
is at least one permutation compatible with $\mathcal B$, since 
the graph underlying $\mathcal B$ is acyclic.
  
Consider an arbitrary distribution $\mu$ over the variables in
$\mathcal X$ (which we also view as a distribution over the variables
in $\mathcal N$, as discussed above).
Recall from \Cref{def:bn2PDG}
that the cpd on the arc in $\PDGof{{\cal B},\beta}$ from $\Pa(X_i)$ to $X_i$
is just the cpd associated with $X_i$ in ${\cal B}$, while the cpd on
the arc in $\PDGof{{\cal B},\beta}$ from $\Pa(X_i)$ to $X_j \in \Pa(X_i)$
consists only of deterministic distributions (i.e., ones that put
probability 1 on one element), which all have entropy 0.  
Thus,
\begin{equation}\label{eq:fact2}
\sum_{\ed aXY \in \Ar^{\PDGof{\mathcal B}}} \H_\mu(Y\mid
X)=\sum_{i=1}^n \H_\mu(X_i \mid \Pa(X_i)). 
\end{equation}

Given a permutation $\rho$, let ${\bf X}_{\prec_\rho i} = \{X_j: j
\prec_\rho i\}$.  Observe that 
\begin{align*}
    \SDef_{\PDGof{\mathcal B,\beta}}(\mu)
 	&= \left[\sum_{\ed aXY \in \Ar^{\PDGof{\mathcal B}}} \H_\mu(Y\mid X) \right] - \H(\mu) \\
	&= \sum_{i=1}^n \H_\mu(X_i \mid \Pa(X_i)) - \sum_{i = 1}^n
\H_\mu(X_i \mid {\bf X}_{\prec_\rho i}) & \text{[by
    \Cref{fact:entropy-chain-rule} and \eqref{eq:fact2}]}\\ 
	&= \sum_{i=1}^n \Big[\H_\mu(X_i \mid \Pa(X_i)) - \H_\mu(X_i
  \mid {\bf X}_{\prec_\rho i} )\Big] \\ 
      &= \sum_{i=1}^n \I_\mu \Big( X_i ~;~ {\bf X}_{\prec_\rho i}
    \setminus \Pa(X_i) ~\Big|~ \Pa(X_i) \Big). & \text{[by
        \Cref{fact:cmi}]} 
\end{align*}

Using \Cref{fact:cmi}, it now follows that,
for all distributions $\mu$,
$\SDef_{\PDGof{\mathcal B}}(\mu) \ge 0$.
Furthermore, for all $\mu$ and permutations $\rho$,
\begin{equation}\label{eq:key}
  \SDef_{\PDGof{\mathcal B}}(\mu) = 0 \quad\mbox{ iff }\quad 
    \forall i.~X_i \CI_\mu {\bf X}_{\prec_\rho i}.
\end{equation}

Since the left-hand side of (\ref{eq:key}) is independent of $\rho$,
it follows that $X_i$ is independent of 
${\bf X}_{\prec_\rho i}$ for some permutation $\rho$ iff $X_i$ is independent of
  ${\bf X}_{\prec_\rho i}$ for every permutation $\rho$.  Since there
is a permutation compatible with $\mathcal B$, we get that 
$\SDef_{\PDGof{\mathcal B,\beta}}(\Pr_{\cal B}) = 0$.
We have now shown that that $\SDef_{\PDGof{\mathcal B, \beta}}$ and $\OInc$ are 
non-negative functions of $\mu$, and both are zero at $\Pr_{0\cal B}$. 
Thus, for all $\gamma \geq 0$ and all vectors $\beta$, we
have that   $\bbr{\PDGof{\mathcal B, \beta}}_\gamma( \Pr_{\cal
  B}) \le \bbr{\PDGof{\mathcal B, \beta}}_\gamma( \mu)$ for all
distributions $\mu$.  We complete the proof by showing that if
$\mu \ne \Pr_{\cal B}$, then 
$\bbr{\PDGof{\mathcal B, \beta}}_\gamma(\mu) > 0$
for $\gamma > 0$.

So suppose that $\mu \ne \Pr_{\cal B}$. 
Then $\mu$ must also match each cpd of $\cal B$,
for otherwise $\OInc_{\PDGof{\mathcal B,
\beta}}(\mu) > 0$, and we are done.  
Because $\Pr_{\cal B}$ is the \emph{unique} distribution that matches the 
both the cpds and independencies of $\cal B$, $\mu$ must not have all of the 
independencies of $\cal B$. 
Thus,
some variable $X_i$, $X_i$ is not independent of some nondescendant $X_j$ in
$\mathcal B$ with respect to $\mu$.  There must be some permutation
$\rho$ of the variables in $\mathcal X$ compatible with ${\mathcal B}$
such that $X_j \prec_{\rho} X_i$ (e.g., we can start with $X_j$ and
its ancestors, and then add the remaining variables appropriately).
Thus, it is not the case that $X_i$ is independent of $X_{\prec \rho,
  i}$, so by (\ref{eq:key}), $\SDef_{\PDGof{\mathcal B}}(\mu) > 0$.
This completes the proof.
\end{lproof}

\subsection{Factor Graph Proofs}
    \label{proof:pdg-is-fg}\label{proof:fg-is-pdg}% target for proof link
Theorems \ref{theorem:fg-is-pdg}(a) and (b) are immediate corolaries of their
more general counterparts, \cref{theorem:pdg-is-wfg,theorem:wfg-is-pdg}, which
we now prove. 


\recall{theorem:pdg-is-wfg}
\begin{lproof}\label{proof:pdg-is-wfg}
	Let $\dg M := (\dg N, \mat v, \gamma \mat v)$ be the PDG in question.
	Explicitly, $\alpha^{\dg M}_a = v_a$ and $\beta_a^{\dg M} =  \gamma v_a$ for all $a \in \Ar$.
	By \Cref{prop:nice-score},
	\[ \bbr{\dg M}_\gamma(\mu)= \Ex_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!a} Y  } \left[
		\beta_a \log \frac{1}{\p_a(y\mid x)} + (
			\alpha_a
		\gamma - \beta_a ) \log \frac{1}{\mu(y \mid x)}
					\right] - \gamma \log \frac{1}{\mu(\mat w)}
			\Bigg\}.  \]
	Let $\{\phi_a\}_{L \in \Ar} := \Phi_{\dg N}$ denote the
			factors of the factor graph associated with $\dg M$. 
	Because we have $\alpha_a\gamma  = \beta_a$, the middle term cancels, leaving us with
	\begin{align*}
	\bbr{\dg M}_\gamma(\mu) &= \Ex_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!a} Y  } \left[
		\beta_a \log \frac{1}{\p_a(y\mid x)} \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\} \\
		&= \Ex_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!a} Y  } \left[
	\gamma v_a \log \frac{1}{\phi(x,y)}  \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\} 
					&\text{[as $\beta_a = v_a \gamma$]}\\
		&= \gamma \Ex_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!a} Y  } \left[
v_a \log \frac{1}{\phi(x,y)}
			 \right] -\log \frac{1}{\mu(\mat w)} \Bigg\} \\
			&= \gamma \VFE_{(\FGof{\dg N}, \mat v)}. 
	\end{align*}
	It immediately follows that the associated factor graph has 
	$\bbr{\dg M}^*_\gamma
 	= \{\Pr_{\Phi(\dg M)}\}$, because the free energy is clearly a constant plus the KL divergence from its associated probability distribution.
\end{lproof}

% \restate{theorem:fg-is-pdg}{
% 	For all WFGs $\Psi = (\Phi,\theta)$ and all $\gamma > 0$,
% 	we have that
% 	$\VFE_\Psi
% 	= \nicefrac1{\gamma} \bbr{{\dg M}_{\Psi,\gamma}}_{\gamma} 
% 	+ C$   
% 	for some constant $C$, so
% 	$\Pr_{\Psi}$ is the unique element of
% 	$\bbr{{\dg M}_{\Psi,\gamma}}_{\gamma}^*$.
% }
\recall{theorem:wfg-is-pdg}
\begin{lproof}\label{proof:wfg-is-pdg}
  In $\PDGof{\Psi,\gamma}$,  there is an arc $1 \to X_J$ for every $J
  \in \mathcal J$, and also arcs 
  $X_J \tto X_j$ for each $X_j
    \in X_J$. 
Because the cpds attached to the latter arcs are deterministic, a
distribution $\mu$ that is not  consistent
with one of the arcs, say $X_J \tto X_j$, has $\OInc_{\dg M}(\mu)
= \infty$.  This is a 
property of relative entropy: if there exist $j^* \in \V(X_j)$ and 
$\mat z^* \in \V(J)$ such that $\mat z^*_J \ne j^*$ and $\mu$ places positive
probability on their co-occurance (i.e., $\mu(j^*, \mat z^*) > 0$),
then we would have
\begin{align*}
\Ex_{\mat z \sim \mu_{J}}\kldiv[\Big]{\mu(X_j \mid X_J = \mat z)}
	{\mathbbm1[X_j = \mat z_{j}]}
 	&= \sum_{\substack{\mat z \in \V(X_J),\\ \iota \in \V(X_j)}} \mu(\mat z, \iota) \log \frac{\mu(\iota \mid \mat z)}{\mathbbm1[\mat z_j = \iota]}
	\\&\geq \mu(\mat z^*, j^*) \log \frac{\mu(j^* \mid \mat z)}{\mathbbm1[\mat z^*_j = j_*]}
	= \infty. 
\end{align*}
Consequently, a distribution $\mu$ that does not satisfy the the projections has
$\bbr{\dg M_{\Psi,\gamma}}_\gamma(\mu) = \infty$ for every $\gamma$.
          Thus, a distribution that 
        has a finite score must match the constraints,
so we can identify such a distribution with its restriction to 
the original  
variables of $\Phi$.
Moreover, for all distributions $\mu$ with finite score and
projections $X_J \tto 
X_j$, the conditional entropy 
$\H(X_j \mid X_J) = -\Ex_\mu\log(\mu(x_j \mid x_J))$ and divergence from
the constraints are both zero. 
Therefore the per-arc terms for both $\SDef_{\dg M}$
and $\OInc_{\dg M}$ can be safely ignored for the projections.
Let $\p_J$ be
the normalized distribution $\frac{1}{Z_J}\phi_J$ over $X_J$,
where $Z_J = \sum_{x_J} \phi_J(x_J)$ is the appropriate normalization constant.
By
\cref{def:wfg2pdg}, we have $\PDGof{\Psi,\gamma} = (\UPDGof{\Phi}, \theta, \gamma\theta)$,
so by \cref{prop:nice-score},
	\begin{align*}
\bbr{\PDGof{\Psi,\gamma}}_\gamma(\mu) 
	&= \Ex_{\mat x \sim \mu} \Bigg\{   \sum_{ J \in \mathcal J } \left[
		\beta_J \log \frac{1}{ \p_J(x_J) } + 
			(\alpha_J \gamma -\beta_J)
		 \log \frac{1}{\mu(x_J)} \right] - \gamma \log \frac{1}{\mu(\mat x)} \Bigg\} \\
		 &= \Ex_{ \mat x \sim \mu} \Bigg\{   \sum_{ J \in \mathcal J } \left[
	 		(\gamma\theta_J) \log \frac{1}{ \p_J(x_J) } + 
	 			(\theta_J \gamma - \gamma\theta_J)
	 		 \log \frac{1}{\mu(x_J)} \right] - \gamma \log \frac{1}{\mu(\mat x)} \Bigg\} \\
		&= \Ex_{ \mat x \sim \mu} \Bigg\{  \sum_{ J \in \mathcal J }\left[
			\gamma\theta_J \log \frac{1}{\p_J(x_J)}  \right] - \gamma \log \frac{1}{\mu(\mat x)} \Bigg\} 
			\\
		&= \gamma \cdot \Ex_{\mat x \sim \mu} \Bigg\{  \sum_{ J \in \mathcal J } \theta_J
			\log \frac{Z_J}{\phi_J(x_J)}   -\log \frac{1}{\mu(\mat x)} \Bigg\} \\
		&= \gamma \cdot \Ex_{\mat x \sim \mu} \Bigg\{  \sum_{ J \in \mathcal J } \theta_J \left[
			\log \frac{1}{\phi_J(x_J)} + \log Z_J \right]  - \log \frac{1}{\mu(\mat x)} \Bigg\} \\
		&= \gamma \cdot \Ex_{\mat x \sim \mu} \Bigg\{  \sum_{ J \in \mathcal J } \theta_J 
			\log \frac{1}{\phi_J(x_J)}  - \log \frac{1}{\mu(\mat x)} \Bigg\}
			 +  \sum_{J \in \mathcal J} \theta_J \log Z_J  \\
        	&= \gamma\, \VFE_{\Psi} + k \log \prod_{J} Z_J,
	\end{align*}
which differs from $\VFE_{\Psi}$ by the value $\sum_J \theta_J \log Z_J$, which 
is constant in $\mu$.
\end{lproof}

\end{subappendices}
