%%%%%%%%%%%% EXTRA COMMANDS %%%%%%%%%%
\newcommand\dotmodels{\mathbin{{\models}^{\mathllap{\bullet\,}}_{\vphantom{l}}}}
\newcommand\emodels{\mathbin{{\models}^{\mathllap{e\,}}_{\vphantom{l}}}}

\newcommand\PSEMs{\mathrm{PSEMs}}
\newcommand\PSEMsA{\mathrm{PSEMs}_{\!\Ar}}
\newcommand\Wits{\mathrm{Wits}}

\newcommand\U{\mathcal U}
\newcommand\enV{\mathcal V} % for endogenous variables
\newcommand\muxor{\mu_{\mathit{xor}}}

\newcommand\hyperarc{hyperarc}
\newcommand\arc{hyperarc}
% \newcommand\SQIM{SIM}
\newcommand\SQIM{QIM}%
\newcommand\scibility{\SQIM-compatibility}
\newcommand\Scibility{\SQIM-compatibility}
\newcommand\SCibility{\SQIM-Compatibility}

\newcommand\scible{\SQIM-compatible}
\newcommand\escible{E\scible}
\newcommand\cible{compatible}
\newcommand\cibility{compatibility}

\newcommand\partl{partitional}
\newcommand\subpartl{sub\partl}
\newcommand\unipartl{uni\partl}
\newcommand\suppartl{super\partl}
\newcommand\suppartlness{\suppartl ity}

\newcommand\hgraph{hypergraph}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In \cref{sec:set-of-distribution-semantics}, we defined what it meant
for a joint distribution $\mu$ to be \emph{quantitatively} compatible with the information in a PDG---it must match all of the cpds. 
But conspiciously absent is a qualitative analogue. 
What should it mean for a distribution to be compatible with the qualitative data of the hypergraph? 



In this chapter, we define what it means for a joint probability distribution to be 
% \emph{compatible}
% compatible
compatible
with a set of independent causal mechanisms,
at a qualitative level---%
or, more precisely, with a directed hypergraph $\Ar$,
i.e., the qualitative structure of a PDG. 
When $\Ar$ represents a qualitative Bayesian network (BN), 
this notion of \emph{\scibility} with $\Ar$
reduces to satisfying the appropriate conditional independencies. 
But giving semantics to hypergraphs using \scibility\ lets us do much more.
For one thing, we can capture functional \emph{dependencies}.
%spencer3: it's not clear what this means
    % (as well as independencies) for arbitrary graphs, including cyclic structures.
%oli22: not much info content; saves a line
% as well as independencies.
%joe17: starting to diverge
For another, we can capture important aspects of causality using compatibility:
%spencer3*: added
we can use compatibility to understand cyclic causal graphs, and
to demonstrate structural compatibility, we must 
essentially produce a causal model. 
Finally, compatibility has deep connections to 
%spencer3:
Shannon information.
% information theory.
%oli22:
% Indeed, 
Applying compatibility to cyclic structures helps to clarify a
longstanding conceptual issue 
%spencer3:
% with Shannon information.
in information theory.
Compatibility also has a close, but far from obvious,
relationship with the original scoring-function semantics for
qualitative PDGs, 
which underlies many of our results.


\section{Introduction}
The 
% oli3:
% graphical
structure of a probabilistic graphical model
    % (such as a Bayesian Network)
    % (like a Bayesian Network or Markov Random Field)
    encodes a set of conditional independencies among variables.
% This interpretation of a graph is called
This is useful because it enables a compact description of
    probability distributions that have those independencies;
%oli16: merging this sentence and previous one to emphasize final sentence with flow
it also lets us use graphs as a visual language for
    describing important qualitative
    properties of a probabilistic world.
    % and also because it makes graphical models a compact, intuitive
 % also because it makes graphical models a compact, intuitive
 %    language for describing independence.
    %     language for describing independence.
    % in part to enable a compact description of probabiltiy distributions
    % that use those independencies.
% Yet independence is not the only qualitative aspect of
%     a probability measure, nor is it
%     unique in enabling a compact representation.
% Yet these kinds of independences are not the only important
%     qualitative aspects of a probability measure, nor are they
%     unique in enabling a compact representation.
% 
Yet these kinds of independencies are not the only important
    qualitative aspects of a probability measure.
%oli16*: starting with a more explicit story
%joe16
%This paper describes a natural generalization of qualitative
%graphical model structures whose consequences extend beyond
%conditional independence.
%oli21*: As I've said many times before, I strongly object to putting PDGs in the first paragraph. It's not something we can expect the audience to recognize, and it's really not the focus of our paper. Hypergraphs also do not need to come up yet, and the first time we say "hypergraph" it needs to be directed. 
%joe17*: but it *is* historically accurate.  And we need to make the connection      
%oli22: historical accuracy is far less important than telling a story. Besides, we'll make the connection soon enough.
%joe18: I agree in general abot the story
%oli21: Trying for a replacement. 
% In this paper, we focus on the qualitative aspects of probabilistic dependency graphs (PDGs) \cite{} or, more precxisely, the hypergraphs that describe their struture, and show how they can be used to describe far more than just conditional independencies.
In this paper, we study a natural generalization of standard graphical model structures that can describe far more than conditional independence. 

%oli16:
% Functional \emph{dependence} of one variable on another, for example, is another structural property of a probability measure that can be exploited to get a compact representation.
%joe16
%    Another qualitative aspect of a probability distribution is that of
For example, another qualitative aspect of a probability distribution is that of
functional \emph{dependence}, 
    which is also exploited across computer science to enable compact representations and simplify probabilistic 
%joe16
   % analysis.
%oli21: this looks weird to me to use a colon to end a sentence.  If the intent was to join with the next sentence, then I dislike how now only the causal model example follows the colon (because the period ends the scope of the colon). Reverting.
    % analysis:
    analysis.
%oli5: reducing
% This practice is very common across many subfields of computer science.
%oli16:
% This practice is common across computer science.
%oli5: this is very verbose, and a place I'm looking to cut from. Rewriting:
\commentout{
    An acyclic causal model, for example, specifies a distribution  
    via a probability over \emph{contexts} (the values of variables whose
    cause is determined outside the model) 
    together with a collection of equations (i.e., functional dependencies)
    describing how each non-exogenous variable can be determined
        as a function of other variables \cite{pearl:2k}.
}%
%joe16
%An acyclic causal model, for example, specifies a distribution  via a
%joe17:
%Acyclic causal models  specify a distribution  via a
%oli22: short range "for example" repetition, which is presumably why you cut it in your last edit.
% For example, acyclic causal models  specify a distribution  via a
Acyclic causal models, for instance, specify a distribution  via a
probability over \emph{contexts}
%spencer2.1:
% (the values of variables whose cause is determined outside the model), 
(the values of variables whose causes are viewed as outside the model),
and a collection of equations (i.e., functional dependencies) \citep{pearl:2k}.
% In deep learning, a popular class of models called normalizing flows \citep{tabak2010density,Kobyzev_2021} determines a (conditional) distribution by composing a fixed distribution with a trainable functional dependence.
%joe17
%In deep learning, a popular class of models called
%oli22: fix typo
And in deep learning, a popular class of models called 
    \emph{normalizing flows}
    %oli5: just taking first citation, to save room; the second citation isn't critical to our story, just a more modern summary.
    \citep{tabak2010density,Kobyzev_2021}
    %oli15: in neurips format, citations are cheaper, and more is better; reinstated both citations 
    % \citep{tabak2010density}
    specify a distribution
    by composing a fixed
    %spencer2.1
    % probability 
    distribution
    over some latent space,
    say a standard normal distribution,
    with a function (i.e., a functional dependence) fit to observational data.
% In much the same way, complexity theorists often regard
% Complexity theorists often regard
%joe17: Look at it now, this is overkill
%oli22: fair enough, although I'd like to keep it in the full version
%joe18: Ut still feels like overkill to me, but I don't feel strongly about it
\vfull{%
Similarly, complexity theorists often regard
    a probabilistic Turing machine as a deterministic function
    that takes as input a uniformly random string \citep{probTuring}.
}%
%oli16: now this is said earlier
% In each case, the probabilistic analysis is simplified through functional dependence.
 %joe2*: If we're going to give these examples, we have to circle
    %back and relate it to our semantics. 
%oli2: what would this look like? 
% The point of this text is to make the case that dependence
% is also worth caring about.  Once we prove Theorem 2, it's obvious
% that we can capture these examples, simply because they are 
% functional dependencies.  It seems like a waste of space to say
% "remember, probabilistic turing machines and normalizing flows
% are practical formalisms where dependence is important". 
% So presumably you mean returning with something deeper and more tailored
% to each example.  But what is there to say?  We haven't done anything
% more specific to these examples; we've only shown how to qualitatively 
% capture dependence in the same framework as independence. 
% I imagine you will conclude we should remove the examples then.  But
% is that really necessary? It seems valuable to motivate capturing 
% functional dependence even if we don't return to the specific examples.
%oli16: removing paragraph break
%
%joe7*: As I've said repeatedly, it seems strange to have these
%examples which we have nothing to say about in the paper.  You use
%the notation \CI exactly once in the remainder of the paper, yet you
%devote a whole paragraph to it in the introduction.  I think we
%should restrict to examples where we can say something useful.  You
%offered in your email to tie this in to the rest of the paper.  I
%think this would be critical.  This will need to be done by 11 AM (my
%time) tomorrow morning.
%oli8: I have added some ties, but the point of this paragraph is not
% to introduce the notation \CI. It is to introduce how the concept of
% dependence can be expressed with independence (a core theme of our 
% paper). The notation \CI is there just to make this paragraph shorter
% and cleaner. We use the concept later in the paper, and occasionally \CI
% as well, but mostly in the appendix.
Functional dependence and independence 
%joe1: your point here is only the interaction
%oli1: Perhaps it wasn't clear, but my intended point here was both:
% I want to say you can't cleanly describe one concept without the other.
% What's written proves that all functional dependencies can be written
% in terms of conditional independencies. Conversely, our work
% shows that conditional independence can be written in terms of 
% indepdenence (of mechanism noise variables) plus functional dependence.
%
are deeply related and interacting notions.
% interact.
For instance,
    if $B$ is a function of $A$ (written $A \tto B$) and $A$ is independent of $C$ (written $A \CI C$), then $B$ and $C$ are also independent ($B \CI C$).
    % note: this is closely related to monotonicity axiom of [Naumov & Nicholls].
% That fact is a key ingredient in many of our proofs (\cref{lem:indep-fun}). 
\unskip\footnote{This well-known fact (\cref{lem:indep-fun}) is formalized
    and proved in \cref{appendix:proofs},
    where all proofs can be found.}
%joe4*: Why is this important for our purposes?  We don't do anything
%with it in the paper!  You can't make a big deal out of something in
%the intro that we don't really do anything with in the paper (beyond
%observing it).  As I've said repeatedly, I would cut this paragraph.
%(If nothing else, you will surely have space problems for the UAI
%submission and will need to cut down to 8 pages.  You'll need to make cuts.)
%oli5*: Yes, we'll need to do some cutting.  But you're looking in the wrong place. We absolutely do something with this concept in the paper. It's essentially the essence of Theorem 2. It's also a very important conceptual foundation on which to start discussing mechanism independence. Through it, it becomes the basis of some of the constructions in section 3. And it plays an important narrative role in this introductory paragraph. 
%joe6*: The reader can't read your mind.  I don't see any discussion
%in the paper about your claims above.  Either you bring it out in the paper.
% The following fact is conceptually even more important:
%oli20: simplifying 
% Still more important for our purposes is the following, perhaps less intuitive fact:
% Here is another fact, which is conceptually even more important:
Moreover, dependence can be written in terms of independence:
    $Y$ is a function of $X$ if and only if
    $Y$ is conditionally independent of itself given $X$ 
    %oli-20: adding, because cutting display mode eqn below
    (i.e., $X \tto Y$ iff $Y \CI Y \mid X$).
%joe1: use "i.e." and "e.g." in parens; otherwise, use "that is" and
%oli1: I accepted the change (and replaced the comma with an em dash),
% but I definitely don't believe that your convention is standard.
% I've seen i.e. and e.g.,
% outside of parens many times, both in academic writing and in the
% the news.  I checked with an English major as well, who also objects
% to this convention, and pointed me to Webster's dictionary
% of English usage, page 520, which says both usages are acceptable.
% i.e.,
%joe2: I accept that both usages are acceptable in most contexts, but 
%*all* copyeditors I've dealt with (both in journals and at MIT
%Press for my books) follow this convinention.  Journals/publishers
%will insist that you follow their style, so I've just taken to doing
%it  everywhere. 
%oli20: in single column format, it is far too expensive to put this equation in display mode. 
%joe16: I find it incredibly arrogant an inapporopriate that, when I
%say "based on my 40 years of experience, this is how it's done" and
%you (based on you zero years of experience) say "I don't believe you".
%oli21*: First, your reprimandment is misplaced; it makes no sense here and I have no idea what you're talking about. Second, having 40 years of experience does not entitle you to be believed; who you are is irrelevant if you're trying to convince me of something.  Surely it has been the case that people with more years of experience than you say things you do not believe, a fact that has nothing to do with whether or not you are arrogant.  Finally, I find these comments incredibly toxic and counterproductive. I am trying very hard to see things from your persepective and engage with you earnestly, and the attacks on my character are not helping anyone. 
%joe17: this is not the place to get into the discussion, but I was
%accurately reporting how I felt (and still feel).  (That said, I
%agree  with much of what you said.)
\commentout{
\begin{equation}
    % \Big(~\text{i.e.,~~}~
    Y \CI Y \mid X 
    \quad\iff\qquad 
    X \tto Y .
        \label{eq:conditional-self-independence-det}
    % ~\Big).
\end{equation}}
Traditional graph-based languages such as Bayesian Networks (BNs) and
    Markov Random Fields (MRFs) cannot capture 
%oli-21:
these relationships.
% functional dependence, let alone the relationship between the two.
%joe1
%Moreover, the graphoid axioms [Pearl \& Paz 1985], and direct axiomitizations
%of conditional independence [Naumov \&
%      Nicholls] do not even consider statements like ``$A$ is
%oli15*: these citations you replaced don't appear to be in %joe.bib, or z.bib (I checked both). Where can I find them? 
%oli20*: Bump. Where do I find these citations? 
%spencer2: nobody knows what the graphoid axioms are, and how they're related to MRFs and BNs. 
%joe17: well, some people do :-)
%spencer3: Oops! This was a stream-of-consciousness paraphrase of something I said to Oli :) 
%oli22: (oops, sorry for transcribing that :/)
%oli21: trying to address
%joe17: the graphohid axioms and axiomatization of conditional
%independence are discussed in Pearl's book on BNs; they're largely
%due to Pearl and his students.  That would be a
%reasonable reference.  (I suspect, but am anot sure, that Koller and
%Friedman discuss them too.)
% Indeed, the graphoid axioms \citep{PP86} and axiomatizations of conditional independence \citep{NN13} 
Indeed, the graphoid axioms (which describe BNs and MRFs) \cite{pearl1987graphoids} and axioms for conditional independence \citep{naumov2013re},
do not even consider statements like $A \CI A$ to be syntactically valid. 
%oli20: shortening
%  ``$A$ is independent of itself ($A \CI A$)'' 
% Moreover, the graphoid axioms [Pearl \& Paz 1985],
%     and direct axiomitizations of conditional independence [Naumov \& Nicholls] do not even consider statements like ``$A$ is independent of itself ($A \CI A$)'' to be syntactically valid.
%joe1
% Yet such statements are perfectly meaningful and can be quite useful.
%oli1
% Yet such statements are perfectly meaningful.
    %  and 
    % and the dependencies tey represent 
% Indeed, $A \CI A$ is no different from
%     $A \CI A'$ where $A'$ is a distinct
%     variable representing the same information, and holds if and only if
%     $A$ is deterministic.
%joe2: you're missing the punchline!
%oli1: I agree.  I'm glad you made this change, but I'm also frustrated
% because it's a reversion of your own edit that I spent a long time staring at
% in the past and then reluctantly came to terms with in an effort to defer to you.
% See %joe1 above where you made almost 
% exactly the opposite change, and %oli1 beneath it, where I copied it, tried
% to reinstate it, and decided it wasn't worth fighting over. 
%
%    Yet such statements are perfectly meaningful.
Yet such statements are perfectly meaningful,
%oli22-TODO: modulo the word choice, I think Spencer's edit is quite good. The point is not that it's useful, but rather that these things are connected
%joe18: To some people the point will be exactly that it's useful
% and useful.
%spencer2.1
% meaningful and are useful for reasoning about distributions.
%joe17*: They definitely do not belie it.  It's the opposite of belie!
%spencer3: revert with better word choice, thanks!
and reflect a deep relationship between independence, dependence, and generalizations of both notions (grounded in information theory, a point we will soon revisit).
%joe16
%oli21: I don't want to talk about PDGs yet, or even hypergraphs; that's coming later on. This also makes the sentence clunkier. Spencer and I have agreed to cut. 
% and can be captured by our semantics for hypergraphs based on PDGS.

%joe16*: added.  We want to bring causality in early
%oli21*: Ok; we'll try to bring it in earlier, but we decided it makes a lot more sense to do this in the middle of the next paragraph, after we've described our contribution and brought up "independent mechanisms". 
%joe17: WE also want to bring in compatibilty early.  Bear with me.
\commentout{%
Yet another critical qualitative aspect of a distribution is the
causal connections it describes. As we shall see, this too can be
captured in our framework, and doing so allows us to capture
connections between causality, independence, and interventions.
Indeed, our approach brings causal reasoning to the forefront.
}%

% joe1*: You were apparently not able to talk about PDGs, despite my
%insistence.  What you wrote is not acceptable.  It gives the
%impression we're introducting a new approach. We'r not. Rather, we're
%giving a new semantics to PDGs.  I feel quite strongly we should
%talk about PDGs at this point.  
%oli*:    Putting aside the pragmatics of how best to tell the story,
% I believe it's mathematically closer to the truth to say this is
% an entirely new way of describing structure in probability
% distributions, than to say this is a previously unexplored 
% aspect of PDGs.  More importantly, I  feel quite strongly that
% we should be careful not to overemphasize this as a new semantics for PDGs,
% because it is a special case of the old PDG semantics,
%    I'll try to bring PDGs closer to the front more carefully, 
% combining elements of our two writeups. 
%oli1*: the rest of the intro has been significantly rewritten with
% aspects of both writeups, and not all changes have been marked. 
%joe2*: I strongly disagree (scientifically and pedagogically).  We
%have to relate this immediately to PDGs.
%oli2*: Why do you disagree? You haven't provided arguments or adressed mine!
% You're just asserting your perspective again. This isn't helpful. As it 
% stands, I don't understand your objections and stand by what I said above.
% If this is what counts for an interation, I certainly will not agree to doing
% things your way after just one iterations. 
% While it may be possible to come to common ground, I strongly object
% to the new changes you made here, which range from misleading to incorrect.
%
%joe2: ((replaced the fragment below))
% This paper describes a simple yet expressive graphical
%
%oli2*: PDGs are not (primarily) a graphical language for describing structure
% such as independence and dependence in probability distributions; they are
% also a language for describing probability distributions themselves, and epistemic
% state.  This sentence also undermines your claim that previous PDG work did not
% interpret or give semantics to the causal PDG part of PDGs.
%
% \emph{Probabilistic dependency graphs (PDGs)}, introduced in
% \citep{pdg-aaai}), are a 
% graphical 
%
%oli2*: I am very annoyed with you for bringing PDGs back to the very
% begining of the paragraph, especially in such a sloppy way. In our last
% in-person meeting of 2023, you agreed that it made sense to begin this with a discussion
% of independent mechanisms. We even mostly agreed on the wording. Then, without
% asking, you modify the draft to be right back to what you were arguing for at
% the start. And what you say is misleading!  
%
%oli2: What is the point of this next paragraph? My objective in writing the
% first 6 drafts was to finally get to our contribution and talk about how 
% we have a language that can cleanly model both independence and dependence
% (among other things, that we needn't name here).
% The focus is "our definition of independent mechanisms" does the right thing.
%
% At your insistance, I've now also integrated a lot of information on  what a
% hypergraph is, and also mentioned qualitative PDGs in the first  
%
%oli2: At a higher level, I still object to your framing, and I don't want
% you to force your story into the document unless we can come to an agreement. 
% I think perhaps the best thing to do is to write this part synchronously,
% together. I think that will dramatically speed up convergence. 
%
%oli2: reverting for now, because your version doesn't make sense. Let's
% figure out how to compromise in our next meeting. 
%joe16: we've now said this above
%oli21: Spencer and I don't think the way you did it above is as effective. For a number of reasons, we can't end the first paragraph with enough detail to replace this text. 
%spencer2: In particular, I think the way this is done in Oli's draft is nice; it ends the first paragraph with a high level comment about graphical mdoel structures, and then is more specific about the connections to dependence and independence in this paragraph. 
%joe17: Here's a tory I thibnk you'll like ...  Bear with me for a paragraph
%oli22: Ok, but I don't get it; this is what we had before, except for the first two words.
%joe18: right; see below (that's the "bear with me" part)
% This paper
So the paper
describes a simple yet expressive graphical
    language for describing qualitative
    structure such as dependence and independence
    in probability distributions.
\commentout{%
%joe2*: I strongly disagree (scientifically and pedagogically).  We
%have to related this immediately to PDGs
%oli2*: While I think I understand why you want to do it this way,
% I strongly disagree with your approach. Let me lay out some reasons.
% First, as far as the writing is concerned, we have put off how our
% paper ties into things long enough.  This is the place where we must
% connect the  motivation we've spent so long building up, to our the 
% present paper.
% It is NOT right place to to do a hard right turn into related work
% (relevant as it is), because understanding PDGs is not in the 
% critical path for understanding what our paper is about, at the highest level.
% Yes, you can view it as a semantics for PDGs (warry as I am of 
% presenting it that way), but you can also understand it without
% PDGs at all. Furthermore, for a reader who does know about PDGs,
% it is important to add this extra distancing material, because this
% is semantics only works for unweighted qualitative PDGs, and it 
% assigns each PDG the old semantics that corresponds to a different PDG.
% I find this very confusing, and it is certaintly not of the flavor
% of when 
%oli2: At a lower level, I really don't like the replacement material.
% I've commented it out and started to explain why.
\emph{Probabilistic dependency graphs (PDGs)}, introduced in
\citep{pdg-aaai}), are a 
graphical 
%oli2: PDGs have not in the past been used explicitly as a graphical langauge
% for describing dependence and independence. Rather, that has been 
% a capability we have occasionally used and never directly addressed.
% One possibility is to emphasize that this is the result of our work,
% e.g., "This paper shows that the graphical structure of PDGs, introduced by [...], when given a new intrerpretation, can be used as a language for describing structure such as dependence and independence.".
% If we omit the "when given a new interpretation", the sentence is true
% but does not accurately reflect what we are doing in our paper. 
% No matter how we do it, we've now brought PDGs into scope and 
% must explain them, which you do ... but that delays the important 
% material motivating independent causal mechanisms (which is what this)
% paper is really about, and moves it further from the motivation. 
    language for describing structure such as dependence and independence
    in probability distributions.
%joe2*: added.  This is the right place for it.
They are a generalization of
BNs and factor graphs, that can capture inconsistent beliefs \citep{pdg-aaai},
Roughly speaking, a PDG is a graphical model where,
like a Bayesian network (BN), nodes are associated with variables, and
are related by \hyperarc s.  
%oli2: people might think there's something wrong here, because in a BN that they're related by ordinary directed edges.
In a (quantitative) PDG, these \hyperarc s
are associated conditional probability tables, in the spirit of a BN.
%oli2: I think the sentence below is deceptive. Qualitative PDGs are defined
% to be directed hypergraphs (plus weights).  "Can be viewed as" is 
% way too weak, and the sentence is technically wrong because of the weights.
In this paper, we focus on qualitative PDGs, which can be viewed as
%joe3*: Since you consider only directed hypergraphs, I would say that
%here, and say that, from here on, when you say "hypergraph", you mean
%"directed hypergraph".
%oli3: I agree that something has to be done; we can't simply abbreviate
% "directed hypergraph" with "hypergraph" silently. I'm also happy to do
% as you suggest.  But in my view even better would be to use a
% new shortened version, e.g., "dhypergraph". 
%joe4*: NO!!!
%oli5: fine, I'll do as you originally asked. 
\emph{directed hypergraph s}.
We provide a novel semantics for qualitative PDGs that allows us to
capture dependence and independence in a way that generalizes the
semantics of qualitative BNs and, as we show, has a variety of useful
applications (including, among other things, allowing us to capture
the reasoning dependency described above).
}%
%joe16
%    The idea is for the modeler to specify 
%oli21: line shaving
% The idea behind our approach is for the modeler to specify 
The idea behind our approach
%joe17: we should acknowledge Pearl early here, otherwise reviewers might be unhappy
%oli22: I disagree; I think Pearl did not quite have this definition or intuition, and that we should not be giving him credit for our contribution. The point of this paragraph is to bring up the structure of a hypergraph. By saying "graph" here, we lose the careful building up we have been doing. 
%oli22: reverted
is to specify 
% goes back to Pearl \cite{pearl:2k}; we view the graph as specifying    is to specify 
%joe18: I actually think that he did have this idea, but I can't look
%inside his head.  More importantly, this is a time to be gracious.
%Even if it's just that he ight have had the idea, we should give him credit.
the inputs and outputs of a set of \emph{independent mechanisms}.
% (a directed hypergraph, which is the graphical structure of a Probabilistic Dependency Graph (PDG) \citep{pdg-aaai}).
%oli21*: new causal material goes here
%joe17*: This breaks the flow.  I moved it to the end fo the
%paragraph, without the net sentence.  I don't see anything here
%that's reminiscent of causality.  It's true that Pearl interprets the
%edgs in a causal model this way.  That's what we should say
%joe18*: NO!  I keep cutting this from here, saying I don't find it at
%all reminiscent of causality.  I think what makes it reminimiscent is
%treating edges as independent causal mechanisms, and we should say
%that.  That's what I did below.  reverting yet again,  What I added
%below (which you didn't change) should be enough.
% This notion is reminiscent of causality, and there is indeed a deep connection. Our graphical language essentially captures, and expands upon, the qualitative structures in causality---namely, causal graphs.
%oli21: further 
%joe17: misplaced; we need to get to compatibility right away.
% We will later see how this framing will allow us to better understand cyclic causal models.
%
%oli21
% By ``mechanism'' we mean a process by which
%spencer2.1:
% By an ``independent mechanism'', we mean a process by which
In slightly more detail, by ``independent mechanism'', we mean a process by which
%joe2
%some (set of)
%    target variables $\mat T$ are determined as a function of 
%    source variables % (the ``parents'' of $\mat X$).   
%    $\mat S$ and independent noise.
%oli2: I like the wording change, but would prefer to introduce the 
% symbols S,T,U early. More importantly, the "plus independent noise"
% part is critical.
%
% the target variables of the \hyperarc\ are determined as a
% (possibly randomized) function of its source variables.
%oli3: revisiting: there's no "\hyperarc" in scope, so this needs to be done differently.
% the target variables of the \hyperarc\ are determined as a
% (possibly randomized) function of its source variables.
some (set of) the target variables $T$ are determined as a
% function of a source variables $S$ and independent noise $U$.
(possibly randomized) function of a (set of) source variables $S$.
%joe2*: cut all this.  The discussion may be useful to clarify what's
%going on for you and, but is totally inappropriate for an
%introduction (and, indeed, for this paper).  It's way too far down
%the tree.  Oliver, at some point you'll have to accept my judgment
%here.  I don't know how to convince you that I'm right, and I'm not
%prepared to go back and forth on this.
%oli4:
% At a qualitative level, the 
So, at a
%oli20
% structural
qualitative
level, the modeler 
%oli20: trying to embelish the graph -> hypergraph thread of the story
% specifies 
specifies not a graph, but rather
% $\{ \mat S \to \mat T \}$
%joe3: I don't think of mechanisms as having inputs and outputs.  I
%would cut "mechanism"
%oli3: ok
    % set of mechanism inputs and outputs form a \emph{directed hypergraph},
%oli3: ... on second thought, we just defined a mechanism as having inputs and outputs, in the previous sentence. Or sources / targets. I'll use those words instead.
    % set of inputs and outputs form a \emph{directed hypergraph},
%oli4:
    % mechanisms form a \emph{directed hypergraph}, which happens to be
    %oli7: 
    % modeler's specified mechanisms form
%joe7*: I strongly object to this; it certainly misrepresents history
%I insist that we do it my way here.
%oli16: I have added the original citation 
\expandafter\commentout
\vjoe{
    a \emph{directed hypergraph}, which can be viewed as 
    capturing the qualitiative structure of a \emph{probabilistic
dependency graph (PDG)} \citep{pdg-infer}.
}%
%oli8*: It is your version (above) that misrepresents history. In the original
% PDG paper, the structure is given by a graph, not a hypergraph. By doing it
% this way, you either imply that our inference paper was the first place
% PDGs were introduced (which is false), or that in our AAAI paper PDGs
% have the structure of a hypergraph (which is also false).  What
% is it exactly that you are objecting to? In order to fix this, I need
% to understand why you think my version (below) misrepresents history. 
%
%oli8: Are you unhappy because of the words "another graphical model"?
% If so, that's it's just the truth that PDGs (as defined in the
% reference) there are a different mathematical concept from what we
% describe here --- even if, historically, we came to this one by
% trying to understand that one.  Nobody cares how we came to work on
% this; what they care about is what's true. 
%oli16: reinstating, with a few minor modifications
%oli20: in NeurIPS format, citations are cheap, so we don't have to 
% do the "by some accounts" thing. 
% a \emph{directed hypergraph}---which by some accounts \citep{pdg-infer} is the qualitative structure of a newer kind of probabilistic graphical model, called a \emph{probabilistic dependency graph (PDG)} \citep{pdg-aaai}.
a \emph{directed hypergraph}---which is the structure of
%joe16: no reason to "date" it 
% more recent
another type of probabilistic graphical model:
%joe17
%called 
%oli23: "a" was out of place; moving from above and putting here
a
\emph{probabilistic dependency graph (PDG)} \citep{pdg-aaai,pdg-infer,one-true-loss}.

% Compared to modeling with BNs, the key conceptual shift is to
%     use a directed hypergraph instead of a directed graph. 
% After all, each ``independent mechanism'',
%     i.e., each random process by which a variable $X$ is determined,
%     corresponds not to a directed \arc\ between a pair of variables, but to 
%     a directed \emph{hyper}arc with multiple sources (the parents of $X$).
% As with PDGs, the key conceptual shift is to use a directed hypergraph
% instead of a directed graph.
% There is a close relationship between the qualitative information
%     of a PDG (\cref{sec:pdgs}), and in particular, we will see how 
% There is a close (but far from obvious) relationship between the
%     qualitative information
%     of a PDG (\cref{sec:pdgs}),
% However, we will later see that this definition
%     of \scibility\ actually corresponds to a different PDG,
%     which explicitly models the noise variables. 
% This allows for the distinction between being causally independent 
%     of all other variables (no parents in a BN) and no .
% Similarly, it enables a distinction between a joint dependence 
%     (two distinct mechanisms that are both responsible for $X$)
%

%oli3: moving this here; 
%joe4: ... where it breaks the flow and is unconnected to anything
%else.  I tried to fix that.
%oli5: Simulatenously, I had fixed it in a way that I think you'll find is much closer to your original writing.
\commentout{
    PDGs
    are a generalization of
    BNs and factor graphs, that can capture inconsistent beliefs,
    and come equipped with a natural measure of that inconsistency,
        which functions as a ``universal loss function'' \citep{one-true-loss}.
}
%oli4: then changing it to look more like your writing; see below.
\commentout{
    PDGs are a generalization of
    BNs and factor graphs that can capture inconsistent beliefs,
    and come equipped with a natural measure of that inconsistency,
        which functions as a ``universal loss function'' \citep{one-true-loss}.
}
%oli4: moving the above closer to what you had above, but with some
% crucial modifications. 
%joe7*: I find this completely unacceptable.  I will not agree to
%submitting a paper with this paragraph.
%oli8*: I will change this paragraph, but I also find your approach unacceptable. I wish we could have talked about this synchronously, instead of wasting so much of both fo our time rewriting things when we're not converging at all. 
%oli8: starting the same way as before, because I don't think that was the primary objection, but rather a change you made as a result of your misguided change above, which I reverted. 
%oli20*: I'm now happy to cut this material.  (I have a better idea for how to say this, that I think Joe will be happier about. )
% \voli{ \emph{Probabilistic Dependency Graphs (PDGs)}, introduced in \citet{pdg-aaai}, are also a graphical language for describing probabilistic dependence and independence, and they too can model structural information. }
%
%oli20: don't want to focus on PDGs so much; let's start by cutting the background material that's not relevant ot the present paper.
% PDGs generalize BNs and factor graphs, can capture inconsistent beliefs,
%     and provide a natural (entropy-based)
%     way to measure the degree of that inconsistency.
%oli-20: bringing up from below
% Prior work \citep{pdg-aaai,one-true-loss,pdg-infer} focused primarily on the quantitative, probabilistic information in a PDG, rather than the qualitative information.
% \citet{pdg-aaai}, for instance, define what it means for a distribution to be compatible with the probabilistic information in a PDG, but do not provide an analogous definition for the qualitative information.
%oli20*: rewriting; new compromise material!
  %joe16: I really don't like this.  Why is it only "seemingly" very
  %expressive?  It's a bad idea to say it's not well understood (as
  %opposed to pointing out cases where it gives problematic answers)
%oli21*: Your replacement is very good, and in particular far better than what I wrote. Thank you! 
%  PDGs have a qualitative semantics (in terms of a   function on
%distributions) that is seemingly very expressive, but not
  %well-understood beyond the BN case.
%Part of the problem is that it is not even clear what the analogue of
%a BN's independencies should be for a cyclic graph, let alone for an
%arbitrary directed hypergraph.
%spencer2.1
% Although the qualitative aspects of PDG were charactrerized
Although the qualitative aspects of PDGs were characterized  
by
%oli21: expanding your citation
% Richardson and Halpern \cite{} 
\citeauthor{pdg-aaai} \cite{pdg-aaai}
% \citet{pdg-aaai}
using a scoring function, that scoring function does not seem to get at the qualitative aspects that we are most interested in here.
% We develop precisely such a notion. 
% In this work, we develop such a notion from first principles.
%oli21: removing implication that this neatly plugs the hole in the PDG framework. 
% In this work, we develop a semantics that does so from first principles.
In this work, we develop from first principles an alternate qualtiative semantics for directed hypergraphs.
%oli20: now, drawing from \vjoe. Below %oli20 modifications are relative to Joe's initial version.
More precisely, we define what it means for a distribution to be 
%oli20: cut
% what we call
%joe16*: I would still really, really like to get rid of QIM.  I don't
%think it adds anything to the reader's understanding, and as a
%general rule of good writing, I strongly prefer to use notation as
%simple as possible.  I would %(unhappily) be willing to use it the
%first time we define the notion, and then get rid of it all later times.
%oli21*: Ok. I see why having QIM QIM QIM over the page is distracting and unnecessarily formal. I'm willing to use the approach you suggest to cut a supermajority of these instances. However, I still find it very important that we occasionally refer to "QIM-compatibility" in various places throughout the paper. "Compatibility" on its own is such a generic English term, and a close match to many of the concepts we describe.
\emph{\scible} (qualitatively independent-mechanism compatible, or just \emph{compatible} when unambiguous) with
% \emph{(QIM-)compatible} (qualitatively independent-mechanism compatible) with
% \emph{(QIM-)compatible} (qualitatively independent-mechanism compatible) with
% \emph{QIM}(qualitatively independent-mechanism)\emph{-compatible} with
%oli20: 
% the set of independent mechanisms represented by
a directed hypergraph $\Ar$.
This definition allows us to use directed hypergraphs as
    a language for specifying structure in 
    probability distributions,
%joe16: No!  The theorem doesn't say that BNs are a special case of
%directed hypergraphs; that would be totally uninteresting!  It says 
%oli21: of which "BNs *and their independencies* are a special case". We see where you're coming from, so we clarified it.
    % of which qualitative BNs and their
    % corresponding independencies are a special case (\cref{theorem:bns}).
%oli21%spencer2:
% which extends the language of qualitative BNs for specifying independencies. 
of which the semantics of qualitative BNs are a special case (\cref{theorem:bns}).
%
%joe16: (new material)
%spencer2: I think the last sentence already captures this, but perhaps we can add one more sentence for emphasis if you think that would help.
%oli21: I don't think the NeurIPS community will appreciate the subtle reference to the different characterizations of BN independencies.
\commentout{
    We show that, although the definition the definition of 
    \scibility\ seems very different from
    any of the standard ways of specifying conditional independencies in a
    BN, a distribution is \scible\ with a hypergraph corresponding to a BN
    iff it satisfies all the conditional independencies associated with
    the BN according to the standard definitions (\cref{theorem:bns})..
    Thus, by reasoning directly about independent causal mechanisms, we
    can recover the standard semantics of BNs.
}

%oli20: removing up old conflict zone. 
\commentout{
%joe7*: Let's be historically accurate.  I'm going to insist that
%we do it this way.
%oli8*: I can't put my name on this story. Your paragraph below accurately describes our personal history of why we started working on this, but it does not accurately describe what we now know about them. We can't say this.  This does NOT fit nicely into the gap in the PDG paper. The math doesn't work out that way.  Nobody cares if we motivate the problem in ways that do not accurately reflect our original motivations, but they will care if the story turns out not to fit the math well, or the other stories people tell about PDGs later.  
\vjoe{
The earlier work on PDGs focused on the quantitative aspects of PDGs,
and
%oli8**: This next sentence is totally unacceptable.  I will not submit a paper with this story. As I see it, it's two very deceptive phrases (if not outright lies) that are just close enough to the truth to believe.  I do not think that our contribution shoud be thought of that way.  I know how desperately you want to have such a semantics for PDGs, but this is not that. You can redefine a variant of a PDG in your own papers, but I can't accept this here.  
did not provide an explicit semantics for qualitative PDGs. We do so here, 
by providing
a novel definition of what it means for a joint probability
%joe7: I really think we can drop the SIM, but I've kept it in for now.
distribution to be what we call \emph{\scible} (structural
independent-mechanism compatible) with
the set of independent mechanisms
represented by
    a directed hypergraph $\Ar$.
This definition allows us to use directed hypergraphs as
    a language for specifying structure in 
    probability distributions,
    of which qualitative BNs and their
    corresponding independencies are a special case (\cref{theorem:bns}).
}
%
%oli4*:
% are a graphical language for describing structure such as 
%joe6*: I have no idea what you're trying to say here.  When you say
%like ours", is "ours" directed hypergraphs?  
%oli7: "ours" is short for "our formalism" in "The PDG Formalism, like ours, ..."; although we haven't gotten there yet, this stands for SIM-compatibility.
%joe6*: In what sense does the
%PDG interperet directed hypergraphs as predicates on probability
%distributions?  
%oli7: through the scoring function IDef. You can look at IDef's sub-zero set, or its set of local minima.  Both are obvious things I've looked into several times before, and both have connections to what we do here. According to both definitions, you recover BNs; you can immediately get weaker analogues of some of our other results. 
%joe6*: This is a *terrible* ay of presenting this material.
%I don't understand what you're trying to say here.  Please go back to
%what we had before.  
%oli7:  By the way, this version has been here since Feb 2nd (5 days).  On Feb 1st, I accidentally gave you a draft with an out-of-place stub in place of this paragraph, and before that, I had something here that you dispised just as much. Let me bring it out of the comments to remind you:
\commentout{
    Consequently, one can interpret this paper as 
        a new semantics for qualitative PDGs. 
    However, the reader should also be aware that
        the concept we introduce here
        can be captured with the original PDG semantics
            (by explicitly modeling the independent noise).
    In light of this fact, 
        it is arguably more appropriate to interpret 
        this paper a stand-alone concept of mechanistic independence 
        that corresponds to a strict subset of PDGs of particular form,
        according to their original semantics.
}
%joe6*: I *really* don't like this paragraph.  If you think it's going 
% to make me happier, you have seriously misjudged my tastes.  
%oli7: 
%joe7*: As I said, I find this unacceptable.
%oli8: ... meaning you refuse to give my perspective any light.  You insist on overwriting PDGs, which I've spent my whole PhD writing about, with a much dinkier concept. If you get your way, this will be the first paper on qualitative PDGs, and in my view it will be wrong, and I'll have to spend the rest of my career putting asterisks on this.  
%oli8*: What I wrote below is what I see as the truth, and I refuse to sign off on anything that is not consistent with it.  But, I am willing to leave some things unsaid, and try for something more anodyne.  
\voli{%
Although it has not been discussed in the literature, the PDG formalism, like ours, can be used to interpret directed hypergraphs as predicates on probability distributions. 
These two formalisms are based on similar intuitions, and agree on simple structures like BNs, but their interpretations of directed hypergraphs in general diverge.  What can be done with the structural component of a PDG is not the same as what can be the language of mechanism independence that we develop here (although they  are related, as we explore in \Cref{sec:pdgs}). In fact, PDGs are more general, in the sense that our concept of mechanism independence can be easily encoded with PDG structures. 
% But PDG structures in general have not (yet) been given a concrete interpretation nearly as clear as the one we offer here. 
%joe14
%But qulaitative PDGs in general do not have a concrete interpretation
But qualitative PDGs in general do not have a concrete interpretation
nearly as clear as the one we offer here, based on independent mechanisms. 
% (More precisely, as we will see in \cref{sec:pdgs}, they are strictly more expressive).
Consequently, there are two equally valid interpretations of the present work: either as
\begin{enumerate}[wide,nosep, label={\textbf{\arabic*.}}]
    \item a more interpretable (but less general or tractable) alternate semantics for PDG structures, or as
    \item 
    an important and previously unexplored standalone concept, that is possible articulate with the language of PDGs (under their current semantics).
\end{enumerate}
%joe4: what two interpretations are you talking about?
%oli5: I heavily modified the above so this should be clear again. (It also used to be clear in the %oli2 version.)
%joe6*: As I said, I really, really don't like what you wrote above.
%I think it makes for an awful story
%oli7: please explain. I like the story. But more than the story, I particularly like how clearly it describes both your view of the paper, and mine. I think it does a good job relating relationship betwen our definition and PDGs, at a high level, without spinning it to be an argument for our present paper. 
%oli8: I've asked you to explain repeatedly why you don't like this, and you've said only that it's unacceptalbe. I've explained myself repeatedly why I don't like what you're proposing, and you haven't responded to that that. 
Under either interpretation, 
our focus is
a novel definition for what it means for a joint probability 
    measure $\mu$ to be
%oli7:
    % structurally compatible (\emph{\underline{IM}-compatible})
    compatible
     with
the set of independent mechanisms
represented by
    a directed hypergraph $\Ar$.
This definition allows us to use directed hypergraphs as
%joe4*: I have no idea what kind of structure you're talking about.
%Neither will the reader.  You're insistence on talking about
%"structure" is leading you astray here.
%oli5*: it will soon become clear what kind of structure I'm talking about. Certaintly it should be crystal clear by section 5, and I intend on keeping passages we have on what I mean by "structural" in sections 1 and 2.  My insistence on talking about "structure" here is brining the readers precisely where I want them, but you're not playing alone with my story. Other readers will. 
a language for specifying structure in 
probability distributions,
    % $\mu$,
    %oli4: acronym introduced both in previous page, and reiterated in the next sentence.
    % of which qualitative Bayesian Networks and their
    of which qualitative BNs and their
    corresponding independencies are a special case (\cref{theorem:bns}).
% It can simultaneously represent arbitrary functional dependencies (\cref{theorem:func})
%
% and can simultaneously describe arbitrary functional dependencies.
% This paper presents a graphical language that can.
% In addition, can also represent independencies of a Bayesian Networks,
% provides a new generalization to cyclic models.
%
% But it can do much more.  
% But qualitative PDGs (with these semantics)
%
% They key is the realization ,
% The key conceptual shift is the observation that the ``independent mechanisms'',
%     i.e., the randomized process by which each variable $X$ is determined by
%     joint settings of its parents, does not in general correspond 
%     to a directed \arc\ between a pair of variables, but rather to 
%     a directed \emph{hyper}arc with multiple sources and one target.
     % $(\mathbf{Pa}(X))$ and a single target $(X)$.
\commentout{
A BN
associates to every variable $X$ an independent mechanism 
    that describes how $X$ depends on its parents $\Pa(X)$.
Observe that these are not pairwise relations between variables, but rather
    joint 
    %oli4: unnecessary
    % (directed)
    relations
    %oli4: using only words
    % $\Pa(X) =: \mat S \to \mat T := \{X\}$.
    between $X$ and $\Pa(X)$.
}
% Compared to modeling with qBNs, the key conceptual shift is to
%     use a directed hypergraph instead of a directed graph. 
This concept
    %(oli7):
    % of \scibility\ is a natural extension of 
    of \emph{\scibility} naturally extends
    BN
    independencies to all directed hypergraphs---%
    not just those arising from directed acyclic graphs. 
}
%
%joe7: \end{commentout}      

\voli{
    Accordingly, this semantics based on mechanism independence
    can do much more than represent this class of conditional
    independencies in acyclic networks.
}
\vjoe{
    This semantics based on mechanism independence
    can do much more than represent conditional
    independencies in acyclic networks.
}
}%oli20: end{commentout}
%oli20:
%joe16
% Our notion of \Scibility\ can do much more than represent conditional independencies in acyclic networks.
%oli21: a bit garbled; trying to extract the critical elements of your version.  I'm also reordering a bit, to mirror the story we told above: not only independence, but also dependence, and more. The causality part for arbitrary networks is better as a finale. 
% But \scibility\ can go far beyond representing conditional independencies in acyclic networks. For one thing, it applies without change to arbitrary networks, allowing us to reason about the effects on independence of assuming independent causal mechanisms in arbitrary networks.
But \scibility\ can do much more than represent conditional independencies in acyclic networks. 
%
%
For one thing, it can encode arbitrary 
    functional dependencies (\cref{theorem:func});
%oli20: added
% Moreover, this approach gives semantics to cyclic models. 
%joe16: already said this
%oli21: Spencer and I would prefer to do it in this order, because we can get the "functional dependence" out of the way quickly, but we have a lot more to say about cylic models below. 
for another, it gives meaningful semantics to cyclic models.
%oli21: the material you added above has been rewritten here. 
%
%joe16: putting causality and information theory ahead of monotonicity.
%Showing that compatibility has deep connections between causality and
%information theory shows that it is deeply embeded into notions that
%we have long recognized are important.  
%oli21: I agree that causality and information theory are clearly more important, but I was going for a story structure that goes from simple and easy to understand, to important and general. 
%joe16: I would frankly cut the material on monotonicity,
%which I view as technical material far down the tree.  Even if you
%need some results from the monotonicity section to prove your results
%on infomration theory, the releveant material can go in the appendix.
%oli21: I hear you and see where you're coming from. That said, I think the primary value of the monotonicity material is conceptual, for, me, and so I would strongly prefer to keep it in the main body of the paper. I will do what I can to cut from that section to make our page limit. 
%joe16*: I cut this.  As I said, it's down the tree.  You can include
%it in the paper (although I would argue against it), but I don't
%think it rises to what belongs in an introduction.
%oli21: I see your point, and will minimize this further. Monotonicity fits well here conceptually, and it's already pretty tucked away and de-emphasized. Let me try to further minimize it. 
% \Scibility\ satisfies an intuitive \emph{monotonicity} property (\cref{sec:monotone}),
%joe17*: I cut this from here.  I hope that you abd Spoebcer can
%disucss it and see that the tighter story (less is ore!) helps make
%your case (or what I understand to be the story that you're trying to
%tell) better.
%oli22: oops! I had already done a lot more minimziation that somehow got lost. But I can live with this cut. 
\commentout{\Scibility\ satisfies a \emph{monotonicity} property (\cref{sec:monotone}) that helps us begin to understand these cyclic models.}%
%oli21: more minimization
% which is the basis of an intuitive calculus that helps us begin to understand these cyclic models. 
% It can handle cyclic models in an non-trivial way, and in doing so, sinks a standard counter-example in information theory (Example \cref{sec:cyclediscussion}). 
% It applies even when there are cycles, and the result
%     can be clarifying; in one case, it undermines 
%     a standard counter-example in information theory
%     (\cref{sec:cyclediscussion}). 
%joe17: Here's a story that I think you'll like; it's what I was
%hinting at above.  Bringing in causality
Indeed, compatibility lets us go well beyond capturing dependence
and independence. The fact that \citet{pearl:2k} also views
causal models as representing independent mechanisms suggests that
there might be a connection between causality and
%joe18
%comptability; in fact, there is.
comptability. In fact, there is.
%oli21: line shaving + getting rid of a QIM
% Furthermore, as we hinted above, \scibility
%joe17: integrated with setences above
%As hinted above, \cibility\
%has deep connections with causality (\cref{sec:causal});
%for instance, a
%oli21: embelishing causal story
% 
%joe16: we haven't defined witness yet; adding \emph
A \emph{witness} that a distribution $\mu$ is \cible\ with a 
\hgraph\ $\Ar$ is an extended distribution $\bar\mu$ that 
%oli21:
% can be naturally converted to
is nearly equivalent to (and guarantees the existence of)
%oli20: want to speak more abstractly; no need to define SEM acronym yet, as it won't be used for until section 3. Besides, it's not even that more more accurate, because really we turn it into a randomized PSEM, and non-deterministically. 
% a \emph{structural equations model (SEM)} \citep{pearl:2k}.
a causal model
%oli21: adding the key property? otherwise what does it have to do with \mu and \Ar? Some options below
%
that explains $\mu$ with dependency structure $\Ar$. 
%joe17: adding more motivation
As we shall see, thinking in terms of witnesses and compatibility
allows us to tie together causality, dependence, and independence.


% that explains the causal origins of $\mu$. 
% that gives rise to $\mu$. %oli21: our terminology
% with structure $\Ar$ that gives rise to $\mu$. %oli21: extra precise, uses \A
%joe16: added next line
Perhaps
%joe17
%oli21: this will be least surprising to an information theorist; I don't want to suggest our own surprisal here, for fear of looking like we don't know that information theory already combines independence and dependence.
% most
surprisingly,
%oli21: cutting a QIM
% \Scibility\ 
\cibility\ 
also has deep connections with information theory
(\cref{sec:info}).
%joe16*: This seem so incredibly weak!   Just reading it (without all
%your preconceptions) does not it seen that way to you?.  Who cares
%that it has a "certain [undefined] information-theoretic property"?
%oli21: Totally agree; thanks for pointing this out. Instead of cutting, Spencer and I have decided to rewrite it to say something more substantial.
%in particular, compatibility with $\Ar$ implies
%that a distribution has a certain information-theoretic property
%(related to the PDG scoring function for $\Ar$).   
% For instance, 
% for instance, 
%
%oli21: cutting the weak material
\commentout{%
    These connections turn out to be quite useful: they yield a generic
    information-theoretic test for complex (e.g., cyclic) dependency
    structures, and enable causality to clarify some important
    misunderstandings in information theory
    (\cref{example:nonneg-ii,example:ditrichotomy}).
}%oli21*:%spencer2*: rewrote
%joe17
%Observe that the
The
conditional independencies of a BN can be viewed as a very specific kind of information-theoretic constraint.
% Our generalized notion of compatibility with a hypergraph A also implies an information-theoretic constraint (related to the PDG scoring function).
% There turns out to be a generalization of this constraint 
Our notion of compatibility with a hypergraph $\Ar$ 
    turns out to imply a generalization of this
    constraint (closely related to the qualitative PDG scoring function)
    that is meaningful for all hypergraphs.
Applied to cyclic models, it yields a causally inspired notion of pairwise interaction that clarifies some important misunderstandings in information theory (\cref{example:nonneg-ii,example:ditrichotomy}).
%joe17: this is much better!
%joe3: As before, I have no idea what "of unknown mechanistic origin"
%even means, an I'm a coauthor.  This should absolutely be cut.
%oli3: ok; for now, changing the wording to something more anodyne. 
% It also gracefully handles ``incomplete'' models containing variables of unknown mechanistic origin, and ``overconstrained'' models that have logical constraints.
%joe4*: I don't see why we could handle undetermined  networks all
%along; Bayesian networks allow isolated nodes. 
%oli5*: ...which, crucially, are a completely different concept, at the opposite end of the spectrum.  I said something to address this above; if you don't understand why I'm saying this, let's please discuss synchronously. 
%joe4*: Yet again, I would
%cut this.   While I left in the part about overdetermined models, I
%see no reason why our framework requires that they ahve logical
%constraints.  That's how you've chosen to interpret it.  I don't
%think you can handle arbitrary models with lgogical constraints.  I
%would cut this too.
%oli5: you're thinking about it backwards: we're not handling arbitrary models with logical constraints. The conversion goes the other direction: we can take an arbitrary witness and convert it to a causal model of constraints in a natural way.  
%oli5: to fix that misunderstanding, I'm rewriting: 
% It also gracefully handles incomplete ``underdetermined''  fragments of a causal picture, as well as ``overdetermined'' models that have constraints.
%joe7*: cut; the reader won't know what this is.  There's no point
%wasting space in the intro to say it.  It adds nothing to the intro,
%and detracts by puzzling the reader.  I've said this before, and I
%believe I know better than you when it comes to these isses.
\vfull{%
    It also gracefully handles incomplete fragments of a causal picture, as well as ``over-determined'' ones.
}%
% It is also \emph{monotonic}, in the sense that adding more 
%     mechanisms 
%oli1: rewriting
% Our definition 
%     is also \emph{monotonic} in that a distribution remains
%     consistent with a set of independent mechanisms
%     if one removes mechanisms, or allows them to depend on 
%     additional variables 
%     % stronger predicate on distributions (see \cref{sec:monotone}). 
%     (see \cref{sec:monotone}).
%joe4*: Yet again, I think you should cut this.  It doesn't belong in
%this paper.  I'll write out my reasons (yet again) in more detail.
%oli5: I didn't see your reasons, but I'm eager to hear them!
%
%
%oli20:
\commentout{
Last but far from least, 
    our definition of \scibility\ 
        also has deep ties to the causality literature.
%joe2*: "witness" comes out of the blue and is not sufficiently explained
    A witness that a distribution $\mu$ is \scible\ with $\Ar$ 
    is an extended distribution $\hat\mu$ that 
%joe7: less is more, and we need the space
%    contains enough information to answer counterfactual queries. 
%Such a witness
    can be naturally converted to a \emph{structural equations model (SEM)}
        \citep{pearl:2k}.
    %oli5: I still think this deserves to be here, but I think it might be better to just hold onto this until section 3
    \commentout{
        (possibly with constraints \citep{beckers2023causal}).
    }
%joe2: I don't know what this means, and it's unnecessary for an intro
% with equations along $\Ar$, 
% and a constraint for each variable that is the target of more than one mechanism.
%oli2: 
    We explore this in \cref{sec:causal}.
}
    

%joe1:
%Despite not having treated it extensively, \citet{pdg-aaai}
%oli1: Interpreted literally, your replacement sentence is false;
% we have an independent scoring function for qualitative PDGs.  
% That scoring also generates a hard definition akin to strucutral
% compatibility, through its (local) minima, and also through
% its zero sublevel-set---and we showed that all three definitions
% coincide with BN independencies for BN structures.
% We just swept this under the rug because I wasn't
% able to justify the scoring function's general form to you.
% 
%oli1: looking for a middle ground... so I'm starting elsewhere to give more context.
% Although they did not give independent semantics for qualitative PDGs,
% \citet{pdg-aaai} 
% Although they did not explicitly give a semantics for qualitative PDGs, 
% Although \citet{pdg-aaai} do not explicitly define compatibiility with, 
%oli20*: new material
%joe16: I have no idea what you're trying to say here, nor will any
%reader not named Oliver Richardson.  I cut this.
%
%oli21*: Alright, I'll to go along with the cut, but only because of the way we've now worked together to carefully avoid opening up the question of qualitative semantics for PDGs in particular. Otherwise, I would find something along these lines critical. 
\commentout{%
Clearly the applicaitons of \scibility\ extend far beyond providing a compatibility semantics for the qualitative information in a PDG. 
Yet, on that original front, the situation is only partially resolved.
% But has our approach 
%joe7
%    Prior work \citep{pdg-aaai,one-true-loss,pdg-infer} has focused on 
%oli20:
% As we said, prior work \citep{pdg-aaai,one-true-loss,pdg-infer} focused on the quantitative, probabilistic information in a PDG, rather than the qualitative information. 
%
\commentout{
\citet{pdg-aaai}, for instance, define what it means for a distribution
    to be compatible with the probabilistic information in a PDG,
    but do not provide an analogous definition for the qualitative information.
They do, however, provide a scoring function quantifying the discrepancy between a probability distribution and the qualitative information in a PDG.  
While that scoring function   has some nice properties,
it is not well-justified, and can also behave unintuitively.
(For example, it can be negative, making it difficult to interpret 
%oli4: saving space
    % as a degree of incompatibility
    \unskip.) 
%joe1*: added
We derive an arguably 
%oli1:
% better-justified scoring
    more intuitive scoring function here, 
    based on \scibility.
%oli1*: added in response
%joe2*: Again, you're going into more depth than is appropriate for an
%intro, and saying things that I don't understand (which means that
%the typical reader won't understand them either).  You either have to
%say more, providing motivation and intuition, or say less.  I opted
%for the latter.
%oli-2: Personally, I'm not a fan of our new scoring function, and I don't want anyone who reads just the intro and skims the rest to be under the false impression that I think it is better.  
% Surprisingly, this new scoring function turns out to be
% an instance of the original PDG scoring function 
%oli-2: clarifying
%% applied to a ``causally extension'' of that PDG explicitly models independent noise. 
% applied to a modified PDG that explicitly models the independent noise. 
%joe2
%    Surprisingly, this new scoring function turns out to 
%    be    an instance of the original PDG scoring function 
%        applied to a ``causal extension'' of that PDG explicitly 
%        models independent noise. 
%
%oli1:
%% There turn out to be 
%This is one of several
%    deep (but far from obvious)
%    connections between 
%%oli1:
%    the scoring function we give here and that
%        used by \cite{pdg-that}
%     gives us a deeper understanding of the latter.  
%oli4: 
% The new scoring function turns out to several surprising and deep
%joe7
%    Our new scoring function turns out to have deep and complex
    Our new scoring function turns out to have deep
connections to the original
%oli4:
% scoring function;
one:
%oli4:
% indeed, although this is far from obvious, it can be viewed
% for example, it can be viewed
% as a special case of the original scoring function.
the former is both a pointwise upper bound on the latter, 
% and also, in a sense, a special case of it.
and also a special case of it (when applied to a transformed PDG).
We explore these connections in \cref{sec:pdgs}. 
}%
%
%oli20: now, drawing from \voli.
%
% At this point, we skip to the end. It is important to realize 
% So as not to leave the reader hanging, at this point we skip to the end of the story. 
% While PDGs ultimately play a major role, 
Although the two are very closely related, we have found that \scibility\ is not quite the same as the qualitative aspect of a PDG (at least, not with its current scoring function semantics). 
In fact, we will see (\cref{sec:pdgs}) that PDGs are actually more expressive, in the sense that our concept of mechanism independence can be naturally encoded within them. 
% But qualitative PDGs do not have an interpretation nearly as clear as the one we offer here. 
% This paper can therefore be interpreted in two ways: 
% With respect to the PDG formalism, this paper has two interpretations: 
So, as far as PDGs are concerned, this paper can be interpreted in two ways: as an alternative, more restrictive but causally grounded semantics for all PDGs, 
% or as causal motivation for the special subclass of PDGs in the image of the encoding, according to their original semantics
or as a case study on just those PDGs in the image of the encoding (under their original semantics).
% Either way, we present a 
Either way, the concept has implications far beyond PDGs.
% Either way, the biggest contributions of this paper are arguably not about PDGs at all.
% In either case, the biggest contributions of the paper lie beyond the PDG formalism.
% We are optimistic that further investigation 
%
% ---objects that, have evidently proved of independent interest.
%
% But the real contributions of our paper lie beyond PDGs. 
}
%joe16: \end{commentout}

%joe16: added some hype
%oli21:  !!
Saying that one approach to
%oli21: streamline
% doing
qualitative graphical modeling has
connections to so many different notions is a rather bold claim.  We spend the rest of the paper justifying it.

% This exemplifies one aspect of a  
%     deep (but far from obvious)
%     connection between this scoring function
%     and our own definition of \scibility.   
    % which we explore in \cref{sec:pdgs}.
%joe2:
% We explore this further in \cref{sec:pdgs}.
%oli1*: After some thought, I no longer think this is accurate in general. 
% From this perspective, our work forms a principled foundation for the qualitative or structural half of the PDG formalism, rooted in causality. 
%joe2: unnecessary and arguably inappropraite
% From this perspective, our work forms a 
%     principled foundation for the structural half 
%     of (a large subclass of) PDGs. 
%
%joe7*: let's cut this; no point in ending the intro on a down note.
%We can mention this issue in the conclusion
%oli8: I had the same thought. 
\commentout{
%joe1*: added.  Can we apply the results of our inference paper to
%the inference problem with these semantics?  We need to say
%something about inference, in any case.
To summarize, our semantics for qualitative PDGs gives us a powerful modeling
tool to capture dependencies and independencies; when combined with the
quantitative information in a full PDG, we can go well beyond standard
graphical models.  
%oli1: cleaning up the next sentence:
% An obvious question is how difficult inference is how difficult inference is in such PDGs. 
An obvious question is how difficult inference is for such PDGs. 
%oli1*: saying something about inference.
%joe2
%Even though this definition is captured by the original PDG semantics, 
%oli2: I like the change, except for the "in a sense" which weakens the statement beyond what I would prefer, and also disrupts the flow of the sentence.
% Although our results show that the current semantics can, in a sense,
Although our results show that 
    %oli7:
    % the current semantics can
    \scibility\ 
    be viewed as a special case of 
    %oli7:
    % the original semantics for PDGs,
    PDG semantics,
    the inference algorithm provided by
    \citet{pdg-infer}, unfortunately, does not apply.
%joe2
%This is because these PDGs do not exhibit the ``observational dominance''
%    property that is required by that work.
%
%oli2: what you say below is true, but much weaker than the point I 
% was trying to make. This scoring function and the original one
% are alike in that we do not know how to do inference for purely qualitative
% PDGs. But the new scoring function is much worse: it is intractable even
% if we have quantiative information (and large values of observational confidence)
% along all of the edges.  
%joe3: I don't understand this point.  The new scoring function does
%not use quantitative information.  It does not apply if you have it
%(which doesn't mean it's intractable if you have it).  Clearly, if
%you have quantitative information, you have to extend the scoring
%function to deal with it.
%oli3: we went over this and clarified in our meeting today. To 
% recap: while the problem is convex if you extend the previous scoring
% function with quantitative information, it is not if you start with 
% this scoring function.  In that sense, this scoring function interacts
% much less well with the quantitative half, and the semantics based on
% that scoring function are far less tractable. 
%
% cannot be applied to purely qualitative semantics, since it applies
% only when the focus is on the probabilistic
% part of the semantics. 
%
%oli2: we're not really talking about inference in this section;
% we're talking about the decision problem that corresponds to 
% SIM-compatibility.  This probablem is actually easy
% if we use semantics induced by the original scoring function, but
% much more difficult with these semantics.
%joe3: What decision problem are you talking about
%oli3: The following decision problem: Given a distribution \mu and
% a directed hypergraph \Ar, tell me whether or not \mu \models \Ar. 
%
% We discuss this issue and possible solutions in \cref{sec:null}.
%
%joe2: While we should be intellectually honest, we on't want to end
%on such a down note
%oli1: 
%oli7: moving this to the discussion:
\commentout{
We describe an alternative approach in \cref{sec:null}
    that works well for small PDGs, 
    but we do not have a proof that it succeeds in all cases, 
    and it scales very poorly.
}
}
    % although provably bounding its running time remains an open problem.

%oli20:
% \section{\SCibility}
\section{Qualitative Independent-Mechanism (QIM) Compatibility}
    \label{sec:scompat}

% {\color{red}FIXME}
%oli15*: adapting the text from you email to build up Theorem 1 more.
%oli16: also moving it to the front of the section.
%oli20:
% \Scibility\ gives us a way to make precise
In this section, we present the central definition of our paper: a way of making precise
Pearl's notion of ``independent mechanisms'', used to motivate Bayesian Networks from a causal perspective. 
%oli20: moving up from the end of the paragraph.
%oli20*: actually, I think the story is better if we cut this. Or we could move it, or trim it down.
\commentout{%
When specialized to hypergraphs $\Ar_G$ that come from directed acyclic graphs (\emph{dag}s), 
we get an intuitive characterization of independencies in BNs that is equivalent to, yet quite different from, the standard characterization.  
}%
%
\Citet[p.22]{pearl2009causality}
states that
% speaks of autonomous mechanisms.
% \begin{quotation}
    % \noindent\it
    \textit{%
    ``each parent-child relationship in a causal Bayesian network represents a
    stable and autonomous physical mechanism.''
    }%
% \end{quotation}
% A mechanism $a$ is
% Formally, a set of mechanisms is represented by
% a directed hypergraph.
%
% Secretly, this is also an unweighted structure
% for a probabilistic dependency graph \cite{},
% If we annotate
%oli1:
% Technically speaking, 
But, technically speaking,
a parent-child relationship only partially describes
the mechanism.  Instead, the autonomous mechanism that determines the child is really represented by that child's joint relationship with all its parents.
%joe1: don't talk about types in an AI paper!
% So, at the qualitative level, the type of a mechanism
%oli1: the sentence is not accurate without the "type" concept. The mecahnism
% is the randomized process by which S determines T.  The type of that mechanism is the directed \arc\ S -> T. The latter is what we are interested in, not the former. If there is a better word to use in the context of an AI paper, but I'd rather use an unfamiliar concept than say something technically confused.
%oli1: here's an alternative that avoids the word altogether.
% So, at the qualitative level, a mechanism is better viewed as
%joe3: I realy don't like this wording.  How about "So the qualitative
%aspect of a mechanism is best represented as a directed
%\emph{hyperedge", which can have multiple sources."  Note that you
%want to change hyperarc to hyperedge here, for consistency.
%oli3: ok
% So, the qualitative aspect of a mechanism is a is a directed \emph{hyper}arc, and can have multiple sources.
So, the qualitative aspect of a mechanism is best represented as a directed \emph{\hyperarc}
%oli6: adding citation
\citep{gallo-dirhypergraphs1993}
%joe16
%\unskip, and can have multiple sources.
\unskip, that can have multiple sources.
%oli16: second part comes here. 
\begin{defn}
    A \emph{directed hypergraph}
    %oli16:
    (or simply a {\hgraph}, since all our \hgraph s will be directed)
    % \unskip---or simply a {\hgraph}, since
    %     all our \hgraph s will be directed---%
    consists of a set $\N$ of nodes
    and a set $\Ar$ of 
    %oli8: trying to implement the spirit of your change without breaking everything
    % directed hyperedges, called \emph{\hyperarc s};
    directed hyperedges, or \emph{\hyperarc s};
    each \hyperarc\  $a \in \Ar$ is associated with
        a set $\Src a \subseteq \N$ of
    source nodes and a set $\Tgt a \subseteq \N$ of target nodes.    %
    We write $\ed {\scriptstyle a}{S}{T} \in \Ar$ to specify a
    \arc\ $a \in \Ar$ together with its sources $S = \Src a$ and targets $T = \Tgt a$.
%
Nodes that are neither a source nor a target of any \hyperarc\ will
%joe16
%seldom have any effect on our constructions, and the other nodes can
seldom have any effect on our constructions; the other nodes can
be recovered from the \hyperarc s (by selecting $\N := \bigcup_{a \in
%joe16
  %  \Ar} \Src a \cup \Tgt a$); thus we often leave $\N$ implicit,
    \Ar} \Src a \cup \Tgt a$). Thus, we often leave $\N$ implicit,
referring to the \hgraph\ simply as $\Ar$.
%oli7*: I was verifying things closely, and this clause interacts annoyingly with partitionality. Partitionality is the more important notion, so I'll drop this to make that definition clearer. 
\commentout{
    We can assume without loss of generality that $\Ar$ contains an identity \arc\ 
    % $N{\xrightarrow{\mathrm{id}_N}}N$ 
    $\ed {\mathrm{id}}NN$
    for each $N \in \N$, 
    %joe3: it should e "all the information"
    in which case $\Ar$ alone contains all the information in the hypergraph.
    For this reason, 
    %oli7:
    
    we refer to the directed hypergraph simply as $\Ar$.
}%
%joe4: my version
%oli5: There's a perfectly good explanation for why we can do this.  (The one I had above.) This sells it short.  I don't see why it muddies the water at all.  
\commentout{
    We sometimes abuse notation and ignore the set of variables and their
    values, referrting to the the directed hypergraph simply as $\Ar$.
}%
    %joe7*: what is it mean for a node to be "relevant"?  What does
    %"almost always" mean?  This is unacceptable in a formal definition!
    %oli8: I mean, if you define the nodes this way, the resulting mathematical object and its smenatics will naturally embed inside the behavior if you included extra nodes, which interact with nothing and are irrelevant. But I just want a throwaway word to communicate this, so I'll move it outside of the formal definition. 
    % Becaue all relevant nodes can almost always be recovered from the \hyperarc s (by setting $\N := \bigcup_{a \in \Ar} \Src a \cup \Tgt a$), we often surpress $\N$, referring to the directed hypergraph simply as $\Ar$.
    %oli16: hiding this, so I can send out a draft. Trying to integrate some pieces of it. 
    \expandafter\commentout\vjoe{
    A \emph{directed hypergraph}
    consists of a set $\N$ of nodes
    and a set $\Ar$ of \emph{directed hyperedges}.
    %joe7*: added.  I will insist on this
    %oli8*: YUCK! I really hate the next line. This is a directed hypergraph, which has a standard definition!  It's a mathematical construction; nodes are an abstract set. The nodes are not not "associated with" anything in the definition of a hypergraph.  Also, hypergraphs are not graphical models, just as graphs are not graphical models. 
    Each node in $\N$ is associated with a variable, just as in other graphical models. 
    A directed hyperedge is a generalization of a directed edge in a
    directed graph; each directed hyperedge  $a \in \Ar$ is associated with
    a set $\Src a \subseteq \N$ of
    source nodes and a set $\Tgt a \subseteq \N$ of target
    nodes.
    %joe7: added
    %oli8: this is a distraction
    (A directed edge in a directed graph can be thought of
    as having a source and targe that are singletons.)
    %joe7*: Why do you need the superscript a?  I would get rid of it.
    %You typically do not write it.
    %oli8: We need it to distinguish between distinct arcs with the
    % same source and target. We always use the superscript when we're
    % carefully using the notation we define here---but when we draw
    % the PDG out completely, we just write two arrows. This notation
    % becomes far less useful if we can't use it to bind a hyperarc a \in \Ar. 
                We write $\ed {\scriptstyle a}{S}{T} \in \Ar$ to specify
    %joe7
    %                an \arc\
    a directed hyperedge
                $a \in \Ar$ together with its sources $S = \Src a$ and targets $T = \Tgt a$.
    %oli7*: I was verifying things closely, and this clause interacts annoyingly with partitionality. Partitionality is the more important notion, so I'll drop this to make that definition clearer. 
    %
    %If we assume that an isolated node corresponds 
    %  Becaue all relevant nodes can almost always be recovered  from the 
    % \hyperarc s (by setting $\N := \bigcup_{a \in \Ar} \Src a \cup  \Tgt a$),
    We assume that every node in $\N$ is in the source or target of some
    directed hyperedge.  
    %oli8: I did say this myself in an earlier version, but this doesn't work
    % because of the definition of partitionality. Has to be cut.
    % (This is without loss of generality, since for a
    % node $X$ not part of any hyperedge, we can always add a hyperedge
    % whose source and target are $\{X\}$.)
    This means that we can recover $\N$ from $\Ar$, so
    we often surpress $\N$, referring to the
    directed hypergraph simply as $\Ar$.
    %joe7: added
    From here on, for ease of exposition, we refer to  a directed
    hypergraph as just a 
    hypergraph, and a directed hyperedge as a \emph{\hyperarc}.  We
    also identify a node $X \in \N$ with the variable associated with
    it, so talk about a variable $X \in \N$.
    }
\end{defn}

%oli16: rewrote both of our paragraphs
\commentout{
%oli9: Although I see that this paragraph is an attempt to communicate the
% spirit of what I wrote before but more clearly, I don't really like 
% the result (particularly, the first half of it).  Comments  within:
\vjoe{
%oli9: as I see it, there's no mathematical distinction between a node
% and a variable, apart from the set of values.  
%joe9: But you are not typical.  A graph theorist would clearly
% distinguish a node from the label of a node.
%oli10: A graph theorist wouldn't bother to make a distinction at all---they would  just fix a set of nodes, and not bother with two different types of things. I've done the same in my definition of a hypergraph, but yours makes a hypergaph somehow about variables, which I think is a mistake. That's now how anyone else defines a hypergraph. In any case, I think this is a place where we're talking past each other somewhat, and should probably address synchronously.
In  typical graphical models, a node is associated not just with a
variable $X$, but a set $V(X)$ of possible values of X.  We
deliberately do not do that here.  It makes perfect sense to talk
about a variable $X$ being independent of a variable $Y$ without
specifying the set of possible values of $X$ and $Y$.  However, when we talk
about a distribution $\mu$ on a set of variables, we assume that the
the variables are associated with values.  For example, we talk about
a distribution $\mu$ on $\X = (\N,V)$ for some choice of $V$ mapping each
variable $X \in \N$ to a set $V(X)$ of values.
%oli9: I don't get the point of the last sentence. 
%joe9: In retrospect, neither do I.
% For ease of exposition (and since a variable is typically associated with a set of values), we talk about ``the set $\X$ of variables''. 
}
%joe7*: I don't know what it means to "identify" a node with a
%variable name.  Nor do I know what it means to "fully" interpret a
%node.  I rewrote the rest of the paragraph.  I found it far too
%complicated and confusing (which you should interpret as "I believe
%that the typical reader will find this complicated and confusing.")
\voli{
%oli8: moved here instead, with modifications to make you happier
% Becaue all relevant nodes can almost always
%oli9: I still think relevant is the better word; putting it in quotes to try to assuage your concerns above.
%joe9: You can't use the word, either in quotes or not, without
%explaining what "relevant" means.  The reader can't read your mind!
%oli10: I was intending to be vague, not to be precise in a way that requires mind reading.  Vague is not the same as meaningless. 
% Because the intended nodes $\N$ can typically
%oli10:
% the rest of the nodes are actually not relevant for SIM-compatibility, so perhaps we can hint at that instead. 
Because the
% relevant nodes can typically 
nodes that are relevant for our purposes  
be recovered from the \hyperarc s (by selecting $\N := \bigcup_{a \in \Ar} \Src a \cup \Tgt a$), we often surpress $\N$, referring to the directed hypergraph simply as $\Ar$.
%
%joe1
%The graphical structure of a PDG is a directed hypergraph
%    whose nodes correspond to variables.
%oli1: I prefer the original, becasue I think the identificaiton
% goes more smoothly one way than the other: the qualitative
% data of a PDG becomes a directed hypergraph when you forget
% the weights, and the fact that the nodes are variables.
%oli1: that said, I see why starting with "a directed hypergraph"
% makes the flow better after the definition. A rewrite attempt:
% A directed hypergraph can be viewed as an \emph{unweighted
%     qualitative} PDG \citep{pdg-aaai}.
%oli7: implementing a change you asked for before, but I hadn't gotten around to
% A directed hypergraph can be viewed as an
A directed hypergraph---henceforth, simply a \hgraph, since it's the only kind we discuss---can be viewed as 
    %oli7: this is the wrong citation here, because that paper defines a PDG structure to be an ordinary graph, not a directed hyeprgraph. We need to cite one of the other two papers.  Also, as you have been saying, we don't want to focus on the weights; adding parens.  
    % an \emph{unweighted qualitative PDG} \citep{pdg-aaai}
    an \emph{(unweighted) qualitative PDG} \citep{pdg-infer}
    %joe4*: this is now uncessary
    %oli5: yet I strongly disagree with your approach.  The variables are not part of the hypergraph. Reinstating what I had. 
    %joe6*: See my comment below.
    %oli7: removing parens, and being more explicit below.
    % by interpreting its nodes as (names of) variables.
    by identifying its nodes with variable names.
%oli1: moving what was previously the next
% paragraph here, before continuing with the nod to qualitative
% PDGs. 
%
%joe1*: 
%The distinction between "variable" and "variable name" may occur 
%in the PL community, but I have never seen it made in the AI
%community.  Nor have I seen the values be part of the definition of a
%variable.  This will hurt *much* more than help.
%joe3: I would cut the rest of this paragraph.   For what it's worth,
%there's no reason that hte set of values of a variable has to be
%finite.  But I don't see what we gain by saying this.  It also breaks
%the flow.  I would just go right to the next paragraph (without a
%paragraph breaak)
%oli3:  I see your point.  At the same time, I think this material
% is very valuable (see next %oli3 for reasons why). Perhaps it can go 
% elsewhere?  But the previous sentence leads into it so nicely...
%oli3: the rest of this paragraph does two important things. The first
% sentence sets up how nodes (the qualitative notion) are related to variables
% (the quantitative notion), and defines the "values of" notation; both 
% are needed before we get to joint distributions in the paragraph after next.
% The second sentence gives an important recurring theme: the values
% of the variables should not be thought of as part of the qualitative
% picture. This is worth emphasizing because it may not be standard, 
% but it is the right way to think of things.  This will come back
% into play in example 6, if not beforehand. 
%joe4*: I strongly disagree.  Note that the definition of
%compatibility talks about probability measures, and to define \mu you
%need to have a set of values.  Now you cay say that you can consider
%consider probability measures for all possible choice of values.
%That would mean that we have this huge class of compatible
%probability measures (no longer a set).
%oli5*: so what? We're not packaging those as a class. We never refer to "the set of all distributions over anything consistent with a hypergraph". We're just talking about a binary relation between a directed hypergraph (which, as a graph, does not have a set of values, just like a qualitative BN does not encode sets of values).  This tell us how, for a SPECIFIC joint probability distribution (which is where the value sets come from), we can define whether or not it's compatible with a directed hypergraph.  I don't see why you feel a need to come up with a an artificial extension of a directed hypergraph, and then afterwards require that the values of that hyeprgraph match the ones specified by the distribuiton. Not only is that unnecessary and different from what's done in the literature, but it would also take more space. As a side note this approach conflicts heavily with my own intuition for what "structural" means. 
%joe4*: You'd also already have a
%problem in Example 2.  The generality you're gaining by not fixing a
%set of values for each variable is simply not worth it.  You'd
%certainly have to explain this issue to the reader if you were going
%to ojmit the set of values.  
%oli5*: I don't see any deep problem with example 2. If we want to be very precise, we could simply replace "variable X" with "node X", which I have now done. 
%joe6*: I know you won't believe me, but since readers can't read your
%mind, they will find this very confusing and completely miss the
%point.  If you insist on doing it this way, you *must* say explicitly
%that variable values are not included in the hypergraph, and explain
%where they're coming from (the distribution).  
%oli7: Great, I'll do that.  (I tried to do something like this some time ago, but you didn't see why and asked me to cut it.)
%oli7: math is the same; rewording so that "variable" is still a valid thing to call a hypergraph node (that doesn't have values). 
To \emph{fully} interpret a node $X \in \N$ as a variable, it must
% To use a node $X \in \N$ as a variable quantitatively, it must
% To interpret a node $X \in \N$ quantitatively, it must
be
% associated with a (measurable) set of possible values $\V(X)$.
% associated with a (finite) set $\V(X)$ of possible values.
%oli3: no longer insisting on finite (but measurable is definitely
% a necessary technical requirement.)
% associated with a (finite) set $\V(X)$ 
%joe7: I've cut the word 'measurable" from my description.  It's not
%defined, as I understand it, unless you have an outer measure or a
%\sigma-algebra 
%(depending on which definition you use; there doesn't seem to be a
%universally agree-upon definition).  I'm more used to thinking of a
%measurable set as being an element of a sigma-algebra, but we don't
%have a sigma-algebra (or outer measure) anywhere in sight.
%
%oli8:  "measurable" has a standard definition: it means precisely
% the thing you say is missing, i.e., coming equipped with a \sigma-algebra.
% A measurable set is a set together with a \sigma-algebra over it.  
% I'd be willing to put this in a footnote, but if we remove "measurable"
% then we need to say "finite" to be precise. Finite might actually be
% better for simplicity, especially once we get to the entropy part of the
% paper. 
% associated with a (measurable) set $\V(X)$
associated with a (finite) set $\V(X)$
%joe1*: NO!  Pleasee add the parens everywhere.  I really find this
%notation annoying.  It makes reading this material nontrivially harder for me.
% (or simply $\V\!X$) 
%oli1: Ok. While I still have a strong preference and several arguments
% in support of this notation, I will cede the parentheses in this paper.
% The compactness lost by adding parens is less important in this
% context than it is for the inference material. 
of possible values.
%oli1: my concession is to delete the following material:
%oli3: pulling this out from below so that the symbol $\X$ is
% bound and we can talk about sets of variables.
% Thus, a set of variables $\X$ is equivalent to 
Thus, a set of variables $\X$ is equivalent to 
    %oli7: I suspect you won't like this, but here I think the types clarify a lot. 
    % the pair $(\N, \V)$---%
    the pair $(\N, \V \colon \N \to \mathbf{Sets})$.
    % a set $\N$ and a map $\V : \N \to \mathbf{Meas}$.
\commentout{
    % Thus, a set of variables $\X$ is equivalent to 
    %     a set $\N$ and a map $\V : \N \to \mathbf{FinSet}$.
    We draw a distinction between $\N$ and $\X$ to emphasize that $\X$ can be decomposed into its qualitative part $\N$ the set of variable symbols, and the map $\V$ which gives a quantiative interpretation of each variable as a set of possible values.
    It is convenient (and common practice) to conflate a variable name $X \in \N$ with  its corresponding variable $(X, \V\!X ) \in \X$.
}%
%oli1: trying to keep this in a different form.
% However, it is important to remember that qualitative properties such
%     as dependence and independence are meaningful no matter how one
%     selects the quantitative interpretation $\V$ of the variables.
%oli2:
% remain valid no matter how one 
% selects the concrete interpretation $\V$ of
% the variables.
%oli3:
% Structural properties of a distribution, however, do not depend on the concrete interpretation $\V$ of the variables.
%oli7: adding emphasis, beginning new sentence. 
% but structural properties of a joint distribution $\mu(\X)$
\emph{Structural} properties of a joint distribution $\mu(\X)$, however,
can depend only on $\N$, not on $\V$.
%oli7: 
With that in mind, we use the same symbol 
    % $X = (X, \V(X))$ 
    for a variable and its name,
    and abuse terminology by calling both objects ``variables'' when no ambiguity can result.
For example, 
    % the statement 
    ``$X$ determines the variable $Y$''
    is meaningful across distributions in 
        % which $Y$ can take on different sets of possible values,
        which $V(Y)$ can vary, 
    because ``$Y$'' here refers only to a variable name.
        % until a distribution is specified.
% Keeping that in mind, we conflate variable names and nodes in the \hgraph, as is standard.
}
}%oli16: end commentout \voli and \vjoe.
%oli16: compromise version:
%oli19*: a peace offering, so that we can confuse variables and nodes
\commentout{Following the graphical models literature, we are interested in \hgraph s whose nodes correspond to variables \unskip---or, more precisely, where $\N$ is a set of variable \emph{names}. It is standard to take $\N$ to be a set $\X$ of variables, each of which implicitly comes with a  (measurable) set $\V(X)$ of possible values; we deliberately do not do that here. 
It makes perfect sense to say that $X$ is independent of $Y$ without specifying possible values of $X$ and $Y$. Of course, when we talk concretely about a distribution $\mu$ on a set of variables $\X \cong (\N, \V)$, those variables must have possible values. The key point is that \emph{qualitative} properties of $\mu$, such as (in)dependence are expressible purely in terms of $\N$, not $\V$. }
%oli19*: replacement
Following the graphical models literature, we are interested in \hgraph s whose nodes represent variables,
so that each $X \in \N$ will ultimately be associated with a
%oli19: because we're talking about entropy later on and don't want to get into the subtleties of differential entrpy, I think it's easier and safer to just say finite here
% (measurable)
(for simplicity, finite)
set $\V(X)$ of possible values.
However, one should not think of $\V$ as part of the information carried by the hypergraph. 
%oli21: cutting for space
%joe17: Why are you cutting this; I actually found it helpful.  There
%are other things I'd rather cut.  Reinstated for now.
% \vfull
{%
It makes perfect sense to say that $X$ and $Y$ are independent without specifying the possible values of $X$ and $Y$.
}%
Of course, when we talk concretely about a distribution $\mu$ on a set of variables $\X \cong (\N, \V)$, those variables must have possible values
%oli21: merging with the below, so it's less belaboring the issue
% \unskip. The key point is that \emph{qualitative} properties of $\mu$,
\unskip---but the \emph{qualitative} properties of $\mu$, such as independence, can be expressed purely in terms of $\N$, without reference to $\V$. 
%joe14: I'm OK with the paragraph above, although it seems like you're
%making a major fuss over what should be a small point.  If you insist
%on making it, you'll have to point out to the reader several places
%where it plays an important role (with perhaps a forward pointer here).
%oli20: deal.  I'll come back to this. %joe15%joe16%joe17%joe18
%joe16* when will you come back to it?
% {\color{red}$\blacksquare$}
%
% As is standard, we conflate variable names and nodes in the \hgraph,     with the understanding that 
%oli1*: I'm thinking it might be a good idea to formally introduce the pictorial PDG notation here, although it might also be better to just get through this material and introduce it by example, as we do currently. 
%
% {\color{red}
% By contrast, a purely quantitative probabilistic
% dependency graph is a tuple $(\X, \mathbb P, \bbeta)$,
% %joe2*: I found this very confusing
% %oli2: oof, on re-reading, I agree.
% % in which each $X \in \N$ has been given a quantative interpretation
% % by associating $X \in \N$ a set $\V(X)$ of possible values (so $\X \equiv (\N, \V)$)
% % and each \arc\ $a \in \Ar$ has been given a quantitative interpretation
% % by associating each $\ed aST \in \Ar$ to a conditional probability distribution
% % $\p_a(T|S)$ on its target given its source.
% %oli2: bringing back and reworking the first part that you cut, because it's a key point: the variables \X are the result of annotating the (qualitative) nodes \N  with concrete values \V; meanwhile the cpds \P are the result of annotating the (qualitative) arcs \Ar  with concrete cpds.
% % deeper reasons for why this is important that have to do with the categorical
% % definition of a PDG, but here it is important.
% %oli2: 
% in which each node $N \in \N$ is concretely interpreted a variable $X_N \in \X$
% by associating it with a set $\V(N)$ of possible values,
% %oli2: continuing with your more compact version below
% and each \arc\ $\ed aST \in \Ar$ is 
% %oli2: adding parallel wording (& a form of the missing word "associated")
%     % interpreted as a mechanism by associating it
% with a conditional probability distribution
% $\p_a(T|S)$ on its target given its source.
% }

Intuitively, we expect a joint distribution $\mu(\X)$ to be 
%oli21: removing parens
% (qualitatively)
qualitatively
%oli4:
% \scible\ with a set of independent mechanisms
compatible with a {s}et of {i}ndependent {m}echanisms
%oli1: now that we're no longer stressing the 
% correspondence \X = (\N, \V), the following is unnecessary.
% (i.e., a \hgraph\ $(\N, \Ar)$ whose nodes $N\in \N$ correspond to     variables $X \in \X$),
%oli1: I rewrote the following, but this is a bit verbose; we can 
% just make it less tecnical, I think.
%
% (i.e., a \hgraph\ $(\X, \Ar)$ whose nodes are the variables 
%     whose joint settings are the sample space of the probabiliy 
%     measure $\mu$)
(whose structure is given by a hypergraph $\Ar$)
if 
%oli1
% we can write down
there is
a mechanistic explanation of how each target
% $\Tgt a$
% Intuitively, a joint distribution
arises as a function of the variable(s) on which it depends
and independent random noise.
% This leads us to the following definition.
%joe1
%oli1: in -> by
This is made precise by the following definition.

\begin{defn}[\scibility]
        \label{defn:scompat}
    %oli7: one approach to your concern; see %oli7 below if it is not crystal clear what I'm trying to do here.  (part 1)
    %(oli7) This is my preferred version, but I'm guessing you won't like the equality.
    %oli17
    % Let $\mathcal Y$ and $\X \cong (\N, \V)$ be sets of variables.
    % Let $\mathcal Y$ and $\X \cong (\N, \V)$ be (possibly identical) sets of variables.
    % Let $\X \cong (\N, \V)$ and $\mathcal Y$ be (possibly identical) sets of variables.
    %oli19: simplifying and identifying nodes and variables
    % Let $\X \cong (\N_{\!\X}, \V)$ and $\mathcal Y$ be (possibly identical) sets of variables, and
    Let $\X$ and $\mathcal Y$ be (possibly identical) sets of variables, and
    $\Ar = \{ \ed{a}{\Src a}{\Tgt a} \}_{a \in \Ar}$ be a hypergraph with 
    % nodes $\N$, the names of $\X$.
    %oli19:
    % nodes $\N_{\!\X}$.
    nodes $\X$.    
    %(oli7) another version:
    % Let $\X$ be a set of variables, with names $\N$.
    %oli15: more general version
    % We say a distribution $\mu(\X)$ is
    We say a distribution $\mu(\mathcal Y)$ is
    %oli2:
%joe3: I could live with this, but would take out "structurally"
%and make the previous references to compatibility consistent with
%whatever you converge on.
%oli3: sounds good. As mentioned before, I actually think your approach
% is a good idea, so long as we make a couple further changes to 
% ensure that the word "structure" is kept in the name of the concept
% when no \Ar is in scope. 
%oli7: (something unrelated; trying to get you "compatible" as an option)
% \emph{structurally independent-mechanism compatible (\scible)}
    % \emph{structurally independent-mechanism compatible (\scible)}
    \emph{%
        %oli15:
        % structurally
        qualitatively
        independent-mechanism compatible}, or (\SQIM-)compatible,
    %oli7: (part 2)
    % with directed hypergraph $\Ar$
    %oli19:
    % with a \hgraph\ $(\N_{\!\X}, \Ar)$
    with $\Ar$
    %oli15: adding diamond, for "possibly" 
%joe8: Please use a larger \Diamond (\Diamond?).  I find this small
%diamond disconcerting
%oli19:
    (symbolically: $\mu \models \Diamond \Ar$)
    iff
    %joe1: added next line
    %        there exists an extended distribution
    %oli1:  My initial several iterations also did it in the order you're 
    %suggesting, but I find the order I had before much nicer. One reason:
    %the joint distribution $\nu(\X,\U)$ is the important object here,
    %(indeed it includes the data \U), so I want it front and center.
    %  I'll adopt some of your verbage, but I'm switching the order back.
    %
    % there exists a set $\U = \{ U_a : a \in \Ar\}$ of ``noise''
    % variables disjoint from $\X$ that consists of one noise variable
    % for each \arc\ $a \in \Ar$ and there exists an extended distribution
    % $\nu(\X, \U)$, according to which
    %
    %oli1: another comment on the above: it doesn't make any difference
    % if the noise variables are disjoint, and some examples are easier
    % to describe without creating a duplicate variable for the express
    % purpose of satisfying this definition. So I don't want to stress disjointness. 
    there exists an extended distribution
    %oli15: I think this notation is better, for various reasons; among them: 
    % - it shows that the extension is related to $\mu$; 
    % - the symbol \nu looks a bit like \V in my notes;
    % - allows for notational separation between a witness of \scibility and a member of {{ M }}, which could be useful for clarity. 
    % $\nu(\X, \U)$
    %oli15: now generalizing notation
    % $\bar\mu(\X, \U)$
    $\bar\mu(\mathcal Y \cup \X \cup \,\U_{\Ar})$
    %oli1: added
    %oli17: the reminder is helpful, I think
    % of $\mu$ to 
    of $\mu(\mathcal Y)$ to 
    %oli15:
    % all of $\X$ and 
    $\X$ and
    %oli15: 
    % $\U = \{ U_a : a \in \Ar\}$
    %oli17: indexed set is nicer. Don't want any collisions...
    % $\U_{\Ar} = \{ U_a : a \in \Ar\}$
    to\, $\U_{\Ar} = \{ U_a \}_{a \in \Ar}$,
    an additional set of ``noise'' variables 
    %oli1: a small nod to your approach
    % (one variable $U_a$ per \arc\ $a$)
    %oli17: expanding, in case we get the whole line anyway
    (one variable per \arc)
    % (one variable per \arc\ $a \in \Ar$)
    %oli1: moving this into the bullet list below.
    % whose marginal on $\X$ is $\mu(\X)$, and 
    % with marginal $\nu(\X) = \mu(\X)$, and
    % according to $\nu$:
    according to which:
\begin{enumerate}[label=(\alph*),itemsep=0pt,topsep=0.0ex,parsep=0.5ex]
% \item the variables $\X$ are distributed according to $\mu$,
\item 
    the variables $\mathcal Y$ are distributed according to $\mu$
    \hfill(i.e., $\bar\mu(\mathcal Y) = \mu(\mathcal Y)$),
\item
%oli15: 
% the variables $\U$ are mutually independent,
the variables\, $\U_{\Ar} $ are mutually independent
%oli15: 
    \hfill (i.e., $\bar\mu(\U_{\Ar} ) = \prod_{a \in \Ar} \bar\mu(U_a)$ ),
    and
%joe6*: You said above " I strongly disagree with your approach.
%The variables are not part of the hypergraph."  If that's the case, 
%you can't talk about the source variables; that's meaningless.  You
%have to assume that the nodes in a hypergraph are labeled with variables, or
%rewrite this somehow (I have no idea how).  After you've done that,
%you'll need a careful discussion of variable values, as I said above.
%oli7*: This is a valid concern. Thanks for taking the time to spell it out for me here in this context. 
%oli7*: Just to be extra sure we're on the same page, here in the comments, let me be a bit more precise about exactly how I want to resolve this up mathematically (although I want to avoid the following verbosity in the actual paper):
% Each variable in \X  is a pair (N, VN), where N is a unique "label" or "variable name", and VN is a set of possible values it can take. The nodes of the hypergraph are precisely those same elements N.  So in this context, for example, the "targets" of a hyperarc are subsets of \N, and thus we can look up the corresponding variables in \X.  So it's not that the nodes of the hypergraph are labeled by variables, but rather the reverse: the variables in the distribution have labels, which are the node elements of the hypergraph.
%
%oli-17:
\item the target variable(s) $\Tgt a$ of each \arc\ $a \in \Ar$ are\\ determined
% \item the target variable(s) $\Tgt a \subseteq \X$ of each \arc\ $a \in \Ar$ are\\ determined
    %oli7: reordering for flow
    % by the source variable(s) $\Src a$ and $U_a$
    by $U_a$ and the source variable(s) $\Src a$
    % by $U_a$ and the source variable(s) $\Src a \subseteq \X$
    %oli15:
    \hfill (i.e., $\forall a \in \Ar.~ \bar\mu \models (\Src a, U_a) \tto \Tgt a$)
    % (i.e., $\sum_{a \in \Ar} \H(\Tgt a | \Src a, U_a) = 0$)
    % (i.e., $\forall {a \in \Ar}.~ \H(\Tgt a | \Src a, U_a) = 0$)
    \unskip.%
    % \footnote{More formally still: $\mu \in \Delta \V!\X$ is compatible with }
    % \qedhere
\end{enumerate}


We call such a distribution
%oli15: last explicit replacement; rest will be done silently. 
% $\nu(\X,\U)$
%oli19:
% $\bar\mu(\X,\U)$
% $\bar\mu(\X \cup\, \U_{\!\Ar})$
$\bar\mu(\X \cup \mathcal Y \cup\, \U_{\!\Ar})$
 a \emph{witness}
%joe3
%that its marginal $\mu(\X)$ is \scible\ with
%oli4:
    % for its marginal $\mu(\X)$ being \scible\ with
    % independent mechanisms along $\Ar$,
    that $\mu$ is \scible\ with $\Ar$.
% and write $\mu \models \Ar$.
    % to indicate that there exists such a witness.
%joe2: Since you allow arbitrary sets \U, this set of witnesses is
%huge.  Technically, it's a class, not a set.  I don't want to make a
%fuss about this here, but I wonder if there's some way to simplify
%it, by restricting \U even further
%oli2: That's a good point.  There's a canonical choice of varaibles U: response variables. 
%joe3: We really shouldn't have to deal with this class at all.  I
%would bet we can get rid of this definition altogether.    
%oli3: Let's revisit this once we've decided exactly what main results
% will go in the paper.  I think it will make it easier to state some
% theorems if we explicitly define this structure.
%oli7: you're right, ultimately don't think it's necessary. 
% and write $\Wits(\mu, \Ar)$ for the class of all such witnesses.
\end{defn}

%joe1: why bother saying this?  Less is more.
%oli1: Do you mean the preface, or the content?  
% because it can be counter-intuitive, and it is very important. If
% the latter, I know for a fact that this fact has tripped both
% of us up more than a handful of times.
% I emphasising this point immediately should
% prevent some of the confusion that we went through.
%oli1: removing the preface. 
% The sense in which noise variables $\U$ are independent
% is simple, but can easily become a source of confusion.
%oli1: rewriting for accuracy + flow
% While $\{ U_a \}_{a \in \Ar}$ are independent of one another, they are not necessarily independent of any variables in $\X$.
While \cref{defn:scompat} requires the noise variables $\{ U_a \}_{a \in \Ar}$ to be independent of one another, note that they need not be independent of any variables in $\X$.
    % \footnote{Clearly the target $\Tgt a$ of a mechanism $a$ cannot be
    % independent of the corresponding noise variable $U_a$ (unless $\Tgt a$ is a
    % constant)%
    % % , and, unless we know the mechanisms are acyclic, there's no reason
    % % to believe any $X \in \X$ is independent of $U_a$
    % . }
%
% Similarly, while we know that $U_a$ and $U_{a'}$ are independent,
%     we have no guarantee that they remain independent
%     once we condition on a variable in $\X$.
%     because it
% of everything; they only need to be independent of one another.  After all,
% the target of the mechanism cannot be independent of the noise,
%oli1: 
% In particular, we do not require that $U_a$ be independent of $\Src a$,
In particular, $U_a$ may not be independent of $\Src a$,
%joe2: I can't parse the rest of this sentence, so I just cut it.
%oli2: Oops! Fixed the garbled part of this sentence.
and so 
% Notably, the way in which $(S,U)$ determine $T$ can diverge
%oli1
% the situation can diverge from what one would expect from a randomized algorithm, whose
the situation can diverge from what one would expect from a randomized algorithm, whose
randomness $U$ is assumed to be independent of its input 
%joe1: 
% $S$, i.e., ``fresh''.
%oli1: I still like "fresh", because ``fresh randomness'' is 
% the most common bigram that evokes the intuitions I'm trying to describe.
$S$. 
%joe2
%joe3: replace "cease to be" by "no longer be".
%oli3: ok
% Furthermore, the variables $\U$ may cease to be independent of one another once we learn the value of some $X \in \X$. 
Furthermore, the variables in $\U$ may not be independent of one another conditional on the value of some $X \in \X$. 
%oli15: this sentence was a poor hybrid of our two sentences. Given that you insist on "may no longer be" over "cease to be", I think we should adopt your end of the sentence as well.
% once we learn the value of some $X \in \X$. 
% Furthermore, the variables $\U$ may not be independent conditional on the value of some $X \in \X$. 
%oli2: why do you prefer the version below?  I find the original lexicaly clearer, easier to think about, and it more directly warns against using the independence if you already know a value of $X$.  So, I'm swapping the below for the above again.
%joe3: What I wrote is, I believe, clearer, but I can live with what
%you wrote once you remove "cease to be".  I'm tired of arguing ...
% Furthermore, the variables $\U$ may not be independent conditional on the value of some $X \in \X$. 
% To build some intuition, we now provide some examples

%oli16: unnecessary; we'll need all the space we can get.
% To illustrate, we start with a simple example.

\begin{example}
    $\mu(X,Y)$ is
    %oli21: cutting QIM
    % \scible\ with
    \cible\ with
    %joe1*: You're using notation that may be clear to you, but will not
    %be clear to many readers.  At a minimum, put in some words to explain
    %it.  (What you wrote earlier is definitely insufficient.)
    %oli21: cut
    % the directed hypergraph\
    $\Ar = \{ \ed 1\emptyset{  \{X\}}, \ed2{\emptyset} {\{Y\}} \}$     
    %oli21
    % \unskip, depicted graphically in PDG notation as
    (depicted in PDG notation as
    %joe2*: what are the superscripts 1 and 2 on the edges?  
    %oli2: those are the labels of the ars.  Formally, we have \Ar = \{1, 2\},
    % with S_1 = S_2 = \emptyset, T_1 = { X }, and T_2 = { Y }. 
    %joe3: This is horrible notation.  Why can't the labels just e the
    %cpts?  We've never used integers as the labels.  This confused
    %me, and I'm a coauthor.  It's bound to confuse others. 
    %oli3: I could call them "a_1" and "a_2" if you prefer?  But note
    % that there are no cpts here; it's all qualitative. We just need
    % to give them names to distinguish between them.  One benefit
    % of setting \Ar = {1,2} is that the corresponding noise variables
    % become U_1 and U_2, rather than U_{a_1} and U_{a_2}.
%joe4*: Your notation makes it look like edges are labeled.  
%We could assume that, but I don't see why.  (We're already assuming
%that nodes are labled by variables, so I don't have an overwhelming
%objection to this assumption.
%oli5*: First, to set things straight, we're not assuming that the *hypergraph* contains the way in which the nodes are labeled by variables; that information is in the distribution \mu.
%joe4*:  But if we're going to make it, it
%should be mentioned in Definition 1.  If we're not going to make it,
%you should remove the labels.)
%oli5:  Actually we *already* introduced this notation in Definition 1, when we say: "we write X --a--> Y to specify a hyperarc a \in |Ar together with its sources and targets". So, the label is the element of \Ar. \Ar is an index set; that's how it's already set up mathematically.
    $\smash{
    \begin{tikzpicture}[center base]
        \node[dpadinline] (X) {$X$};
        \node[dpadinline,right=0.5em of X] (Y) {$Y$};
        %oli21: removing \tiny 1 and \tiny 2 in PDG notation.
        \draw[arr1, <-] (X) -- node[above,inner sep=2pt]{} +(-.7,0);
        \draw[arr1, <-] (Y) -- node[above,inner sep=2pt]{} +(.7,0);
    \end{tikzpicture}
    }$
    % if and only if $X$ and $Y$ are independent in $\mu$, i.e.,
    %oli21: 
    % if and only if
    ) iff
     $X$ and $Y$ are independent, i.e., 
    $\mu(X,Y) = \mu(X)\mu(Y)$.
    %oli21: shaving line
    % This is because, if $U_1$ and $U_2$ are independent and determine
    % $X$ and $Y$,  respectively, then $X$ and $Y$ must be independent.%
    For if $U_1$ and $U_2$ are independent and respectively determine
    $X$ and $Y$, then $X$ and $Y$ must also be independent.%
        % \footnote{see \cref{lem:indep-fun} in \cref{appendix:proofs}, where all proofs can be found.}
\end{example}

This is a simple illustration of a more general
%joe1
%phenomenon: when the $\Ar$ corresponds to the structure of a
%oli2: "set \Ar of \hyperarcs" is more stilted than "directed hypergraph", 
% which itself is unnecessary because $\Ar$ is the same type throughout 
% the paper, and it would be extremely verbose to do this throughout.
% We may as well start dropping the $\Ar$ here. 
%joe3: I don't understand your point.
%oli3: sorry; what I wrote above was not clear.  What I meant to
% say was that there's no reason to say "when the set \Ar of hyperarcs..."
% nor even "when the directed hypergraph \Ar$ ...", because the symbol \Ar
% is already reserved for that.  We can just say "when \Ar ...". We'll
% eventually need to start abbreviating in this way, so we may as well start here.
%
% phenomenon: when the set $\Ar$ of \hyperarc s corresponds to the structure of a
phenomenon: when $\Ar$ describes the structure of a
%oli3:
% Bayesian Network, then 
Bayesian Network (BN), then 
%oli2: this is now too far away from the theorem to connect so explicitly.
% Replacing with some words.
% $\mu \models \Ar$ iff it has the appropriate independencies.
%joe3*: you have to explain what "the appropriate independencies"
%are.  I would refer to d-spearation.
%oli3: adding + references
%joe14: added some italics
% \scibility\ with it coincides with the appropriate independencies.
\scibility\ with $\Ar$ coincides with satisfying the independencies
    of that BN (which are given, equivalently, by the \emph{ordered Markov properties} \citep{lauritzen-dag-indeps}, 
    %oli19: added
    \emph{factoring} as a product of probability tables,
    or \emph{d-separation} \citep{geiger-pearl-d-separation}).
%oli3:
To state the general result (\cref{theorem:bns}),
    we must first 
    %oli15:
    % explain how graphs that underlie usual graphical models 
    clarify how the graphs of standard graphical and causal models
    give rise to directed hypergraph s. 
% we return to this in \cref{theorem:bns}, after 

%
% Here are a collection of properties to build intuition.
% \begin{prop}
%     \begin{enumerate}
%         \item
%             % Every deterministic distribution $\delta_\omega$,
%             % supported by a single $\omega \in \V\!\X$,
%             % is \scible\ with every set $\Ar$ of hyperarcs.
%             For all $\omega \in \V\!\X$, and directed hypergraph s
%             $(\X,\Ar)$, $\delta_\omega \models \Ar$.
%         % \item
%     \end{enumerate}
% \end{prop}

% \section{Bayesian Networks, Markov Random Fields, and Independent Mechanisms}
% \section{Bayesian Networks, Markov Random Fields, and Independent Mechanisms}
% \subsection*{Capturing Bayesian Networks}
% \subsection{Capturing Bayesian Networks.}
    \label{sec:bns}
%    
Suppose that
$G = (V,E)$ is a graph, whose edges may be directed or undirected.
Given a vertex $u \in V$,
%joe1*: can we use more standard notation like "Par" instead of a
%weird symbol?  Weird symbols are definitely not helpful for a reader.
%oli1: While "Par" is standard for directed graphs, it is very 
% non-standard for undirected graphs. In the graphical models 
% literature and sometimes in graph theory, "\partial" is standard
% for undirected graphs.  Are you OK with "Par" even for undirected
% and semi-directed graphs?  I've changed the macro \Pa for now.
write $\Pa_G(u) := \{ v : (v,u) \in E\}$
    for the set of vertices that can ``influence'' $u$.
There is a natural way to interpret the graph $G$ as giving rise to a set of mechanisms:
    one for each variable $u$, 
%joe1
    %    that determines its value based
    %        the values of the variables on which it can depend.
    which determines the value of $u$ based
    the values of the variables on which $u$ can depend. 
%oli21: saving space by doing this inline
% \commentout{
% \begin{equation}
%     \label{defn:graph-to-hypergraph}
%     \Ar_G := \big\{ ~{\Pa}_G(u) \xrightarrow{u} \{ u \} ~\big\}_{u \in V}~;
% \end{equation}
% We call $\Ar_G$ the \hgraph\ \emph{corresponding} to the graph $G$.}
Formally, let
$\Ar_G := 
    \smash{\big\{} 
    ~{\Pa}_G(u) \ed{u}{~}{~} \{ u \} ~
    \smash{\big\}_{u \in V}}$
be the \hgraph\ \emph{corresponding} to the graph $G$.
%oli2*: expanding here by introducing "exact / partitional" hypergraphs.
%joe8: this is misplaced.   You want to get right to the theorem
%(which does not use these definitions).
\commentout{
The \hgraph s generated by \eqref{defn:graph-to-hypergraph}
    form a special class, and share an important property.
    
\begin{defn}
%oli16*: rewriting this definition to have just the two most important categories
\commentout{
    A directed hypergraph $(\N,\Ar)$ is called \emph{\partl} if the targets of its \arc s $\Ar$ form a partition of its node set $\N$,
    and \emph{non-\partl} otherwise.
    A \hgraph\ is \emph{\subpartl} if it contains a node 
    that is not a target of any \arc\
    (i.e., $\cup_{a \in \Ar}\Tgt a \subsetneq \N$),
    \emph{\suppartl} if it contains 
        one   that is a target of multiple \arc s
    (i.e., $\Tgt{a} \cap \Tgt{a'} \ne \emptyset$ for $a \ne a' \in \Ar$),
    %oli14: moved this up from above, because it'll be useful to refer to it more than once. 
    and \emph{\unipartl} if its \hyperarc\ targets form a singleton partition 
    (i.e., $\cup_{a \in \Ar}\Tgt a \subsetneq \N$).
}%
%oli16:
    A directed hypergraph $(\N,\Ar)$ is \emph{\subpartl} if the targets of its \arc s $\Ar$ are disjoint sets, and \emph{\partl} if they form a partition of its node set $\N$.
\end{defn}
}
%joe8: \end{commentout}

%joe8: where do you use next sentence?  If you don't use it, cut it.
%It's true, of course, but doesn't need to be said
\commentout{
The targets $\{ \Tgt a \}_{a \in \Ar}$ of $\Ar$ form the singleton partition if and only if $\Ar = \Ar_G$ for some graph $G$.
}
% Moreover, if $\Ar$ is a partitional hypergraph and $\tilde$ is the equivalence relation generated by the partition, then 
%joe3: You've already said what happens with a dag (that's your equation (2) above). 
%olie: Equation (2) above is more general; it explains what happens for an arbitrary graph (even if not acyclic, or in a sense, undirected). 
%oli3: I'm cutting what I had at your suggestion, although I feel we still need something to transition to the theorem below. Do you agree?
% In the special case where $G$ is a directed acyclic graph, $\Ar_G$ consists of one \arc\ for each $X \in \X$, pointing to $X$ from the parents of $X$.


\begin{linked}{theorem}{bns}
        % [\scibility\ captures BNs]
    % If $G$ is a directed acyclic graph, then $\mu$ is \scible\ with $\Ar_{G}$ if and only if
%joe2
  %  Let $G$ be a directed acyclic graph, and $\mathcal I(G)$ the
%    independencies of its corresponding Bayesian Network.
If $G$ is a directed acyclic graph and $\mathcal I(G)$ consists of the
independencies of its corresponding Bayesian network,
% Then a joint distribution $\mu$ is structurally compatible with $\Ar_{G}$ if and only if
%joe2
%    Then $\mu \models \Ar_G$ if and only if
%oli15: diamond
then $\mu \models \Diamond \Ar_G$ if and only if
    $\mu$ satisfies $\mathcal I(G)$. 
    % $\mu \models \mathcal I(G)$
    % $\mu$ has the independencies of the corresponding Bayesian Network.
\end{linked}

%joe2* The story in the rest of the section leaves much to be
%diesred.  First, I would give hypergraphs whose targets form a
%partition a name (regular hypergraph? normal hypergraph?  something
%else?) and do this just before or just after Theorem 1.
%Then point out that they are the analogue of acyclic graphs.
%
%oli2: (as we discussed in our last meeting, they're not the analogue of cyclic graphs, but I did add some material on them above)
%
%joe2: Then you need a better story after Theorem 1.  Instead
%of just saying that not all directed hypergraph s are regular, say
%something like  "directed hypergraph s that are not normal are also of
%interest.  For example, they can be used to capture determinism.
%
%oli2: OK.  That's the same story I was trying to tell, although a bit
% more explicit about threading the motivation through, and with language
% I think is slightly less powerful.  (I imagine that  "not all hypergraphs
% are \partl\" would invite a reader to think more about the implications, and wonder whether they were useful on their own --- which would make them more engaged than if we tell them. )  Of course, it's far worse for someone to lose track of the motivation, so I'll take your approach.
%
%joe2: With regard to Theorem 2, isn't it the case that if if there are
%edges from both X to Y and X' to Y (where X may be different from X')
%then we get determinism?  If so, this should be part of Theorem 2.  
% The rest of the section, on equivalence, is out of place, at best.
%
%oli2: Yes you're right that if there are edges from X->Y and X'->Y, then
% we often get some kind of determinism, but not in a way that's always easy to describe.  It also might be trivial determinism; for example, it might turn out that X' = Y. We do get the same kind of determinism X ->> Y if X' is the empty set (see the example), but this is a far less robust phenomenon (i.e., it also has other effects). So I disagree that Theorem 2 should contain any of this.  The expanded answer to why I disagree was the rest of the section, on equivalence.
%joe3: what do you mean by "some kind of deterinism"?
%oli3: I mean something very vague and intuitive.  In general, the arcs
% "X -> Y" and "X' -> Y" together imply that any randomness in $Y$ is 
% already present in at least one of $X'$ or $X$ --- but without additional
% arcs for context, there's not always a functional dependency here. 
%
%
%joe2:  The story in the rest of this section should be about non-regular
%hypergraphs.  You should review the arguments of the Beier et
%al. paper saying that cycles are interest, and our approach gives
%them perfectly reasonable semantics.  (A comparison with the Beier et
%al. paper should go here.)  % Can we say anything of interest about
%cyclic hypergraphs?  If so, it should go here.  
%
%oli2: I disagree that this is the right place to talk about cyclic BNs
% or give the comparison with the Berier paper. Yes, cycles are of interest,
% and yes our approach gives them reaosnable semantics. But we don't have clean answers to exaclty what those semantics look like in general, and we won't get the tools to get anywhere close to analyzing them until we've finished the sections on monotonicity and on what I view as the most important theorem of this paper: the structural deficiency bound.
%joe3: somewhere around here you should discuss that types of PDGs
%we're intersted in.  Cyclic PDGs fit into that story.  I could live
%with just a pointer to th Beier et al. paper, with a more detailed
%coparison to follow.
%oli3: ok; I've now added some text with a forward pointer, as you suggest.
%
%We definitely need
%some discussion.  I find the material on equivalence weak.  You don't
%do anything intersting with it.  If you can't find an interesting
%application, I would cut it.  It weakens the paper. In any case, it
%doesn't belong in this %section.  
%
%oli2: The material on equivalence is conceptually very important, and the proposition about equivalence of -> X -> Y and X <- Y <- is incredibly useful for playing with and understanding these graphs. It makes the definition far easier to work with.  Most of what I've proved with it so far aren't so important as to make it into the main paper, but I would be surprised if we can't find a really compelling use case.  If we can prove my conjecture that all equivalent BN structures can be proved equivalent using instances of this theorem, would that fit the bill? 
%joe3: Conceptually important to whom?  Not to me (at least, not for this paepr).  I don't think it fits our story well.
%oli3: I think the story should be molded by the important mathematical properties of the object in question.  So the concept of equivalence ends up being important for the I'm trying to tell, which is about motivating, understanding, and detailing the most important properties of mechanism independence.
%
% One direction of \cref{theorem:bns}---that an acyclic causal model
%joe1: This is premature and breaks the flow.  I'm also not so sure
%it's well known.  (If you think it is, please add a reference.)
%oli1: I haven't seen it formalized carefully, but reading through Pearl's book
% strongly gave me the impression some version of this fact is is common knowledge. 
%joe2: It certainly isn't common knowledge! I don't know it.
\commentout{
    In \cref{sec:causal}, we will see how
        a witness of compatibility $\mu(\X, \U)$ can be seen
        as a (partially specified) causal model.
    From this perspective, \cref{theorem:bns} is a form of an equivalence
        well-known in the causlity community:
    every acyclic causal model with independent per-variable noise induces a distribution with the independencies of the appropriate Bayesian Network---%
    and, conversely,
    every distribution with those independencies
    arises from such a causal model.
    % These facts are well-understood in the causality literature.
}
%joe1: added next line.
%oli1: removing the heading altogether. (But also I don't like line
% break without a section header)
% \textbf{Capturing Functional Dependence.}

% \cref{theorem:bns} describes one important class of graphs
%     for which 
%     our definition of \scibility\ (\cref{defn:scompat})
%     is well-understood.
%joe4: you haven't defined "BN-shaped".  
%\cref{theorem:bns} shows how, for BN-shaped \hgraph s,
%oli5:
% \Cref{theorem:bns} shows how, for hypergraphs that regresent a BN,
%oli21: 
% \Cref{theorem:bns} shows how, for hypergraphs that 
\Cref{theorem:bns} shows, for hypergraphs that 
% So, for hypergraphs that 
%oli19: using your new terminology
% arise from directed acylic graphs,
correspond to directed acyclic graphs (dags),
    our definition of
    %oli21: cutting a QIM
    % \scibility\ 
    \cibility\ 
    reduces exactly to the well-understood independencies of 
BNs.
%joe8*: you need to SLOW DOWN here and get the reader to appreciate
    %what you've done.
%joe16*: slowing down
%This means that \scibility\ gives us a completely different way of
This means that \scibility, a notion based on the independence of
causal mechanisms, 
%oli21: I like where you're going with this, but the below needs some adjustments.  The *notion* of conditional independence does not intuitively seem unrelated, although the formalization is very different.
% and apparently unrelated to the standard notion of conditional independence in BNs (such as d-separation)
and seemingly unrelated to other notions of independence in BNs,
gives us a completely different way of 
%
characterizing these independencies
%oli21: sentence is long and full of commas; for readers like me, this will help it hold together. 
% \unskip,
\unskip---\unskip
%joe16:
% in a way 
one
%spencer1:
%  that can be generalized to much larger classes of graphical models.
that can be generalized to much larger classes of graphical models,
%oli19: I would like to add something like this... but not quite this.
% including cyclic variants \citep{Baier_2022}.
%joe14: why not "that includes, for example, cyclic variants
%\citep{Baier_2022}."?
%oli20: happy to try it out. I was worried that even the shorter version would make the sentence feel a bit run-on.
%joe17
%oli22: I prefer "variants", as it is more precise and "graphs" is getting pretty overloaded.
%joe18:  I disagree (about it being more precise).  But since you
%didn't change it, that's fine
%oli23: I think "cyclic graph" is non the mathematical object at play here, so although it wasn't high enough priority for me to think about and fight over at the deadline, I still believe "variants" is more appropriate.
% that includes, for example, cyclic graphs \citep{Baier_2022}.
that includes, for example, cyclic variants \citep{Baier_2022}.
% including cyclic variants \citep{Baier_2022}.
%oli21: 
% Moreover, we can use  \scibility\ to capture properties other than
Moreover, \scibility\ can capture properties other than
independence.  
As the following example shows, it can capture determinism.

%joe8: this is much too "rambly".  It is subsumed by the few sentences
%I wrote above.  Notice how I will avoid talking
%about "partitionality"
%oli19: I'm willing to try this approach, although I would have preferred to bring up this distinction here so that it is paralleled again in section 3.
\commentout{
But not all \hgraph s are of this form.
Among \partl\ \hgraph s, cyclic models \citep{Baier_2022} are
of particular interest. Our definition of \scibility\ 
gives rise to an interesting generalization of BN independencies 
for such \hgraph s, but we will need the tools developed in 
\cref{sec:monotone,sec:info} to better understand what \scibility\ means in these cases.
Non-partitional \hgraph s are also of interest since they allow us to express determinism, as the following example shows.
}

% \begin{linked}{example}{two-edge-det} % \mu compatible with  [  --> X <---  ] iff \mu det
\begin{example}\label{example:two-edge-det} % \mu compatible with  [  --> X <---  ] iff \mu det
If $\Ar = \{ \ed1{}X, ~ \ed2{}X \}$ consists of just two \arc s pointing
to a single variable $X$, then a distribution $\mu(X)$ is \scible\ 
with $\Ar$ iff
% $\mu = \delta_x$ places all mass on a single value of $x$.
%oli1: adding parens for you
% $\mu$ places all mass on a single value $x \in \V X$.
%joe6*: you must explain where V is coming from, since it's no longer
%part of the definition of the hypergraph.  Trust me, readers will not
%figure it out without an explanation.  
%oli7: With the material I added above, I hope it is clear how it works in this case in particular. But I would bet large sums of money that nobody would have noticed a problem here in the previous version. 
$\mu$ places all mass on a single value $x \in \V(X)$.
\end{example}

%oli19*: adding intuition, per Spencer's suggestion
Intuitively, if two independent coins always give the same answer (the value of $X$), then neither coin can be random. 
%joe8
%To be explicit: we have captured determinism by
%    having multiple \arc s point to the same variable.
    %oli2: making connection to new material
%    (i.e., via \suppartlness). 
%oli19: it's false that we've captured functional dependence.  I also find the "in a hypergraph" to be unnecessary.  But I'll go along with your plan to avoid "subpartitionality".
% This simple example shows that we can capture functional dependence by having multiple \arc s in a hypergraph point to the same variable.
This simple example shows that we can capture determinism with multiple \arc s pointing to the same variable.
%oli19
% Such structures are impossible to model using a BN; in a BN, two arrows pointing to $X$
Such hypergraphs do not correspond to graphs; 
    recall that in a BN, two arrows pointing to $X$
    (e.g., $Y \to X$ and $Z \to X$)
    represent a single mechanism by which $X$ is jointly determined
        (by $Y$ and $Z$),
    %oli19:
    % not two independent mechanisms.
    rather than two distinct mechanisms.
%joe1: not you're talkking about PDGs here, which makes sense now that
%I've introduced them, but didn't make sense with your earlier structure.
%    A primary motivation for using a PDG instead of a BN
%oli1: Good point. 
% Here I want to emphasize that this is not just one advantage, but
% arguably THE thing that a PDG buys you
% One advantage of using a PDG instead of a BN
%oli2: trying to shift emphasis to quantitative PDGs
% A key advantage of using a PDG instead of a BN is the 
%joe4: I don't think this buys us anything; I'm not even sure I
    %believe it
%oli5: I would consider cutting this to save space in a pinch, but I think it is a very important conceptual point. I don't understand why you don't believe this; it's litterally the emphasis of our first three examples.  
%oli5: I've introduced a "vfull" environment to make things gray, that will be in the arXiv version, but may need to be cut for space in the conference version.
\vfull{
A central thrust of \citeauthor{pdg-aaai}'s original argument for PDGs over BNs is their
    ability to describe two different probabilities describing a single variable,
    such as $\Pr(X|Y)$ and $\Pr(X|Z)$.
%joe8: I have no idea what the following sentence is saying
%oli19: effectively cutting, by moving it inside the "vfull" environment.  It makes more sense here, if we ever use the material with this extra context. 
The qualitative analogue of that expressiveness is precisely what allows us to capture functional dependence.
% is the ability to describe multiple probabilities about the same variable.
}

%oli16: making this better; there's a cleaner way to state!
\commentout{
\begin{linked}{theorem}{func}
    If $\Ar_0$ is a directed hypergraph and $\Ar := \Ar_0 \sqcup \{ \ed 1XY, \ed2XY \}$ is the same \hgraph\ augmented with two additional \arc s from $X$ to $Y$, then 
    \begin{enumerate}[label={(\alph*)}]
    \item If 
    $\mu \models \Diamond \Ar_0$ and $\mu \models X \tto Y$, then $\mu \models \Diamond \Ar$.
    \item
    If $\Ar_0 = \Ar_G$ for a directed acyclic graph $G$, 
    then
    $\mu \models \Diamond \Ar$ iff $\mu \models \Diamond \Ar_0$ and $\mu \models X \tto Y$. 
        \item 
        $\mu \models \Ar_0$ and $\mu \models X\tto Y$
        if and only if $\mu \models \Ar_0 \sqcup \{X \to Y\}^n$ for all $n > 0$.
    \end{enumerate}
\end{linked}
}


%joe8: definitions should not be buried inside theorem
%oli19*: I strongly disagree.  A more important rule of thumb is that definitions that you only use once should not be given headings and numbers.  For what it's worth, Spencer agrees with me, although he doesn't think it's a battle worth fighting.
%oli19*: Spencer agrees with me that this should go in the Theorem, because its entire purpose is to describe the result and it will never appear elsewhere in the document.  But, if you insist that it cannot go in the Theorem, I can put it above the theorem without a definition header
%joe14: It is generally recognized as bad writing style to put
%definitions in theorems.  (I have been doing this for a while ...)
%It's fine with me to leave out the definition number, although I
%don't see what you gain from doing so.
%oli20: I don't view it as a full-blown definition; it's essentially just a "let binding", which I encounter often in theorem statements. By not defining it globally with a number (and even moreso by bringing the binding inside the theorem statement) what one gains is a less polluted namespace, and an assurance that you don't need to hold onto this definition any longer once you finish reading the theorem. 
%spencer1: I personally feel it's better inside the theorem.
% \begin{defn}
Given a \hgraph\ $\Ar = (\N, \Ar)$, 
$X, Y \subseteq\N$,  and a natural number $n \ge 0$,
%joe8*: I have some latex bug here, but I don't know what it is, since
%I just copied the text below.
%oli19: fixed by replacing "##" with "#" as we're not in a linked environment anymore
\gdef\ArXY#1{%
        % \Ar_{X{\to}Y}^{(+##1)}
        % \Ar\genfrac[]{0pt}2{+##1}{X{\to}Y}
        % \Ar{\sqcup}\genfrac{}{}{{0pt}{}{+##1}{X{\to}Y}
        \Ar{\,\sqcup}\genfrac{}{}{0pt}{}{(+#1)}{X{\to}Y}
        %\Ar_{X{\to}Y}^{+##1}
        % \Ar + {(\ed{}XY)}^{##1}%
        % \Ar + {X\smash{\xrightarrow{\!(##1)\!}}Y}
    }%
    let $\ArXY n$ 
%joe8
    %    denote the \hgraph\ resulting by augmenting $\Ar$ with $n$
        denote the \hgraph\ that results from augmenting $\Ar$ with $n$
    additional (distinct) \arc s from $X$ to $Y$.
%oli19:
% \end{defn}
%oli16:
\begin{linked}{theorem}{func}
    %joe8: moved this above
    %oli19: brought old version together; I'd really like to revisit this, but I'll keep your version for now.
    \commentout{
    Let $\Ar = (\N, \Ar)$ be any \hgraph, and $\mu$ a distribution over its nodes. 
    Given $X, Y \subseteq \N$ and a natural number $n \ge 0$,
    \gdef\ArXY##1{%
        % \Ar_{X{\to}Y}^{(+##1)}
        % \Ar\genfrac[]{0pt}2{+##1}{X{\to}Y}
        % \Ar{\sqcup}\genfrac{}{}{{0pt}{}{+##1}{X{\to}Y}
        \Ar{\,\sqcup}\genfrac{}{}{0pt}{}{(+##1)}{X{\to}Y}
        %\Ar_{X{\to}Y}^{+##1}
        % \Ar + {(\ed{}XY)}^{##1}%
        % \Ar + {X\smash{\xrightarrow{\!(##1)\!}}Y}
        }%
    let $\ArXY n$ denote the \hgraph\ resulting by augmenting $\Ar$ with $n$ additional (distinct) \arc s from $X$ to $Y$. For all $\mu$ and $\Ar$, we have that 
    }
    %oli21: since this is literally the same as the above, we can save a line and some attention by cutting this.
    % Given a \hgraph\ $\Ar = (\N, \Ar)$ with $X, Y \subseteq \N$ and
    % distribution $\mu$  over its nodes,  
    %
    \begin{enumerate}[label={(\alph*)}, itemsep=0pt,topsep=0pt,parsep=0.4ex]
    \item 
    % $\mu \models X \tto Y$ and $\mu \models \Diamond \Ar$    
    $\mu \models X {\tto} Y \land \Diamond \Ar$    
    ~~if and only if~~
    %oli16: I originally put this, but it's ambiguous depending on how you bracket the "for all"
    % $\mu \models \Diamond \Ar_{X{\to}Y}^{(+n)}$ for all $n \ge 0$. 
    $\forall n \ge 0.~ \mu \models \Diamond \ArXY n$ . 
    % $\mu \models \bigwedge_{n \ge 0} \Diamond \Ar_{X{\to}Y}^{(+n)}$.
    \item
    %joe8
    %      When $\Ar = \Ar_G$ for a dag $G$,
            if $\Ar = \Ar_G$ for a dag $G$, then
    % the conditions in (a) are equivalent to 
    $\mu \models X {\tto} Y \land \Diamond \Ar$ if and only if
    $\mu \models \Diamond \ArXY 1$.
    % and indeed $\exists n \ge 1.~\mu \models \Diamond \ArXY n$.
    
%joe8*: Putting this here is completely unacceptable.  You can't
%state a result that will be meaningless to a reader without knowing a
%definition that occurs two sections later.
%oli19: Spencer strongly agrees with you, and even I find what was there before to be in somewhat poor taste.  Let me try to take a stab at rewriting it without the forward reference.
%oli19*: new material; this is a bit weaker but much simpler and more pallatable. 
\item 
% if $\Ar$ contains a \hyperarc\ without sources targeting $X$, 
% if there is some $a \in \Ar$ with $\Src a = \emptyset$ and $X \in \Tgt a$, 
if $\exists a \in \Ar$ such that $\Src a = \emptyset$ and $X \in \Tgt a$, 
% or a sequence of arcs that can be composed to produce such a \hyperarc, 
then $\mu \models X {\tto} Y \land \Diamond \Ar$
% if and only if
iff
 $\mu \models \Diamond \ArXY 2$.
\commentout{
    \item When
    % $\Ar \rightsquigarrow \{ \ed{}{}\{X\}\}$, 
    % some sequence of hyperarcs of $\Ar$ can be composed to unconditionally  $X$
    % $\Ar$ contains a sequence of hyperarcs that can be composed to target $X$ without any inputs
    $\Ar$ describes an unconditional mechanism for $X$
    (formally, when $\Ar \rightsquigarrow \begin{tikzpicture}[center base]
        \node[dpadinline] (X) {$X$};
        \draw[arr1,<-] (X) to[] +(0.6,0);
    \end{tikzpicture}$, a condition we will define in \cref{sec:monotone}),
    % the in (a) are equivalent to
    $\mu \models X {\tto} Y \land \Diamond \Ar$
    if and only if
     $\mu \models \Diamond \ArXY 2$.
}
    \end{enumerate}
\end{linked}
%joe8*: The discussion after the theorem is far too rambly,
% and is really unnecessary for the paper. I cut the rest
%oli19: I feel some of it is necessary.  Let me try to dramatically reduce it and produce a less rambly critical path.
\commentout{
Part (c) generalizes \cref{example:two-edge-det}. 
Based only on that example, it may seem unnecessary to ever add more than two arcs, as in part (a). 
Intuitively, if two independent randomized procedures (always) obtain the same value of $Y$ from input $X$, then neither one can be using its randomness, and so $Y$ is a (deterministic) function of $X$.
However, this intuition (based on randomized algorithms) implicitly assumes not only that the randomness $U_1$ and $U_2$ for the two mechanisms are not only independent, but also conditionally independent given $X$. 
Indeed the right hand side of (a) cannot be replaced by any fixed finite number of arcs without similarly strong assumptions on $\Ar$; see \cref{sec:func-counterexamples} for (non-trivial) counterexamples when $\Ar$ comes from a cyclic graph. 
}
%joe14: I can live with this, but what or whom is it necessary for?
%oli20: for people who want to understand the theorem more deeply, or people who might be convinced to work with QIM-compatibility as a notion of structure.  This is a critical aspect of the intuition that, from personal experience, is easy to get very wrong. 
%oli19*: replacement. 
Based on the intuition given after \cref{example:two-edge-det}, it may seem unnecessary to ever add more than two parallel \hyperarc s to ensure functional dependence in part (a). 
% Indeed, that is true if $X = \emptyset$ (part (c)).
However, this intuition implicitly assumes that the randomness $U_1$ and $U_2$ of the two mechanisms is independent conditional on $X$,
%joe14
%which we have warned may not be the case.
which may not be the case.
%joe16: Even if you leave in the counterexamples, it feels like
%overkill to discuss them here.  It's *way* down the tree.
%oli21: I think they're quite important. Not important enough to discuss here, but more than important enough to have a link to here. 
See \cref{sec:func-counterexamples} for counterexamples
%oli21: if it helps, I'll cut this next part. 
% when $2 < n < \infty$.
\unskip.


%oli22*: critial shard (from monotonicity section) on cycles goes here.
Finally, as alluded to above, \scibility\ gives meaning to cyclic structures,
    a topic that we will revisit often in \cref{sec:causal,sec:info}. 
We start with some simple examples. 
% We'll return to this in \cref{sec:info}, but for now we content ourselves with two simple examples. 
% As we discuss in \cref{sec:monotone}, 
\begin{example}
        \label{example:xy-cycle}
    Every $\mu(X,Y)$ is \cible\ with
    \begin{tikzpicture}[center base]
        \node[dpadinline] (X) {$X$};
        \node[dpadinline,right=1.2em of X] (Y) {$Y$};
        \draw[arr1] (X) to[bend left=13] (Y);
        \draw[arr1] (Y) to[bend left=13] (X);
    \end{tikzpicture},
    %oli22:
    % This is because this cycle is weaker than a \hgraph\ 
    % that can already represent any distribution, i.e.,
    because every distribution is \cible\ with
    \begin{tikzpicture}[center base]
        \node[dpadinline] (X) {$X$};
        \node[dpadinline,right=1.0em of X] (Y) {$Y$};
        \draw[arr1] (X) to (Y);
        \draw[arr1,<-] (X) to +(-0.68,0);
    \end{tikzpicture},
    %oli22: before monotonicity
    % $~\rightsquigarrow~$
    % \begin{tikzpicture}[center base]
    %     \node[dpadinline] (X) {$X$};
    %     \node[dpadinline,right=1.2em of X] (Y) {$Y$};
    %     \draw[arr1] (X) to[bend left=13] (Y);
    %     \draw[arr1] (Y) to[bend left=13] (X);
    % \end{tikzpicture}~. 
    and a mechanism with no inputs is a special case of one that can depend on $Y$.
    % and intuitively, allowing $X$ to depend on $Y$ only expands the set of compatible distributions.
    \qedhere
    % \]
    % $\to X \to Y$.
\end{example}
The logic above is an instance of an important reasoning principle,
    which we develop in \cref{sec:monotone}. 
Although the 2-cycle in \cref{example:xy-cycle} is straightforward, generalizing it even slightly to a 3-cycle raises a not-so-straightforward question, 
    whose answer will turn out to have surprisingly broad implications.
    % and has which we will return to later on. 
% We now apply it to 
%     a more complex example, which will play a surprisingly large role 
%     % in \cref{sec:info}
%     later on
%     \unskip.
     
\newlength{\cycleboxlen}
\newsavebox\cyclebox
\sbox\cyclebox{\begin{tikzpicture}[center base, scale=0.8]
        \node[dpad0] (X) at (0:.8) {$X$};
        \node[dpad0] (Y) at (120:.8) {$Y$};
        \node[dpad0] (Z) at (-120:.8) {$Z$};
        \draw[arr2] (X) to 
            % node[above right]{\small 1}
            (Y);
        \draw[arr2] (Y) to
            % node[left]{\small 2}
            (Z);
        \draw[arr2] (Z) to 
            % node[below]{\small 3}
            (X);
    \end{tikzpicture}}
\settowidth{\cycleboxlen}{\usebox{\cyclebox}}
\begin{wrapfigure}[5]{o}{\cycleboxlen}
    \vspace{-0.8em}
    \usebox\cyclebox
\end{wrapfigure}
% \begin{example}
\refstepcounter{example}
\label{example:xyz-cycle-1}
\textbf{Example {\theexample}.~} 
    What $\mu(X,Y,Z)$ are \cible\ with the 3-cycle shown, on the right?
    %oli22: change for new context
    % By monotonicity,
    By the reasoning above,
     among them must be all distributions consistent with a linear chain ${\to}X{\to}Y{\to}Z$. Thus,  
    any distribution in which two variables are conditionally independent given the third is compatible with the 3-cycle.
    Are there
     % any 
    distributions that are \emph{not} compatible with 
    this hypergraph? It is not obvious. We return to this
   %oli21: line shave
    %  question
     in \cref{sec:pdgs}. 
% \end{example}
\hfill$\triangle$

% \commentout{


Because \scibility\ applies to cyclic structures,  one might wonder if
    it also captures the independencies of undirected models 
    %oli6: not helpful here, and breaks the flow
    % (i.e., Markov Random Fields).
    \unskip.
%oli22: compressing:
% Undirected edges $A {-} B$ are commonly identified
% with a (cylic) pair of directed edges $\{ A{\to}B, B{\to}A\}$,
% as we have implicitly done in defining $\Ar_G$. 
Our definition of $\Ar_G$, as is common, implicitly identifies a undirected edge $A {-} B$ with the pair $\{ A{\to}B, B{\to}A\}$ of directed edges;
in this way, it naturally converts even an \emph{undirected} graph $G$ to a (directed) \hgraph. 
Compatibility with $\Ar_G$, however, does not coincide with any of the standard Markov properties
corresponding to $G$ \citep{koller2009probabilistic}.
This may appear to be a flaw in \cref{defn:scompat},
%oli21:
% (\scibility),
% but it is unavoidable.
%oli22
but it is unavoidable (see \cref{sec:monotone}) if we wish to also capture causality, as we do in the next section. 
% We explore the implications further in \cref{sec:monotone}. 


%oli19: I had thought you would like this because of the storytelling and the roadmap, but I'm OK with cutting it.
\commentout{In this section, we have seen how \scibility\ with a single graph can represent many (in)dependencies of interest. But we have only scratched the surface here; in the remaining sections, we will come to understand how \scibility\ works in general: its connection to causality (\cref{sec:causal}), to information theory (\cref{sec:info}), and a simple principle for reasoning about it (\cref{sec:monotone}). 
}


\section{%
    % Witnesses of Structural Compatibility as Causal Models%
    % Structural Compatibility
    %oli2: "witnesses as causal models" is the main takeaway,
    % but based on our conversations, we may also want to explicitly
    % touch on the other direction: how to convert a causal model to
    % a PDG. (It's obvious, but OK). 
    %
    %oli15:  trying to be a bit more general here.
    % Witnesses as Causal Models%
    \SCibility\ and Causality
    % \SCibility\ and Causality%
}
    \label{sec:causal}

%oli15*: illustration
% \begin{figure}
%     \begin{tikzpicture}
%         \def\hgraphtypes{
%             Hypergraphs/GSEMs,
%             Subpartitional/SEMs,
%             Partitional/Randomized PSEMs,
%             Graph-derived/Fully Randomized SEMs,
%             DAG-based/Acyclic SEMs}
%         \def\insetleft{0.2}
%         \def\insetright{0.1}
%         \def\insettop{0.5}
%         \def\insetbot{0.1}
%         \foreach \hg/\cm [count=\xi from 0] in \hgraphtypes {       
%             \draw[rounded corners=5pt] 
%                 ({2+\xi*\insetleft},{-1.5+\xi*\insetbot}) 
%                 rectangle ({6-\xi*\insetright},{1.5-\xi*\insettop});
%             \node[anchor=west,fill=white,inner sep=0pt] at ({2.3+\xi*\insetleft},{1.5-\xi*\insettop}) {\hg};
%         }
% 
%         % \draw[rounded corners=5pt] (2,-1) rectangle (5,1);
%         % \node[anchor=west,fill=white,inner sep=0pt] at (2.3,1) {Hypergraphs};
%         % % \fill[rounded corners=5pt,fill opacity=0.2,fill=blue] (-5,-1) rectangle (5,1);
%         % 
%         % \draw[rounded corners=5pt] (2.4,-.9) rectangle (4.6,.7);
%         % \node[anchor=east,fill=white,inner sep=0pt] at (2.7,.7) {Non superpartitional};
%         % 
%         % \draw[rounded corners=5pt] (2.8,-.8) rectangle (4.2,.4);
%         % \node[anchor=west,fill=white,inner sep=0pt] at (3.1,.4) {Partitional};
%         % 
%         % \draw[rounded corners=5pt] (3.2,-.7) rectangle (3.8,.1);
%         % \node[anchor=east,fill=white,inner sep=0pt] at (3.2,.1) {Unipartitional};
%         % 
%         % \draw[rounded corners=5pt] (3.6,-.6) rectangle (3.6,-.2);
%         % \node[] at (0,0) {DAG-based};
%     \end{tikzpicture}
%     \caption{nested classses of hypergraphs, and the corresponding classes of causal models to which they correspond}
%     \label{fig:hgraphmap}
% \end{figure}
%oli15:
% In this section, we spell out the connection between a witness of structural compatibility and causal models.
%oli18*:
\commentout{
In this section, we spell out the fundemental connection between causality and \scibility. In brief: $\mu \models \Ar$ if and only if it is possible for $\mu$ to arise from a causal model with dependency structure $\Ar$ (\cref{ssec:cm-arise}); furthermore, a witness of \scibility\ can be naturally converted to such a causal model. To make this precise, we must review some standard definitions from causality. }

% \Scibility\ interprets a hypergraph as a set of independent mechanisms. 
Recall that in the definition of \Scibility, each hyperarc represents an independent mechanism. 
%joe8*; this is the first I've heard of causality involving modeling a
%data generating process as a collection of independebnt mechanisms,
%and I work in the area.  Rewritten
%This should be reminiscent of causality, in
%which a data-generating process is modeled as a collection of
%independent mechanisms. Indeed, the concepts are directly connected:
%oli19*: I don't think this will be contentious because the fix is so simple, but I strongly object to the next sentence; a critical point we make in this paper is that all edges together, i..e, the hyperarc, represent the mechanism. An individual edge represents only the possibility of causal influence.
% Edges in a causal model are also viewed as representing independent (causal) mechanisms.  
Equations in a causal model are also viewed as representing independent
%oli21: I don't like "causal" here; it's already part of the model.
 % (causal) 
mechanisms.  
%oli21: not much content; densifying:
% This suggests a possible connection between \scibility\ and causal models. Indeed, as we now show, there is one.
This suggests a possible connection between the two formalisms, which we now explore.
We will show that \scibility\ with $\Ar$ means exactly that
a distribution can be generated by a causal model with the corresponding dependency structure (\cref{ssec:ssec:cm-arise}). Moreover, such causal models and \scibility\ witnesses are themselves closely related (\cref{sec:witness-to-causal-model}).
%joe8:reinststate.  Yoyu need soe luge
%oli19: I can't read the comment, but I am fully on board. 
%This section therefore establishes a causal grounding for \scibility.
In this section, we establish a causal grounding for \scibility.
To do so, we must first review some standard definitions.

%oli2: already said this
% To do so, we must first review some standard definitions.
%oli2: adding subsection for clairty; happy to remove later for space.
%joe5*: This should be  shortened,  you should use more standard
%notation (e.g., like in my book), and you should allow for cyclic
%oli6: this notation is pretty close to standard, but it has been modified slightly (replacing \V with \X and \R with \V) so that it is also compatible with the overwhelming standard in the graphical models literature.  I agree it should be shortened.
%oli6: what do you mean by "allow for cyclic"? What I do below does allow for cyclic models!
%oli6: removing header
% \subsection{Causal Primer}

%%% MATERIAL LIFTED FROM CAUSAL MODEL PAPER
\begin{defn}[\citet{pearl2009causality}]
        \label{defn:SEM}
    A \emph{structural equations model} (SEM) is a tuple
    %oli15: being more consistent about reserving \cal M for PSEMs
    % $\mathcal M = (\U, \enV, \mathcal F)$,
    $M = (\U, \enV, \mathcal F)$,
    % $\mathcal M = (\U, \X, \mathcal F, \V)$,
    % $M = (\U, \enV, \mathcal F, \mathcal I)$,
    where
    % $\V$ is as before,
    \begin{itemize}[left=0pt,itemsep=0pt,topsep=0pt,parsep=0.3ex]
    \item $\U$ is a set of exogenous variables;
%joe3*: It is ore standard to write \V for the endogenous variables.
%Why are you using \X?
%oli3*: because, the way I'm telling the story, this set
% of variables coincides with \X from earlier, so it will simplify
% our notation.  (When talking about grpahical models more generally,
% or even PDGs or BNs in particular, \X is the standard notation for
% the set of variables that correspond to the nodes of the graph.)
% In addition, I want to reserve \V for the "Values" of a variable;
% I like it better than \R, used in the causality literature. 
    %oli15: changing to \enV
    % \item $\X$ is a set of endogenous variables (disjoint from $\U$);
    \item $\enV$ is a set of endogenous variables (disjoint from $\U$);
    %oli15: was garbled; fixing. Also adding back in a few symbols, which really clarify things for me. 
    % \item $\mathcal F$ is an indexed set of functions consisting of an equation $f_Y : \V (\U \cup \enV - Y ) \to \V(Y)$ that determines is value as a function of the values of the other variables.
    \item $\mathcal F = \{ f_Y \}_{Y \in \enV}$ associates to each endogenous variable $Y$ an \emph{equation}
    $f_Y : \V (\U \cup \enV - Y) \to \V(Y)$
     that determines its value as a function of the other variables. 
%oli3: this last bit is no longer necessary, with my new approach 
\commentout{
    \item $\mathcal I$ is a set of allowable interventions,
    %oli2:
    % typically 
    typically taken to be the set of all strings
    of the form $X \gets x$
    %oli2:
    % for $X \in \enV$
    for $X \subseteq \enV$
    and $x \in \V(X)$.
}
        \qedhere
    \end{itemize}
\end{defn}

%oli3: the definition in this sentence may not be necessary; graying out for now
%joe4: I would prefer to cut the definition.
%oli5: as you wish
\commentout{
Given a SEM $M = (\U, \enV, \mathcal F)$, and a joint setting $\mat u \in \V(\U)$ of exogenous variables, let
\[
    % M(\mat u) :=
    \mathcal F(\mat u)
    := \Big\{
        \mat x \in \V\! \enV ~\Big|~
            \forall Y \,{\in}\, \enV.~
            f_{Y}\big(\mat u,~ \mat x[ \enV {-} Y ]\big) = \mat x[Y]
    \Big\}
        % \quad \subseteq \V\! \enV
\]
be the set of joint endogenous variable settings that are consistent with $\mat u$, according to the equations $\mathcal F$.
}
%
% A SEM is called \emph{acyclic}
% if for every $\mat u \in \V \U$, there is a total order $\prec_{\mat u}$ on $\enV$ such that, $f_{X}(\mat u, \mat z)$ does not depend on $\mat z[Y]$  when $X \prec_{\mat u} Y$,
% and \emph{structurally acylic}%
%     %
%     \footnote{for Joe: this is important because structural acyclicity,
%         which is standard in some settings, is the qualitative proeprty, and
%         the one that leads to a correspondence with BNs.}
%     %
% if $\prec_{\mat u}$ can be chosen
% so that it does not depend on $\mat u$.
%oli6: taking your approach even further:
In a SEM $M$, a variable $X \in \enV$ \emph{does not depend} on $Y \in \enV\cup\U$ if $f_X(\ldots, y, \ldots) =
f_X(\ldots, y', \ldots)$ for all $y, y' \in \V(Y)$.
%oli6: moving up from next section, and significantly compressing; it belongs here.
Let the parents $\Pa_{M}(X)
 % \subseteq \V(\enV \cup \U - X)
$
%joe7
%of $X$ to be the set of variables on which it depends.
of $X$ be the set of variables on which $X$ depends.
%joe4
%A SEM is called \emph{acyclic}
% A SEM is \emph{acyclic}
%oli6:
$M$ is \emph{acyclic}
%oli6: 
% if there is a total order $\prec$ on $\enV$ such that
%joe4: Why do f_X need arguments?  What is their role in the
%definition.  What is z[Y]?
%$f_{X}(\mat u, \mat z)$ does not depend on $\mat z[Y]$  when $X \prec Y$.
%oli5: z[Y] is the value of Y in joint setting z. We'll need this notation
% eventually, either way.  I prefer my version, but let's go with yours. 
%oli5: fixing up your version so it compiles (and removing duplicate parens):
% $f_{X}$ does not depend on $Y$ (i.e., $f_X(\ldots, y, \ldots) =
% f_X(\ldots, y', \ldots)$ for all $y, y' \in \V(Y)$) when $X \prec Y$.
%oli6: a few possibilities:
% if $X \prec Y$ implies $X$ does not depend on $Y$. 
% if this is the case whenever $X \prec Y$. 
% if $X$ does not depend on $Y$ whenever $X \prec Y$. 
%oli16*: this is an ambiguous definition, and arguably wrong, because of how it treats exogenous variables
% if $\Pa_M = \Pa_G$ for a directed acyclic graph $G$. 
% iff there is a total order $\prec$ on $\enV$ such that $X$ does not depend on $Y$ when $X \prec Y$. 
iff $\Pa_M(X) \cap \enV = \Pa_G(X)$ for some dag $G$ with vertices $\enV$. 
% iff there is a total order $\prec$ on $\enV$ such that $X$ does not depend on $Y$ when $X \prec Y$. 
% if the negation of this relation is a directed acyclic graph. 
%joe4: I on't know what it means to determine the value of each
%exogenous variable according to this order.  
%oli5: I thought it was obvious that this means you go one by one through the variables in this order, and determine them by applying their corresponding functions.  If this is not obvious, I don't see how your replacement statement is obvious.
%joe4: Moving up a level, it
%seems ot me that you're making things too complicated here.
%oli5: I disagree that what I wrote is more complicated (I think it
% gives more intuition, yet is shorter). We also still need to bind
% f $f_\enV$ for several purposes (e.g., the next sentence). But this is
% not the hill I want to die on, so I'll start from your version.
% (I've consolidated my version for reference below.)
\commentout{
    It is easy to see that,
    by determining the value of each exogenous variable according to this order,
    $\mathcal F(\mat u)$ is a singleton for acyclic models, in which case
    $\U$ determine $\enV$ via a function $f_{\enV} : \V (\U) \to \V(\enV)$.
}
% It is easy to see that,
% in an acyclic SEM, 
In an acyclic SEM, it is easy to see that 
a setting of the exogneous variables determines the
values of the endogenous variables
%oli14: reminding of this notation, because it will come in handy to refer to this property more briefly than the English above. 
(symbolically: $M \models \U \tto \enV$).
%oli5: adding this, because we use the notation below. (I still like my approch better, but I can live with this once we define $f_{\enV}$.)
%oli5.2: the extra bit I added was clunky and long. Making it shorter, but like I had before.
% ; let $f_{\enV} : \V (\U) \to \V(\enV)$ be a function capturing this determination.
%oli6:
% by a function $f_{\enV} : \V (\U) \to \V(\enV)$.
%oli8: no longer used; I know you dislike it; we can now safely delete.
% let  $f_{\enV} : \V (\U) \to \V(\enV)$ capture this relationship.
% \begin{remark}
%     This definition depends heavily on variable identity.
%         This is an artificial limitation, and due to the fact that there is precisely
%         one equation per variable.
%     We illustrate with an example construction.
%     Given a SEM $(\U, \enV, \mathcal F)$,
%     let's construct a new SEM $(\U, \enV \sqcup \enV, \mathcal F')$, with two copies of each endogenous variable $X \in X$: one which we call $X.0$, and one which we call $X.1$.
%     How shall we set the equations?
%     \[
%         F'_{X.0}(\mat u, \mat x.0, \mat x.1) = F_X(\mat u, \mat x.0)
%     \]
%     Intuitively, all of these should be true.
%
%     \TODO
% \end{remark}
%oli8: merging paragraphs.
%
A \emph{probabilistic SEM} (PSEM)
%oli2: compressing and changing \nu to P to avoid confusion. (\nu
% changed to P later on silently without marking.)
% $\mathcal M = (\U, \enV, \mathcal F, \nu)$ is a SEM in which
%oli8: shortened
% $\mathcal M = (\U, \enV, \mathcal F, P)$ 
$\mathcal M = (M, P)$ 
is a SEM,
%joe8: As I've said repeatedly, yoyr quest for precision will (a)
%confuse the reader and (b) is misguided.  If you can explain to
%Spencer why you need this and he's convinced, then I'd be willing to
%diss it further.  If your goal is to snow the reader, you're doing well.
%oli19: I've convinced Spencer that it creates a big technical problem if we don't assume variables are measurable.  However, I'm willing not to say it out loud here if we leave in the parenthteical early on when we defined a variable as taking on a value from "a (measurable) set of possible values"
%joe14*: One author convincing a second author of something when the
%third author is still unconvinced is absolutely, positively not the
%basis for adding it to a paper.  Do you really not see how absurd
%your coment below is?  Since it seems you're reoved "measurable", it
%seems that we've converged.
%oli20: I've removed "measurable" here, and put "(finite)" earlier in section 2 (because it will make the discussion about entropy a thousand times simpler).  I mentioned Spencer here not as a basis for forcefully changing the paper, but rather because above you say "If you can convince Spencer, I'd be willing to discuss it further". 
% in which $\V(Z)$ is measurable for all $Z \in \U \cup \enV$,
% (in which all variables have measurable domains)
together with a probability 
%oli21: shaving
% measure
$P$ over the exogenous variables.
%
% If $M = (\U, \enV, \mathcal F, \nu)$ is an acyclic PSEM,
%oli6:
\commentout{
If $\cal M$ is acyclic, then it
    determines a 
    %oli6:
    % joint distribution over the endogenous variables $\enV$,
    distribution over the endogenous variables $\enV$,
    by the pushforward of $P(\U)$ through
    %oli5: compressing
    % the function $f_{\enV}$.
    $f_{\enV}$.
}%
%joe4*: I don't understand your notation, and find it *far* too complicated.
%oli5: I fixed a typo that should help you understand the notation. I don't think the notation is at all complicated here, and for people like me, the notation is better than the words.  But I'm trying to cut things, so and what you wrote is shorter, so I'll write something based on your version.
\commentout{
    In this case, $P$ extends uniquely to a joint distribution
    $\nu(\U, \enV) = P(\U) \delta\! f_{\enV}(\enV \mid \U)$, where the notation $\delta g(B|A)$ describes the (deterministic) cpd
    %oli1:
    % that corresponds to the function $f : \V \! A \to \V \! B$.
    %oli5: here's one reason for confusion: a typo in which I replaced f with g.
    % that corresponds to the function $f : \V (A) \to \V (B)$.
    that corresponds to the function $g : \V (A) \to \V (B)$.
}%
%oli5: your version, which I've improved
%oli6: and then later merged with the previous setnence
{%
    %oli5: What is the point of "easily"?  Also, this distribution over the exogenous variables has a name. 
    % It easily follows that a distribution on the exogenous variables $\U$ can be
    %oli14: previously broken sentence. Adding a sensible first part. 
    When $\mathcal M \models \U \tto \enV$ 
    %oli21: can't afford the extra reminder; we just defined this
    \vfull{%
    (such as when $M$ is acyclic)%
    },
    the distribution
    %oli6:
    % It follows that $P(\U)$  
    $P(\U)$
    %oli5:
    % extended uniquely to a distribution on $\U \times \enV$.    
    extends uniquely to a distribution over $\V(\enV \cup \U)$.    
}%
%joe4*: what does "correspond" mean?  You haven't defined it.  (I know
%what you mean, of course, but a reader can't read your mind.  Rewrote
%oli5: I don't understand why you're asking such questions. I define it immediately below. Also, there's an easy fix here: replace "correspond" with "extend". Or, as you do, below, with "induce".
% A cylic PSEM, however, may correspond to more than one such distribution, or none at all.
A cylic PSEM, however, may induce more than one such distribution, or none at all.
% (for example, if $\U = \emptyset$, $\enV = \{X, Y\}$,  $F_X(x) = x$, and $F_Y(x) = x$), or none at all (when $\mathcal F(\mat u) = \emptyset$, but $\mu(\mat u) > 0$).
%oli5: Again, your version is the 
%joe4: 
% This is not, in general, the case in a cyclic PSEM.  
%oli16: merging paragraphs
%
%joe4: added next sentence
%oli5: why is "induce" any more precise than "corresponds to"? 
%oli5: why is the distribution $P$ special? It's the whole PSEM that does this.
%oli5: why specify "cyclic?" It's true even for acyclic PSEMs; we're now speaking in general.
%oli5: missing the type of what the distributions are over, which is important
% In general, a distribution $P$ on the exogenous variables $\U$ in a
% cyclic PSEM induces a (possiby empty) convex set of distributions.
%oli6:
% In general, a (P)SEM
%oli21:
% In general, a (P)SEM $\cal M$
In general, a PSEM $\cal M$
induces a (possiby empty) convex set of distributions over $\V(\U \cup \enV)$.
%oli2: new paragraph break above, and also:
% So, in general, a PSEM corresponds to a convex set of joint distributions over $\enV \cup \U$.
%joe4: cut
% To simplify the presentation of our results, we now
%  introduce new notation to formally capture that correspondence.
% In general, a (P)SEM corresponds to a convex set of joint distributions over $\enV \cup \U$.
% This set is defined by two (linear) constraints:
This set is defined by two (linear) constraints:
%oli2: updating so it applies to both. Put parens around (P) above,
% and adjusting all of this.
%     the marginal probability over $\U$ must be the one specified by the PSEM, and
%     the equations must hold with probability 1.
    the equations $\mathcal F$ must hold with probability 1, 
    and
    %oli21:
    \vfull{, in the case of a PSEM,}
    the marginal probability over $\U$ must
    %oli7:
    % be the specified one.    
    equal $P$. 
% Formally, for a PSEM $\mathcal M = (\U, \enV, \mathcal F, P)$, define
%joe4*: this notation will only confuse th rader.  If you're going to
%keep it (and I see no reason to), you have to explain it better.
%oli5: I can run with your replacement explanation in English, but I find the symbolic version far more satisfying and precise. There is value to having both.  English is full of ambiguities, and I don't want definitions like this one to rest on interpretation. I've already compiled it to math, for the reader who would rather read math than English.  
\vfull{
Formally, for a PSEM $\mathcal M = (M, P)$, define
%oli2: slight notational change that might better suit our uses
% $\SD[\big]{\mathcal F, \nu} := $\\[-0.2ex]
$\SD{\mathcal M} := $
% \\[-0.2ex]
% \\
% \vspace{-1.5ex}
% $\displaystyle
\[
    \bigg\{
    %joe3
       \nu {\,\in\,} \Delta\!\V(\enV \cup \, \U)
    %oli3: I agreed to give you your parens for the arguments of \V,
    % but I still object to putting parens around the arguments to 
    % \Delta.  In this particular case, the spacing only barely works as is, and the parens make the left half twice as difficult to parse.  The \Delta also fits very nicely well next to the \V.  I suggest we dont think of \Delta$ as being meaningful on its own; define "\Delta\V" together as meaning distributions over values.
    %oli3: reverting to above
        % \nu {\,\in\,} \Delta(\V(\enV \cup \, \U))
            \,\bigg|\!
        \begin{array}{l}
            %oli2: swapped order of two equations to match English
            \forall Y \!\in\! \enV.~~
            % \text{ and }
            % \forall Y \in \enV.
            % \\
            \nu\Big( f_Y(\mathcal U,
                % \enV {\setminus}\{Y\}
                \enV {-} Y
            ) \,{=}\, Y\Big) \,{=}\, 1
            % \\ ~\text{for all $Y \in \enV$}
            ,
            \\[0.2ex]
            % \qquad
            \nu(\U) = P(\U)
        \end{array}\!\!
        \bigg\}
% $
% \vspace{-1ex}
\]
and define $\SD{M}$ for an ``ordinary'' SEM $M$ in the same way,
    except without the constraint involving $P$.
% This definition might be clarified if we unpack it slightly
% The final line may be difficult to read;
% To unpack the last line:
%oli2: expanding
% To unpack the first constraint:
%oli3: essentially reverting
% The other constraint may be harder to parse. Unpacking it, 
To unpack the other constraint,
% $f_{Y}(\U, \enV \setminus \{Y\})$
$f_{Y}(\U, \enV - Y)$
is a random variable
%oli2: should we hide this, because it's the only sample space in sight?
%joe3: I don't feel strongly about it
on the outcome space $\V(\enV,\U)$,
and that it
%joe3*: I find this really confusing.  I have never thought of Y as a
%random variable that takes inputs from \U and X-\{Y\}.  I can't
%imagine that anyone other than you has either.   If I were to think of it
%as a function all, it would be a function of the context.  At a bare
%minimum, this needs much more comment.
%oli3: What else is there to say?  You're also making it more complicated than it needs to be.  Every function f_Y is a function of \U and \enV, and so all of these functions are variable for the same sample space \V(\U,\enV) which is the same as the sample space of the distribution \nu.  It's not that each function is a special random variable with a restricted sample space. In any case, your way of thinking about it is totally compatible with this one. As a function of the context, f_Y is a function \U -> (\enV -> Y). But we still need to say that, according to the distribution $\nu$, the equation always holds.  But now you need to specify "always" with respect to the marginal on \enV.  I think this approach to thinking of what's going on is not just clunkier, but also less standard. It's far cleaner just to use the sample space \V(\U,\enV) throughout. 
has the same value as $Y$ is an event
 % of interest that must always occur
which, according to the equation $f_Y$,
must always occur.
}%
% In a probabilistic context such as this one, a (set of) variable(s) $\mat X \subseteq \U \cup \enV$ is also a \emph{random variable}, i.e., a function from the sample space $\Omega = \V(\enV \cup \U)$ to the range $\V \mat X$, by projection.
% This makes $F_X(\mathcal U, \enV - \{ X \}) = X$,
% an event $\{ (\mat u, \mat x) :  \mat u \in \V\U, \mat x \in \mathcal F(\mat u) \} \subseteq \Omega$, and hence we can require that it has probability 1, which is the second condition above.
%%% END MATERIAL LIFTED FROM CAUSAL MODEL INTRO %%%
Given a 
%oli21: we don't use the generality anymore for ordinary SEMs, I think
% (P)SEM 
PSEM
 $\mathcal M$, let $\SD{\mathcal M}$ consist of all 
joint distributions $\nu(\U, \enV)$ that satisfy the two constraints above
%cutting down
\vfull{%
%oli21: this is clearly implied,  not helping the reader, and long (and unused, I think)
(or just the first of them, in the case of a non-probabilistic SEM).
%oli21: this is not as useful as the next sentence, which subsumes it; cutting for space. 
$\SD{\cal M}$ can be thought of as the set of distributions compatible
wth $\cal M$. It
}%
%oli21: merging sentences
\unskip; this set
captures the behavior of $\cal M$ in
the absence of interventions.  
%oli14: defining what it means to "arise", because it will be easier and shorter this way.
A joint distribution $\mu(\mat X)$ over $\mat X \subseteq \enV \cup \U$ \emph{can arise from} a (P)SEM $\mathcal M$ iff there 
is
% exists
some $\nu \in \SD{\mathcal M}$ whose marginal on $\mat X$ is $\mu$.
%oli16: some extra words, for clarity, brought on by Spencer's comment
%joe8*: This is a distraction.  Readers won't havea clue what youy
%eabn y "set-of-distribution semantics".  You should have to explain
%it.  I cut this
%oli19: OK; I'll leave it cut, but we may want to revisit it depending on how much we talk about PDGs.
\commentout{
We remark that a (P)SEMs $\cal M$ is naturally a special case of a PDG, and that $\SD{\mathcal M}$ is the set-of-distributions semantics of that PDG. 
}
% Keep in mind that $\SD{\cal M}$ captures the behavior of $\cal M$ in the absence of intervention. 
% While much of causality is about the effects of intervention, .
%oli5: this is not an efficient use of space; moved the critical part above.
\commentout{
    For a SEM $M$, we take $\SD{M}$ to consist of all 
    %oli5:
    % distributions in $\SD(M,P)$ for some distribution $P$ on the exogenous variables of $M$; 
    distributions $\nu \in \SD{M,P}$ for some distribution $P$ on the exogenous variables of $M$; 
    equivalently
    %oli5:
    % SDM $M$ consists of all
    $\SD{M}$ consists of all
    distributions that satisfy
    the first of the two constraints above.
}



%oli7
% after all, our definition can capture such dependencies without needing to explicitly include these varaibles, yet still captures the independencies between them.
%oli9: no longer needed; reference to incorrect construction from bygone 3.1.
% in this case, only one \arc\ for each equation is necessary, and the noise variables can be omited. 
%oli5: 
% Much more interestingly,
%     in this case the translation also works in the opposite direction,
%     allowing us to interperet witnesses of \scibility\ as causal models.
%oli7: 
% But much more interesting is the converse: 
%     witnesses of \scibility\ can be converted to randomized causal models.
%oli9: wrong transition, now that it's no longer in section 3.1
\commentout{
    %oli8*: trying to bring out why this is important:
    % This connection is particularly interesting when stated in reverse: witnesses of \scibility\ can be converted to randomized causal models.
    The connection is particularly important in the other direction: witnesses of \scibility\ can be converted to randomized causal models,
    %oli8*: ... which is what the rest of the section is about!
    as we now show.
}
%oli14:
% For randomized causal models, the set of distributions $\SD{\mathcal M}$ is important because it captures the behavior of the causal model $\cal M$; for instance, a a distribution over $\V(\enV,\U)$ can be used to ascribe probabilities to causal formalas, as we now describe.

% So far, we have describe the semantics of causal models;
We now review the syntax of a language for describing causality. 
%joe16
%A basic causal formula is the of the form $[\mat Y {\gets} \mat
%    y]\varphi$, where $\varphi$ is a boolean expression over the
A \emph{basic causal formula} is one of the form $[\mat Y {\gets}
    \mat y]\varphi$, where $\varphi$ is a Boolean expression over the
endogenous variables $\enV$, $\mat Y \subseteq \enV$ is a subset of them, and $\mat y \in \V(\mat Y)$.
% A joint setting $\omega \in \V(\enV)$, or a \emph{world}, determines the truth of $\varphi$; if $\varphi$ is true in world $\omega$, we write $\omega \models \varphi$.
The language
%oli21: we never use the notation
% $\mathcal L(\enV, \U)$ then consists of all boolean combinations 
then consists of all Boolean combinations 
 of basic formulas.
% We write $\omega \models \varphi$ if $\varphi$ is true in the world $\omega \in \V(\enV)$. 
% We now define a truth relation with two important base cases. 
In a causal model $M$ and context $\mat u \in \V(\U)$, 
a Boolean expression $\varphi$ over $\enV$ is true iff it holds
for all $(\mat u, \mat x) \in \V(\U,\enV)$ consistent with the equations of $M$.  
Basic causal formulas are then given semantics by
%oli15:saving space with inline
% \[
$
    (M, \mat u) \models [\mat Y {\gets} \mat y]\varphi
%oli15: 
    % \quad\iff\quad
$ iff $
    (M_{
    \mat Y \gets \mat y}, \mat u) \models \varphi,
%oli15: 
% \]
$
%oli21: compressing / smoothing
% where $M_{\mat Y \gets \mat y}$ is the SEM that results from changing the equation for every $Y \in \mat Y$ to
where $M_{\mat Y \gets \mat y}$ is the result of changing each $f_Y$, for $Y \in \mat Y$, to
%joe16*: YUCK!  First, this is incredibly clunky.  Second, there will
%be a nontrivial fraction of the NeurIPS community that does not deal
%with \lambda.   Why confuse them? (Note that even it's 10% of the
%readership that is unfamiliar with lmabda notation -- and I suspect
%it's much more -- it's not worth using.) Please get rid of this
%notation everywhere you use it.
%oli21: ok; I'll remove \lambda notation.  I think what you proposed is far worse (especially given that we didn't define that surface level syntax for equations), but I think the following will make us both happy. 
% the constant function $\lambda \mat s. \mat y[Y]$,
the constant function $\mat s \mapsto \mat y[Y]$, 
%oli21: declunkifying
% where $\mat y[Y]$ is the value of $Y$ in a joint setting $\mat y$ of a set of variables that includes $Y$. 
which returns (on all inputs $\mat s$) the value of $Y$ in the joint setting $\mat y$.
%oli21: here's what you had before. Note that Y := y has not been defined for equation notation, and that it doesn't define the notation \mat y[Y], which will be critical in several other places. 
% (the constant function) $Y := y$, where $y$ is the component of $Y$ in $\mat y$.
% returning the appropriate components of $\mat y$. 
%oli21: not sure the detail is necessary.
\vfull{%
From here, the truth relation can be extended to arbitrary causal formualas by structural induction in the usual way.
    \unskip\footnote{
    $M\! \models\! \varphi_1 \land \varphi_2$ iff $M \!\models\! \varphi_1$ and $M \!\models\! \varphi_2$;
    $M \!\models \lnot \varphi$ iff $M \!\not\models \varphi$. 
    }
}%
The dual formula
$\langle \mat Y {\gets} \mat y\rangle \varphi := \lnot [\mat Y{\gets }\mat y]\lnot \varphi$
is equivalent 
%oli17: needed, I think
to $[\mat Y {\gets} \mat y]\varphi$
in SEMs where each context $\mat u$ induces a unique setting of the endogenous variables \cite{halpern-2000}.
A PSEM $\mathcal M = (M, P)$ assigns probabilities to causal formulas according to $\Pr_{\mathcal M}(\varphi) := P(\{ \mat u \in \V(\U) : (M, \mat u) \models \varphi \})$.
%By supplying values of the variables $\U$, one can also determine the truth of counterfactual statements.  
%To make this precise, we need some more notation.
% If $\mat X$ is a set of variables and $\mat x \in \V(\mat X)$, we write $\mat x[\mat Z \gets \mat z]$ for the joint setting of $\mat X$ obtained by starting with $\mat x$, and setting the values of $\mat Z \subseteq \mat X$ to $\mat z$. 
% For $\mat Y \subseteq \enV$, write $U_{\mat Y}$ for the set of variables $\{ U_Y : Y \in \mat Y\}$.
% Let $\mathrm{do}(\mat Y{\gets}\mat y) := \cap_{Y \in \mat Y} (\hat U_Y = \lambda z.~\mat y[Y])$ denote the event in which each variable $\hat U_Y$ takes on the constant function that ouputs $\mat y[Y]$ for all inputs. 
% Let $\mathrm{do}(\mat Y{\gets}\mat y) := \cap_{Y \in \mat Y} ( \forall z.~f_Y(U_Y, z) = \lambda z.~\mat y[Y])$ denote the event in which each variable $\hat U_Y$ takes on the constant function that ouputs $\mat y[Y]$ for all inputs. 
% \[
% % \Pr_\nu \Big(
% %     [\mat Y \gets \mat y]\varphi
% %     \Big)
% %     := 
%     \nu\Big(\{ \omega : \omega \models \varphi\} ~\Big|~ 
%         \mathrm{do}(\mat Y{\gets}\mat y)
%         % \{ U_Y = \lambda \textunderscore.y \}
%         \bigcap_{Y \in \mat Y}\{ U_Y = \mat y[Y] \}
%         \Big),
% 
% \]
% new subsection?
% \subsection{An alternative definition of \scibility\ based on causal models}



%oli9*: cutting this section; based on flawed direction of Thm 2. 
\commentout{
%oli5:
% \subsection{Explicitly Capturing Causal Models with directed hypergraph s}
\subsection{Capturing Causal Model Structures with directed hypergraph s}
%oli6:
% One immediate consequence of \cref{theorem:func} is that directed hypergraph s
One consequence of \cref{theorem:func} is that directed hypergraph s
can capture the structural aspects of causal models
%oli6:
% (which are essentially just functional dependencies). 
(which are functional dependencies). 
%oli6: removing paragraph break
%
%oli6: thanks for pointing out the connection with the above. Upon revisiting, I'm moving the entire discussion there. 
\commentout{
More precisely, given a SEM $M = (\enV, \U, \mathcal F)$,  
define the parents
%oli6:
$\Pa_{M}(X) $
 of endogenous variable $X \in \enV$ to be
the set of variables 
%oli5:
% that $X$ depends on 
on which $X$ depends 
 (recall that the notion of $X$ not depending on $Y$ was defined above).
}%
%joe4: unnecessary; defined above
\commentout{
can affect $X$ directly, i.e.,
$\Pa_{M}(X) := $
% \vspace{-1.6ex}
\[
    % \Pa_{\mathcal M}(X) :=
    \bigg\{
        Y \in \U \cup \enV
     ~\bigg|~
        \begin{array}{@{}l@{}}
            \exists \mat z \in \V(\U {\cup }\enV - \{X,Y\}), 
            \exists y,y' \in \V(Y).\\ 
            f_X(\mat z, y) \ne f_X(\mat z, y')
        \end{array}
    \bigg\}.
\]
}%
% Then, we can define
% we can define a directed hypergraph with nodes $\N := \enV \cup \U$
% and \hyperarc s
%joe3
%Because each equation represents a functional dependence, in light
%oli6: inverting sentence
% Because each equation represents a functional dependency, in light of \cref{theorem:func}, we need two 
More precisely, given a SEM $M$,
%oli6:
% copies of each \arc\ to get a \hgraph\ 
%joe6: what does "along each equation" mean.
%oli7: I feel this is both extremely intuitive and also precisely defined with symbols, so it needn't be spelled out in great detail in English. Still, let me see if I can do it compactly for you.
% we need two parallel \arc s along each equation to get a \hgraph\ 
we need two parallel \arc s with the dependency structure each equation, to get a \hgraph\ 
%joe3*: Please don't use undefined notation ("2 x").  Moving up a
%level, I'm lost here.  I don't see what you're trying to do or why
%you're doing it.
%oli3*: First, I'll address the motivation. I'm showing that the dependencies of the functional causal model, which I've extracted in the form of \Pa, can be precisely captured by taking two parallel arcs along each equation (recall that Theorem 2 shows we capture dependencies with parallel arcs).  
%oli3: @notation: modulo the (obvious) specification of sources and 
% targets, this notation lines up with the usual cartesian product on 
% sets.  In addition, I spelled it out in words, so I was hoping there
% would be no need to go into greater depth (it's not something I want
% to spend the reader's time on.) If this is unacceptable, then
% can look into alternatives.  What I've done below is to double the
% index set with a disjoint union. It has the benefit of being slightly
% more precise and taking up similarly little space, but the drawback of not 
% emphasizing the "two copies of each arc", which is what I'm looking 
% to emphasize. 
$
    \Ar_{M} := 
    %oli3: as requested:
    % 2 \times
    \big\{
        %oli6:
        % \Pa_{M}(X) \to X
%joe6*: you may understand this notation, and I now understand after
%our discussion yetesrday, buta typical reader who cannot read your
%mind will not understand it.  It comes out of the blue.  It needs to
%be explained
%oli7:  !! annoyed that the changes I made to fix your confusion created different confusion.  Let me see what I can do.
    %     \Pa_{M}(X) \rightrightarrows X
    % \big\}_{X \in \enV}
        \Pa_{M}(X) 
        \mathrel{\raisebox{-2pt}{$\xrightarrow{(\iota,X)}$}}
        X
    \big\}_{(\iota,X) \in \{0,1\} \times \enV}
    %oli6: with double arrow notation, do the simpler thing above
    %oli3: doing this instead
    % \big\}_{X \in \enV \sqcup \enV}
    %oli3: if you want full precision but the po  ssibility of confusion, we can instead do the following. The idea is that X \sqcup X (which is the same as 2 \times \enV) is defined as all pairs (i,X) where i \in {0,1}, and X \in \enV, and the target/sources depend on X but not i. 
    % \big\}_{(i, X) \in \enV \sqcup \enV}
$
%oli3:
% that represents all of the (in)dependencies of $M$. 
%oli6: these are just dependencies, so this might clairfy somhow
% representing all of the (in)dependencies of $M$. 
representing precisely the structural information in $M$.  
%joe7*: You might think that what you wrote is intuitive.   Maybe it
%is to you.  But let me assure you that yuo're not typical, and it
%won't be intuitive to many readeres (who will also find your notation
    %extremely hard to parse.  The next sentence should be much more
    %accessible to readers
What this notation is saying is that for each node $X$ in the causal
model $M$, there is a pair of hyperarcs in $\Ar_M$, each one going
from the parents of $X$ to $X$.

%oli2: If we're going to go through this translation, we should probably
% prove something about it. I think this is the obvious thing. 
\begin{linked}{prop}{causal-model-structure-capture}
    %joe4: added next line
    If $M$ is a SEM, then
    $\mu(\enV,\U) \models \Ar_{M}$ 
    %oli5: compressing
        % if and only if 
        iff
%joe4*: this seems false.  Take M = M', Surely Pa_M = Pa_{M'}.  
%oli5*: I'm not sure I understand your complaint. You don't get to choose M'; this says that if \mu(\enV,\U) is SIM-compatible with a hypergraph \Ar_M, then there exists a way of filling in the equations and probabilities to recover $\mu$.  Note that when we form $\Ar_M$, we have forgotten about the equations. So the fact that some arbitrary distribution \mu is compatible with the structure, does not mean that distribution models the same equations. But it does model the same *structural dependencies* 
%joe4*: On top
%of that, why should the fact that we have the right functional
%dependencies ensure that all the required independencies hold.  
%oli5: there are no independencies in this construction (except insofar as they are being used to model functional dependencies). A SEM just has functional dependencies, no (traditional) independencies. 
%joe4: I'm lost.  I see no reason for this to be true.
%oli5: I'm adding the proof in the appendix. 
        there exists $M'$ such that $\Pa_M = \Pa_{M'}$ and
        $\mu \in \SD{M'}$.
\end{linked}

%joe7*: You need to add a short paragraph here explaining why,
%intuitively, this result is true.  
%oli8: ok, after I finish writing up the proof. 

%joe3
% variables $\U$ are not targets of any mechanisms.
% variables in $\U$ are not targets of any mechanisms. 
%oli3: can you explain the change above? I feel "in" is unnecessary and breaks the flow, especially the way I rewrote it below (omitting your extra "in"). Do you still feel the change is necessary in the new version?
%
%joe3: I don't see why the next sentence is important.  I would cut
%it (or just say that the variables in \U are not targets of any mechanism).
%oli3: You have a point; reducing this text a bit. But also I'm also trying to give some intuition (by example) of how the concepts of \subpartl\ and \suppartl\ hypergraphs  interact with causal models. 
% Note that $\Ar_M$ is an \subpartl\ hypergraph, because the exogenous variables in $\U$ are not targets of any mechanisms. 
%joe4*: cut; this is premature (and will probably not make it to the
%UAI paper in any case.)
%oli5: what do you mean by premature? Just that you don't like it? Why? This is an important and unintuitive phenomenon with our definition, that I feel readers need to get a sense of. 
%joe6: No; "premature" means it's the wrong place in the text for it.
%It should come later.  
%oli5: I've changed your \commentout to \vfull, because I'm willing to reduce this to make room for more important things, but I think it makes the paper significantly worse to lose all this conceptual cross-linking. 
\vfull{
$\Ar_M$ is \subpartl, because the exogenous variables $\U$ are not targets of any mechanism. 
%oli3: adding transition, to strengthen tie.  Also, I forgot that I'm using "P" instead of "\nu" now for the PSEM's distribution, which may have caused some confusion!
% In the case of a PSEM $\mathcal M = (M, \nu)$,
A PSEM $\mathcal M = (M, P)$, however,
%joe3: I have no idea what the next line is trying to say.
% oli3: combining with next sentence
% the causal model $M$ is completed with distribution over $\nu$ over $\U$.
    augments $M$ with a distribution $P$,
%joe3: I also can't parse the next line
%oli-3: missing "an"; adding it and strengthening the coreference
    % This is essentially  independent mechanism by which they arise;
%oli3: actually, merging with previous sentence
    % This distribution $\nu$ is essentially an independent mechanism
%joe4: what does it mean for the variables \U to arise?  I just cut this.
% describing how the variables $\U$ arise;
%oli5: attempting to redo in a way you find clearer
which can be viewed as a randomized mechanism 
    % by which the variables $\U$ arise;
    % that describes how $\U$ arise;
    explaining where the variables $\U$ come from;
    thus $\Ar_{\cal M} := \Ar_M \cup \{ \emptyset \to \U \}$
    is \partl.
%joe3: nor the next line.  
%oli3: It seems well-formed to me.  Let me try saying it another way.
% \Scibility\ with the the two structures is the same: $\mu \models \Ar_M$ if and only if $\mu \models \Ar_{\mathcal M}$---%
%     yet there is still an important sense in which they 
%     are not equivalent, as we will see in \cref{sec:equivalence}.
Perhaps surprisingly, \Scibility\ with $\Ar_M$ and $\Ar_{\cal M}$
    pick out the same set of distributions
    (i.e., $\mu \models \Ar_M$ iff $\mu \models \Ar_{\mathcal M}$).
But that does not make the two structures interchangable,
    as we will see in \cref{sec:equivalence}.
}    

%joe2*: You can't bring in partitions out of the blue here!  If you
%follow my earlier suggestion of defining regular (?) hypergraphs
%before Theorem 1, you can remind the reader that this is an important
%subclass of hypergraphs that we've seen before.  By the way, i've
%downgraded the role of response variables in the latest version of my
%paper with Sainyam; I'll send it to you.  I suspect that you can
%downgrade it in a similar way here; you shouldn't need to assume this
%particular form for the context variables.
%oli2: I believe our discussion cleared up one reason that we need
% the structural compatibility
% \textbf{Witnesses as SEMs.}
% \subsection{Witnesses of \SCibility\ with \partl\ structures, as Causal Models}
%joe4: this is  an incredibly clunky title.  Shortened
%oli5: it is less precise, but I accept. cleaning up comments so old title is here:
% \subsection{\SCibility\ Witnesses
%     for \partl\ \hgraph s as
%     Fully Randomized Causal Models
% }
%oli5: if we're not going to be precise, I would have preferred
% \subsection{Witnesses as Causal Models}
% ... but I'll let you have this one. 
}
%oli9: end cut

%joe3: I would cut the next paragraph.  I don't really undersatnd it.
%oli3: I can't cut it because this is a critical point that we have to flag to prevent misunderstanding. I'm trying to explain that the way we capture the structural aspects of causal models above does not line up exactly  with the more interesting "witnesses as causal models" direction that we are going to focus on next.  
%oli3: the paragraph has two other benefits. First, it gives us an opportunity to mention another talking point (being able to directly capture what one might call "randomized" causal models, in which there is a special independent noise variable for each equation). Second, it makes for a smooth transition to the next section.
%oli3: I'll see what I can do to make it more understandable.
%joe4*: Cut; I still have no idea what you're talking about  I don't
%know in what sense SIM-compatibility captures "this situation"; I'm
%not even sure what "this situation" is. 
%oli5: "this situation" is the setting in which each variable is associated with a special, independent "noise variable". It's common in Pearl's work to give models like this.
%joe4*: Why confuse the reader, who
%cannot read your mind and has no idea of what point you're trying to make.
%oli5*: It seems to me you didn't put much thought towards trying to understand what point I was trying to make, or why it was important (and so I imagine many readers will get more out of it than you did). You also did not address either of my two %oli3 comments above.
%joe7*: this riff on randomized PSEMs is at best misplaced.  It
%detracts from teh main story line.  I cut it.
%oli8*: I reinstated it, because it is critical to the story. The main
% stroy is that from witnesses, you get randomized causal models. Again,
% you're clearly not tracking properly.
%oli9: moving this out of section 3.1; that section is getting cut, 
% and it deserves to be here anyway.
%oli14*: This paragraph is very important, and it makes a nice segue to the material that follows. However, I can also see an argument for putting it earlier: closer to the semantics, and not putting the material on syntax in the middle. What are your thoughts?
Some authors assume that 
%oli6:
% each variable $X$ comes with a special
%oli7:
% each variable $X$ can be associated with a special
for each variable $X$, there is a special
``independent noise'' exogenous variable 
%oli3:
% $\epsilon_X$, effectively allowing randomized transitions.
$U_X$
%oli21: no space for this
\vfull{%
(often written $\epsilon_X$ in the literature)
}%
on which only the equation $f_X$ can depend;
%joe8: cut; it's a bad idea to talk about "wisfhul" simplifying assumptions"
% \unskip \footnote{Seldom in reality are distinct measurable quantities functions of one another; $U_X$ is intended to account for all factors beyond the model that could influence $X$. The requirement that $U_X$ and $U_Y$ be independent is a wishful simplifying assumption that is often effective.}
%oli7:
% We shall call a SEM \emph{randomized} if it contains 
we call a PSEM $(M, P)$
%joe8*: I really don't like the terminology "randomized".  It doesn't
%have anything to do with the notion of interest.  Maybe you can say
%that a PSEM has independent noise if it's what you're now calling
%ranodoized, If you really need "fully reandomized" (and I hope you
%don't) perhaps that can be a PSEM "has only indepdent noise". 
%
%oli19*: I'm really unhappy about "has (only) independent noise" as a replacement for "randomized", although I'm willing to entertain other alternatives. For example, with this terminology, SEMS "with independent noise" would includes a special case where the independent noise is constant, i.e., "SEMs without independent noise".  I want to point out that there is a is a direct connection to randomized algorithms here.
%joe14: Which is what?
%oli20: Perhaps this discussion is superceded by Spencer's latest email, but just for completeness: just as a randomized algorithm is an ordinary algorithm that takes additional independent bits of randomness as input, a randomized PSEM is an ordinary PSEM in which each equation takes as input additional independent randomness. 
%joe16: But the resulting object is still a PSEM!
%oli19*: Furthermore, it suggests how ordinary PSEMs can be embedded within randomized PSEMs: the same way in which deterministic algorithms can be viewed as randomized algorithms .
%joe16*: I still really don't like using "randomized" in this context.  
%oli21*: I still really like "randomized" in this context. I think very few readers will share your objections to it. For what it's worth, I think Spencer feels the same way. 
%spencer3: Sure, this can wait. Oli is right that I'm fine with ``randomized''
\emph{randomized} if it contains 
%oli21: cleaning up run-on sentence
% such exogenous variables, and they are mutually independent according to $P$   
such exogenous variables that are mutually independent according to $P$
%oli16: this is necessary for now; open to cut it again later
% \vfull
{%
    \unskip, and \emph{fully randomized} if {all} its exogenous variables are of this form
    %oli21: the notation is a luxury we cannot afford :(
    % (i.e., $\U = \{ U_X : X \in \enV \}$)
}%
\unskip.
%joe8: [replacement material]
\commentout{
but one that turns out to be important in
presenting our results. Given a PSEM $\cal M$, there is a randomized
PSEM $\cal M'$ such that $\cal M$ and $\cal M'$ agree on the
probability of all forulas in the causal language.  [[Perhaps add one
    sentence about the construction]]  Thus, in a sense, we do not
lose expressive power if we use randomized PSEMs.  
}%
%joe8: this is not how most readers will view it.  You're not helping
%the reader by adding this.  (Recall that I said you needed to
%simplify things. YOu do NOT have to tell the reader everything you
%know about a topic.
%oli19*: I'm on board with simplification, but this is not the place. I particularly like this sentence, as does Spencer.  This intuition gets right at the heart of what randomized PSEMs are, why we don't lose expressive power by using them.  
%joe14*: I'm *strongly* opposed to calling a special case of a PSEM a
%generalization of a PSEM.  I hope that you can understand why.  You
%can point out what you want to point out without calling it a
%gneeralization (which will no doubt confuse soe readers).
%oli20*: Those readers who will be confused by this are precisely those readers who need to hear it. I hope Spencer's email helped convince you of this.  I will try to rewrite this material so it goes over better.
%
%joe16: I think I've left in the essentials.  Unfortunately, what you
%wrote won't help to unconfuse any readers.
\commentout{
Randomized PSEMs are a special class of PSEMs%
---yet, at the same time, may also be viewed as a generalization of a PSEM in which each equation is not deterministic but a randomized algorithm, taking as input an independent random seed. 
}%
%oli20: replacement:
%joe16*: I see no Enlgish reading where allowing an equation to depend
%on independent random noise (which it always could) 
%oli21: (I don't see how it always could; in a PSEM, you can depend on variables. Other equations can also depend on those variables. To make it depend on independent random noise, you have to not only add a new variable, but also have a guarantee that none of the other equations can use it.)
%joe16*: ... gives you a generalization.  I don't see what you gain
% by saying this, and you may confuse/lose some readers.  
%
%oli21: There's a lot to be gained from this intuition: it says that PSEMs and randomized PSEMs are equivalent, neither truly a special case of the other. 
%
Randomized PSEMs are clearly a special class of PSEMs
%oli21*: I'm not sure how exactly, but I suspect there's some misunderstanding between us. Let me try to be really precise.  Fix a PESM M. In that PSEM, each equation is a (deterministic) function of the variables. It is indeed a generalization to allow those not to be deterministic functions, but randomized ones, that can flip coins.  This is because we've swapped the notion of a function out for the notion of a stochastic map, which is more general. Yet, becasue stochastic maps are equivalent to deterministic functions that take an additional argument of additional random bits, this "generalization" of a PSEM does not buy any expressive power. 
%oli21: trying for a more explicit compromise. 
%\unskip. But it is clear that any PSEM
%joe17: (a) it shaves a line not to have it; (b) by removing it,
%you'll make people like me less confused; and (c) I think you're
%already aking the point by saying they have ore expressive power in
%the qualitative case (which I'm happy to leave in).  I'm cutting it
%in the probably vain) hope that this will convince you.
\commentout{\unskip---yet, at the same time, correspond precisely to a
``generalization'' of PSEMs whose equations need not be deterministic.}
%spencer3: I see your point about using ``generalization'',  although I still feel we should convey the intuition that randomized PSEMs can be viewed as PSEMs with randomized equations. (I'm still not sure why you disagree with this.) But, cutting this is fine for NeurIPS.
%oli22: I agree with Spencer that this intuition is important, but not so important to fight a war over right now. Keeping it as is.
%oli21: squishing
% From this perspective, it is clear that
% This makes it clear that 
%joe17:
% Indeed,
but
any PSEM can be
%joe8: I don't know what "losslessly" means in this context.  (It's a moot point; this shoudl be cut)
%oli19: cutting just "losslessly"
% losslessly 
converted to an equivalent randomized PSEM by extending it 
% with new variables $\{U_X\}_{X \in \enV}$ that can only take a single value.
with additional dummy variables $\{U_X\}_{X \in \enV}$ that can
%joe16
%only take a single value.
take only a single value. 
% While PSEMs and randomized PSEMs are equally expressive, \scibility\ has a particularly close relationship with the latter.
% While PSEMs and randomized PSEMs are equally expressive at a quantitative level, randomized PSEMs allow a modeler to represent independencies in the qualitative structure. 
%joe8*: I have no idea what th next sentence is telling me.  A
%randomized PSEM is a special case of a PSEM, so how could it possible
%capture strictly more information.
%oli19: they're both special cases of one another, but randomized PSEMs capture the information qualitatively, so their structures carry more information.  Does that help?
%
%oli19*: commenting out and replacing
% So, one should view a randomized PSEM as capturing strictly more qualitative information---namely,  independencies between exogenous variables. Furthermore, randomized PSEMs have a particularly close relationship with \scibility.
%joe14: the fact that randomized PSESMs capture the infornation
%qualitatively is worth explaining to the reader.
%oli20: I've moved the %joe14 comment above to right here, where I believe I've addressed the concern.
%
%oli19*: replacement, drawing from your material above.
Thus, we do not lose expressive power by using randomized PSEMs
%oli21:
% \unskip; indeed, at a qualitative level, 
\unskip. In fact, \emph{qualitatively}, 
%oli20:
% randomized PSEMs are arguably more expressive, because their structures imply certain independencies. 
randomized PSEMs are more expressive: they can encode independence.
%oli20: adding back some transition
%oli21: removing transition for space
\vfull{%
It should come as no surprise that randomized PSEMs and \scibility\ are related.
}

%oli16*: New story!
%joe8: "Shape" is undefined.  People can't read your mind, oliver.
%oli19: That was an oversight, and I like your change. But let me quickly point out that "QIM-compatibility" was undefined when we gave that section title, but this isn't a problem (and here, shape will be defined later as well).
%joe14: there's no need to define it at all
%\subsection{\SCibility\ Means Arising from a Randomized PSEM of a Given Shape}
% \subsection{From \SCibility\ to Randomized PSEMs and Back}
%oli19*: actually, your proposed subsection title fits section 3.2 far better than it fits section 3.1. How about this?
%joe14: Your suggestion is awfully clunky,  Does "Arising from" really
%need to be part of the title?
%oli20: to be this precise, yes, but perhaps you'd prefer to be vaguer...
% \subsection{The Equivalence Between \SCibility\ and Arising from a Randomized PSEM}
\subsection{The Equivalence Between \SCibility\ and Randomized PSEMs}
    \label{ssec:ssec:cm-arise}
%oli18: replacing this section header
\commentout{In this section, we give the top-level contour of the relationship between \scibility\ and causality: $\mu \models \Ar$ iff there is a causal model whose dependency structure is $\Ar$ that gives rise to $\mu$. To do this, we must be more precise about what it means to ``have dependency structure $\Ar$''. We do this by considering increasingly general classes of \hgraph s, which turn out to coincide with special classes of causal models. }

%oli18: new material
We are now equipped to formally describe the connection between \scibility\ and causality.
At a high level, this connection should be unsurprising:
 % mentioned at the beginning of the section.
%oli21: removing orange
% {%
% \color{orange!50!black}%
witnesses and causal models both relate dependency structures to distributions, 
but in ``opposite directions''.
% but in opposite directions. 
\Scibility\ starts with distributions and asks what dependency structures they are compatible with. Causal models, on the other hand, are explicit (quantitative) representations of dependency structures that give rise to sets of distributions.
% }
% We do this at three levels of increasing generality.
% We show that the two are equivalent at three levels of generality.
% We now show, at three levels of generality, that the two coincide qualitatively.
%joe8
%We now show, at three levels of generality, that existence a causal
%model coincides with existence of a witness.
%We start with the hypergraphs generated by graphs (like Bayesian
We now show that the existence of a causal model coincides with the existence of a witness. 
We start by showing this for the hypergraphs generated by graphs (like Bayesian
%joe8
%Networks, except possibly cyclic), which we find coincide exactly with
networks, except possibly cyclic), which we show
%oli19: in retrospect, this was too strong: 
% coincide exactly with
correspond to
fully randomized causal models (\cref{prop:sc-graph-arise}).  
%joe8*: I actually don't see why this is an expanion.  In any case,
%it's close enough to the first case not to merit a proposition
%oli19*: I actually agree with your second statement, but for reasons that are probably very different from yours. Proposition 3 corresponds to fully randomized causal models, Proposition 4 corresponds to randomized causal models. As I see it, the two are equivalent, and the former is cleaner because we can get the hypergraphs from graphs. 
\commentout{ We then expand the set of hypergraphs we consider by allowing variables not to be the target of any mechanism (i.e., with additional exogenous variables); these are the dependency structures of PSEMs (\cref{prop:sc-psem-corresp}).}
%joe8
%Finally, we give a natural generalization of a causal model that
We then give a natural generalization of a causal model that exactly captures \scibility\ with an arbitrary hypergraph (\cref{prop:gen-sim-compat-means-arise}). 
%joe8:
% In each case,
In both cases,
the high-level result is the same: $\mu \models \Ar$ iff there is a 
%joe8:
% causal model ``of dependency structure is $\Ar$''
causal model that ``has dependency structure $\Ar$''
that gives rise to $\mu$. 
 % we must be more precise about what it means for a PSEM to ``have dependency structure $\Ar$''.

%oli18: splitting out a definition.
%joe17
%More precisely, we say a randomized causal model $\cal M$
More precisely, we say that a randomized causal model $\cal M$
%joe8: clunky
%oli19: I actually tried this variant, but I found the opposite was true and went with "of". One reason for this is that a causal model can simultaneously be of many dependency structures. Another is that this is shorter and less clunky in many contexts ("a model of structure A" instead of "a model that has structure A"), including when we define GRPSEMS
%joe14: I'm afraid that I really don't like "of".  Why not "with"?
%oli20: I'm not wedded to "of", but I like it far more than the alternatives so far. To my mind, "with" is closer to "has" than it is to "of" in terms of implying uniqueness of a structure. In the context "a model with structure \Ar" its flow is not much different from the alternative with "of", but in predicative form, it's FAR clunkier. Imagine if I were to replace the below with, "a model M is said to be 'with dependency structure A' iff.."  or the statement of a result that says "M is with A$. 
%oli20: Ok, after talking with Spencer, I've decided I can live with a fast-and-loose aproach that combines "has" and "with" wherever it's conveneint. 
%joe16: I wish we had Spencer around more often :-(
%oli21: me too :-(
% is \emph{of (dependency) structure $\Ar$}
\emph{has dependency structure $\Ar$}
iff there is a 1-1 correspondence between $a \in \Ar$ and the equations of $\cal M$, such that the equation $f_a$ produces a value of $\Tgt a$ and depends only on $\Src a$ and $U_a$. 
% In the special case of a fully randomized PSEM, this means 
% The definition in the previous sentence emphasizes properties of the hypergraph for intuition and applies generally. For PSEMs as we defined them, here is a more concrete alternative:  
This definition emphasizes the hypergraph; here is a more concrete alternative emphasizing the randomized PSEM: $\cal M$ is of dependency structure $\Ar$ iff the targets of $\Ar$ are disjoint singletons (the elements of $\enV$), and $\Pa_{\cal M}(Y) \subseteq \Src Y \cup \{ U_Y \}$ for all $Y \in \enV$. 
%oli21: merging paragraphs
%
%joe8
%We start with an important special case: when $\Ar$ is generated by a
%oli21: 
% We start with 
We start by presenting the result in 
the case where $\Ar$ corresponds to a directed graph. 

\commentout{
We start with an important special case:
\scibility\ with a \hgraph\ $\Ar_G$ coming from a graph $G$
means precisely that a distribution can arise from a fully randomized causal model in which each variable depends only on its parents and random noise.
}

%oli16: The statement of this theorem is a bit sneaky and has a few issues: it implicitly all but assumes that M is a fully randomized causal model, and it implicitly assumes \X is a subset of \enV by talking about U_Y for Y in \X. These issues can be resolved by defining U_X = 1 for X \notin \X, and adding a side condition, but it's easier to just make this explicit.
\commentout{
\begin{linked}{prop}{sc-graph-arise}
    %oli15:
    % For all graphs $G$, 
    % For all graphs $G$ and joint distributions $\mu(\enV)$ over variables corresponding to the nodes of $G$, 
    % For all graphs $G$ and distributions $\mu(\enV)$ over joint settings of values of $G$'s nodes,
    If $G$ is a graph whose nodes correspond to variables $\X$,
    and $\mu$ is a joint distribution over $\X$, then
    $\mu \models \Diamond \Ar_G$ iff there exists a randomized PSEM
    $(M, P)$
    from which $\mu$ can arise
    %oli15: cutting; this is now part of the definition of a randomized PSEM
    % , whose noise variables $\{ U_X \}_{X \in \X}$ are mutually independent according to $P$, 
    %oli15:
    % and that satisfies $\Pa_M = \Pa_G$.
    \unskip,
    satisfying $\Pa_M(Y) \subseteq \Pa_G(Y) \cup \{ U_Y \}$ for all $Y \in \X$.
    % satisfying $\Pa_M(Y) \cap \X = \Pa_G(Y)$ for all $Y \in \X$.
\end{linked}}
%oli16: new version
\begin{linked}
    % [\scibility\ and fully randomized causal models]
    % [\scibility\ and fully randomized causal models]
        {prop}{sc-graph-arise}
    \commentout{
        If $G$ is a graph whose nodes correspond to variables $\enV$,
        % and $\mu$ is a joint distribution over $\enV$,
        then $\mu \models \Diamond \Ar_G$ iff there exists a fully randomized PSEM $\mathcal M = (\{U_Y\}_{Y \in \enV}, \enV, \mathcal F, P)$ from which $\mu$ can arise, satisfying $\Pa_M(Y) \subseteq \Pa_G(Y) \cup \{ U_Y \}$ for all $Y \in \enV$.
    }
    % If $G$ is a graph whose nodes correspond to variables $\enV$,
%joe8: (a) you don't use \enV anywhere, and (b) I don't know what
%it means for a graph to have nodes that represent variabes
    %    If $G$ is a graph whose nodes represent variables $\enV$, then
    Given a graph $G$ and a distribution $\mu$, 
    $\mu \models \Diamond \Ar_G$ iff there exists a fully randomized PSEM
    % that has 
    of
    %joe8: removed comma before "which"
    dependency structure $\Ar_G$ from which $\mu$ can arise.
    % $\Pa_M(Y) \subseteq \Pa_G(Y) \cup \{ U_Y \}$ for all $Y \in \enV$.
% \item Let $\mathcal M = (\U_0 \sqcup \U_{\Ar}, \enV, \mathcal F, P)$ be a randomized causal model, and $\mathcal Y \subseteq \U_0 \sqcup \U_{\Ar} \cup \enV$. 
% Then $\mu(\mathcal Y) \models \Ar_{\mathcal M}$ iff 
% \end{enumerate}
\end{linked}

%oli18: 
\Cref{prop:sc-graph-arise} shows that, for those hypergraphs induced by graphs,
\scibility\ means arising from a fully randomized PSEM of the appropriate dependency structure.
%oli16: 
%oli18:
% In light of this,\cref{theorem:bns} can be viewed as result 
% This implies that 
%joe8:
% In light of this, 
\cref{theorem:bns} 
%joe8: I work in the area, and am not aware of this "folklore"
%result.  Moreover, people don't tend to work with fully randomized PSEMs.
%oli19: Fair enough. I like your edit, but let me just push back against your "moreover".  Pearl's book works mostly with randomized PSEMs (but he just calls them SEMs); also, structural linear models with noise are also randomized PSEMs of a very special form. 
%oli19:  some tweaks to your text
% makes precise an assumption that seems to be almost universally impilicitly made
makes precise a phenomenon that seems to be almost universally impilictly understood
but, to the best of our knowledge, has not been formalized before:
% that implicitly undergirds much of  causality:
%joe8*: I assume you meant *fully* randomized.  If you can get away with
%randomized, shouldn't this be reflected in the proposition?
%oli19: Oops, you are correct. 
%every acyclic randomized SEM
every acyclic fully randomized SEM
%joe8
%induces a distribution with the independencies of the appropriate
induces a distribution with the independencies of the corresponding
Bayesian Network---% 
and, conversely,
every distribution with those independencies
arises from such a causal model.
% \Cref{prop:sc-graph-arise} provides formal causal grounding for this claim, and a modest extension of it to certain cyclic models. 
%joe8: I'm not sure what it means that it "grounds the result in
%causality". Also, the rest of the material is far too rambly
%\Cref{prop:sc-graph-arise} grounds the result in causality,
%    and provides similar grounding for cyclic models,
%    which we will exploit in \cref{sec:info}. 


%joe8*:  I cut Proposition 4.  First, I don't see why it's not
%  subsumed by Proposition 3.  If we start with an arbitrary directed
%  acyyclic graph G,   it can certainly have nodes that do no have
%  parents, and can be viewed as corresponding to exogenous variables.
%  Second, even if Proposition 4 is not subsumed by Proposition 3,
%  unless it's a special case that you have a lot more to say about, we
%  should not clutter up the paper with it.  It doesn't really tell us
%  more than Proposition 3.
%oli19*: I am happy to cut proposition 4 to reduce clutter, but I feel an obligation to clarify what's going on first.  Proposition 3 is about the relationship between they hypergraphs that correspond to graphs, and *fully* randomized causal models.  Proposition 4 is about how *arbitrary* randomized PSEMs correspond to hypergraphs that are are almost like that but may be missing some hyperarcs (corresponding to the exogenous variables, which do not have equations).  The latter is harder to state, less nicely tied into our "corresponding to a graph" motif.  But, it is more general, and describes what happens with arbitrary PSEMs. 
%joe14: OK; I missed the fact that we were going from fully randomized
%graphs to randomized graphs (in part, I suspect, because it wasn't
%pointed out!).  But we won't have space, so let's keep it suppressed
%in any case.
\commentout{
What about causal models that are not fully randomized, and \hgraph s that are not induced by graphs?

\begin{prop} \label{prop:sc-psem-corresp}
     If $(\N, \Ar)$ is a \hgraph\ whose targets are distinct singletons, then $\mu \models \Diamond \Ar$ iff $\mu$ can arise from a randomized PSEM of dependency structure $\Ar$. 
 \end{prop}
 \commentout{%oli18: second version; decided instead to go with the first one, above.
     \begin{prop} \label{prop:sc-psem-corresp}
         % If $(\N, \Ar)$ is a \hgraph\ whose targets are distinct singletons, then
         If $\Ar = \Ar_{\cal M}$ for some PSEM $\cal M$, then $\mu \models \Ar$ iff $\mu$ can arise from a (possibly different) randomized PSEM of the same dependency structure $\Ar$. 
     \end{prop}
    }%

This is just \cref{prop:sc-graph-arise} 
for the graph of the PSEM's ``\emph{depends on}'' relation,
except for a small difference: since the exogenous variables $\U$ have no equations, they are not associated with \arc s.
}

%oli19: transition is missing now; adding text
It is easy to extend this result to the dependency structures of all randomized PSEMs.  But what happens if $\Ar$ contains
%oli21: cut
% two
\arc s with overlapping targets?
% We now turn to 
%joe8
%Here the correspondence starts to break down for the simple reason
%that, by definition, there is at most one equation per variable in
%(P)SEM---and
Here the correspondence starts to break down for a simple reason:
by definition, there is at most one equation per variable in a
(P)SEM;
%oli18:
% thus such \hgraph s are applicable to these PSEMs. 
%joe8
%thus no PSEM is of dependency structure $\Ar$.
thus, no PSEM can have dependency structure $\Ar$. 
% Even if $\Ar$ is \subpartl, our construction of $\PSEMsA(\bar\mu)$ still makes sense, except the resulting object has more than one equation for some variable, and so it is no longer a causal model in the traditional sense. 
Nevertheless, 
    the correspondence between witnesses and causal models persists if we simply drop the
%oli18:
% (in our view, artificial)
(traditional)
requirement that $\mathcal F$ is indexed by $\enV$.
%oli18: reworked somewhat
%, we get
This leads us to consider
a natural generalization of a 
%oli19: added
(randomized)
PSEM that has an arbitrary set of equations---not just one per variable. 
% [Comment on something]

%oli21*: I'm trying to figure out how much I need to cut using both of these versions. While it's not a ton more space in principle, somehow the layout of the document means it costs 3 extra lines to avoid GRPSEM, which I don't think is worth it. But if you insist, it can be changed by swapping the definition below. 
%joe17: I insist on dropping GRPSEM.  Having it makes things worse for
%the reader.  I'm very sensitive to the need for space, but there are
%other places where we can shave lines.  I can help with this.  (I
%also am concerned that your current estimates of where we  stand on space aren't quite right, at least not if you haven't taken references into account.)
%spencer3: Fortunately NeurIPS references are numbered, so that shouldn't be a problem!
\gdef\GRPSEM{generalized randomized PSEM}
% \gdef\GRPSEM{GRPSEM}
\begin{defn}
%oli19*: this definition really is about randomized PSEMs; there's no need to define any other variant, and it already generalizes the traditional causal models (randomized and non-randomized). Changing the definition this way also unnecessarily complicates the results. Plus, I think this definition is far clunkier. 
%joe14: In what sense is it *really* about randomized PSEMs?  Perhaps
%we don't use it for PSEMs that aren't randomized, but it seems
%meaningful even if it's not randomized.  I really want to get rid of
%"GRPSEM" from the paper.  You'll have to trust me that a nontrivial
%subset of readers will find it offputting.
%oli20: I think Spencer's email covered this point; the short version is that Proposition 5 is about the randomized variant.  The two are equally expressive (quantitatively), so it is unnecessary to define both. 
\commentout{
%joe8: we should be able to define a generalized PSEM without assuming
  %it's randomized.  It's also more consistent with the English above
%oli19: I've now changed the English above. 
  A \emph{generalized PSEM} is a tuple $M = ((\U,\enV, \mathcal F), P)$,
  where $U$, $\enV$, and $P$ are just as in a PSEM, and $\mathcal F$
  is [[PLEASE FILL IN]].   $\SD{\dg M}$ and \emph{can arise} are
  defined for generalized PSEMs just as for PSEMs.
  A generalized PSEM $M = ((\U,\enV,{\mathcal F}),P)$
  \emph{corresponds} to a \hgraph   $(\N,\Ar)$ if (a) $M$ is
  randomized,
%joe8*: I continue to *strongly* object to separating \N from V.  I
%think this makes life much more complicated for the reader, with
%essentially no benefit
%oli19*: I'm willing to identify them in sections 3-5, so long as we continue to properly emphasize their conceptual separation in section 2 when we introduce them first.  I think the benefit comes in section 5. 
  (b) $\U = \{ U_a \}_{a \in \Ar}$, (c) $\X = \N$, and (d)
      $ \mathcal F = \{
  f_a : \V(\Src a) \times \V(U_a) \to \V(\Tgt a)    \}$.
%joe8: I don't think we need to say anything more about P.
}
    Let $(\N, \Ar)$ be a \hgraph. 
    A \emph{generalized randomized PSEM}
     $\mathcal M = (\X, \U, \mathcal F, P)$
    %oli20: trying to implement what I assume was in %joe15. Cutting:
    % \unskip, or \emph{GRPSEM}, 
    %joe14*: I strongly prefer "with" to "of"
    %oli20: as you wish
    % \emph{of
    \emph{with
    %oli19: since you don't like "shape", and to match our previous definition 
    % shape
    structure
    $\Ar$} consists of
    %oli19: removing the separation of $\N,\V$ here
    % sets of variables $\X = (\N, \V)$ 
    sets of variables $\X$ 
    and $\U = \{ U_a \}_{a \in \Ar}$, 
    together with a set of functions
    % \[ \mathcal F = \Big\{
    $ \mathcal F \!=\! \{
            f_a : \V(\Src a) \times \V(U_a) \to \V(\Tgt a)
    % \Big\}, \]
    \}_{a \in \Ar},
    $
    %oli19:
    % and a probability $P_a \in \Delta \V(U_a)$ for each $a \in \Ar$. 
    %joe14: why do we need "independent" in the next line?
    %oli20: we need "independent" because, to be extra precise about the definition of {{ M }} for a GRPSEM (as I was in the commented out material below), we have to require that the noise variables not only have the appropriate marginal, but are also independent.
    and a probability $P_a$ over each independent noise variable $U_a$.
    %oli18: cutting down on definition.
    \commentout{
        Let $\SD{\mathcal M}$ consist of all joint distributions $\nu(\X,\U)$ satisfying the equations with probability 1, and whose marginal on $\U$ is $\nu(\U) = \prod_{a \in \Ar} P_a(U_a)$. 
        Just like in an (ordinary) SEM, 
        a joint distribution $\mu(\mat X)$ over $\mat X \subseteq \X \cup \U$ \emph{can arise from} $\mathcal M$ iff there exists some $\nu \in \SD{\mathcal M}$ whose marginal on $\mat X$ is $\mu$.
    }%
    %oli18: Spencer's replacement
    The meanings of $\SD{\cal M}$ and \emph{can arise} are the same as for a PSEM.
    % GRPSEMs also assign truth values to causal formulas in the same way as before
    %
    % A generalized randomized PSEM assigns truth to $\mat u \in \V(\U)$
    % A generalized randomized PSEM assigns probabilities to causal formulas just as an ordinary PSEM does: 
\end{defn}
%joe8: cut
%oli19: fine; it does save space. 
\commentout{
% In the special case 
% When $\Ar = \Ar_G$ for some directed graph $G$, 
%oli16: 
%oli18: cutting
% We observe a few special cases of this definition, for when $\Ar$ is a member of a special class.
% When $\Ar$ is a member of some of . 
%oli18:
% If $G$ is a directed graph, a
If $\Ar = \Ar_G$ is a directed graph, a
% generalized randomized PSEM
\GRPSEM\ 
of shape $\Ar_G$ is 
% precisely a
a
%oli16*: fixing, and expanding. 
% (ordinary) randomized PSEM
%oli18:
% fully randomized PSEM with endogenous variables corresponding to the nodes of $G$,
fully randomized PSEM with endogenous variables represented by the nodes of $G$;
% and
%oli18: cutting
\commentout{In particular, if $G$ is a dag, then a GRPSEM of shape $\Ar_G$ is a causal Bayesian Network with underlying graph $G$. }
% More generally, 
%oli18:
% if $\Ar = \Ar_M$ for a SEM 
% $M$, then a GRPSEM of shape $\Ar_M$ is a PSEM 
% if $\cal M$ is a randomized PSEM of dependency structure $\Ar$, 
if the targets of $\Ar$ are disjoint singletons, 
then a \GRPSEM\ of shape $\Ar$ is a randomized PSEM 
%oli18: Spencer asked me to cut, and I'm sure we don't have room for this---although I would have preferred to be more precise here...
\commentout{
    with the same sets of endogenous and exogenous variables,
    %oli15: being more precise
    in which the equation $f_Y$ can depend only on the variables on which it depends in $M$ (and also independent noise $U_a$).
    }%
%oli18:
of dependency structure $\Ar$.
% with the same dependencies.
%oli18:
% \GRPSEM s have a simple and direct relationship with \scibility.
\GRPSEM s have a simple and direct relationship with \scibility\ with an arbitrary \hgraph.
}

\begin{linked}{prop}{gen-sim-compat-means-arise}
    $\mu \models \Diamond \Ar$ iff
    there exists 
    % a generalized randomized PSEM
%joe8: getting rid of GRPSEM
%    a GRPSEM of shape $\Ar$,
%oli19: as I've said, it needs to be randomized, and it needs to be "of dependency structure \Ar" or "of shape \Ar", which I prefer. 
% a generalized PSEM 
%joe14: And, as I said, I would strongly prefer "with structure A"
%oli20: ok; I can live with that. 
    % a GRPSEM of structure $\Ar$\,
%oli20: this extra line is really bothering me; can we afford it? I don't know how to get rid of it  without compressing "generalized randomized PSEM". 
    % a generalized randomized PSEM 
    a \GRPSEM\
    %oli20: as you wish
    % of
    with
     structure $\Ar$\,
    from which $\mu$ can arise.
    % \vspace{-1ex}
\end{linked}
% In light of this result, a generalized randomized SEM of shape $\Ar$ 

%oli14*: previously had added in section 3.0. Unfortunately, it turns out that we need to effectively make the same choices needed to determine a causal model {\cal M} \in PSEMs(\nu) in order to ascribe probabilities to formulas this way. 
% Let 
% \[\mathrm{do}(\mat Y{\gets}\mat y) := \cap_{Y \in \mat Y} ( \forall z.~f_Y(U_Y, z) = \lambda z.~\mat y[Y])
% \]
% denote the event in which each variable $\hat U_Y$ takes on the constant function that ouputs $\mat y[Y]$ for all inputs. 
% Suppose $\nu \in \Delta \V(\X,\U)$ is a distribution over extended contexts. 
\commentout{
%-(oli14): this was originally the point, but it turns out not to be very useful to do this, for the reason stated above. 
We now describe
    the same connection from another perspective:
    like causal models, witnesses of \scibility\ can be used to ascribe probabilities to causal formalas.
    % First some more notation.
    % If $\mat X \supseteq \mat Z$ are sets of variables and $\mat x \in \V(\mat X)$, we write $\mat x[\mat Z {\gets} \mat z]$ for the joint setting of $\mat X$ obtained by starting with $\mat x$, and setting the values of $\mat Z$ to $\mat z$. 
    % For $\mat Y \subseteq \X$, write $U_{\mat Y}$ for the set of variables $\{ U_Y : Y \in \mat Y\}$.   
    % Now, let $\nu$ be a witness to $\mu \models \Ar$. 
    A basic causal formula $[\mat Y {\gets} \mat y]\varphi$ is true in 
    % an \emph{extended context} $(\omega, u) \in \V(\X,\U)$, written $(\omega, u)\models [\mat Y \gets \mat y]\varphi$,
    $\nu$ for a context $\mat u \in \V(\U)$, written
    $(\nu, \mat u) \models [\mat Y {\gets} \mat y]\varphi$, iff
    \[
        % \nu(\varphi \mid \mat u[U_{\mat Y} \gets \lambda\textunderscore.y]) = 1.
        \nu(\varphi \mid \U{=}\mat u[\hat U_{\mat Y} \gets 
            \Lambda Y. \lambda\textunderscore.\mat y[Y]]) = 1,
    \]
    where $\Lambda Y.\lambda \textunderscore.\mat y[Y]$ is just system F syntax for the indexed set of functions (one for each $Y \in \mat Y$) that ignore their inputs and return components of the constant $\mat y$. 

    \begin{linked}{prop}{all-models-formula-equiv}
        Let $\varphi \in \mathcal L(\U,\X)$ be causal formula, and $\nu(\X,\U)$ a witness to $\mu \models \Ar$.
            % and $\mathcal M \in \PSEMs(\nu)$ a .
            Then
        \[
        (\nu, \mat u) \models \varphi
        \quad\iff\quad
        \forall \mathcal M \in \PSEMs(\nu).~~
        (\mathcal M, \mat u) \models \varphi.
        \]
    \end{linked}
}

%joe8*: cut.  Again, too rambly.  You need to stay focused.  At best,
%this is a discussion that belongs in the 
%conclusion, when discussing future work.  ("We would like to
%udnerstand the connections between generalized PSEMs and other
%nnodels that can capture functional dependencies anbd constraints
%...".)  Also, as I said repeatedly in our discussion, you can't just
%plop causal models with constraints in here out of the blue, wiothout
%saying something about what they are and why we should care.  Again,
%if this belongs anywhere, it's in the conclusion.
%oli19*: I've talked with Spencer, and we both very much disagree with
% your decision to cut this material from here. Here, I'll speak for myself so as to avoid putting words in his mouth. 
%oli19*: First, it's helpful to understand what these things are and what they can represent; note that it's only a few lines. I'll also reduce it so it's even fewer. While we can leave some of this to the future work section, it's in scope here, and doesn't require a reader to page it back in. I'll try to make some edits to make this mroe pallatable. 
%oli20: as requested
% GRPSEMs 
% Generalized randomized PSEMs
%joe17: unded (to avoid lower case g)
Generalized randomized PSEMs
can capture functional dependencies, and constraints. 
%joe17: removing, yet again.    To repeat for the nth time, they are
%*not* similar (intriguingly or otherrwise) to these other models
%(This will also save space)
%oli22: I disagree and like this intuition, but I'll keep it surpressed for NeurIPS
% and constraints, making them intriguingly similar to other generalizations of SEMs that have been proposed \cite{peters-halpern-GSEMs,blom2019structural,beckers2023causal}. 
%oli19: cutting back on examples:
% Recall \cref{example:two-edge-det,theorem:func} for examples of functional dependencies; for an example of a constraint, note that
%joe17
%oli22: already said above.
% They can also capture constraints.
For instance, an equality (say $X = Y$) 
%oli22:
% can be enforced by adding to 
can be encoded in a
\GRPSEM\
with a second equation for $X$.
%joe14: added; you absolutely need this glue if you're going to talk
%about causal models with constraints.  I hope this will be an
%acceptable compromise
%oli20: looks good to me!
Indeed, we believe that 
% generalized randomized PSEMs
%oli22: too expensive
\GRPSEM s
% this class of generalized causal models
can capture a wide class of constraints, 
and are closely related to
\emph{causal models with constraints} \cite{beckers2023causal},
%oli22: line shave
% although we defer this discussion to future work.
a discussion we defer to future work.
% We believe that GRPSEMs are actually eqiuvalent to causal models with constraints \cite{beckers2023causal}, but defer that discussion to future work.
%oli18: cutting old material
\commentout{
GRPSEMs, and hence \scibility\ in its most general form, 
are closely related to several existing generalizations of SEMs. 
They can be viewed as causal models with constraints 
\citep{beckers2023causal} by splitting each variable that is a target of multiple \hyperarc s into multiple copies (so that each copy is only a target of a single \hyperarc), and then add a constraint requiring that all copoes take the same value.
They can be viewed as GSEMs \citep{peters-halpern-GSEMs} by selecting the defining function from $\V(\U)$ to subsets of $\V(\X)$ to be the set of states that satisfy the equations in a given context.
}

    
% \subsection{From Witnesses to Causal Models}
%oli18: (Spencer's Idea) changing name to emphasize interventions. 
% \subsection{The Correspondence Between \SCibility\ Witnesses and Causal Models}
\subsection{Interventions and the Correspondence Between Witnesses and Causal Models}
    \label{sec:witness-to-causal-model}

%oli18: rewrote; cutting old material here.
\commentout{
%oli16: new text        
The relationship shown in the previous section, culminating in \cref{prop:gen-sim-compat-means-arise}, is simple and exact, but it does not fully capture the depth of the connection between \scibility\ and causality.
One unsatisfying aspect has been the focus the behavior of these causal models in the absence of interventions. 
This is by necessity: 
in the discussion so far, both the witness of \scibility\ $\bar\mu$
(implicit in the assertion $\mu \models \Ar$) and the causal model $\mathcal M$ (implicit in the statement that $\mu$ can arise from some causal model) have been obfuscated behind existential quantifiers. 
% and hence focused on the behavior of causal models in the absence of intervention by necessity).
Yet $\bar\mu$ and $\mathcal M$, previously left implicit, are themselves closely related. 
% By understanding, we will see that 
% (1) a proof of $\scibility$ 
% By looking more closely at the details, we will be able to 
A closer look at the details will reveal 
a clean relationship between \scibility\ witnesses and causal models
(\cref{prop:witness-model-properties}),
and, moreover,
extend the correspondence to say something interesting about
    causal formulas with interventions
% see how the correspondence extends to intervention
 (\cref{theorem:condition-intervention-alignment}).
}
%oli18:
We have seen that \scibility\ with $\Ar$ (i.e., the existence of a witness $\bar\mu$) coincides exactly with the existence of a causal model $\cal M$ from which a distribution can arise. But which witnesses correspond to which causal models? The answer to this 
%joe8
%question will be critical to extend the correspondence to
%interventions, because different causal models may give rise to the
question will be critical to extend the correspondence we have given
so that it can deal with interventions.  
Different causal models may give rise to the
same distribution, yet handle interventions differently.  


% The correspondence actually goes in two directions. 
There are two directions of the correspondence. 
% The correspondence has two directions.
% One direction of this correspondence is straightforward:
% Given a randomized causal model, producing appropriate \scibility\ witnesses for distributions arising from it is trivial (we defer the formal details to \cref{appendix:sem2witness}).
% given a randomized causal model $\cal M$, the distributions arising from it are compatible with its dependency structure, and the corresponding witnesses are exactly the solutions to $\cal M$, i.e., the elements of $\SD{\cal M}$ (see \cref{appendix:sem2witness}). Consequently, if $\cal M$ is acyclic, it leads to a unique witness. 
% The distributions that can arise from a given randomized PSEM $\cal M$ are compatible with its dependency structure, and the corresponding witnesses are exactly the solutions to $\cal M$, i.e., the elements of $\SD{\cal M}$ (see \cref{appendix:sem2witness}). Consequently, if $\cal M$ is acyclic, it leads to a unique witness. 
Given a randomized PSEM $\cal M$, distributions arising from it are
%oli21: removing QIM
% \scible\
\cible\
with its dependency structure, and the corresponding
%joe8
%witnesses are exactly the solutions to $\cal M$, i.e., the elements of
%$\SD{\cal M}$ (see \cref{appendix:sem2witness}). So, if $\cal M$ is
%acyclic, it leads to a unique witness.  
%oli19: I accept, but I find it a bit dangerous to say "distributions compatible with M" when "distributions that can arise from M" is a very closely related concept for which the statement is not true. Also, your edit lost the link to the appendix where I treat the issue more carefully.
%joe14: the loss of the link was certainly unintentional!
% witnesses are exactly the distributions compatiable with $\cal M$.
witnesses are exactly 
%oli21: line shave; also I cut the "compatible" definition above for space.
% the distributions compatible with $\cal M$  (i.e., the elements of $\SD{\mathcal M}$; see \cref{appendix:sem2witness}). 
the distributions in $\SD{\mathcal M}$ (see \cref{appendix:sem2witness}).
%oli19: 
% So, if $\cal M$ is
In particular, if $\cal M$ is
acyclic, there is a unique witness.  
%oli18:
% the \scibility\ witnesses for distributions arising from it are the elements of $\SD{\cal M}$
%     (we defer the straightforward formal details to \cref{appendix:sem2witness}).
%oli18:
% Here, we focus on the more interesting converse: how can we turn a witness into a causal model?
The converse is more interesting: how can we turn a witness into a causal model?


% More precisely, we show that there 

%oli16: removing old header
% We now describe how a winess to \scibility\ can be converted to a SEM, at three different levels of generality.
%oli7: removing the list from the document; I'll cut things from here and paste them below as I use them.
\commentout{
\begin{itemize}[wide]
    \item 
    When $\Ar = \Ar_G$ for some is a directed acyclic graph $G$ (i.e., a qualitative BN), then a witness to $\mu \models \Ar$ is
    %joe4*: this is not a useful summary, since I have no idea of what it
        %means to be "essentially an acyclic causal model".
    %oli5: what's to know? It means that, in this setting, a witness of compatibility is (in some sense naturally isomorphic to) an acyclic causal model. 
         essentially an acyclic causal model
    %joe3*: A witness is a distribution.  At a minimum, you have to
    % explain how you're viewing the distribution as a causal model, and
    % why it should be acylic in this case.  I'm lost here.
    %oli3*: this is meant as a preview; I'd like to prime the readers for
    % the technical dedatils they'll see in the next section. I'll add some
    % extra verbage to make the intended purpose of this section clear.
        in which the equation for $X$ depends on $\Pa_G(X)$ and
    %joe3*: In what sense does it "ultimately give rise to \mu$?  This has
    %to be clarified
    %oli3: in the sense of being the distribution over the endogenous variables that results when no interventions are taken. But again, I want to  get into the details later. Here, I'm just giving an overview of our results.
    %joe4*: Readers can't read your mind, Oliver!  It's not a useful
    %overview of the results if a reader can't understand what it says.  Either
    %explain "ultimately gives rise to" means, or cut it.
        independent noise, and which ultimately gives rise to $\mu$ in the absence of intervention.
    %oli6: already mentioned
        % Because the causal model is acyclic, $\mu$ is the unique such distribution. 
        %oli6: irrelevant
        % In practice, most causal modeling is done with acyclic models.
        
        %oli3: moving material from above here, with some modification
            %joe1: This is premature and breaks the flow.  I'm also not so sure
        %it's well known.  (If you think it is, please add a reference.)
        %oli1: I haven't seen it formalized carefully, but reading through Pearl's book
        % strongly gave me the impression some version of this fact is is common knowledge. 
        %joe2: It certainly isn't common knowledge! I don't know it.
        %oli3: first part is now no longer necesary because it's in the right place
        % In \cref{sec:causal}, we will see how  a witness of compatibility $\mu(\X, \U)$ can be seen  as a (partially specified) causal model.
        %oli3: modifications from your comments:
        % From this perspective, \cref{theorem:bns} is a form of an equivalence  well-known in the causlity community:
        From this perspective, \cref{theorem:bns} can be viewed as result 
            that implicitly undergirds much of the work on causality:
        every acyclic causal model with independent per-variable noise induces a distribution with the independencies of the appropriate Bayesian Network---%
        and, conversely,
        every distribution with those independencies
        arises from such a causal model.
    \item
    More generally, if $\Ar$ is a \partl\ hypergraph,
        then a witness to 
        $\mu \models \Ar$ 
%joe3*: Now you talk about a witness containing a causal model rather
%than being a causal model.  I'm even more lost.
%oli3*: the reader isn't supposed to understand why yet; that will come leter. 
% but to answer your question: as you point out, a witness isn't literally a causal model, but an extended distribution.  In the case of an acyclic model, the information in that witness (distribution) is essentially precisely what is needed to get a causal model.  Now, in the the cyclic case, the extended distribution not only gives the functional model, but also a choice of a distribution over its fixed points. 
        contains a (possibly cyclic) causal model 
        with respect to which $\mu$ is a possible 
        (distribution over) fixed-point solution(s).    
    
    \item 
    Finally, a witness of \scibility\ with an arbitrary directed hypergraph $\Ar$
    corresponds to a causal model with logical constraints \citep{beckers2023causal}.
%joe3*: I have no idea why this should be true, and neither will the
%reader.  You can't just bring it in out of the blue.    This needs
%much more discussion.  You need to slow down significantly and
%rewrite this material signficantly.  
%oli3*: yes, and that slowing down will come in section 3.4. Here, 
\end{itemize}
}

%oli7:
% \subsubsection{Partitional directed hypergraph s}
%oli16:
% \subsubsection{Partitional Hypergraphs}
\commentout{
    In the section we show that if $\nu(\U,\X)$ is a witness to a distribution
    $\mu(\X)$ be \scibile\ with a directed hypergraph $(\X,\Ar)$, then there is a
    unique causal model $M$ with endogenous variables $\X$ and exogenous
    variables $\U$ such that \ldots.
}%
%oli18*: (Spencer's Idea;) replacing paragraph with construction plus discussion
\commentout{\color{gray!80}
\paragraph{Causal models from \scibility\ witnesses.}
%joe4
%Suppose $(\X,\Ar)$ is a \partl\ directed hypergraph,
%and that $\mu(\X)$ is \scible\ with $\Ar$.
%A witness of \scibility\ $\nu(\X, \U)$
%can then be converted to a 
%oli5: I see where you're going here, but this is slightly too much to ask; there will not be a unique causal model. Instead, each \nu naturally corresponds to a set of causal models. I agree with what you're trying to get me to do, and I'll see if I can pull these properties further up.  I'll also try to use some of your text, but for now I'm keeping it commented out as a reference. 
%oli14:
% Suppose that $(\X,\Ar)$ is  a \partl\ \hgraph,
% and $\mu(\X)$ is \scible\ with $\Ar$.
% Suppose that $\mu(\X)$ is \scible\ with a directed hypergraph $\Ar$. 
Suppose that $\mu(\X)$ is \cible\ with a \hgraph\ $\Ar$, 
and for the sake of more closely matching the standard causality literature, assume that $\Ar$ is \subpartl. 
%oli7:
% A witness of \scibility\ $\nu(\X, \U)$
A witness $\bar\mu(\X,\U_{\Ar})$ 
%oli14:
% of \scibility\ 
to this \scibility\ 
can be converted to a 
randomized SEM
% with endogenous variables $\X$ and exogenous variables $\U$
with endogenous variables $\enV := \cup_{a \in \Ar} \Tgt a$ and exogenous variables $\U := \U_{\Ar} \cup (\X{-}\enV)$
\unskip, as follows.
%joe4*: Oliver, I have no idea what "converted to" means.  There are
%lots of causl models with endogenous variables \X and exogenous
%variables \U.  What's special about the model you're constructing?
%
%joe3*: You need more of a story at the beginning of this section,
%saying that you're going to consider sim-compatibility witnesses  as
%causal models for each of the three types of PDGs that you discussed
%in the previous section.  This is a story that you should repeat for
%other notions of interest (like the semantics for qualitiative models_)
%You haven't said anything about the case of dags.   I would start
%with that.  You're issing the chance to tell a nice story here.  I
%would be happy if all you said about the third case (which I would
%simply call non-paritional -- I still don't find the split into
%subpartitional and superpartitional at all interesting) is that
%things get more complicated, and point out way.  No need to prove a
%theorem, unless it tells a nice story.
%oli16:merge paragraphs
%
% The endogenous variables are
% $\X' :=
%  % \{ X \in \X : \exists a \in \Ar.~ \}
% \cup_{a \in \Ar} \Tgt a \subseteq \X$,
% and the exogenous variables
% are $\mathcal U' := \U \cup (\X \setminus \X')$.
% The equations are straightforward.
%joe4*: "obvious choice" is not what you want to say here!  You want a clear
%description of the properties you want of the causal model, from
%which it follows that you have to take P to be the marginal.
%oli7*: please suggest! I have been trying to formulate the properties, in Proposition 4, but I'm not sure that what's there fully captures the essence of why this construction is appropriate. 
%oli7: changing the wording, so that at least this doesn't look as glaring
% The marginal $\nu(\U)$ is the obvious choice for the distribution $P$ over exogenous variables.
For the distriubution $P(\U)$ over the exogenous variables, 
    select the marginal $\bar\mu(\U)$. 
%oli2: adding
% The equations, for the most part, fall out of condition (c) of \cref{defn:scompat}. 
%joe4*: Again, you must make clear what properties you want to hold.
%All this will need to be rewritten once you've made that clear.  It
%should be straightforward.
%joe7: this doesn't help
%The equations, for the most part, fall out of \cref{defn:scompat} (c). 
%For each variable $X \in \X$, use the following procedure to define $f_X$.
\commentout{
The equations, for the most part, fall out of \cref{defn:scompat} (c). 
For each variable $X \in \enV$, use the following procedure to define $f_X$.}%
%oli16: I didn't see this before, but the %joe7 cut made above cuts a few critical words that bind $X$; reinstating them.
For all $X \in \enV$, define the equation $f_X$ as follows.
%oli2:
% For each $X \in \X$, there is a unique $a_X \in \Ar$
% such that $X \in \Tgt {a_X}$ (by the assumption that the targets form a partition of $\X$).
% Because $\Ar$ is \partl, 
Because of the definition of $\enV$ and the fact that $\Ar$ is \subpartl, 
    there is a unique
    \arc\ $a_{\!X} \in \Ar$ whose targets $\Tgt {a_{\!X}}$ contain $X$.
% Choose $u \in \V U_{a_X}$ and $\mat s \in \V \Src {a_X}$.
%oli2:
% Choose $u \in \V (U_{a_X})$ and $\mat s \in \V (\Src {a_X})$.
Since
$\bar\mu \models ( U_{\!a_{\!X}}, \Src{a_{\!X}}) \tto \Tgt {a_{\!X}}$
%oli14:
(by \cref{defn:scompat}(c)),
% we know in particular that $X$ is a function of $\Src{a_X}$ and $U_{a_X}$.
%joe3
%we know in particular that
%oli3: your version is nicer, except that it admits multiple parses :(
% To eliminate, I want a word rather than a mathematical symbol after the comma. 
%oli14:
% it must be that $X$ is 
$X \in \Tgt {a_{\!X}}$ must also be 
a function of $\Src{a_{\!X}}$ and $U_{\!a_{\!X}}$.
%oli14:
% Thus,
Roughly speaking, we take $f_X$ to be this function. 
More precisely,
for each $u \in \V (U_{\!a_{\!X}})$ and $\mat s \in \V (\Src {a_{\!X}})$
%oli3: delete comma
% ,
for which $\bar\mu(U_{\!a_{\!X}} {=}\, u, \Src {a_{\!X}} {=}\, \mat s) > 0$,
% we know in particular that $\Src{a_X}$ and $U_{a_X}$ together determine $X $(in $\mu$).
%oli3: reclaimin some extra words here
% we know there is a unique value
there is a unique
 $t \in \V (\Tgt {a_{\!X}})$ such that $\bar\mu(u,\mat s, t) > 0$.
%oli2:
% We then set $f_X(u, \mat s) := t[X]$, in this case.
%oli14: being more precise: the definition of an equation in a SEM takes in values of all variables. 
% In this case, choose $f_X(u, \mat s) := t[X]$.
In this case, choose $f_X(u, \mat s, \ldots) := t[X]$.
On the other hand, if $\bar\mu(U_{\!a_{\!X}} {=}\, u, \Src {a_{\!X}} {=}\, \mat s) = 0$,
%oli14:
% then any choice of $f_X(u, \mat s)$ will do.
then any choice of $f_X(u, \mat s, \ldots)$ that depends only on $\Src a \cup \{U_a\}$ will do.
%joe3*: If I've understood it right, your previous sentence says that
%\nu does *not* determine the causal model.  Given \nu, there may be
%many queries we cannot answer (involving interventions in contexts to
%which \nu assigns probability 0.  You need to discuss this issue, and
%the extent to which we should care.  This is an important part of the story.
%oli3: Good point. I'll try to make sure this is a clear part of the story (if not in this iteration, then the next).
%joe2
% In this case, fix a choice of $f_X(u, \mat s)$, and call the result
% SEM $\dg M$.  
% In this case, to avoid a non-deterministic choice,
% we will instead make the intervention $X\gets x$ inadmissable,
% by removing it from $\mathcal I$.
Let 
%oli14: don't sweep dependence on \Ar under the rug
% $\PSEMs(\nu)$
$\PSEMsA(\bar\mu)$
be the set of PSEMs that can can be obtained from $\bar\mu$
    in this way.
%joe3*: I think you may want to say something about the extent to
%which \nu determines the causal model according to this recipe; that
%is, make clear when PSEM_S(\nu) is not a singleton.  
%oli3: Sounds like a good plan. 
%oli16: 
Even if $\Ar$ is not \subpartl, our construction of $\PSEMsA(\bar\mu)$ still makes sense, except the models will have more than one equation for some variable and thus be GRPSEMs, rather than causal models in the traditional sense. 
}

% We can use a witness to (non-deterministically) construct a PSEM as follows.

\begin{constr}
        \label{constr:PSEMs}
    % Let $\Ar$ be a \subpartl\ \hgraph. 
    Given a witness $\bar\mu(\X)$ to \cibility\ with
    %joe8
    %    a \subpartl\ \hgraph\ $\Ar$, construct a PSEM according to the
    %
    %oli19: this is not the definition of subpartitional!
    % a \hgraph\ $\Ar$ where each target has at most one source,
    a \hgraph\ $\Ar$ with disjoint targets,
    construct a PSEM according to the 
    following (non-deterministic) procedure.  
    % Take $\V := \cup_{a \in \Ar}
    Take $\enV := \cup_{a \in \Ar} \Tgt a$,  $\U := \U_{\Ar} \cup (\X{-}\enV)$, and $P(\U) := \bar\mu(\U)$. 
    %
For each $X \in \enV$, there is a unique $a_{\!X} \in \Ar$ whose targets $\Tgt {a_{\!X}}$ contain $X$. 
%  (as $\Ar$ is \subpartl).
Since
$\bar\mu \models ( U_{\!a_{\!X}}, \Src{a_{\!X}}) \tto \Tgt {a_{\!X}}$ 
%(witness property (c)),
(this is just property (c) in
%oli19: more precise and saves a line
\cref{defn:scompat}%
 % the definition of ``witness''
 ),
$X \in \Tgt {a_{\!X}}$ must also be 
a function of $\Src{a_{\!X}}$ and $U_{\!a_{\!X}}$;
take $f_X$ to be such a function. 
More precisely,
for each $u \in \V (U_{\!a_{\!X}})$ and $\mat s \in \V (\Src {a_{\!X}})$
for which $\bar\mu(U_{\!a_{\!X}} {=}\, u, \Src {a_{\!X}} {=}\, \mat s) > 0$,
there is a unique
$t \in \V (\Tgt {a_{\!X}})$ such that $\bar\mu(u,\mat s, t) > 0$.
In this case, set $f_X(u, \mat s, \ldots) := t[X]$.
%joe8
%On the other hand, if $\bar\mu(U_{\!a_{\!X}} {=}\, u, \Src {a_{\!X}} {=}\, \mat s) = 0$, 
If $\bar\mu(U_{\!a_{\!X}} {=}\, u, \Src {a_{\!X}} {=}\, \mat s) = 0$, 
% non-deterministically select $f_X(u, \mat s, \ldots)$ to be any function of $\Src a$ and $U_a$.
%joe8
%take $f_X(u, \mat s, \ldots)$ to be any function of $u$ and $\mat s$.
$f_X(u, \mat s, \ldots)$ can be an arbitrary function of $u$ and $\mat s$.
    %
% Let $\PSEMsA(\bar\mu)$ denote the set of PSEMs that can be obtained from $\bar\mu$ in this way.
Let $\PSEMsA(\bar\mu)$ denote the set of PSEMs that can result.
    % \unskip\footnote{When $\Ar$ is not \subpartl, the construction still makes sense, except the result will be a GRPSEM.}
\end{constr}

%oli18: adding
It's clear from \cref{constr:PSEMs} that
$\PSEMsA(\bar\mu)$ 
is always nonempty, and is a singleton
iff $\bar\mu(u,s) > 0$ for all $(a,u,s) \in \sqcup_{a \in \Ar} \V(U_a, \Src a)$
%oli14:
%\unskip, and it is possible to find such a witness $\nu$ when $\mu(\mat x) > 0$ for all $\mat x \in \V(\X)$.
\unskip.
A witness with this property exists when
%oli19:
$\mu$ is positive (i.e.,
$\mu(\X{=}\mat x) > 0$ for all $\mat x \in \V(\X)$
%joe8
%So, if $\mu$ is positive, it leads to a unique causal model.
%oli19: merging with the above, instead of the below
%joe14: yes, that's good
% So, if $\mu$ is positive, the construction gives a unique causal model,
\unskip), in which case the construction gives a unique causal model.
% so if $\mu$ is positive, the construction gives a unique causal model.
%
%oli18*: added
% At a high level, indeed causal models $\cal M$ with dependency structure $\Ar$ and witnesses $\bar \mu$ are equivalent. But there are two caveats.
%joe8
%So, when everything is sufficiently nice,
%oli19: not quite the right story when compressed like this; let me try to fix it
% \unskip, and 
Conversely, we have seen that an acylic model $\mathcal M$ gives rise to a unique witness.
So, in the simplest cases, models
$\cal M$ with structure $\Ar$ and
%oli21: removing QIM
 % witnesses $\bar \mu$ to \scibility\ with $\Ar$
 witnesses $\bar \mu$ to \cibility\ with $\Ar$
 are equivalent.
But there are two important caveats.
\begin{enumerate}[topsep=0pt,itemsep=0pt]
    \item A causal model $\cal M$ can contain more information than a witness $\bar\mu$ if some events have probability zero. For instance, $\bar\mu$ could be a point mass on a single joint outcome $\omega$ of all variables that satisfies the equations of $\cal M$. But $\cal M$ cannot be reconstructed uniquely from $\bar\mu$ because there may be many causal models for which $\omega$ is a solution. 
    %
    \item A witness $\bar\mu$ can contain more information than a causal model 
    $\cal M$ if $\cal M$ is cyclic. For example, 
    %oli19:
    %if
    suppose that
    $\cal M$ consists
    %joe8
    % of two variables $X,X'$ and equations $f_X(X') = X'$ and
    of two variables, $X$ and $X'$, and equations $f_X(X') = X'$ and
      $f_{X'}(X) = X$
    % the model does not contain information about the unconditional distribution of $X$, which is present in $\bar \mu$. 
    %oli19: trying to parallel the above:
    %\unskip, then $\cal M$ does not contain the information about the distribution of $X$ present in $\bar \mu$. 
    \unskip. In this case, $\bar\mu$ cannot be reconstructed from $\cal M$, because $\cal M$ does not contain information about the distribution of $X$. 
\end{enumerate}
% Although these two caveats appear to be very different, they actually represent opposite views of the same mathematical phenomenon. 
%oli19: saving a line
% Although these two caveats appear to be very different, they fit
These two caveats appear to be very different, but they fit
%joe8
%together in a surprisingly elegant way: it turns out the two ways of
%describing dependency structure---witnesses and causal models---are
%adjoint views of the same mathematical relationship between
%distributions and hypergraphs.
together in a surprisingly elegant way.
%oli18: cutting
\commentout{
But even when the correspondence is not unique, the causal models ${\cal M} \in \PSEMsA(\bar\mu)$ still contain much of the same information as the witness $\bar\mu$ from which they are derived. 
}


%joe3*: I don't see how the next paragraph fits into the story at all.  I
%have no idea what you're trying to show here.  The obvious thing to
%show (which  I don't think you're showing) is that for a cusal model in
%PSEM(\nu) for some \nu, we can recover \nu (at least, to soem extent)
% In turn, this SEM (if the underlying \hgraph\ is
% partitional).   You should either cut the rest of this section or
% explain how it fits in the story.  (A I Ais, I have no clue).
%oli3*: Hmm. The result you are talking about is Proposition 4, and 
% I think you're correct that it should go here. The other material,
% which I will try to clarify, can come later. 
%oli3: moving up from below
%oli3*: parts 2 and 3 may need to be weakened; I haven't fully vetted them yet. 
%joe4*: I have no idea of why I should be interested in this result or
%what it's telling me (and I'm an expert on causal models!).
%Neeedless to say, neither will the typical reader.
%oli5: I agree that more motivation is necessary, but I wish you would have done more work to try to understand it and help me motivate it, rather than just pointing this out. ..
\begin{linked}{prop}{witness-model-properties}
        % \label{prop:witness-model-properties}
    %oli18*: redoing theorem
    \commentout{
    %oli7: I'm not sure I see the value of the other parts; just reducing to part 1.
    %oli14: stating implicit assumption, common to both parts:
    Suppose $\Ar$ is \subpartl\ and $\bar\mu(\X,\U)$ is a witness
    %oli15: diamond
    to $\mu \models \Diamond \Ar$. Then:
    \begin{enumerate}[label=(\alph*), nosep]
    \item 
    $\bar\mu$ can arise from every causal model in $\PSEMsA(\bar\mu)$.
        \hfill
        $\Big(\text{i.e., }\displaystyle
        \bar\mu \in~
        \bigcap_{\mathrlap{\!\!\!\!\mathcal M \in \PSEMs(\bar\mu)} } ~~\SD[\big]{\cal M}
        %oli14: now that common context has been factored out, no longer necessary
        % \quad\text{for all witnesses $\nu$}.
        .
        \Big)
        $
    %oli7: removing th e other parts; they're just not so important.
    % \item $\displaystyle
    %     \{ \,\Ar\, \} = ~~
    %     \bigcap_{\mathclap{\nu \in \Wits(\mu, \Ar)}}~~
    %     % \bigcap_{{\mathcal M \in \PSEMs(\nu)}} \Ar_{\cal M}
    %     \big\{ \Ar_{\cal M} : {\mathcal M \in \PSEMs(\nu)} \big\}.
    % $
    % %oli3: moved conjeture up from above to form a third point
    % \item $\displaystyle
    %     \Wits(\mu, \Ar) = 
    %         \bigcup_{{\nu \in \Wits(\mu, \Ar)} }
    %         %oli3: there's a problem here
    %         % \SD[\big]{\mathcal M_\nu}
    %         \bigcap_{{\mathcal M \in \PSEMs(\nu)} } \!\!\!
    %         \SD[\big]{\mathcal M}
    %             .
    % $    
    % \item
    % If $\mathcal M$ is a causal model and $\nu \in \SD{\mathcal M}$, 
    % then
    \item Moreover,
    $\PSEMsA(\bar\mu)$
        is the set of all PSEMs 
        %oli15:
        % that can give rise to $\bar\mu$ and  have dependency structure consistent with $\Ar$.
        from which $\bar\mu$ can arise, with equations consistent with a derandomization of the dependency structure $\Ar$.
        % have equations representable with functions of type $\Ar$.
    That is, 
    %oli15: expanding + displaymode 
    % $
    \[
        %oli15: do I want to add types here?
        % \PSEMsA(\nu) = \{   
        % \PSEMsA(\bar\mu(\X,\U)) = \{ 
        \PSEMsA(\bar\mu) = \{ 
        \mathcal M 
        % = (\U,\X,\mathcal F)
        ~|~ \bar\mu \in \SD{\mathcal M}
        ~~\land~~ \forall X {\in }\X.\, {\Pa}_{\mathcal M}(X) \subseteq \Src {a_{\!X}} \cup \{U_{a_{\!X}}\} \}.
    \]
    \end{enumerate}
    }
    %
    %oli18*: end comment out above; new material below
    If $\bar\mu(\X,\U_{\Ar})$ is a witness for \scibility\ with $\Ar$ and $\mathcal M$ is a PSEM with dependency structure $\Ar$,
    then $\bar\mu \in \SD{\cal M}$ if and only if $\mathcal M \in \PSEMsA(\bar\mu)$. 
\end{linked}

%oli16: this is a weak restatement of the theorem. Replacing with something more useful:
\commentout{
In other words, if you convert a distribution $\bar\mu$ witnessing $\mu \models \Ar$ to a causal model $\mathcal M \in \PSEMs(\nu)$, then 
% (a)
$\bar\mu$ can arise from $\mathcal M$; moreover,
% (b)
$\PSEMs(\bar\mu)$ is the set of all SEMs from which $\bar\mu$ can arise,
with equations consistent with the dependency structure of the \hgraph\ $\Ar$. 
}
%oli18: Now put directly into the theorem; bringing other takeaway here.
% In other words, if $\bar\mu$ is a \scibility\ witness for a \hgraph\ $\Ar$ and $\mathcal M$ is a causal model of shape $\Ar$, then $\bar\mu \in \SD{\cal M}$ if and only if $\mathcal M \in \PSEMsA(\bar\mu)$.
%oli18:
% Our next result shows that the non-deterministic procedure used to define $\PSEMsA(\bar\mu)$ above corresponds exactly with the set of causal models with structure $\Ar$ that could have given rise to $\bar\mu$. 

Equivalently, this means that $\PSEMsA(\bar\mu)$, the possible outputs of \cref{constr:PSEMs}, are precisely the randomized PSEMs 
%oli19:
% that have
%joe8
% a
%oli19: this "a" changes the meaning; I meant that the randomized PSEMs (which have dpenedency structure A) can give rise to \bar \mu, not that their dependency structure can. 
%oli19: rewording--- "of" works much better here actually. 
of dependency structure $\Ar$ that can give rise to $\bar\mu$. 
This is already substantial evidence that causal models $\mathcal M \in \PSEMsA(\bar\mu)$ are closely related to the \scibility\ witness 
%joe8
% $\bar\mu$---but everything we have seen so far only describes the
$\bar\mu$. But everything we have seen so far describes only the
 correspondence in the absence of intervention, a setting in whch many causal models are indistinguishable.  
We now show that the correspondence goes deeper, by extending it to interventions. 
%oli16: cutting this; replacement material above and below
% We now further develop the relationship between a \scibility\ witness $\bar\mu$ and the causal models $\mathcal M \in \PSEMsA(\bar\mu)$ derived from it: intervention in $\mathcal M$ amounts to ordinary conditioning in $\bar\mu$.
In any randomized PSEM $M$, we can define an event 
% over contexts $\V(\U)$
\begin{equation}
    % oli14: perhaps worth pointing out notationally that this depends on
    % the equations, and hence on M
    % \mathrm{do}(\mat X{=}\mat x) := 
    \mathrm{do}_{M}(\mat X{=}\mat x) := 
    % \mathrm{do}(\mat X\overset{M}{=}\mat x) := 
    %oli18:
        % \bigcap_{X \in \mat X} \forall \mat s.~f_X(U_{{X}}, \mat s) = \mat x[X].
        % \Big\{ \mat u \in \V(\U) ~\Big|~ \forall X \in \mat X,\mat s \in \V(\Pa\nolimits_M(X)).~f_X(\mat u[U_X], \mat s) = \mat x[X]         \Big\}
%joe8 What does x[X| mean.  Can't we just get rid of the X?  This
%would make things clearer for the reader.  
%oli19: We can get rid of the X if we only intervene on one  variable at a time, but that weakens the theorem for no real reason. This means the value of the variable X in the joint setting \mat x = (x_1, x_2, x_3, ... ).  One of those components corresonds to the variable $X$. This notation needs to be defined, but it needs to be defined earlier, because it's also used in the construction. 
%joe14*: you saved me the bother of stressing that the notation has to
%be defined.  It's usage also to be explained.
%oli20: now done. 
%joe16: it wasn't done, but is now
        \bigcap_{X \in \mat X} \bigcap_{\mathrlap{\mat s \in \V(\Pa(X))}~~}~~~f_X(U_{\!{X}}, \mat s) = \mat x[X],
    %oli21: saving a line by moving the below into the equation
    \qquad\parbox{3cm}{\centering
    %joe16
    %oli21: here, X is a single variable
    % where $\mat x[X]$ is the restrition of $\mat x$ to the variables in $X$.
    where $\mat x[X]$ is the value of $X$ in $\mat x$.
    }
\end{equation}
%joe8: cut.  This is a distraction.  You need to stay focused
%oli19: Spencer strongly believes there needs to be intuition here, and I agree with him.  I really like both this and the footnote, which I think makes an important connection to the literature. The whole point of response variables is do do stuff like this. 
%oli19: That said, in search of compromise, I will reluctantly cut the footnote for the NeurIPS submission.
%joe14: I'm a big fan of intuition.  We definitely don't want the
%footnote, either for NeurIPS or the full paper.  It's a distraction
%anbd absolutely does not give intuition.
This is intuitively the event in which the randomness is such that $\mat X = \mat x$ regardless of the values of the parent variables.
\vfull{%
\unskip\footnote{\vfull{This is essentially the event in which, for each $X \in \mat X$, the \emph{response variable} 
$\hat U_X := \lambda \mat s. f_{X}(\mat s, U_X)$,
whose possible values $\smash{\V(\hat U_X)}$ are functions from $\V(\Pa_{M}(X))$ to $\V(X)$ \citep{Rubin74,BP94}, takes on the constant function
%oli23: scrubbing \lambda notation in \vfull
$\lambda \mat p. ~x$.%
% $\mat p \mapsto x$.
}}%end footnote
}%end vfull
%
%joe16
%Thus,
%oli21: shaving a line
% As the following result shows,
As we now show,
conditioning on 
$\mathrm{do}_{M}(\mat X{=}\mat x)$ has the effect of intervention.
%joe8: cut.  Yet another distraction.  Hardly any of the readers will
%know what nonstadard probabilities are.  It feels like you're doing a
%core dump of interesting observations you've made.  While they may
%interest some people (among the few who will know what you're talking
%about), they don't belong in the paper.
%
%oli19: Again, I'll cut it in search of compromise, but it does make an important point conceptually  (one that addresses a question you keep asking: what about when the probability is zero? Doesn't the theorem say nothing then?). I also think there's enough there that a reader who isn't familiar with nonstandard probabilities could still get something out of it if interested. 
%joe14: If the best you can say is "a reader who isn't familiar with
%nonstandard probabilities could still get something out of it if
%interested", then the material should absolutely be cut.  That's not
%how to decide what gets into a paper!
%oli23: merging this with other footnote for \vfull
% \vfull{
% \unskip\footnote{Even if $\mathrm{do}_M(\mat X{=}\mat x)$ has probability zero, \cref{theorem:condition-intervention-alignment} places bounds on the conditional probabilities that are possible limits of sequences of distributions $(\nu_k)_{k\ge 0}$ where $\nu_k(\mathrm{do}_M(\mat X{=}\mat x)) > 0$, i.e., the results of conditioning a non-standard conditional probability on this probability-zero event.}
% }
%
%oli7: not the right story
% Intervening in $\hat{\mathcal M}$ corresponds to ordinary conditioning in the distribution $\nu$. 
%oli18: removing; more clutter than help
% For the sake of simplicity and concordence with the causality literature, take $\PSEMsA(\bar\mu)$ to consist only of ordinary (i.e., not generalized) PSEMs, effectively restricting scope of the theorem statement to \subpartl\ \hgraph s $\Ar$.

% The resulting model, called a \emph{functional causal model} is the
% minimal one that allows all interventions.
% Let $\mathcal M_{\nu}$ denote the resulting SEM.
%joe2*: I've completely lost the thread here.  I don't see what the
%theorem is telling me.  I would have expected a theorem saying that
%you can can convert a regular hypergraph to an equivalent causal
%model, but Theorem 7 doesn't seem to e saying that, nor does
%Proposition 9.  If we don't have this result, then we should cut
%this material (and the corresponding material in the introduction).
%We should then discuss the situation with hypergraphs that aren't
%regular.  
%oli2: While I see your point that this may not quite be the right thing to state here, I want to point out that you can't naturally convert the hypergraph to a causal model; the hypergraph contains only qualitative information, while the causal model contains lots of quantitative information. Rather, you can convert a *proof* of structural compatibility with a hypergraph, to a causal model. This result is that conditioning the witness is like intervening in the corresponding causal model.
%joe3: It's not the proof that you're converting to a causal model, but the instations of \nu.  

%oli3: I'm not sure if you're being deliberately pedantic or not, but I stand by my point above.  The instantiation of \nu and a certificate that it satisifies properties (a-c) is necessary and sufficient for a (constructive) proof of structural compatibility.  My broader point --- that your previous comment was confusing the qualitative/structural/proposition/type level (e.g., hyper-graphs, independencies, structural compatibility) with the quantitative/observational/proof/value level (e.g, a causal model, or a witness of structural compatibility \nu, which is arguably a proof).
%joe5: the notion of constructive proof is coming out of the blue.
%The reader won't have any idea of what you're talking about.  I think
%you're missing my point: your presentation will confuse the reader.


\begin{linked}{theorem}{condition-intervention-alignment}
%joe2
  %  Suppose $\nu \in \Wits(\mu,\Ar)$.
%    Let $D_x := \hat U_{X_a}=\lambda\textunderscore.x$ be the event
%    in which $\hat U_{X_a}$ equals constant function that returns $x$.
%    It then holds that
%oli2: expanding. Your "if" construction was better for what I had,
% except I actually need a few more premises to be perfetly precise. 
% If $\nu \in \Wits(\mu,\Ar)$ and
%joe4*: I can't make sense of this theorem, and I don't see how it
%fits into the story.  In the introduction, you say that
% witness contains enough information to answer counterfactual
% queries. Is this supposed to show that?  If so, how?  If not, where
% do you show it?  (If you don't show it, you certainly shouldn't say
% it in the introduction.)
%oli5*: The purpose of this theorem is to further justify the deep correspondence I've been talking about between a witness distribution \nu \in Wits(\mu, \Ar) and the causal models that we can derive from it. 
%oli14:
% Suppose $\nu(\X,\U) \in \Wits(\mu,\Ar)$
%oli15: 
% Suppose $\bar\mu(\X, \U)$ is a \scibility\ witness to $\mu \models \Ar$,
Suppose that
%oli21: getting rid of a QIM and otherwise shortening
% $\bar\mu(\X, \U_{\Ar})$ is a \scibility\ witness to $\mu \models \Ar$,
$\bar\mu$ is a witness to $\mu \models \Diamond \Ar$,
% \scibility\ witness, 
 $\mathcal M \in \PSEMsA(\bar\mu)$, $\mat X \subseteq \X$ and $\mat x \in \V(\mat X)$.
%oli14: factored out one more thing:
If $\bar\mu(\mathrm{do}_{\cal M}(\mat X{=}\mat x)) > 0$, then:
%oli14:
% If $X \in \X$, $x \in \V(X)$, and
% For $\mat X \subseteq \X$ and $\mat x \in \V(\mat X)$, 
%joe2: I've never seen the notation \lambda_.x before.  Please remove
%it.  
%oli2: yes you have; if I write \lambda y.x or \lambda z.x or \lambda a.x,
% all of these represent the constant function.  The underscore is no
% different from y or z or a, except that has a strong connotation of
% "the variable in which you store things you want to discard", across
% many different programming languages, and also in the theory of programming
% languages. I think it's very clean, and it's also the most standard
% notation for the constant function I know of.  Plus, I spelled it out
% in words already, so readers who aren't familiar with the standard
% shouldn't have any trouble.
%joe3: I've seen lambda notation since (quite literally, I suspect)
%before you were born.  I've never seen the underscore.  
%Please get rid of it.  You may find it clean; I found it onfusing. I
%suspect that I'm far more typical of readers.
%oli3: This is not so important to me, so I'll give you this one.  However, I still want to say that I generally disagree with your approach to making stylistic judgements such as this one.  I think "traditional" notation does not trump "better", especially if the notation is accompanied by words. And, if I'm right and the underscore has become relatively popular in recent years, I feel that  "traditional" has no benefit on its own, unless it's also "better". 
%joe5: It'm not arguing for "traditional" vs. "modern".  I'm arguing
%for what the reader is comfortable with.  In this case, it's your
%only use of lambda notation in the paper, and the typical UAI reader
%won't be familiar with it at all.
%oli3: as you requested... although I think it's now more confusing; I suspect some readers will think: what is "s"? 
 % $D_x := (\hat U_{\!a_{\!X}}=\lambda\mathunderscore.x)$ is the event
%joe5*: I also think "what's s", and have no clue of what you're trying
%to do here.  What this says is that instead of worrying about
%notation, and arguing about what flavor of lambda notation to use,
%you next to explain and motivate what's going on here much better
%than you've done so far.  This is the main point I've been trying to
%make all along.  (Notice all my "I have no clue what you'rje doing"
%comments.)  
%oli6: I still don't understand why you're confused; $s$ is the argument to the function, in \lambda notation, and I've written in words "the event in which U_a_X equals the constant function that returns x". 
 %oli3: alternatively, if you're ok with underscore as a "blank" but not in lambda notation, we can say "...,  ( _ \mapsto x ) is the constant function that always returns x, then ...". 
%oli14: now defined above, in more standard terms
 % $D_x := (\hat U_{\!a_{\!X}}=\lambda s.x)$ is the event in which $\hat U_{\!a_{\!X}}$ equals the constant function that returns $x$, then
 %
 % ~~~(a)~$\bar\mu(\X \,|\, \mathrm{do}_{\cal M}(\mat X{=}\mat x))$ can arise from $\mathcal M_{\mat X {\gets} \mat x}$;
\begin{enumerate}[label={(\alph*)}, topsep=0pt,itemsep=0pt]
\item
    $\bar\mu(\X \,|\, \mathrm{do}_{\cal M}(\mat X{=}\mat x))$ can arise from $\mathcal M_{\mat X {\gets} \mat x}$;
% 
    % And, in general,
    % every causal model $\mathcal M$
    % \[
    %     \inf_{\substack{
    %         \nu \in \Wits(\mu, \Ar) \\
    %         \nu(D_x) > 0
    %     }} \nu(\X \mid D_x)
    %     \le
    %     \mathcal M(\X \mid \mathrm{do}(X{=}x))
    %     \le
    %     \sup_{\substack{
    %         \nu \in \Wits(\mu, \Ar) \\
    %         \nu(D_x) > 0
    %     }} \nu(\X \mid D_x)
    % \]
    % where $\Wits(\mu, \Ar)$ denotes the set of witnesses
    % that $\mu$ is compatible with $\Ar$.
    
    \item
    for all events $\varphi \subseteq \V(\X)$, 
    $ \displaystyle
    % \[
        \Pr\nolimits_{\mathcal M}
        \big([\mat X {\gets} \mat x]\varphi\big)
    \le
        \bar\mu\big(\varphi \,\big|\, \mathrm{do}_{\cal M}(\mat X{=}\mat x)\big)
    \le
        \Pr\nolimits_{\mathcal M}
        \big(\langle\mat X {\gets} \mat x\rangle\varphi\big)
    $ \\
    % \]
    % for all events $\varphi \subseteq \V(\X)$, 
    % with equality when $\mathcal M \models \U \tto \X$. 
    and all three are equal when $\mathcal M \models \U \tto \X$
    (such as when $\mathcal M$ is acyclic). 
\end{enumerate}
\end{linked}

%oli21: another attempt at returning to the main story and reminding the reader why this is related to the main story. My last attempt at the end was cut; I'm hoping that this is both better and in the right place.
\cref{theorem:condition-intervention-alignment} shows that the relationship between witnesses and causal models extends to interventions. 
%
\commentout{%
To those familiar with the causality literature, it may be surprising that $\mathrm{do}_M(\mat X{=}\mat x)$ is actually an event \citep{sep-causal-models,halpern-2000}, and even moreso that 
ordinary conditioning on this event has the effect of intervention, since conditioning and intervention 
%oli18:
% are typically (and for good reason!) regarded as fundementally different. 
are usually thought of as fundmentally different. 
(Indeed, intervening on $\mat X{=}\mat x$ remains fundementally different from conditioning on $\mat X{=}\mat x$.)
Nevertheless, 
    % both facts are true, 
intervention on $\mat X{=}\mat x$ can be viewed as conditioning on $\mathrm{do}_M(\mat X{=}\mat x)$, a state of affairs that has allowed us to demonstrate that the relationship between causality and witnesses extends to interventions.
}%
%
%joe8: I have no idea why the references are relevant.  More
%importantly, most people who work on causality don't deal with
%randomized PSEMs, so they won't find this result surprising.  It's a
%result that doesn't apply to the SPEMs they work with.
\commentout{%
To those familiar with the causality literature, it may be surprising that $\mathrm{do}_M(\mat X{=}\mat x)$ is actually an event \citep{sep-causal-models,halpern-2000}, and that conditioning on 
    % $\mathrm{do}_M(\mat X{=}\mat x)$ 
    % this event
    it has the effect of intervention, since conditioning and intervention are 
    conceptually very different. 
}%
%
%oli19: The references are relevant because they are places where people make a big deal out of "do(X=x)" not being an event.
%joe14: Maybe you know why you added the references, but the typical
%readef won't have a clue.  (Abd I'm not a typical reader -- I'm an
%expert in the area!)  If you were going to include this (which you
%shouldb't) it would beed much more discussion.
%oli19: Randomized PSEMs are no less expressive that PSEMs largely, and this result applies to their PSEMs as well.  The only question is whether or not the probability of do(X=x)=0, and even if it is, the theorem still says something because of the footnote that I reluctantly agreed to cut, but added to address this very point. I've cut the footnote as you asked, but I'm willing to do this only if we don't get hung up on this question of probability zero and whether or not the theorem applies. 
%joe14*: We need to find a compromise here.  As I said above, I
%strongly suspect that in the PSEMs that people actually use, this
%event will have measure 0.  It's thus important to understand what the
%result is saying in that case.  I don't think that what you cut
%really addresses this point.  This may be something we should discuss
%in our meeting, although if you could say something beforehand, that
%would be useful.
%oli20: ok, I agree--- if you're not willing to sweep the issue under the rug, we need to figure out how to communicate the conceptual content of the footnote in a more friendly way. 
%joe16*: You cannot introduce nonstandard probabilities out of the
%blue in a footnote.  This will not help the reader, and it's way down
%the tree.  In any case, it doesn't address my concern at all.  How
%about the following
%oli21: I've talked with Spencer, and we're not convinced your replacement is true; it's not the event that is close, but the probability. Commenting out and trying for a replacement. 
\commentout{%
(We remark that although $\mathrm{do}_M(X{=}x)$ is often an event of
probability 0, \cref{theorem:condition-intervention-alignment} can be
extended to events ``close'' to 
$\mathrm{do}_M(X{=}x)$ that have positive probability; conditioning on
them has an effect close to that of intervention.  We defer formal
datails to the full paper.)
}%
%
Even when $\mathrm{do}_M(\mat X{=}\mat x)$ has probability zero, 
    it is always possible to find a nearly equivalent
    %oli21:
    % model $M'$ that doesn't and the bounds of the theorem apply.
    setting where the bounds of the theorem apply.
    % Conditioning on 
    % Intervention in $M$ can then be captured by conditioning on 
     % to which the theorem still applies.
 %oli20: for the more precise version, I'm recussitating footnote, although I'm happy to change the content if we can say something better.
    \unskip\footnote{More precisely, 
    %oli21%spencer2:  replacing material 
    % the ... will have \epsilon close ... all formulas
    % and conditioning on do M' in M' will give the same probabilities
    % as conditioning ... up to \epsilon error. 
    for all $\epsilon > 0$, there exists some $\mathcal M'$ that differs from $\mathcal M$ on the probabilities all causal formulas by at most $\epsilon$, and a distribution $\bar\mu'$ that is $\epsilon$-close to $\bar\mu$, such that $\bar\mu'(\mathrm{do}_{\mathcal M'}(\mat X{=}\mat x))>0$.
    % Thus the theorem also applies in 
    %
    %oli23: tweaking and reintroducing material for vfull
    \vfull{%
     As a result, \cref{theorem:condition-intervention-alignment} places bounds on the conditional probabilities that are possible limits of sequences of distributions $(\nu_k)_{k\ge 0}$ where $\nu_k(\mathrm{do}_M(\mat X{=}\mat x)) > 0$, i.e., the 
     %oli23: 
     % results of conditioning a non-standard conditional probability
     possible outcomes of conditioning a \emph{non-standard} probability measure
        \cite{halpern-RAU}
     on this probability-zero event.}% end vfull
    } % end footnote
\commentout{
%joe8:
The connection between conditioning and intervension here may seem
surprising, because they are viewed as being conceptually very different.
%oli19: I'm happy to even admit that they *are* conceptually different
}%
%oli19: trying for a compromise with some of both of our material.  I really want to make the point about being surprising that do(X=x) is an event, though. 
%joe14: My concern is that it's a nontrivial event in only aa few
%cases.  People won't be surprised because they simply haven't
%encountered the issue.  That said, I have no problem with what you wrote.
Intervention and conditioning are conceptually very different, so
it may seem surprising that
%joe16: why should it be surprising that it's an event?
% $\mathrm{do}_M(\mat X{=}\mat x)$ is not just notation but actually an event \citep{sep-causal-models}, and that conditioning on it has the effect of intervention. 
%oli21: because many people, including the citations I gave below make a big fuss by saying "do(X=x) is not actually an event, but just notation". It's relevant because it again validates Pearl's intuition, in a way that wasn't previously made explicit.
% conditioning on $\mat X{=}\mat x$ has the effect of intervention.
conditioning can have the effect of intervention
    (and also that the Pearl's $\mathrm{do}(\,\cdot\,)$ notation actually corresponds to an event \citep{sep-causal-models}). 
% (Indeed, intervening on $\mat X{=}\mat x$ remains fundementally different from conditioning on $\mat X{=}\mat x$.)
% Nevertheless, 
%     intervention on $\mat X{=}\mat x$ can be encoded as conditioning on $\mathrm{do}_M(\mat X{=}\mat x)$,
% While true, 
We emphasize that the conditioning (on $\mathrm{do}_M(\mat X{=}\mat x)$) is on the randomness $U_X$ and not $X$ itself; 
%joe8
%intervening on $\mat X{=}\mat x$ is indeed fundmenetally different
intervening on $\mat X{=}\mat x$ is indeed fundamentally different
from conditioning on $\mat X{=}\mat x$.
% but we have shown how to encode the capcacity for interventions latent in a causal model directly into a distribution (a witness). 
%joe8
%Yet, as we have shown, even the former can be encoded as a
%probabilistic conditioning on an extended distribution (a witness).  
%joe16*: cut; this adds nothing
%oli21: even if this attempt didn't add anything, I still want to turn the story back to a focus on QIM-compatibility. I've added above. 
\commentout{%
    Yet, as \cref{theorem:condition-intervention-alignment}
    shows, even the former can be encoded as 
    probabilistic conditioning in an extended distribution (a witness), 
    %oli19: aren't we trying to tell a story here? The story is about how this has allowed us to connect witnesses and causal models. 
    %This fact has allowed us to deepen the connection between cauality
    %and \scibility\ by making sense of interventions on \scibility\ witnesses.
    %oli19: a shorter version that still brings us back to the main story
    deepening the connection between witnesses and causal models.
}%
% Nevertheless, the fact that intervention can be encoded with conditioning
% 
%     \begin{linked}{prop}{formula-bounds}
%         If $\nu$ is a \scibility\ witness and 
%         $\varphi \subseteq \V(\X)$ is an event, then
%         for all $\mathcal M \in \PSEMs(\nu)$, 
%         % we have that
%         \[
%             % \inf_{ \mathcal M \in \PSEMs(\nu)} 
%             \Pr_{\mathcal M}
%             ([\mat Y {\gets} \mat y]\varphi)
%                 \le
%                 \nu(\varphi \mid \mathrm{do}_{\mathcal M}(\mat Y{=}\mat y))
%                 \le
%             % \sup_{ \mathcal M \in \PSEMs(\nu)} 
%             \Pr_{\mathcal M}
%             (\langle\mat Y {\gets} \mat y\rangle\varphi)
%         \]
%         with equality when $\nu \models \U \tto \X$. 
%     \end{linked}
%
%oli14: In retrospect, I don't think this commutative diagram is helpful, 
% and it's certainly not space efficient. 
% \begin{center}
%     \begin{tikzcd}
%         \Delta \V(\X,\U)
%             \ar[d,"|\mathrm{do}(X{=}x)"']
%             % \ar[r,"\in"description]
%         % & 2^{\Delta\V(\X,\U)}
%         & \mathrm{PSEMs}(\X,\U)
%             \ar[d,"{[X{\gets}x]}"]
%             % \ar[l,"\SD-"]
%         % & \mathcal L(\X,\U)
%     \\
%         \Delta \V(\X,\U)
%             % \ar[r,"\in"description]
%         % & 2^{\Delta\V(\X,\U)}
%         & \mathrm{PSEMs}(\X,\U)
%             % \ar[l,"\SD-"]
%         % & \mathcal L(\X,\U)
%     \end{tikzcd}
% \end{center}

% In other words, intervention can both be viewed as conditioning in the witness distribution $\nu$.  
% We have now seen how a witness $\nu$ for structures $\Ar$ whose targets do not overlap, can be viewed as a causal model $\mathcal M_\nu$.  
% Here's another illustration of their relationship: we can reconstruct the 
% set of all witnesses from the extended-distribution semantics of their causal models.
% \begin{prop}
%     % \[
%     %     \Wits(\mu, \Ar) = 
%     %     \bigcup \Big\{
%     %         \SD{\mathcal M_\nu} : \nu \in \Wits(\mu, \Ar) 
%     %         \Big\}.
%     % \]
%     $\displaystyle
%         \Wits(\mu, \Ar) = 
%             ~~\bigcup_{\mathclap{\nu \in \Wits(\mu, \Ar)} }~~
%                 \SD[\big]{\mathcal M_\nu}.
%     $
% \end{prop}

%oli16: moving elsewhere:
\commentout{
\subsubsection{Acyclic Hypergraphs}
When $\Ar = \Ar_G$ for some is a directed acyclic graph $G$ (i.e., a qualitative BN), then a witness to $\mu \models \Ar$ is
%joe4*: this is not a useful summary, since I have no idea of what it
    %means to be "essentially an acyclic causal model".
%oli5: what's to know? It means that, in this setting, a witness of compatibility is (in some sense naturally isomorphic to) an acyclic causal model. 
%oli7: 
essentially an acyclic causal model
%joe3*: A witness is a distribution.  At a minimum, you have to
% explain how you're viewing the distribution as a causal model, and
% why it should be acylic in this case.  I'm lost here.
%oli3*: this is meant as a preview; I'd like to prime the readers for
% the technical dedatils they'll see in the next section. I'll add some
% extra verbage to make the intended purpose of this section clear.
%
%
in which the equation for $X$ depends on $\Pa_G(X)$ and
%joe3*: In what sense does it "ultimately give rise to \mu$?  This has
%to be clarified
%oli3: in the sense of being the distribution over the endogenous variables that results when no interventions are taken. But again, I want to  get into the details later. Here, I'm just giving an overview of our results.
%joe4*: Readers can't read your mind, Oliver!  It's not a useful
%overview of the results if a reader can't understand what it says.  Either
%explain "ultimately gives rise to" means, or cut it.
%oli14: now that I've defined "gives rise to", I can simplify the wording and just lean on my definition
% independent noise, and which ultimately gives rise to $\mu$ in the absence of intervention.
independent noise, 
%oli14:
% from which $\mu$ can arise.
that uniquely determines the distribution $\mu(\X)$ in the absence of intervention.
%oli6: already mentioned
    % Because the causal model is acyclic, $\mu$ is the unique such distribution. 
    %oli6: irrelevant
    % In practice, most causal modeling is done with acyclic models.
    %
    %oli3: moving material from above here, with some modification
        %joe1: This is premature and breaks the flow.  I'm also not so sure
    %it's well known.  (If you think it is, please add a reference.)
    %oli1: I haven't seen it formalized carefully, but reading through Pearl's book
    % strongly gave me the impression some version of this fact is is common knowledge. 
    %joe2: It certainly isn't common knowledge! I don't know it.
    %oli3: first part is now no longer necesary because it's in the right place
    % In \cref{sec:causal}, we will see how  a witness of compatibility $\mu(\X, \U)$ can be seen  as a (partially specified) causal model.
    %oli3: modifications from your comments:
    % From this perspective, \cref{theorem:bns} is a form of an equivalence  well-known in the causlity community:
    %oli14:
    % From this perspective, 
    In light of this,
    \cref{theorem:bns} can be viewed as result 
        that implicitly undergirds much of the work on causality:
    %oli14:
    % every acyclic causal model with independent per-variable noise variables
    every acyclic randomized SEM
    induces a distribution with the independencies of the appropriate Bayesian Network---%
    and, conversely,
    every distribution with those independencies
    arises from such a causal model.

    %oli3: what about a result like this? 
    % (I haven't proved it yet, but I don't think it should be too bad)
    %oli6: I don't think it's necessary; we have enough results; cutting. Please chime in if you like this one.
    %oli7: this result still seems like the wrong one, but putting the commeted out version here instead of above. 
    \commentout{
    \begin{prop}
        If $\Ar = \Ar_G$ is acyclic,
        $\mu \models \Ar_G$, and $G$ is minimal 
            (i.e.,$\mu \not\models \Ar_{G'}$ for all $G' \subsetneq G$),
        then for all $\nu \in \Wits(\mu, \Ar)$ and $\mathcal M \in \PSEMs(\nu)$, $\Ar_{G} = \Ar_{\mathcal M}$.
    \end{prop}
    }
}

%oli16: this material has been reworked into the section above.
\commentout{
%joe7*: I find this section unacceptable.  Wile I agree that a witness can
%probably be intepreted as an causal model with constraints, the
%converse is certainly not true (there are lots of constraints that I
%don't think you'll be able to capture).
%oli8*:  You are correct, but the point is irrelevant. The subsection is 
% called "From Witnesses to Causal Models".  What's relevant is that 
% witnesses can still be thought of causally for these other cases. 
%
%joe7*: Moreover, we don't have
%nything like an analogue Proposition 4 or THeorem 5 in this case.
%All we have is the extremely handwavy argument below.  I think we
%should cut it.
%oli8*: there are two different cases. For subpartitional hypergraphs
% we have something concrete and absolutely worth saying.  For 
% superpartitional causal models, we still have something that will
% clarify the state of the affairs. Instead of cutting it, I will 
% tighten up the wording so it's not so hand-wavy.
\subsubsection{Non-partitional Hypergraphs}
In the case where $\Ar$ is \subpartl\ (but not \suppartl), 
%oli14:
% things are very similar to the \partl\ case. 
the situation is very similar. 
The only difference is that the variables that are not targets of any \hyperarc\ do not naturally correspond to an equation, and hence should be treated as exogenous.
%oli9:
% Thus, a witness to \scibility\ with a \hgraph\  $(\X, \Ar)$ corresponds to causal model that with
Thus, a witness $\bar\mu$ to \scibility\ with a \subpartl\ \hgraph\  $(\X, \Ar)$ corresponds to causal model(s) $\PSEMs(\bar\mu)$ with 
endogenous variables $\bigcup_{a \in \Ar} \Tgt a$ and exogenous variables $\{ U_a \}_{a \in \Ar} \cup (\X \setminus \bigcup_{a \in \Ar}\Tgt a)$.
}

%oli2*: while I am fond of this material, and I feel
% some ownership of it because I thought I discovered it,
% I reluctantly agree that it's not the focus of this story,
% and so it doesn't belong here.  It might fit better,
% for example, as a part of your causal BN paper. It applies
% to much more general structures of causal models than the
% acyclic ones you focus on, but has the distinct downside
% that I haven't proved it carefully yet. And, even if I had,
% the result wouldn't be important enough to this story to merrit
% putting here. 
%
\commentout{
\begin{conj}
%oli1:
% For every event $B \subseteq \V\!\X$,
% and every PSEM $\mathcal M$ that has shape $\Ar$
% with $\mathcal M(\X) = \mu(\X)$ we have that
% Let $\mathcal M$ be a PSEM with structure $\Ar$, that determines
% Fix a distribution $\mu(\X)$, and an event .
Every PSEM $\mathcal M$  with structure $\Ar$ determining a distribution
    $\mu(\X)$ in the absence of interventions, satisfies
\[
    \Wits(\mu, \Ar\mid D_x)_*( B )
    \le
    \mathcal M(B \mid \mathrm{do}(X{=}x))
    \le
    \Wits(\mu, \Ar \mid D_x)^*( B )
\]
for every event $B \subseteq \V(\X)$.
Here $\Wits(\mu, \Ar \mid D_x)$ denotes the set of witnesses
that $\mu$ is compatible with $\Ar$ conditioned on the event $D_x$,
while $\Wits(\mu, \Ar)_* := \sup_{\nu \in \Wits(\mu,\Ar)} \nu(B \mid D_x)$,
$\Wits(\mu, \Ar)^* := \inf_{\nu \in \Wits(\mu,\Ar)} \nu(B \mid D_x)$ 
are the corresponding lower and upper probabilities, respectively.
\end{conj}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%joe17*: I still feel (quite strongly) that the paper would be better
%without this section.  Have you discussed it with Spencer?
%oli22*: MONOTONICITY section gone :( :( :( --- implementing fragement + link in previous section
% \section{Monotonic Reasoning about \SCibility}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \clearpage
%joe2*: This is the wrong story.  We should tie everything to the new
%type 1 semantics.  I changed the section title and added the next paragraph
% \section{Qualitative Probabilistic Dependency Graphs}
% \section{Qualitative PDGs}
%oli3: a different focus, broader story
% \section{Scoring Functions for (In)compatibility}
% \section{\SCibility\ and Entropy}
%oli20:
% \section{\SCibility\ and Entropy}
\section{\SCibility\ and Information Theory}
    \label{sec:pdgs}
    \label{sec:info}
    
%oli12*: a rewrite from scratch, as requested. I will not look at (or copy) any material from the previous draft while writing this, with the exception of the sentence that defines and explains IDef. 
%oli14: removing heading
% \subsection*{New \S5 Intro: Telling the Story in Explicit Detail}

%oli20: cutting old section intro:
\commentout{
%oli14:
% In this section, we will investigate the rich relationship that \scibility\ has with information theory, which is mediated by the theory of probabilistic dependency graphs. 
%oli15: 
% In this section, we investigate the rich relationship between \scibility\ and information theory, which is mediated by the theory of probabilistic dependency graphs. 
In this section, we investigate the rich relationship between \scibility\ and information theory, which is mediated by the theory of probabilistic dependency graphs. 
%oli15**: a nod to your story. 
One aspect, which we will develop in \cref{sec:scoring-funs}, is an entropy-based scoring function that measures a distribution's degree of \SQIM-incompatibility. That function is quite different from (although can be viewed as essentially a special case of), the original qualitative PDG scoring function \citep{one-true-loss,pdg-infer}. 
%oli15: removed paragraph break
%
%oli14:
% Perhaps the most important of aspect of this relationship is a strong information-theoretic bound that is a consequence of \scibility. 
%oli15: expanding story
% Perhaps the most important of aspect of this relationship is an information-theoretic consequence of \scibility. 
% Along the way (\cref{sec:sc-infobound}), we will see an arguably more important aspect of the relationship: 
% Perhaps even more importantly, 
Before that (\cref{sec:sc-infobound}), we present a more useful and fundmental
 % and arguably far more important
 aspect of the relationship, based on the same scoring function: a generic and highly non-trivial information-theoretic consequence of \scibility.
}
%oli20*: new section intro
The fact that the dependency structure of a (causal) Bayesian network describes the independencies of the distribution it induces is fundamental to both causality and probability. 
%spencer3:
It makes explicit the distributional consequences of BN structure.
%spencer3: moved below
% Yet, despite
% substantial interest \cite{Baier_2022},
% giving a similar explanation for more complex (e.g., cyclic) dependency structures
% remains largely an open problem. 
%joe17*: This seems to have come out of the blue.  We haven't talked
%about information theory yet, let alone how this fact is at all
%related to information-theoretic constraints.
%spencer3*: This is important. It TODO
%This fact characterizes
%the distributions that can arise from a given Bayesian network
%structure in terms of information-theoretic constraints.
Yet, despite
substantial interest \cite{Baier_2022},
%joe17
%generalizing this fact to more
%spencer3: reverting. This is probably the single edit I feel strongest about :)
% characterizing independencies in more
generalizing the BN case to more
complex (e.g., cyclic) dependency structures remains largely an open
problem.  
%joe16: I don't find this substantial at all; why don't we let the
%reader be the judge
%problem. In \cref{sec:sc-infobound}, we substantially generalize the
In \cref{sec:sc-infobound}, we 
%spencer3:
% generalize the BN case by 
% address this problem by
generalize the BN case by
%spencer3:
% providing a generic information-theoretic constraint
providing an information-theoretic constraint,
%spencer3: 
capable of capturing conditional independence, functional dependence, and more,
% (which reduces to conditional independencies in the BN case)
%spencer3:
% on the distributions that can arise from an arbitrary dependency structure $\Ar$
on the distributions that can arise from an \emph{arbitrary} 
%spencer3:
% dependency structure $\Ar$, in terms of $\IDef$, 
% the scoring function used for the qualitative structure of PDGs \cite{pdg-aaai}.
dependency structure.
%spencer3: moving below
% in terms of $\IDef$, the scoring function used for the qualitative structure of PDGs \cite{pdg-aaai}. 
This connection between causality and information theory has implications for both fields. It grounds the cyclic dependency structures found in causality in concrete constraints on the distributions they represent. 
%joe17*: You had promised references here.  We cannot include this in
%the paper without such references.
%oli22: I promised references when for when this comes up below, which I had already provided. I'll copy them here as well. 
At the same time, it allows us to resolve longstanding confusion about structure in information theory, clarifying the meaning of the so-called ``interaction information'', and
%oli20: some options:
% { reframing, recontextualizing }
% overturning a standard counterexample. 
recasting a standard counterexample to substantiate the claim it was intended to oppose.
In \cref{sec:scoring-funs}, we strengthen this connection. Using entropy to measure distance to (in)dependence, we develop a scoring function to measure how far a distribution is from being QIM-compatible with a given dependency 
%oli20: connect new scoring function in terms of idef, as agreed:
% structure, and
structure.  
This function turns out to have an intimate relationship with the qualitative PDG scoring fucntion $\IDef$, which we use to 
%spencer3
%oli22: I think the flow got worse in some ways; reverting, but will come back to this.
% Interestingly, both our constraint and our scoring function have a close relationship with the qualitative PDG scoring function $\IDef$. We use this to show that our constraint not only applies to compatible distributions, it also applies (with some loss) to distributions that are only ``near-compatible''.
show that our information-theoretic constraints degrade gracefully on ``near-compatible'' distributions.


    % forming the beginnings of a bridge between information theory and causality. 
 % generalizing the (in)dependencies we have already seen.
%oli14:
% What constraints does $\mu \models \Ar$ place on a distribution $\mu$?
%oli15*: moved the paragraph that was here below, into section 5.1
%
%(oli14: moving figure here; cut from below)
%oli21: removing figure; its components have been moved elsewhere
\commentout{
\begin{figure}
    %oli20*: Ok; I have some ideas for how to use the space in this figure more effectively, but I'm pushing the implementation to the next iteration.
    \centering
    % \begin{tabular}{c}
    %     % $\H(A,B,C) = 3$ & $\H()$ & \\ 
    %     $\H(A | B,C) = \H(B|A,C) = \H(C | A,B) = 0$ \\
    %     $\I(A;B) = \I(A ; B | C ) = \I(B ; C | A) = \I(A ; C | B) = 1\,\text{bit}$ \\
    % \end{tabular}
    \def\circsize{1.6}
    \def\radsize{0.8}
    % \edef\outerrad{{\circsize/2+\radsize}}
    \edef\outerrad{1.6}
    \def\innerrad{1.00}
    % \def\twosize{0.5}
    \scalebox{0.7}{
    \begin{tikzpicture}[center base]
        \node at ({-\circsize-\radsize+0.1},{\circsize+\radsize-0.2}) {\underline{\textbf{(a)}}};

        \path[fill=white!90!black] (210:\radsize) circle (\circsize) ++(225:\circsize) node[label={[label distance=2pt,inner sep=0pt]left:\Large $X$}]{};
        \path[fill=white!90!black] (90:\radsize) circle (\circsize)
            % ++(125:\circsize) node[label={left:$Y$}]{};
            ++(65:\circsize) node[label={[label distance=1ex]right:\Large $Y$}]{};
        \path[fill=white!90!black] (-30:\radsize) circle (\circsize) ++(-45:\circsize) node[label={[label distance=1pt,inner sep=0pt]right:\Large$Z$\!}]{};


        \begin{scope}[gray]
            \draw[] (-30:\radsize) circle (\circsize);
            \draw[] (210:\radsize) circle (\circsize);
            \draw[] (90:\radsize) circle (\circsize);
        \end{scope}
        
        %oli9: replacing this mutual info circle with one more crescent.
        % \begin{scope} % I(B;C)
        %     \clip (-30:\radsize) circle ({\circsize+0.03});
        %     \clip (90:\radsize) circle ({\circsize+0.03});
        %     \draw[ultra thick,blue,fill=blue,fill opacity=0.3] (-30:\radsize) circle (\circsize);
        %     \draw[ultra thick,blue] (90:\radsize) circle (\circsize);
        % \end{scope}
        \begin{pgfinterruptboundingbox} % H(Z|X)
            \begin{scope}[even odd rule]
                \clip (210:\radsize) circle ({\circsize-0.03}) (-3,-3) rectangle (3,3);
                \clip (-30:\radsize) circle ({\circsize+0.03});
                \draw[ultra thick,blue,fill=blue, fill opacity=0.3] (-30:\radsize) circle (\circsize);
                \draw[ultra thick,blue] (210:\radsize) circle (\circsize);
            \end{scope}
        \end{pgfinterruptboundingbox}
        \begin{pgfinterruptboundingbox} % H(X|Y)
            \begin{scope}[even odd rule]
                \clip (90:\radsize) circle ({\circsize-0.03}) (-3,-3) rectangle (3,3);
                \clip (210:\radsize) circle ({\circsize+0.03});
                \draw[ultra thick,purple,fill=purple, fill opacity=0.3] (210:\radsize) circle (\circsize);
                \draw[ultra thick,purple] (90:\radsize) circle (\circsize);
            \end{scope}
        \end{pgfinterruptboundingbox}
        \begin{pgfinterruptboundingbox} % H(Y|Z)
            \begin{scope}[even odd rule]
                \clip (-30:\radsize) circle ({\circsize-0.03}) (-3,-3) rectangle (3,3);
                \clip (90:\radsize) circle ({\circsize+0.03});
                \draw[ultra thick,violet!50!black,fill=violet, fill opacity=0.2] (90:\radsize) circle (\circsize);
                \draw[ultra thick,violet!50!black] (-30:\radsize) circle (\circsize);
            \end{scope}
        \end{pgfinterruptboundingbox}
        \node at (-90:\innerrad) {\small$\I(X;Z|Y)$};
        \node[rotate=-60] at (30:\innerrad) {\small$\I(Y;Z|X)$};
        \node[rotate=60] at (150:\innerrad) {\small$\I(X;Y|Z)$};
        \node at (0:0){\small$\I(X;Y;Z)$};
        \node at (90:\outerrad) {\small$\H(Y|X,Z)$};
        \node[rotate=60] at (-30:\outerrad) {\small$\H(Z|Y,X)$};
        \node[rotate=-60] at (-150:\outerrad) {\small$\H(X|Y,Z)$};
        
        %oli9:
        % \node[purple!50!black,text opacity=1,fill=purple!15!white,fill opacity=1,
        %         % rotate=0] (HAB) at (164:2.7) 
        %         rotate=45] (HAB) at (164:2.3) 
        %         % rotate=0] (HAB) at (164:2.3) 
        %     {\small$\H(X|Y)$};
        \node[purple!50!black,text opacity=1,fill=purple!15!white,fill opacity=1,
                rotate=0] (HAB) at (-110:2.0) 
            {\small$\H(X|Y)$};
        % \node[violet!20!black,text opacity=1,fill=violet!15!white,fill opacity=1,
        %         % rotate=0] (HBC) at (148:2.60) 
        %         rotate=45] (HBC) at (125:2.40) 
        %         % rotate=0] (HBC) at (125:2.40) 
        %     {\small$\H(Y|Z)$};
        \node[violet!20!black,text opacity=1,fill=violet!15!white,fill opacity=1,
                rotate=0] (HBC) at (145:2.10) 
            {\small$\H(Y|Z)$};
        % \node[blue!50!black,text opacity=1,fill=blue!15!white,fill opacity=1,
        %     % rotate=0] (IBC) at (28:2.55)
        %     % rotate=-50] (IBC) at (32:2.05)
        %     rotate=0] (IBC) at (32:2.05)
        %     {\small$\I(Y;Z)$};
        \node[blue!50!black,text opacity=1,fill=blue!15!white,fill opacity=1,
            rotate=0] (IBC) at (15:2.1)
            {\small$\H(Z|X)$};
        % \draw[purple,ultra thick] (
    \end{tikzpicture}
    }
    \!   
    \def\circsize{0.7}
    \def\radsize{0.42}
    \def\twosize{0.5}
    % \begin{tabular}{@{}c@{}}
    \begin{tabular}{@{}c@{}c@{}}
    \begin{tikzpicture}[center base,scale=0.9]
        \node at ({-\circsize-\radsize+0.1},{\circsize+\radsize-0.2}) {\textbf{\underline{(b)}}};
        \begin{scope}
            \path[fill=green!60!black,fill opacity=0.25]
                (210:\radsize) circle (\circsize) ++(210:\circsize)
            % node[label={[label distance=0pt,inner sep=0pt]left:\small$A$}]{}
                (210:\radsize) circle (\circsize) ++(210:\circsize)
            % node[label={[label distance=0pt,inner sep=0pt]left:\small$A$}]{}
                (-30:\radsize) circle (\circsize) ++(-30:\circsize);
             % node[label={[label distance=0pt,inner sep=0pt]right:\small$C$}]{};
        \end{scope}
        \begin{scope}[even odd rule]
            \clip (-30:\radsize) circle (\circsize)
                    (210:\radsize) circle (\circsize)
                    (90:\radsize) circle (\circsize);
            \fill[fill=white!90!black] 
                % (-2,-2) rectangle (2,2);
                    (-30:\radsize) circle (\circsize)
                    (210:\radsize) circle (\circsize)
                    (90:\radsize) circle (\circsize);
         \end{scope}
         \begin{scope}
            \clip (-30:\radsize) circle (\circsize);
            \clip (210:\radsize) circle (\circsize);
            \clip (90:\radsize) circle (\circsize);
            \fill[fill=orange!80!black!40!white] 
                (90:\radsize) circle (\circsize);
          \end{scope}
        \begin{scope}[gray]
            \draw[] (-30:\radsize) circle (\circsize);
            \draw[] (210:\radsize) circle (\circsize);
            \draw[] (90:\radsize) circle (\circsize);
        \end{scope}
        \node at (-90:\twosize) {\small +1};
        \node at (30:\twosize) {\small +1};
        \node at (150:\twosize) {\small +1};
        \node at (0,0) {\small -1};
    \end{tikzpicture}
    %oli21: horlizontal layout
    % \\[7ex]
    &
    \begin{tikzpicture}[center base,scale=0.9]
        \node at ({-\circsize-\radsize+0.1},{\circsize+\radsize-0.2}) {\underline{\textbf{(c)}}};
        \begin{scope}
            \path[fill=green!60!black,fill opacity=0.25]
                (210:\radsize) circle (\circsize) ++(210:\circsize)
            % node[label={[label distance=0pt,inner sep=0pt]left:\small$A$}]{}
                (210:\radsize) circle (\circsize) ++(210:\circsize)
            % node[label={[label distance=0pt,inner sep=0pt]left:\small$A$}]{}
                (-30:\radsize) circle (\circsize) ++(-30:\circsize);
             % node[label={[label distance=0pt,inner sep=0pt]right:\small$C$}]{};
        \end{scope}
        \begin{scope}[even odd rule]
             \clip (-30:\radsize) circle (\circsize)
                    (210:\radsize) circle (\circsize)
                    (90:\radsize) circle (\circsize);
            \fill[fill=white!90!black] 
                % (-2,-2) rectangle (2,2);
                    (-30:\radsize) circle (\circsize)
                    (210:\radsize) circle (\circsize)
                    (90:\radsize) circle (\circsize);
         \end{scope}
        \begin{scope}[gray]
            \draw[] (-30:\radsize) circle (\circsize);
            \draw[] (210:\radsize) circle (\circsize);
            \draw[] (90:\radsize) circle (\circsize);
        \end{scope}
        \node at (-90:\twosize) {\small +1};
        \node at (30:\twosize) {\small +1};
        \node at (150:\twosize) {\small +1};
    \end{tikzpicture}
    \end{tabular}
    \caption{
        \textbf{(a)} 
        %oli8:
            % A diagrammatic representation of the components of an information profile for variables $X,Y$, and $Z$;
            The components of the information profile over three variables;
        \textbf{(b)} the information profile of $\muxor$;
        \textbf{(c)} the information profile of $P$ and $Q$ from \cref{example:ditrichotomy}.
    }
        \label{fig:info-diag}
\end{figure}
}

%joe12*: Why is this helpful?  This is far to major a digression at
%this point.  You don't need to understand any of the details to
%understand the points you want to make in this intro.
%oli13*: I disagree that this is a digression at all. This material helps explain why independence and dependence are so closely related, and its relationship to information. This material builds up the concepts needed to motivate the coming examples, to talk about why IDef is a linear function of an information profile, and gives the tools needed to start interpreting IDef. 
%joe13: It does not really explain (certainly not to me) why
%dependence and independence are closely related, and the material os
%not particularly relevant to what the current story.  I think you can
%greatly shorten it.
%oli14: I know it can be greatly shortened at the cost of losing the effect I'm going for.  Let me instead try again to rewrite it looking for a sharper effect. For this particular paragraph, I know you are predisposed not to want to engage deeply with the different entropy quantities, but very few people are like you, and in reading it again, I'd like you to simulate someone who is willing to slow down and think about these things. 
%oli13: shortening
% In order to interpret this bound, fully appreciate its significance for information theory, or understand its relationship to  probabilistic (in)dependence, it is helpful to have a high-level picture of the primitive quantities in information theory, and how they fit together. 
%oli14:
% To interpret our bound or appreciate its significance for information theory, it is helpful to understand how the primitives of information theory are related to one another.
%oli15: telling a better story; beginning with rhetorical question
% To motivate the relationship between entropy and \scibility, 
% But first, let's look at things from a higher level
% But first, we pose a high-level question: 
%
%oli20: not quite the emphasis we want anymore
% \commentout{
% But first: why should there be any relationship between entropy and \scibility\ at all? To build intuition, we review a special case: the relationship between entropy and probabilistic (in)dependence.
% }
%oli21: inline figure
\begin{wrapfigure}[10]{o}{3.2cm}
    \vspace{-4.5ex}
    \def\circsize{1.6}
    \def\radsize{0.8}
    % \edef\outerrad{{\circsize/2+\radsize}}
    \edef\outerrad{1.6}
    \def\innerrad{1.00}
    % \def\twosize{0.5}
    \scalebox{0.7}{
    \begin{tikzpicture}[center base]
        % \node at ({-\circsize-\radsize+0.1},{\circsize+\radsize-0.2}) {\underline{\textbf{(a)}}};

        \path[fill=white!90!black] (210:\radsize) circle (\circsize) ++(225:\circsize) node[label={[label distance=2pt,inner sep=0pt]left:\Large $X$}]{};
        \path[fill=white!90!black] (90:\radsize) circle (\circsize)
            % ++(125:\circsize) node[label={left:$Y$}]{};
            ++(65:\circsize) node[label={[label distance=1ex]right:\Large $Y$}]{};
        \path[fill=white!90!black] (-30:\radsize) circle (\circsize) ++(-45:\circsize) node[label={[label distance=1pt,inner sep=0pt]right:\Large$Z$\!}]{};


        \begin{scope}[gray]
            \draw[] (-30:\radsize) circle (\circsize);
            \draw[] (210:\radsize) circle (\circsize);
            \draw[] (90:\radsize) circle (\circsize);
        \end{scope}
        
        %oli9: replacing this mutual info circle with one more crescent.
        % \begin{scope} % I(B;C)
        %     \clip (-30:\radsize) circle ({\circsize+0.03});
        %     \clip (90:\radsize) circle ({\circsize+0.03});
        %     \draw[ultra thick,blue,fill=blue,fill opacity=0.3] (-30:\radsize) circle (\circsize);
        %     \draw[ultra thick,blue] (90:\radsize) circle (\circsize);
        % \end{scope}
        \begin{pgfinterruptboundingbox} % H(Z|X)
            \begin{scope}[even odd rule]
                \clip (210:\radsize) circle ({\circsize-0.03}) (-3,-3) rectangle (3,3);
                \clip (-30:\radsize) circle ({\circsize+0.03});
                \draw[ultra thick,blue,fill=blue, fill opacity=0.3] (-30:\radsize) circle (\circsize);
                \draw[ultra thick,blue] (210:\radsize) circle (\circsize);
            \end{scope}
        \end{pgfinterruptboundingbox}
        \begin{pgfinterruptboundingbox} % H(X|Y)
            \begin{scope}[even odd rule]
                \clip (90:\radsize) circle ({\circsize-0.03}) (-3,-3) rectangle (3,3);
                \clip (210:\radsize) circle ({\circsize+0.03});
                \draw[ultra thick,purple,fill=purple, fill opacity=0.3] (210:\radsize) circle (\circsize);
                \draw[ultra thick,purple] (90:\radsize) circle (\circsize);
            \end{scope}
        \end{pgfinterruptboundingbox}
        \begin{pgfinterruptboundingbox} % H(Y|Z)
            \begin{scope}[even odd rule]
                \clip (-30:\radsize) circle ({\circsize-0.03}) (-3,-3) rectangle (3,3);
                \clip (90:\radsize) circle ({\circsize+0.03});
                \draw[ultra thick,violet!50!black,fill=violet, fill opacity=0.2] (90:\radsize) circle (\circsize);
                \draw[ultra thick,violet!50!black] (-30:\radsize) circle (\circsize);
            \end{scope}
        \end{pgfinterruptboundingbox}
        \node at (-90:\innerrad) {\small$\I(X;Z|Y)$};
        \node[rotate=-60] at (30:\innerrad) {\small$\I(Y;Z|X)$};
        \node[rotate=60] at (150:\innerrad) {\small$\I(X;Y|Z)$};
        \node at (0:0){\small$\I(X;Y;Z)$};
        \node at (90:\outerrad) {\small$\H(Y|X,Z)$};
        \node[rotate=60] at (-30:\outerrad) {\small$\H(Z|Y,X)$};
        \node[rotate=-60] at (-150:\outerrad) {\small$\H(X|Y,Z)$};
        
        %oli9:
        % \node[purple!50!black,text opacity=1,fill=purple!15!white,fill opacity=1,
        %         % rotate=0] (HAB) at (164:2.7) 
        %         rotate=45] (HAB) at (164:2.3) 
        %         % rotate=0] (HAB) at (164:2.3) 
        %     {\small$\H(X|Y)$};
        \node[purple!50!black,text opacity=1,fill=purple!15!white,fill opacity=1,
                rotate=0] (HAB) at (-110:2.0) 
            {\small$\H(X|Y)$};
        % \node[violet!20!black,text opacity=1,fill=violet!15!white,fill opacity=1,
        %         % rotate=0] (HBC) at (148:2.60) 
        %         rotate=45] (HBC) at (125:2.40) 
        %         % rotate=0] (HBC) at (125:2.40) 
        %     {\small$\H(Y|Z)$};
        \node[violet!20!black,text opacity=1,fill=violet!15!white,fill opacity=1,
                rotate=0] (HBC) at (145:2.10) 
            {\small$\H(Y|Z)$};
        % \node[blue!50!black,text opacity=1,fill=blue!15!white,fill opacity=1,
        %     % rotate=0] (IBC) at (28:2.55)
        %     % rotate=-50] (IBC) at (32:2.05)
        %     rotate=0] (IBC) at (32:2.05)
        %     {\small$\I(Y;Z)$};
        \node[blue!50!black,text opacity=1,fill=blue!15!white,fill opacity=1,
            rotate=0] (IBC) at (15:2.1)
            {\small$\H(Z|X)$};
        % \draw[purple,ultra thick] (
    \end{tikzpicture}
    }
    \caption{$\mat I_\mu$.
    % and their relationships.
     % for $\X {=} \{X,Y,Z\}$.
     }
        \label{fig:info-diag-a}
\end{wrapfigure}
%oli20: replacement
We now review the critical information theoretic concepts and their relationships to
    (in)dependence
    (see \cref{appendix:info-theory-primer} for a full primer).
% oli13: shortening:
% The entropy $\H_\mu(Y)$ of a (set of) variable(s) $Y$ measures how far $\mu$ is from making $Y$ a constant, and its interdefinable conditional variant $\H_\mu(Y|X) = \H_\mu(X,Y) - \H_\mu(X)$ measures how far $\mu$ is from satisfying the functional dependency $X \tto Y$. 
% The mutual information $\I_\mu(Y;Z) = \H_\mu(Y) + \H_\mu(Z) - \H_\mu(Y,Z)$ measures how far $\mu$ is from modeling the independence $Y \CI Z$, and the conditional mutual information $\I_\mu(Y;Z| X)= \H_\mu(Y| X) + \H_\mu(Z| X) - \H_\mu(Y,Z|X)$ measures how far $\mu$ is from satisfying the conditional independence $Y \CI Z \mid X$. 
Conditional entropy $\H_\mu(Y|X)$ measures how far $\mu$ is from satisfying the functional dependency $X \tto Y$
%oli20: short sentences are better here, I think
%\unskip, and conditional
\unskip. Conditional
mutual information $\I_\mu(Y;Z| X) $ measures how far $\mu$ is from satisfying the conditional independence $Y \CI Z \mid X$. 
%
%oli13: shortening:
% It can also be shown that $\H_\mu(Y) = \I_\mu(Y;Y)$, $\H_\mu(Y|X) = \I_\mu(Y;Y|X)$, and that the conditional variants reduce to the unconditional ones by taking $X$ to be a constant (like the empty set of variables, which always takes on its unique value).
%oli14: swapping sentence order, so that we start with the motivation and then get the details, rather than the details and then the high-level summary. Should make it easier to follow. 
%oli15:
% Furthermore, the two quantities, as well as their unconditional variants, are interdefinable, and generated by linear combinations of one another. 
%oli20*: streamlining this, and not going out of my way to point out connections here. Trying to tell a story...
\commentout{
The two kinds of quantities and their unconditional variants are all interdefinable
(indeed, $\H(Y|X) = \I(Y;Y|X)$, a strengthening of \eqref{eq:conditional-self-independence-det}) \unskip, and generated by linear combinations of one another; moreover, they are related by an inclusion-exclusion rule (illustrated for three variables in 
%oli21
% \cref{fig:info-diag}(a)
\cref{fig:info-diag-a}
). Thus, to specify all standard information-theoretic quantities for a joint distribution $\mu(\X)$,
it suffices to give a vector of $2^{|\X|}-1$ numbers, which we will call the \emph{information proile} of $\mu$, and denote $\mat I_\mu$. 
}
%oli20:
%spencer1: can remove "integer" here in two places
%oli20.1: I'll go with this for now because it's simpler and I like the flow without "Integer" better.  However, keep in mind that if even one of these quantities is non-zero, then all scalar quantities are linear combinations of it, so it's a trivial statement...
%
%spencer1:
% Integer linear combinations of these quantities 
Linear combinations of these quantities 
% (or indeed, of just entropy or of just mutual information) 
%spencer1: I think we can cut this
%oli20.1*: I disagree; it has three effects that I really like: (1) it bounds the scope of set of variables so that $\X$ is defined below (it was previously unbound); (2) taking the English at face value, one might think that we're talking about just two numbers, not the general quantities applied to different arguments, and (3) it shows that we're talking about subsets of variables, not just individual variables one-at-a-time.  An expert information theorist doesn't need this context, but I feel it has a chance of really helping someone out. Reinstating.
(for $X,Y,Z \subseteq \X$)
%spencer1:
% can be viewed as the inner product between a vector $\mat v$ of integers
can be viewed as the inner product between a coefficient vector $\mat v$
and a $2^{|\X|}-1$ dimensional vector $\mat I_\mu$ that we will call the \emph{information profile} of $\mu$. For three variables, the components of this vector are illustrated in 
%oli21:
% \cref{fig:info-diag}(a).
\cref{fig:info-diag-a} (right).
%
%oli15: moved above:
% This is illustrated for three variables in \cref{fig:info-diag}(a).
% illustrated for the case of 3 variables in \cref{fig:info-diag}(a). 
%oli15: 
% It is easy to see that arbitrary sets of (conditional) (in)dependencies can be enforced by requiring that
%oli20:
% It is easy to see how an arbitrary conjunction of (conditional) (in)dependencies is equivalent to a constraint
It is not hard to see that an arbitrary conjunction of (conditional) (in)dependencies 
%spencer1:
% is equivalent to
can be expressed as
a constraint $\mat I_\mu \cdot \mat v \ge 0$, for 
some appropriate vector $\mat v$
%oli21: shave a line
 % of integers. 
\unskip.
%joe17: I don't think the next twop sentence are adding anything, given the
%following paragraph
%But constraints of this form can describe much
%more.
%joe17: I'm not even sure shat the "generic" information-theoretic
%constraint is.
%Indeed, our generic information-theoretic constraint 
%oli21: shaving
% will be
%is
%of this form.

%oli20*: rewriting this story as well
% The origin of this bound is actually the scoring function defined by \citet{pdg-aaai} to give qualitative semantics to PDGs.
%spencer1
% There is already a way to convert a \hgraph\ $\Ar$ to a function of the form $\mat I_\mu \cdot \mat v_{\!\Ar}$, originally introduced for the purposes of giving qualitative semantics to PDG \cite{pdg-aaai}. 
We now formally introduce the qualitative PDG scoring function $\IDef$, which interprets a hypergraph structure $\Ar$ as a function of the form $\mat I_\mu \cdot \mat v_{\!\Ar}$.
This \emph{information deficiency}, given by
\begin{equation}
%joe13: There's somethng wrong with the layout here when I latex it.
%I think the problem is with the definition of \Idef
%oli14: I suspect your pdg-preamble-v3.tex is out of date. I'll make sure to send you an updated version of that file
    \IDef_{\!\Ar}(\mu) 
    %oli20: adding extra context
    = \mat I_\mu \cdot \mat v_{\!\Ar} 
    := - \H_\mu(\X)  + \sum_{a \in \Ar} \H_\mu(\Tgt a \mid \Src a)
    ,
        \label{eq:idef}
\end{equation}
is the difference between the number of bits needed to (independently) specify the randomness in $\mu$ along the \arc s of $\Ar$, and
%joe13: I don't know what it means to specify a sample of \mu
%"according to its own structure".  What is the sturcture of \mu?
%oli14: Since \mu is an unconditional distribution on \X, its structure is the hyperarc with empty source, and target \X.
the number of bits needed to specify a sample of $\mu$ according to its own structure 
%oli14: I've expanded for extra clarity. I think this is overkill, but I'll shorten it later. 
% $(\to \X)$.
%oli15: 
% (i.e., the hyperarc $\to \X$, corresponding to a random mechanism that determines all variables with no input)
%oli20: answering your prevoius %joe13 in depth lead this to be out of proportion. Shortening to something that still hopefully makes sense.
% (i.e., the hyperarc $\to \X$, a mechanism that jointly determines $\X$ without input).
($\emptyset \to \X$).
% ($S {=} \emptyset \to \X {=} T$).
% ($S = \emptyset \to \X = T$).
%joe13*: Why is it important that it's a linar combination?  Why are
%you making a fuss about this.  I would cut all this and the long
%discussion in the previous paragraph on the information profile. 
%It's not appropriate introductory material.  
%oli14*: This is an important concept. The fact that it is a linear function of the information profile is important because all standard information theoretic quantities (entropy, conditional entropy, mutual information, etc), as well as all independences, dependencies, and sets thereof, can all be written as linear combinations of the information profile.  This is a natural generalization. 
%oli20: we now lead with this, so we can cut this whole thing
% Because it is a linear combination of (conditional) entropies of $\mu$,  $\IDef_{\!\Ar}(\mu) = - \mat v_{\!\Ar} \cdot \mat I_{\mu}$, for some vector $\mat v_{\!\Ar}$ determined by the \hgraph\ $\Ar$. 
While $\IDef$ has some nice properties\footnote{%
%joe13*: this is *way* too much for an introductory few paragraphs.
%This should ge shrunk to at most two examples.  Bringing in vector
%fields here is a sure-fire way to confuse the reader.
%oli14: fair enough; I wasn't proposing this as a final draft, I was just trying to be accurate and complete and get us on the same page about what is true. I'll cut it down to a couple now. 
\commentout{
These properties include:
\begin{itemize}[nosep]
    \item capturing BN independencies,
    \item capturing the dependencies in of \cref{theorem:func},
    \item nice interactions with the qualitative information in a PDG given by a sum of relative entropies,
    \item capturing factor graphs and their associated exponential families (as a result of the previous point),
    \item reducing to maximum entropy in the absence of structural information, 
    \item being easy to calculate from $\mu$ in closed form, and
    \item being the opposite of 
    the corresponding quantitative term in $\Inc$,
     in the sense of having the opposite vector field.
\end{itemize}}%
%oli14: a much shorter replacement
%oli20: improved writing in footnote and rebalanced it to fit on two lines. Changes not carefully marked. 
It 
% For instance, $\IDef$
%spencer1: for readability
% captures BN independencies, the dependencies of \cref{theorem:func}, 
captures BN independencies and the dependencies of \cref{theorem:func}, 
reduces to maximum entropy for the empty \hgraph, and
% combines conveniently with the other PDG scoring function to capture factor graphs,
%spencer1: for line shaving
% interacts seamlessly with the quantitative PDG scoring function \cite{pdg-aaai} to capture factor graphs.
combines with the quantitative PDG scoring function \cite{pdg-aaai} to capture factor graphs.
},
it can also behave unintuitively in some cases; for instance, it can be negative.
% when $\Ar$ is \subpartl\ or has cycles. 
%oli-20: new *exactly and remove "in general"
Clearly, it does not measure how close $\mu$ is to being structurally compatible with $\Ar$, in general. 
%oli20: now that we've done a better job at the beginning, we can talk about this in the next section when we get there. 
\commentout{%
Nevertheless, if $\mu\models \Ar$, we prove that $- \IDef_{\Ar}(\mu) = \mat v_{\!\Ar} \cdot \mat I_\mu \ge 0$. 
This is quite useful; by inspecting $\mat v_{\!\Ar}$, one can often directly interpret the constraints imposed by \scibility\  in terms of well-studied information-theoretic primitives. The same connection also allows \scibility\ to elucidate some of the less inuitive information-theoretic quantities. 
}%
Nevertheless, there is still a fundamental relationship 
between $\IDef$ and \scibility, as we now show.
%oli15*: removing the paragraph beginning " there is a school of thought" from here; moving it below. 
%
%oli15: this material breaks the flow; cutting it from here, and putting some of it in section 5.2 and as a transition at the end of section 5.1
\commentout{
%oli13: compresssing
% While the connection made by \cref{theorem:sdef-le0} is the most important, $\IDef$ still does not capture \scibility\ exactly out of the box, as mentioned above.
%oli14: this isn't quite true
% Still, $\IDef$ does not exactly capture \scibility.
Still, the quantity $\IDef_{\!\Ar}(\mu)$ does not determine whether or not $\mu \models \Ar$. 
%oli14: minor tweaks to this to make it read better, that I forgot to mark precisely. 
In \cref{sec:scoring-funs}, we derive an entropy-based scoring function that does, by directly translating conditions (a-c) to  scoring functions on witnesses, and then adding an infemum to take care of the existential quantifier (i.e. the choice of witness) implicit in \cref{defn:scompat}.
%oli13: shortening:
% Perhaps surprisingly, it turns out that the inner part of this formula (i.e., without the existential quantifier) is in fact
Perhaps surprisingly, without the infemum, this function is
an instance of $\IDef$---yet not for the original \hgraph, but a transformed one.
Thus the new scoring function is essentially an instance of the old one, tailored for \scibility.  When combined with a modest extension of \cref{theorem:sdef-le0}, this observation also gives us natural easy-to-compute bounds 
%oli13: adding
(\cref{theorem:siminc-idef-bounds})
for
%oli13
% our new
this new \scibility-based
scoring function.
}

% Finally, we show that \emph{even} \scibility\ has an even closer relationship to  $\IDef$. 


%oli14*: cutting old S5 intro
\commentout{
\clearpage
\subsection*{Previous \S5 Intro}
%joe3*: As you know, I strongly disagree with your next comment.  The
%"story" of the paper should be the new semantics.  That's what you
%need for the causality section and what you used in Section 2.  That
%means that you should use a scoring function that fits naturally with
%the new semantics.  I view that scoring function as one of the main
%contributions of this paper.  Of course, you should relate it to the
%scoring function that we used originally.  Following your email, I
%did not read the rest of the paper; I just responded to a few of your
%comments.  But on a fast scan, it seems to me like a completely
%random collection of observations, with no story.  Even if you don't
%like my story, you need to have a much better story.
%oli3: We agree that this section needs a significant overhaul, which I will now attempt. I still think the emphasis you are trying to bring forward is deceptive and muddies the picture of what a PDG is. I will try to tell a crisper story that avoids what I see as the issues with your approach, and I will make sure to tie it into the rest of the paper nicely. 
%oli2*: This is the wrong order.  The original scoring function needs
% to come first. It's more important, and I don't want to highlight 
% the new one. 
%joe3: I strongly disagree, but, as always, I don't know how to
%convince you.  The new one is what this paper should be about, in
%my opinion. 
%oli4: Oops, in the previous version I accidentally reinstated this whole paragraph instead of telling tell. Although the start is OK, I'm replacing the rest of it with something more in line with what I intended.
\commentout{    
\citet{pdg-aaai} provided a scoring function that 
    was intended to measure 
    how far a probability distribution was from being structurally  compatible with a PDG, viewed as a hypergraph.
    We discuss this
    scoring function below; for now, we just mention that the scoring
    function was rather difficult to interpret as a degree of
    incompatibility since, for example, it could be negative.
    Fortunately, the definition of structural incompatibility we have
    given here leads to a natural scoring function, which we now describe.
}
%
%
%oli6*: I think one reason you're not seeing my story is that I based my writing on yours, so in many ways this is really motivating your story.  I'm commenting out the old text at the top of this section, and trying to tell my story properly, rather than something halfway between them. See my other %oli6* comments responding to you inside this environment for more details. 
%joe6*: Even if you were going to start with the old scoring function
%(which I continue to think is a bad idea), why not just cut the first
%two paragraph in the section, and the accompanying figure.  (If
%necessary, you can add a sentence to explain entropy and conditional
%entropy.  
%oli7*: You're missing the point.  The first two paragraphs are laid out this way on purpose, and that purpose is not just to introduce conditional entropy and mutual information.  There is a deeper relationship I was trying to point to. Below, I will annotate the first two paragraphs with the purposes they serve.
\commentout{
    \citet{pdg-aaai} defined an entropy-based scoring function ($\SDef$) that 
        was intended to measure discrepancy between a probability distribution
        and a PDG's underlying (weighted) directed hypergraph structure.
    Although that scoring function has many nice properties,
    %joe5: what nice properties does it have?  You can't just say "it has
    %nice properties" any more than you can say "it is useful".  You have
    %to say something about hose properties.
        it is rather difficult to interpret directly as a ``degree of
        incompatibility'' since, for example, it could be negative.
    %joe5*: The natural thing to do at this point, if you were trying to
    %tell a good story, is to say "So here's a nice scoring function that
    %is never negative and captures directly the discrepancy between a
    %probability distribution and a PDG's digraph structure".  (No need to
    %confuse the issue and get into weighting.)  To me this is obvious.
    %What you're talking about hext does not make for a good story.
    %You haven't motivated the issues that you discuss, and don't follow
    %the story line that you've set yourself up for.
    %oli6**: You are right: the beginning of a story above does naturally lead to the story you have in mind.  This is because you wrote it with that story in mind; it's here in my story because I liked your words, agreed with them, and thought I could get you to like it better by drawing from your story. But, because you are a good writer, your story leads to more of your story---especially for you, because you wrote it.  I have a different story in mind, which I think is far more fundemental and has the potential to be be much more engaging.
    In \cref{sec:sc-infobound}, we discuss an important
    % information-theoretic connection between $\SDef$ and \scibility,  which works in two directions.
     connection between \scibility\ and the information theory behind $\SDef$, which works in two directions.
    On one hand, it gives us an entropy-based criterion to conclude that a distribution cannot be \scible\ with a directed hypergraph; on the other, our definition of \scibility\ can be used to shine light on a (seeming) counterexample in the information theory literature.
    Nevertheless, $\SDef$ still does not exactly coincide with \scibility.
        In \cref{sec:scoring-funs}, we derive a new scoring function that
        directly measures how far a distribution is being \scible\ with a 
        given directed hypergraph---%
        %oli4:
        which, perhaps surprisnigly, turns out to be an instance of the original scoring function, applied to a transformed directed hypergraph.
    To understand these information theoretic issues, we begin by reviewing some standard definitions.
}
%oli9: new lead: 
Just as \scibility\ has 
%oli11:
% a deep relationship with causal models
%joe11: "important" seems like the wrong word.  What makes it
%important?   Who is it important to?  Note that "important" is in no
%way a weakening of "deep".  It's saying something different.
an important relationship with causal models
 (\cref{sec:causal}), so too does it have 
%oli10: parallel phrasing
%oli11: 
% a deep relationship with information theory.
an important relationpship with information theory.
%
We have seen how \scibility\ can capture 
%oli10:
% the conditional indepependencies of BNs (\cref{theorem:bns})
certain conditional indepependencies (\cref{theorem:bns})
%oli9: since Theorem 2 is now weaker...
% dependencies (\cref{theorem:func}),
%oli10:
% and dependencies (in some contexts, \cref{theorem:func}),
and dependencies (\cref{theorem:func}),
%oli9:
% and the structures of causal models (\cref{prop:causal-model-structure-capture}),
by restricting to special classes of \hgraph s. 
% But what can be said about \hgraph s more broadly?
% It turns out that \scibility\ can 
But, for \hgraph s outside these classes (such as those generated by cyclic graphs), \scibility\ can 
% mean something much subtler. 
impose much subtler constraints on a
    distribution
%oli9:
% \unskip.
%oli10: I don't really want to be precise about this yet. This is for the next paragraph. 
%joe10*: the fact that it requires information theory to describe
%these constraints (which are just an inequality connecting SIM
%compatibility and various quantities involving entropy) doesn't make
%me feel like there's a deep relationship between compatibility and
%information theory. 
%oli11: Of course, that's only one of several strands of the story, and that inequality is far tighter than you might imagine. (Empirical results suggest that the converse holds on the vast majoroity of the volume of the probability simplex.)  Perhaps the wording can be clarified to make this feel less jaring for people jaded with infromation theory, but there's enough here to support the claim that there's a deep relationship.
%joe11: Oliver, readers can't read your mind, as I've said
%repeatedly.  Based on what you wrote below, there is not a deep
%relationship.  If there are other facts that make you feel like
%there's a deep relationship, you have to spell them out *in the
%paper*.  FOr what it's worth, based on what you said, I don't find
%the case compelling, but it's hard to know, because I don't know
%what's in uor head.
%oli12: ok, but we can't have to describe everything all at the beginning. I think the next sentence is a good way to motivate why we're talking about this without getting into the details yet.
\unskip, which require information theory to describe
%oli10: if not the above, then the below is better than what I had before:
% \unskip---namely, inequalities between integer linear combinations of information-theoretic primitives
% \unskip---namely, linear inequalitites between 
%     the entropies of subsets of variables
     % (\cref{theorem:sdef-le0})
     % (\cref{sec:sc-infobound})
     \unskip.
%oli10:
% Conversely, \scibility\ can clarify important misunderstandings in information theory
%joe10*: What is the following sentence a converse to?  In any case,
%this doesn't make me feel like there's a "deep relationship".  It
%just says that SIM compatibility is a useful tool.  This is very
%different from the relationship between causality and compatibility.
%oli11: The previous sentence says information theory can elucidate SIM-compatibility; this sentence says the connection works in reverse, and SIM-compatibility can elucidate information theory.  Together the suggest a bi-directional connection. 
%joe11: I In what way does one elucidate the other?  ("Elucidate" means
%"make clear".  I don't think an inequality makes SIM compatibility
%clear (at least, not to me).  Nor does explaining what going on in
%one example involving three nodes make nformation theory clear.
%oli12: without this inequality, we really have no idea what the implications of SIM-compatibility are for most hypergraphs (aside from those of very special forms). If \mu is SIM-compatible with the 3-cycle, what can you tell me about \mu? Without this inequality, all we you can do is regurgitate the definition.  This information theoretic perspective allows people who understand some information theory to understand the implications of SIM compatibility. THat's one form of elucidation.  In the other direction: I think examples 5 and 6 clearly demonstrate how a misunderstanding in information theory can be clarified by SIM-compatibility.  Yes, this is only one (particularly important) simple example, but it is enough to claim that we can clarify information theory. Surely we don't need to have a positive impact on every single paper in information theory in order to claim that we can clarify what's going on in information theory. 
%oli11:
% Conversely,
The connection also useful in the other direction:
 \scibility\ can clarify important misunderstandings in information theory
    % (\cref{example:ditrichotomy})
    \unskip.
%oli11: undecided as to whether something like this is appropriate.
%joe11: I really don't like this sentence.
%oli12: OK; why not?  
It is also possible to directly capture \scibility\ with 
an entropy-based scoring function and an infemum.
%oli10: rather than summarizing at the beginning, like you suggested, I still think the story is better if we leave this for later. 
\commentout{
    %oli10:
    We detail both phenomena in \cref{sec:sc-infobound}.
    %joe9: the next sentence seems to be missing some words.
    %oli10: adding missing words, and other minor restructuring
    % In addition, can
    In \cref{sec:scoring-funs}, we derive an entropy-based scoring function that measures how far a distribution is from being \scible\ with a given \hgraph. 
    % and discover that it is intimately connected to the original scoring function for PDGs 
    %oli10: splitting second half into new sentence
    But, perhaps surprisingly,
    % To our surprise, 
    this new scoring function coincides with the original PDG scoring function
    % , applied to a transformation of the hypergraph
    %oli10:
    % if we first transform the hypergraph appropriately
    % if we first appropriately transform the underlying hypergraph
    %joe10
%    if underlying hypergraph is first transformed appropriately
    if the underlying hypergraph is first transformed appropriately
        % (\cref{theorem:siminc-idef-bounds}). 
        \unskip.
    %oli10: 
%joe10: I don't have any problem interepreting PDGs.  I have a problem interpreting the scoring function.   I also don't think that the way it captures scibility is the least bit natural
%oli11: "PDGs as originally defined" includes their scoring function.
%joe11: the scoring function is absolutely *not* part of the
%defintion.  We hae a definition (Definition 2.1).  It doesn't mention
%scoring functions.
%oli12: I take "PDGs" to mean both their syntax and their semantics, and I find it hard to imagine doing otherwise. If you think it is meaningful to say "PDGs can capture BNs", then you agree with me at some level.  
%joe11: Socring functions are one approach to giving PDGs semantics. 
%
%oli12: OK, but any reasonable other semantics we give for PDGs should coincide with the scoring function semantics at some level. Otherwise, we're creating confusion. If it has different behavior, it needs to be thought of differently. We could "give an alternate semantics" to first order logic, that interperets all quantifiers as \exists, and variables containing the letter "e" as the constant 7.  Yet it would be counterproductive to think of this as a different interpretation of first order logic, and also call such formulas "first order logic" under this interpretation---you'd want to think of it as its own thing. Similarly, because SIM-compatiility does NOT coincide with the scoring-function for the same PDG hypergraph, but rather for a different hypergraph, I maintain that SIM-compatibility should not be thought of as a semantics for PDGs, but its own separate thing. (And, just as in the example above, the new conept can be compiled to the old one, and thought of in those terms). 
%
%oli11: You're saying you don't have a problem re-interpreting PDG syntax differently, but I do. I think the way that it captures SIM-compatibility is very natural, and I would like an opportunity to discuss that in person. 
%(Fortunately for you, this part is now commented out.)
    Thus PDGs as originally defined, difficult as they may be to interperet in general, capture \scibility\ in a natural way.
    % To understand these connections, we have to understand a little bit about information theory itself.
}

%oli10: instead, say
% Why should \scibility\ be related to information theory?
% Why might this be so?
% To understand these connections, it is important to realize that
%oli11: removing again, to say something shorter below.
% To understand how such a connection is possible, note that
%
%(oli7): paragraph break?
%oli8: 
% To understand why (or, indeed, to even articulate the constraints),
%oli9:
% To understand why (or indeed, to even describe these constraints),
% To understand these interactions,
% we need some ideas from information theory---a field with much to say 
% Information theory has much to say
% % about precisely the topics we have discussed so far.
% about  
% %(oli8?)
% % generalized
% probabilistic (in)dependencies.
%oli9:
%joe9*: It does nothing of the kind.  Perhaps you can express
%independence using information theory.  BUt that doesn't make
%information theory a "generalization" of independence.
%oli10*: It very much a generalization of independence! Independence is the special case in which mutual information is zero, but with information theory you can relax that and say things like: "variables A and B share less than 1% of their information". I think your position is very extreme, and that you'll find it very difficult to convince anyone that information theory does not generalize (in)dependence. 
%joe10: I think you misunderstood myu intent.    I agree that
% information theory (or, more precisely, entropy) can capture
% independence.  But it seems like a type mismatch to say that
% information theory generalizes independence.  Saying "probability
% theory generalizes independence" sounds equally strange to me.
%oli11: hmmmm... I think I understand where you're coming from regarding the wording. Your example of "probability theory generalizes independence" is a much more flagrant violation, since probability theory revolves around structural questions like independence less than information theory. 
%oli11:
% information theory generalizes probabilitic (in)dependence;
%joe11: I *really* don't like this (I actually think you're selling
%information theory short here).  It's much more than a continuous
%relaxation of independence.  But it does let us define a continuous
%relaxation of independence.
%oli12: OK; I will change it. I see how this could be contentious, although I still think it's accurate. I'm selling information theory short because extolling the many nice properties and applications of this particularly natural relaxation of probabilistic independence would take us far away from the story. 
In some sense, information theory is a continuous relaxation of probabilitic (in)dependence:
%oli10: adding, although I believe this is overkill, and overfit to explaining things to you:
%joe10: cut this, once byou get a better version of the above
%oli11:
% for example, it let us us talk about variables being only \emph{mostly} (in)dependent.
%
%joe5*: This takes us way to far afield.  The fact that you feel you
%need to put this here is a strong indicuation that you're not telling
%the right story. 
%oli6: I hear you, and I will now work to tighten up this material and relationships it has with other concepts in our paper, so it does not feel as far afield.  But, while I undersand why you feel this way, I very much do not buy the structure of your argument: you view the fact that this is here (when you don't find it necessary to tell your story) as evidence that it's the wrong story.  But in fact it only tells you that it's not *your* story.
%oli6: I am removing this section title, to better integrate this material with the rest of the motivation. 
% \subsection{Primer on PDGs and Multivariate (Shannon) Information Theory}
% \subsection{Primer on Multivariate (Shannon) Information Theory and PDGs}
%
% {\color{red}\small [This paragraph to be redone much better, but it is not the top priority]}
%oli4: redoing this where $\mu$ is general, not just over a product space.
% Let $\X$ be a set of variables, 
%     % with $\V(X)$ a finite set for all $X \in \X$
%     each taking on finitely many values,
%     and $\mu(\X)$ be a joint distribution over them. 
% Let $\mat X, \mat Y \subseteq \X$ be subsets of variables.
%oli6: significantly cutting this down, putting in appendix for conference version
%oli6*: replacement material, for conference version.
% Observe, for instance, that the standard quantities in information
% Observe: the directed versions
%joe10: What would be an "undirected form" of the quantity
%oli11: the "undirected forms" are ordinary (unconditional) entropy H(X), and ordinary (unconditional) mutual information I(X;Y), respectively.  They measure distance from determination, and distance from independence.  Since we're working with directed hypergraphs and focused on their conditional versions (which are more expressive anyhow), I think it's worth the 2 extra words to make this distinction.  
%joe11*: It's not worth it if the reader doesn't understand it.  I
%didn't understand it.  Either cut it or explain it.
%oli12: I did change it, so I don't understand why you left this comment and gave it a star.
%oli11: here two possible improvements to keep people from getting hung up on this (1) use consistent lettering "S" for the conditioned variable, just like the source "S"; (2) say "conditional" instead of "directed", although the latter ties in better with our focus on directed hypergraphs. 
%oli11:
% Observe: the directed forms
% Observe: the conditional forms of the two most basic quantities in information theory
the conditional forms of mutual information $\I_\mu(Y;Z|X)$
and entropy $\H_\mu(Y|X)$%
\footnote{
Either quantity can be derived from the other. 
In particular, $\H_\mu(Y|X) = \I_{\mu}(Y;Y|X)$, deepening 
the observation made in \cref{eq:conditional-self-independence-det}.
See \cref{appendix:info-theory-primer} for definitions and a short review.}---the most important quantities in the field---measure
%oli11: silently rearranged the letters so we're always conditioning on X below. 
%oli11: worked this material into the above.
%of its two most basic quantities%
%---conditional entropy $\H_\mu(Y|X)$ and conditional mutual information $\I_\mu(Y;Z|X)$\footnote{See \cref{appendix:info-theory-primer} for definitions and a short review.}---are 
% natural ways of quantifying 
% ways to quantify 
how far a distribution $\mu$ is from satisfying
%oli11: swapping order of the below, for new sentence structure above:
a conditional independence $Y \CI Z \mid X$ and
a functional dependence $X \tto Y$,
respectively. 
%oli9: removing
% It turns out that SIM-compatibility with any (even cyclic) hypergraph implies interpretable inequalities between such quantities, generalizing many of the results we've seen so far (\cref{theorem:sdef-le0}). 
%
% All of these quantities are related with the inclusion-exclusion rule.
%oli9: paragraph merge
%
%oli6 reorder
%oli6: completely rewrote, but forgot to copy oldest version.
% Multivariate information theory promises to capture precisely the structural relationships between a set $\X$ of variables as a vector of $2^{|\X|}-1$ numbers such as these ones,
% The vector of all $2^{|\X|}-1$ numbers needed to describe all such quantities is sometimes called $\mu$'s \emph{information profile}.
% Multivariate information theory promises to capture exactly the {structural} relationships between a set $\X$ of variables as a vector of $2^{|\X|}-1$ numbers such as these ones,
%joe11: Yet again "multivariate" comes out of teh blue.  WHyu do we
%even need to use the word.
%oli12: Many information theorists use only mutual information and entropy; I've seen "multivariate information theory" as code for "stuff you can do with the whole information profile". 
Multivariate information theory promises
% The central promise of multivariate information theory is
to distill \emph{all} {structural} relationships between a 
set $\X$ of variables 
%oli11:
% (in a distribution $\mu$)
(distributed according to $\mu$)
into a vector of $2^{|\X|}-1$ 
%oli11: adding
such
numbers 
% set $\X$ of variables into a vector of $2^{|\X|}-1$ numbers 
% such as these,
%oli11: removing
% such as those depicted in \cref{fig:info-diag}(a)
%oli6: next two lines are out of place now.
% (all sums and differences of entropies of subsets of $\X$), 
% i.e., the mathematical object depicted above, 
%oli11: bringing back one aspect of the below 
that fit together with an inclusion-exclusion rule,
called
    %oli6:
    the
    % an
    % $\mu$'s
    \emph{information profile}
    %oli11: 
    of $\mu$.
%oli11:
This is depicted for $\X = \{X,Y,Z\}$ in \cref{fig:info-diag}(a).
%oli9: 
% These quantities are related through an inclusion-exclusion rule,
%oli10: perhaps not critical to the story, althgouth getting
% people to think about this will make it easier to understand
% some of what comes later
%joe10: If it's not critical to the story (which it definitely isn't), it doesn't belong here!  Tell a story
\commentout{ 
This representation is made possible by the fact that
    these quantities are related (defined, in fact)
    by an inclusion-exclusion rule, 
which allows us to read off
identities like
%oli9: 
\commentout{
    %oli8: more relevant example
    % $\H(X,Y) = {\color{purple!60!black}\H(X|Y)} + {\color{violet!65!black}\H(Y|Z)} + {\color{blue!50!black}\I(Y;Z)}$ 
    $\I(X;Y;Z) = \H(X,Y,Z) -  {\color{purple!60!black}\H(X|Y)} + {\color{violet!65!black}\H(Y|Z)} + {\color{blue!50!black}\H(Z|X)}$ 
        % can easily be read 
        %oli7:
        % off a diagram like the one in \cref{fig:info-diag}(a).
        %oli9:
        (which will soon play an important role)
        off such a diagram.
}
%oli9: replaced the above with:
$\H(X|Y) = \H(X|Y,Z)+ \I(X;Z|Y)$. 
}%oli10: endcommentout
%
%oli4: previous sentence
% The relationships between them are perhaps most easily seen with a diagram like the one below.
% The relationships between them
%
% But does 
%oli9:
Does multivariate information theory deliver on its promises? 
%oli11: I think the below is where I most need to put a few citations.
Many have argued that the answer is no.
Yet one of the most intuitively powerful arguments rests on 
a poorly motivated notion
 of structure in distributions
%oli10:
% , and  when
\unskip. When
%joe10: You've never mentioned a counterexample before.  You've just
%mentioned an argument for information theory not delivering on its
%promises.  
we replace that notion with \scibility, the
%oli11: addressing your comment above
% counterexample
argument
is not only defused, but 
%oli11:
% also
 supports the opposite of its intended purpose (\cref{example:ditrichotomy}).

%oli4: This isn't actually important for the current story.
\commentout{
\begin{defn}[Qualitative PDG]
    % A Qualitative Dependency Graph (QDG), is a directed weighted
    % hypergraph $(\N, \Ar, \balpha)$.
    % Concretely, it consists of
    % \begin{itemize}[nosep]
    % \item a set $\N$ of nodes interpreted as variable names;
    % \item a set $\Ar$ of (hyper)arcs connecting them,
    %     each $a \in \Ar$ of which is
    %     associated with a set of source nodes $\Src a \subseteq \N$ and
    %     target nodes $\Tgt a \subseteq \N$;
    % \item
    %     % a weight $\alpha_a \in \mathbb R$,
    %     % which together form a vector
    %     % $\balpha = [\alpha_a]_{a \in \Ar} \in \mathbb R^{\Ar}$.
    %     weights $\balpha = \{ \alpha_a \}_{a \in \Ar} \in \mathbb R^{\Ar}$ for each arc.
    % \qedhere
    % \end{itemize}
    A Qualitative Dependency Graph (QDG) $\dg Q = (\N, \Ar, \balpha)$,
    is a directed hypergraph $(\N, \Ar)$, together with a vector
    $\balpha = \{ \alpha_a \}_{a \in \Ar} \in \mathbb R^{\Ar}$ of
    weights for each \arc.
\end{defn}

For completeness, a (purely) quantitative probabilistic
dependency graph is a tuple $(\X, \mathbb P, \bbeta)$,
%joe2*: I found this very confusing
%oli2: oof, on re-reading, I agree.
% in which each $X \in \N$ has been given a quantative interpretation
% by associating $X \in \N$ a set $\V(X)$ of possible values (so $\X \equiv (\N, \V)$)
% and each \arc\ $a \in \Ar$ has been given a quantitative interpretation
% by associating each $\ed aST \in \Ar$ to a conditional probability distribution
% $\p_a(T|S)$ on its target given its source.
%oli2: bringing back and reworking the first part that you cut, because it's a key point: the variables \X are the result of annotating the (qualitative) nodes \N  with concrete values \V; meanwhile the cpds \P are the result of annotating the (qualitative) arcs \Ar  with concrete cpds.
% deeper reasons for why this is important that have to do with the categorical
% definition of a PDG, but here it is important.
%oli2: 
in which each node $N \in \N$ is interpreted a variable $X_N \in \X$
by associating it with a set $\V(N)$ of possible values,
%oli2: continuing with your more compact version below
and each \arc\ $\ed aST \in \Ar$ is 
%oli2: adding parallel wording (& a form of the missing word "associated")
    interpreted as a mechanism by associating it
with a conditional probability distribution
$\p_a(\Tgt a | \Src a)$ on its target given its source.
}
%oli4: end of cut
%oli4: merging paragraphs
%
%oli3: removing heading
% \textbf{Structural Information Deficiency.}
%joe2*: let's not proliferate names.  We called it information
%deficiency in the original paper
%oli2*: Ok, but after years of playing with the name, I've come to think
% "structural deficiency" is the better term.  The more papers we 
% publish where it is called the wrong thing, the harder it is to change.
% Also, my AISTATS paper (which is the only of our papers that is cited)
% outside of our own work calls it structural deficiency, 
% as do my ICML workshop papers. 
%joe3: I still have no clue with "deficiency" means in this context,
%and find it a horrible name.
%oli3: "Deficiency", because it's the amount information that is
%lacking in the structure, relative to what one would expect from a
%generic structure for describing a distribution \X.  I sent you a
%different writeup (on Wed, Jan 24, at 12:42am EST) that explains it
%in more detail. I'd be happy to discuss.  If I recall correctly,
%I believe you also find "information deficiency" to be a terrible name.
%oli6: paragraph break

%oli10: rewriting this to explicitly tie things together. 
%joe10: Another non sequitur here.  You've just talked about the
%counterexample not fulfilling its intended purpose, and then
%suddently switch to the "subtler constraints", which involves the
%information profile.  At a minimum, you have to rearrange the
%sentences so that you're not jumping back and forth between ideas, 
%to make for a better flow.
%oli11: I see what you're saying but I don't find it problematic, and I don't see what you are suggesting to fix it. This sentence can't come before we describe the information profile, and if we place it immediately after the that, the material at the end of the last paragraph becomes a bigger nonsequitur. The sentences in this paragraph are already in a very linear order, and cannot be reordered. 
%joe11: I don't know how to convince you that this is a badly written
%story, but it is.  If anything, it's gotten worse.  I have no clue of
%what the first few sentences are trying to tell me, how they relate
%to the rest of the papragraph, which seems to me like a bunch of
%disconnected sentences.  I see no discrenible story in this
%paragraph, and I don't see how it first int othe larger story of the
%first three paragraphs of this section (which is not surprising,
%since I don't think they're a good story at all).
%oli12: All the sentences are connected, but I see a way of making it smoother by putting the second one into the footnote. The story is that multivariate information theory (paragraph 2) give us tools to talk about the constraints we motivated with (paragraph 1), via the IDef (the next paragraph).
%oli12: minor rewrite:
\commentout{
%oli11: the best resolution I can see is a minor rewording to ease the suddenness. 
% We can now be more precise about the ``subtler constraints'' imposed by \scibility\ that we alluded to in the first paragraph:
With multivariate information theory, we can describe the ``subtler constraints'' imposed by \scibility:
%oli11:
% we mean constraints
% we mean conditions
they are linear inequalities
of the form $\mat{i}_\mu \cdot \mat v \ge 0$, 
    where $\mat i_{\mu}$ is $\mu$'s information profile, and $\mat v$ is a vector of integers.
% The constraints on $\mu$ that can be specified by requiring the inner product of the information profile with some vector of integers to be non-negative. 
% By this, we mean
}
%oli12: instead
SIM-compatibility with a hypergraph imposes a fundemental constraint on a distribution's information profile,
% It is the information profile---and, more precisely, linear inequalities of it
%oli9: adding
%oli10: rewrote above
% It is easy to capture arbitrary (conditional) (in)dependencies by requiring the inner product of the information profile with some vector of integers to be non-negative. 
%oli9: adding
%joe10*: Yet another example of a bad story.  You're trying (should be
%trying) to explain the deep connections between entropy and
%compatibility.  WHy bring you the issue of representing independence
%in a compact, graphical manner.  What does it have to do with the
%conections  between entropy and compatibility?
%oli11: hmm. I like the story here, even though it does not directly supporing the main claim. 
%joe11: If it doesn't support the claim, then it doesn't belong here!
%oli11: One role it plays is to head off the obvious doubts that might come to mind---after all, we motivated SIM-compatibility by wanting to capture independencies and unify them, and at this point it is clear that information theory fills both roles strictly better. I think some defense of SIM-compatibility here would be worthwhile.  In addition, the conceptual transition (if not the one I wrote, exactly) to the next paragraph is smooth.  
%joe11: I found what you wrote makes for a transition that's not the
%least bit smooth.  Maybe you have something better in your head, as
%uyo hinted "not the one I wrote, exactly"), but in our head doesn't
%count.  The reader can't read your ind.
%oli11: defering to you by demoting this to a footnote
\unskip\footnote{
%oli12: moved here (i.e., into footnote) from above
Specifically, a constraint of the form $\mat{i}_\mu \cdot \mat v \ge 0$, where $\mat i_{\mu}$ is $\mu$'s information profile, and $\mat v$ is a vector of integers.
It is not hard to see that arbitrary collections of (conditional) (in)dependencies can be captured by specifying $\mat v$ appropriately. 
%
    Given this, the reader might wonder: why bother with \scibility\ at all? 
    Here are two reasons: the dimension of the information profile is exponential in $|\X|$, and its components become increasingly unintuitive as the number of variables grows. 
    Our goal is not only to capture (in)dependence 
    %joe9
    %(indeed, (in)dependence itself does this),
    %oli10:
    % but rather to do so in 
    \unskip, but to do so in 
    %oli10:
    % an intuitive, visual, and causally motivated manner. 
    a compact, graphical, and causally motivated manner. 
    %oli11: giving a smaller paired down version of the cut material below to bring out connection to the next paragraph.
    %oli11: then commenting it out, as the transition is not as useful in footnote form.
    % One solution is to compile a \hgraph\ to a linear inequality like the one above.
}
%oli10: this text is not as valuable as the space it takes up. Making connection to IDef more directly.
\commentout{
To this end, we would like to compile a \hgraph\ to a function of an information profile, which we do directly in \cref{sec:scoring-funs}. 
But there is a simpler and more fundemental scoring function in the literature that has a more intricate and useful relationship with \scibility, an even closer relationship with \scibility, and ultimately will be able to capture both notions indirectly. 
}
%oli11: merge paragraphs
%
%oli10: instead, say:
%joe10*: similar to what? 
%oli11: Similar to our goal in the previous sentence. But perhaps it will be gramatically clearer if I say "like this" because the point of coreference is a verb phrase, not a noun phrase. 
%joe10*: In any case, you're jumping to yet another part of the story now.  
%oli11: Perhaps there is a rough transition in the wording, but fs conceptually it's a natural next step. 
%joe11: Maybe that's true in your head, but certainly not in mind,
%based on what you wrote.
%oli11: We just talked about the kinds of constraints that are possible; we now show how the original PDG scoring function compiles a hypergraph to such a constraint. This ties together all of the threads.  
%joe11: In what sense does the scoring function "compile a hypergrah"
%to anything.  I'm lost.
%oli12:  This formula (IDef) can be seen as converting a hypergraph into a linear function of a distribution's information profile. Is that much clear?
%oli11: I played with a few possibilities in the wording, but I still like what I had before best. 
%joe10*:Here's what I've gotten so far: (1) There is
%a constraint on compatibility that can be expressed in terms of
%entropy; (2) Thinking in terms of compability helps you better
%understand what is going on in a certain example (which is way down
%the tree!), and stops it from being an argument that information
%theory cannot capture all structural relationships between variables
%(but ceratinly does not indicate that information theory can capture
%all structural relationships between variables); (3) the scoring
%function for PDGs (that was designed by you), which uses entropy, is
%closely related to compatibility.  All this comes nowhere close to
%indicating a "deep relationship" between entropy and compatibility
%(at least, not to me).
%oli11: Okay, well I think you got the techncal content I wanted to convey, even if I think you are not adequately appreciating the importance of (1,3) or the fact that (2) is merely an illustrative example and not the extent of the relationship (and I disagree that it's far down the tree; it's the key application. What other problems that people care about have we addressed?). I've slightly toned down the language at the beginning of the section to keep expectations more reasonable, because I think the deeper story about how they're connected requires a different approach, using SDef. 
%joe10*: Moreover, the story itself is poorly told.
%If you really want to tell this story, you need to tell it much better
%oli11:
% \citet{pdg-aaai} attempt something similar in giving qualitative semantics for PDGs. 
% \citet{pdg-aaai} attempt something like this in giving qualitative semantics for PDGs. 
%
%oli12:

\citet{pdg-aaai} define
%oli8:
% an entropy-based
a 
scoring function 
% they call \emph{information deficiency}
%oli7: 
% which is $\IDef_{\!\Ar}(\mu)$ is a linear function of $\mu$'s information profile.
%oli9:
% that is a linear function of $\mu$'s information profile. It was intended to quantify discrepancy between a distribution $\mu$ and a PDG's \hgraph\ structure $\Ar$;
that converts a \hgraph\ to 
%oli11: adding
such
a linear function of $\mu$'s information profile, which was intended to quantify discrepancy between $\mu$ and a PDG's underlying structure.
%oli6: replacing
\commentout{
    %oli6: compressing:
    % In the original paper on PDGs, \cite{pdg-aaai}
    \Citet{pdg-aaai}
    %oli6: although it doesn't really reduce the proliferation of names, because both names are present and I will go back to "structural deficiency" in my thesis, I admit that is fewer words to just use what it was called where we cite it. 
    % define what we will call the \emph{structural (information) deficiency} of
    define the \emph{information deficiency} of
    a distribution $\mu$ with respect to a directed hypergraph $\Ar$
    %joe2
    %as the following quanitity:
    %oli6:
    % as follows:
    as:
}
% this so-called \emph{information deficiency} is given by
%oli9:
% this so-called \emph{information deficiency}, given by
This \emph{information deficiency}, given by
\begin{equation}
    \IDef_{\!\Ar}(\mu) := - \H_\mu(\X)  + \sum_{a \in \Ar} \H_\mu(\Tgt a \mid \Src a),
\end{equation}
%oli6.2: replacing these two sentences with one
\commentout{
    %
    The first term is the number of bits needed to specify an outcome of
    $\mu$
    %joe5*: Why does the first term specify \my in an *unstructured*
    %fashion.  You seem to have some intuitions about structure that you
    %haven't let the reader in on.
    %oli6: good point; I have something interesting to say here, so I'll expand. 
    % in an unstructured fashion,
    according to $\mu$'s own structure $(\to \X)$
    while the second
    %oli6
    % is the number of bits need to specify randomness in a structured way, following the \arc s of $\Ar$.
    is the number of bits needed to (independently) specify randomness along the \arc s of $\Ar$.
    The structural deficiency $\IDef_{\!\Ar}(\mu)$ is
    therefore the difference between the cost of a sample of
        $\mu$ along the structure $\Ar$, and describing it directly.
}
is the difference between the number of bits needed to (independently) specify the randomness in $\mu$ along the \arc s of $\Ar$, and the number of bits needed to specify a sample of $\mu$ according to its own structure $(\to \X)$.
%oli5: altering this sentence (which used to be at the beginning of this paragraph), and moving it here.
% Note that $\SDef_{\!\Ar}(\mu)$ is a sum of conditional entropies, and thus a linear function of $\mu$'s information profile.
% Observe that 
%oli5: adding some important links to section 4. 
%oli9: making this less of a nonsequitur, and introducing another important property.
$\IDef$ has many nice properties.%
%oli10: these properties can go in a footnote. 
\footnote{
%oli10:
% When $\Ar = \Ar_G$ for a dag $G$, then it
When $G$ is a dag, then $\IDef_{\!\Ar_G}(\mu)$
is non-negative, and
%oli10
%joe10
%equals zero iff $\mu$ has the independencies of the corresponding BN, 
is zero iff $\mu$ has the independencies of the corresponding BN, 
%oli10:
% an
which is a direct
analogue of our \cref{theorem:bns}.
% It is easy to verify that $\IDef_{\!\Ar}$
%oli10:
% it is also easy to see that $\IDef_{\!\Ar}$
$\IDef$ also
% equivalence and monotonicity (\cref{sec:equivalence,sec:monotone})
%oli6: because I cut the equivalence material from the main body, this can be compressed and done inline
\ifvfull\vfull{
also satisfies analogues of
\cref{prop:equiv-factorizations-cnd,theorem:strong-mono}.%
    \footnote{
    % By inspecting \cref{fig:info-diag}, $\H(X,Y \mid Z) = \H(X \mid Z) + \H()$. 
    Specifically, if $\Ar_1, \Ar_2, \Ar_3$ are the three \hgraph s in \cref{prop:equiv-factorizations-cnd}, then it is easy to prove that $\IDef_{\!\Ar_1} = \IDef_{\!\Ar_2} = \IDef_{\!\Ar_3}$.
    In addition,
    If $\Ar \rightsquigarrow \Ar'$, then it is easy to show that $\IDef_{\!\Ar} \ge  \IDef_{\!\Ar'}$.
    See appendix for details.}
}
\else
%oli6: for the conference version:
satisfies an analogue of \cref{theorem:strong-mono}: if $\Ar \rightsquigarrow \Ar'$, then $\IDef_{\!\Ar}(\mu) \ge \IDef_{\!\Ar'}(\mu)$.
%oli10: save a line in the footnote
 % for all $\mu$. 
\fi
%oli10: end footnote
}
%
%
%oli5: reintroducing subject because it is now further away, and fixing double "note".
% Note, however, note that it can be 
%oli11:
% Note, however, that $\IDef_{\!\Ar}(\mu)$ can be
However $\IDef_{\!\Ar}(\mu)$ can be
%oli11:
% negative---for instance, if $\Ar$ is empty.
negative in some cases (e.g., when $\Ar = \emptyset$)),
% This can make it somewhat confusing, despite its very nice properties. 
and so clearly it does not measure ``how far a $\mu$ is from being compatible with $\Ar$''
%oli6: flourish?
% for any sense of the word ``compatible''
%oli10:  partial reinstatement of flourish
% \unskip.
%oli11:
% \unskip, in any sense.
\unskip, in general.
%oli9:
% But, as we shall see, 
%oli10:
% Nevertheless, as we shall soon see, 
Nevertheless,
%joe5*: indeed, this is clear.  What's not at all clear (at least, not
%to me) is why this is a good scoring function.  But even if you could
%give intuition for that, THIS IS NOT THE PAPER YOU SHOULD PUT IT IN.
%That intuition belongs in the (jhournal version of) the first paper.
%oli6*: As you correctly point out, explaining why this is a reasonable scoring function is not the point of this section. The point is not to *defend* the scoring function, but to lay out what it is as clearly and concisely as we can.  
% the two constructions are
\scibility\ and $\IDef$ are
%oli11:
% closely related.
deeply connected.
%oli10:
% , 
% as we will now see.
%oli11:
% We will demonstrate this close relationship in a different way in each of the the three remaining subsections.
We will demonstrate a different aspect of that connection in each of the coming subsections.

%oli10: this summary was now moved up to the front, and can be cut
\commentout{
In \cref{sec:sc-infobound}, we describe a deep and nontrivial connection between \scibility\ and
%oli8:
% $\SDef$, 
% the information profile (via $\IDef$)
information deficiency ($\IDef$),
which works in two directions.  In one direction, it gives us an entropy-based criterion to conclude that a distribution cannot be \scible\ with a \hgraph; in the other, 
%oli8:
%this connection allows
it enables
 our definition of \scibility\ to clarify some important
%oli8: I think we've earned the right to be more forceful here
 % points
 misunderstandings
in information theory. 
%
% Then, in \cref{sec:scoring-funs}, we derive our own scoring function, 
We then (\cref{sec:scoring-funs}) derive a new entropy-based scoring function to directly measure how far a distribution is being \scible\ with a given \hgraph.
%oli6:
% ---%
% which, perhaps surprisnigly, turns out to be an instance of the original scoring function, applied to a transformed directed hypergraph.
But, perhaps surpringly, the new function turns out to be an instance of 
% the original scoring function, 
$\IDef$ applied to a transformed \hgraph.
% a (seeming) counterexample
% the more opaque aspects of 
% in the information theory literature.
% some historically contentious
}

\commentout{
In the PDG formalism
%  \citep{pdg-aaai},
the idea is to find a distribution $\mu$ that minimizes $\SDef_{\!\Ar}(\mu)$
plus an observational incompatibility $\Inc_{\dg M}(\mu)$ with the qualitative information.
}
% \begin{prop}
% \end{prop}
% Recall how one benefit of independence is that it allows for a more compact
% representation of a probability distribution. Note that $\SDef(\mu) \le 0$
% if and only if it is cheaper to specify randomness along the edges of $\Ar$
% than it is to specify
%oli2*: Although I'm very unhappy about it, I'm willing to try to 
% write a version in which the material interpreting qualitative PDGs
% is part of the appendix instead of the main body. 
%
% \input{original-scoring-function-intuition.tex}
%oli14: end of long \commentout
}

%oli4:
% \subsection{ \SCibility, and an Information Bound}
%oli6: a bit more precise
% \subsection{An Information Bound for \SCibility}
%oli15: moving section heading above
\subsection{A Necessary Condition for \SCibility}
    \label{sec:sc-infobound}
%oli15: moved from intro
What constraints does \scibility\ with $\Ar$ place on a distribution $\mu$?
%oli14: minor fixes. Tried to shorten, but wasn't able to do it well, so mostly reverted. 
% In \cref{sec:bns} we saw how, for \hgraph s that come from directed graphs, this means $\mu$ must satisfy the independencies of a Bayesian Network, and that additional \arc s can impose functional dependencies---but these results apply only to special classes of \hgraph s. 
% We saw in \cref{sec:bns} that $\mu \models \Ar$ implies dependencies for \hgraph s of  
%oli15: compressing, sharpening
% In \cref{sec:bns} we saw how, for \hgraph s that come from directed graphs, $\mu\models \Ar$ implies $\mu$ must satisfy the independencies of a Bayesian Network, and that additional \arc s impose functional dependencies---yet these results apply only to very special classes of \hgraph s. 
When $G$ is a dag, we have seen that if $\mu \models \Diamond \Ar_G$, then $\mu$ must satisfy the independencies of the corresponding Bayesian network (\cref{theorem:bns}); we have also seen that additional \arc s impose functional dependencies (\cref{theorem:func}). But these
%joe16
%results only apply when $\Ar$ is of a very special form.
results apply only when $\Ar$ is of a very special form.  
%joe12*: In what sense do we "weaken" SIM compatibility in Section 4?
%I'm confused.
%oli13: monotonicity allows us to weaken one statement of SIM compatibility to another statement of SIM compatibility. So,  consequences of \mu \models \Ar inluclude \mu \models \Ar', for every \Ar' obtained by weakening.  
%oli13: in any case, it's not the most important strand of the story, and we have to make it significantly shorter. So, cutting:
\commentout{%
    In \cref{sec:monotone} we saw how we can weaken \scibility\ relationships, but information is lost in the process, and the resulting \scibility\ statement often no easier to interperet.
}%
% (\cref{example:xyz-cycle-1}).
% In \cref{sec:sc-infobound}, we describe a 
%oli14:
% More generally, we know only that
%oli15:
% In general, we know
More generally, $\mu \models \Diamond \Ar$ implies that $\mu$ can arise from some 
%oli21: shave a line
% (generalized)
randomized causal model whose equations
%joe13: To reoeat what I've said many times before, I have no idea
%what "shape A" means, which means that most readers won't either.
%Nor do I know what it means to "give rise to %\mu".  I have no idea
%what this sentence is trying to say, and I'm an expert in the area!
%Why confuse the reader?  
%oli14: it's short and intuitive. I should probably just define what the "shape" of a causal model is. In the meantime, here's a longer rewording that's no more precisely defined, but perhaps more to your liking. 
% have shape $\Ar$---yet,
%oli20: saving a line.
% have the dependency structure of $\Ar$
have dependency structure $\Ar$
%oli15: adding
(\cref{prop:sc-graph-arise,prop:gen-sim-compat-means-arise}).
Still,
%oli14:
% unless $\Ar$ is of some particularly convenient form, 
unless $\Ar$ has a particularly special form, 
it is not obvious 
%oli14:
% if 
whether or not
this says something about $\mu$.
% turns out to be a generalization 
%joe12*: It's not "tight" in the way the word is normally used.  It
%gives just one directon.  Indeed, the fact that it's not tight makes
%it less interesting to my mind (and, I suspect, will also make it
%less interesting to most readers).  I know you think this result is
%really important, but it does not help to inflate it.
%oli13*: It is possible that I am inflating the result slightly, and  I'll bring it back to proportion, to the extent that I am. Still, I think you're mistaken about the degree, and also about how the term "tight" is normally used.  A tight upper bound doesn't say anything a "reverse direction" of the same problem; it means that no better upper bound is possible (perhaps because of a matching lower bound). 
%joe13: "Tight upper bound" means there's a matching lower bound.  In general, tight means you've proved that you can't do better.  That's not the case here. You're misusing the word.  Please remove it.  I don't want to go back and forth on this, nor do I want to argue the point further.  
%oli14: I'll remove "tight", but you cannot do better, in the sense that no smaller bound is possible. We have a matching lower bound. 
%oli13*: Indeed, it is not possible to get a sharper bound on SIM compatibility as a function of \mu's information profile; this is to to see, because for every hypergraph \Ar, there are distributions \mu that are SIM-compatible with \Ar, that have IDef_\Ar(\mu) = 0. This gives a "matching upper bound" so the result is, in this standard sense, "tight".  The fact that the reverse directions of these other theorems follow from it, as well as example 6, are additional points of evidene suggesting that the result is tight. 
%joe13: This "evidence" is not a proof.  Please remove the word tight.
%oli14: Perhaps I weakened my argument by adding extra intuition and calling it "evidence". As I understand things, there is also a straightforward proof. 
%joe17
%The primary result of this section is a
The primary result of this section is an
%oli14: since you insist, although I still want a satisfying explanation for why "tight" is not applicable here. 
% tight
information-theoretic bound (\cref{theorem:sdef-le0}) that
%joe13: As near as I can tell, this is not true.  It generalizes only one direction of theorem 1 and 2.  What other "direct consequences" of compatibility have we seen so far?
%oli14: Yes, it generalizes one direction of THeorems 1 and 2. "consequences of \scibility" refers to everything you can prove about \mu by assuming that \mu \models \Ar, which is only one direction. 
%  generalizes all of the direct consequences of \scibility\ that we have seen so far (\cref{theorem:bns,theorem:func,theorem:strong-mono}).
generalizes most of the concrete consequences of \scibility\ 
%oli21: shave a line
% that
we have seen so far (\cref{theorem:bns,theorem:func}).
%joe12: why do we need to say "multivariate".  What does "working in
%both directions" mean?  (I think I know what you intend, but the
%reader won't)
%oli13: I explain "both directions" immediately afterwards, so I'm not sure what more you are looking for.
%oli13: As for "multivariate", I'm trying to emphasize the information profile, but unfortunately it seems the place I explain this is later on in the exposition. I've removed the word below because it's better to wait to get into that. 
%oli20*: softening language for Joe. 
\commentout{%
The result is a bridge between information theory and causality---%
in one direction, giving us a useful information-theoretic consequence of \scibility,
and in the other, enabling \scibility\ to dispell misconceptions in information theory.
}%
%oli20: replacement
The result is a connection between information theory and causality; it yields an information-theoretic test for complex causal dependency structures, and enables causal notions of structure to dispel misconceptions in information theory.
%oli13: cutting for space, to merge with next sentence
% that works in both directions.
%joe12: What does "interpretable" mean in this context
%oli13: It means that people already have intuitions for the quantities in the inequalities, and that it can be interpreted in terms of communication costs.
%joe13: There may exist people who have these intuitions, one of them
%being you, but (a) it is unlikely to be true for many readers of the
%paper and (b)  this is not discussed anywhere in the paper.  I certainly
%have no clue what you have in mind.  It's a bad idea to make a 
%claoim that you're not going to substantiate!  It's much better to
%cut it here, and point out an interpretation later, if you
%have a reasonable interpretation.  
%oli14: (a) if we do our job, people will gain some intuition by working through the examples, and (b) we do discuss the interpret it somewhat, in these examples. 
%oli13:
% In one direction, it gives us an interpretable inequailty between information-theoretic primitves 
%joe12*: I think that is *far* too strong a claim.  You've shown that
%compatibility helps us better understand one example.  This does not
%establish it as a suitable notion of strutural informatoin!  Nor have 
%you shown that it eliminates confusion and misconceptions in anything
%other than this one example.  
%oli13*: Well, it's actually examples based on the same hypergraph, but that's just because we don't have enough space to go into other examples in depth, and this one illustrates what can be done extremely well. If you would feel more comfortable, I'll add some more examples, with the intention of putting them in the appendix. 
% in the other direction, this bound establishes \scibility\ as a suitable notion of ``structural information'' in information theory, by eliminating confusion and misconceptions that arise from the current de-facto occupants of that role. 
%joe13: removed "de-facto"
%oli14: went further...
% and in the other, suggesting that \scibility\ may play an important conceptual role for information theory, by dispelling misconceptions surrounding the current de-facto accounts of structural relationships in probability distributions. 
%oli20: moving up out of comments to consolidate
% and in the other, enabling \scibility\ to dispell misconceptions in information theory.
% and in the other, positioning \scibility\ as a notion of structure in information theory, to dispell misconceptions in information theory.
%joe13*: One example, or multiple examples based on the same hypergraph
%is not enough to dispel whatever "misconceptions" you have in mind.
%Would all the people who have claimed that the information profile is
%not enough suddenly say "Oh, I was wrong" when they saw your single
%example (or the others based on the same hypergraph)?
%oli14: Who knows how sudden it would be, but I am confident that this would shake their faith.
%oli14*: The fact that the example is based on a single hypergraph is technically not true, but more importantly, completely irrelevant; the focus on this example doesn't diminish the power of the argument at all.  The information theory in question is a quantity about three variables, and the argument is an argument about all hypergraphs on three variables. It just so happens that the 3-cycle is at the crux of the argument, because it is the most expressive graph without a joint dependence. 
%oli15: end moved material
    
    
% At this point, we have
% But what does this have to do with the definition
% of \scibility\ (\cref{defn:scompat}) that we introduced in \cref{sec:scompat}?
%
%joe5*: Again, you say that a result is useful, without giving any
%reason for the reader to think it's useful.  Either illustrate
%"useful" or (my preference) cut it.
%
%oli6*: I don't get it. The text below and example 6 are in-depth illustrations (perhaps too verbose, but certainly good reasons) why Theorem 12 is so useful. 
%oli6: cutting this text, because you don't like it and neither do I. I thought it was weird to start with 
\commentout{
    We start by stating a useful result that relates \scibility\ and the original PDG scoring function. 
}
\begin{linked}{theorem}{sdef-le0}
    %oli20: adding \Diamond
    If $\mu \models \Diamond\Ar$, then
    $\IDef_{\!\Ar}(\mu) \le 0$.
\end{linked}

%joe5:  this is avery weak argument for usefulness.  There could well
%be lots of other techniques; you just haven't found them.  You're
%better off illustraijg usefulness by showing that it can be used, in
%a number of intersting cases, to show that a distribution is not
%compatible with a PDG.  
%oli6: We *DO* illustrate how the technique can be used in some in-depth examples. And the fact that we don't know any other source of negative examples may not be a formal result we cna state, but it certainly supports the position that Theorem 12 is important. 
%joe5:  But in any case, this makes for a lousy
%story. You've introduced a scoring function, which is supposed to
%measure how far a distribution is far satisfyihg the indepedence
%constraints, 
%oli6: What you said above is inaccuarate. The scoring function I introduce is not "supposed to measure how far a distribution is from satisfying the independence constraints". That's your story, not mine. Please try to read my story. 
%joe5: and instead of explaining why it's a useful scoring
%function (i.e., why it really gives insight into q distribution's
%"distance" from satisying the independece assumptions -- which, by
%the way, is not the least bit clear to me) you focus on this
%particular use of the scoring function, which doesn't seem to me
%particularly important.
%oli6: Here, you're contradicting what you said in your email and in our meeting. You said that it didn't matter how important Theorem 12 was.  Above, you're instead arguing that we should tell a different story because Theorem 12 doesn't seem particularly important.  I'm telling you that Theorem 12 is far more important than the new scoring function. 
%oli6: rewriting.
% Generally speaking, \cref{theorem:sdef-le0} is our only
%     reliable source of negative examples---that is, 
%     our only general technique for
%     proving that $\mu \not\models \Ar$. 
%oli10:
% \Cref{theorem:sdef-le0} connects the question of \scibility\ between $\Ar$ and $\mu$ with a simple, easy-to-calculate function of $\mu$.
%oli20.1: on second thought, we might all be happier if we skip this first sentence, and fold the material into the above.
\commentout{
\Cref{theorem:sdef-le0} gives  
%oli-20: compromising by taking out "easy-to-calculate", also condensing 
% a simple and often easy-to-calculate condition necessary  
a direct information-theoretic test  
for \scibility
%oli-20: putting my emphasis on generality somewhere else, because I cut it below
% .
\ with an arbitrary hypergraph.
%oli-20:
% Moreover, it is the only
Moreover, it subsumses every
}
% Not only does \cref{theorem:sdef-le0} apply to all hypergraphs, but it
%oli20: replacement:
\Cref{theorem:sdef-le0} applies to all hypergraphs, and subsumes every general-purpose technique we know of for proving that $\mu \not\models \Ar$. 
Indeed, the negative directions of \cref{theorem:bns,theorem:func} are
 immediate consequences of it.
%oli2:
% To illustrate, we now return to the 3-cycle in example 5.
%oli8: moving paragraph break one sentence later
%
%joe5*: added paragraph break.  More importantly, to me this is a bad
%story.  You're getting  into the weeds of an example that doesn't
%even seem to me particularly.  I'm willing to take your word for it
%that the information theory community cares about this, but you
%haven't motivated it, and in any case, the motivation would have
%absolutely nothing to do with what I view as the main story of our paper.
%oli6: @above: you're still stuck on your story and not tracking mine! 
%oli6: updating wording
% To illustrate, let's return to the 3-cycle in \cref{example:xyz-cycle-1}.
%oli7: we're not illustrating what's above; new words.
% To illustrate, 
%oli8: forgot to add some words here. Also, paragraph break afterwards.
%oli10: emphasis
%oli20: Although I really like it, I'm removing the emphasis for Joe
% Yet \cref{theorem:sdef-le0} applies to all \hgraph s.
%oli11: Now that we have the sentence above, the start below is too repetetive.
% To illustrate its implications for \scibility\ with other \hgraph s, 
To illustrate
% some of its implications, 
some of its subtler implications,
let's return to the 3-cycle in \cref{example:xyz-cycle-1}.

%oli8: wrapping in example environment
%oli21: moving figure here
\begin{wrapfigure}[5]{o}{1.5cm}
    \vspace{-2ex}
    \def\circsize{0.7}
    \def\radsize{0.42}
    \def\twosize{0.5}
    
    \scalebox{0.85}{
    \begin{tikzpicture}[center base,scale=0.9]
        %oli21: removing label (b)
        % \node at ({-\circsize-\radsize+0.1},{\circsize+\radsize-0.2}) {\textbf{\underline{(b)}}};
        \begin{scope}
            \path[fill=green!60!black,fill opacity=0.25]
                (210:\radsize) circle (\circsize) ++(210:\circsize)
            % node[label={[label distance=0pt,inner sep=0pt]left:\small$A$}]{}
                (210:\radsize) circle (\circsize) ++(210:\circsize)
            % node[label={[label distance=0pt,inner sep=0pt]left:\small$A$}]{}
                (-30:\radsize) circle (\circsize) ++(-30:\circsize);
             % node[label={[label distance=0pt,inner sep=0pt]right:\small$C$}]{};
        \end{scope}
        \begin{scope}[even odd rule]
            \clip (-30:\radsize) circle (\circsize)
                    (210:\radsize) circle (\circsize)
                    (90:\radsize) circle (\circsize);
            \fill[fill=white!90!black] 
                % (-2,-2) rectangle (2,2);
                    (-30:\radsize) circle (\circsize)
                    (210:\radsize) circle (\circsize)
                    (90:\radsize) circle (\circsize);
         \end{scope}
         \begin{scope}
            \clip (-30:\radsize) circle (\circsize);
            \clip (210:\radsize) circle (\circsize);
            \clip (90:\radsize) circle (\circsize);
            \fill[fill=orange!80!black!40!white] 
                (90:\radsize) circle (\circsize);
          \end{scope}
        \begin{scope}[gray]
            \draw[] (-30:\radsize) circle (\circsize);
            \draw[] (210:\radsize) circle (\circsize);
            \draw[] (90:\radsize) circle (\circsize);
        \end{scope}
        \node at (-90:\twosize) {\small +1};
        \node at (30:\twosize) {\small +1};
        \node at (150:\twosize) {\small +1};
        \node at (0,0) {\small -1};
    \end{tikzpicture}}
\end{wrapfigure}
%oli21: klling example environment for 
% \begin{example}
\refstepcounter{example}
    \label{example:nonneg-ii}
\textbf{Example \theexample.~}
% Applied to that context, 
%oli8:
% In that context,
It is easy to see 
%oli21:
% (e.g., by inspecting \cref{fig:info-diag}(a)) 
(e.g., by inspecting \cref{fig:info-diag-a}) 
that
%oli8: expanding, so reader can follow
% $\IDef_{\text{3-cycle}}(\mu) = -\I_\mu(X;Y;Z)$
$\IDef_{\text{3-cycle}}(\mu) = 
    \H_\mu(Y|X) + \H_\mu(Z|Y) + \H_\mu(X|Z) - \H_\mu(XYZ)
    =
    -\I_\mu(X;Y;Z)$.
% (see \cref{fig:info-diag}(a)),
%
%oli8: changing "all... must be non-neg" to "a ... cannot be negative"
%
\Cref{theorem:sdef-le0} therefore tells us that 
%oli10: missing
a
distribution
    $\mu$ that is \scible\ 
    with the 3-cycle 
    %oli8: framing everything in negative to streamline discussion
    % must have non-negative interaction information 
    %oli10: perhaps most concise to never actually use the name "interaction information" for I(X;Y;Z)
    cannot have negative interaction information
% \begin{align*}
$
    \I_\mu(X; Y; Z)
    % = \I_\mu(X ; Y) - \I_{\mu}(X ; Y \mid Z)
        . 
% \end{align*}
$
%oli8:
What does this mean?
%oli4.2
%oli6: removing
% (i.e., the quantity in the center of \cref{fig:info-diag}).
%joe5:
% The fact that $\I(X;Y;Z)$ can be negative can be counter-intuitive.
%oli I really meant "can"; I will reference McKay exercise 8.8 to back it up. if I say "may", then the next sentence feels like it should be exploring this. 
%oli6: cutting this.
% The fact that $\I(X;Y;Z)$ can be negative may be counter-intuitive.
% The fact that $\I(X;Y;Z)$ might be negative can be counter-intuitive.
%$$ In general, negative interaction information means that two variables
% The total entropy in the a distribution goes down whenever 
%     one conditions on a variable, and so typically one also expect the amount of
%     information shared between variables to go down as well.
% In general, negative interaction information means that observing one
%     variable 
%oli6: more space-saving cuts; trying not to torque story. 
\vfull{
    Overall, 
    %oli6:
    % learning 
    conditioning on
    the value of one variable can only reduce the amount of 
    remaining information in other 
    %oli6: gotta be more careful here
    % variables.
    variables (in expectation).
}
%oli6:
% But, when three variables have negative interaction information, learning the value of one of them causes the other two to share more information than they did before. 
When $\I(X;Y;Z) < 0$, 
%oli10:
% learning the value of
conditioning on
one variable causes the other two to share more information than they did before. 
%oli8*: moving up from commented material below + new words
The most extreme instance is $\muxor$, the distribution in which two variables are independent and the third is their parity 
%oli21:
% (see \cref{fig:info-diag}(b)).
(illustrated on the right).
%
%
It seems intuitively clear that $\muxor$ cannot arise from 
%oli20:
% a causal model with these dependencies. 
the 3-cycle, a causal model with only pairwise dependencies.
This is difficult to prove directly, but is an immediate consequence of \cref{theorem:sdef-le0}.
%oli21: making example play with wrapfig, part 2
% \end{example}
\hfill$\triangle$
    
    
%oli8*: restructuring this paragraph to reflect on the example; commenting out this version of it, along with its history, to start fresh. 
\commentout{
% Playing with such examples quickly builds up a strong intuition
%     that that negative interaction information ought to imply some
%     kind of 3-way interaction between variables.
%joe5*: This is just not a useful things to say.  "One" may build up a
%strong intuition, if that one is a clone of you, who has worked on
%this.  But the typical reader of this paper hasn't worked on this.
%In any case, what if your intuition is wrong?
%oli6*: this is not a pargraph about me. I should add references. Your question of "what if our intuition is wrong?" is precisely the resolution that many people come to in the literature.  But, because of our definition, we can validate peoples' intuitions for the first time! 
%oli6: reordering.
% One quickly builds up a strong intuition that this requires a fundementally ``3-way'' interaction
% This may seem to require a fundementally ``3-way'' interaction between the variables, rather than pairwise relationships
This may seem to require a fundementally ``3-way'' interaction between the variables, rather than pairwise relationships
---but, according to the traditional ways of making ``pairwise relationships'' precise (e.g.,  maximum entropy subject to pairwise marginal constraints, factoring over pairwise factors), this is incorrect. 
However, if we take it to mean there is a causal model without any joint dependencies, then the 3-cycle is the most expressive structure, and we have already seen that \scibility\ with it implies non-negative interaction information.
%oli: 
% validating a common ``misconception''.
%joe5*: I"m lost.  Why should we take "can be captured by 2-way
%interactions" to mean SIM-compatibility?  You have some deep
%intuition here that I'm just not getting.
%oli6: It makes sense that you're not following here; I skimmed over this way too quickly. The intuition is that each equation is a 2-way interaction between variables.
%joe5*: The rest of the paragraph
%does tell a good story.  That's being polite; the story is actually
%quite bad.  You talk about "historically" when you haven't explained
%the concerns or intuitions of the information theory community, an
%exlaining them would take us way to far afield.   This is not a paper
%about information theory.   There may be people who find this example
%really interesting, but they will *not* be the typical readers of
%this paper.  
%oli6: the readers of this paper depend on what we write. I know for a fact that there are people interested in information theory in the UAI community. It doesn't make sense to ask me to write a paper for the audience you would have chosen for your paper. 
%oli6: cutting this; I rewrote it in the reverse order above
\commentout{
to mean \scibility\ with the cycle, then, by \cref{theorem:sdef-le0}, this intuition is well-founded. 
But historically, that intuition has been a sticking point for interpreting interaction information, because other ways of making ``2-way interactions'' precise, such as matching pairwise marginals, or factoring as a product of 2-variable factors, do not rule out negative interaction information $\I(X;Y;Z)$. 
}
%oli8: reordering; moving this material above
\commentout{
    %joe5*: Now you're brining in a distribution that will most likely be
    %buried in the appendix out of the blue.  
    %oli6*: It's not in the appendix; this proof (sketch) is in the main body of the paper. This instance is the second of three times \muxor will be used.
    %joe5*: Yet again, this does not
    %make for a good story.  At the end of the day, you're showing that a
    %particular distribution is not compatible with the 3-cycle, and I
    %have no clue of why I should care.  
    %oli6: You should care because it's extremely hard to imagine this particular distribution arising from a causal model with this structure, and now we have shown why. Let me add some more here. 
    More concretely, \cref{theorem:sdef-le0} means that $\muxor$,
    %oli6: this is not footnote material; it's main body material.
        % \footnote{$\muxor$ is the distribution over 3 variables in which two are independent, and the third is their parity, introduced in the proof of \cref{theorem:mrf-bn-monotone-impossible}.
    the distribution in which two variables are independent and the third is their parity, cannot come from a causal model without a joint dependence
    (as one would expect).
    %oli6: adding
    It is worth noting that $\muxor$ is
    the most extreme example of negative interaction information,
    %oli6: adding
    because $\I(X;Y)$ is non-negative 
    %oli6:
    % is not compatible with the 3-cycle 
    (see \cref{fig:info-diag}(b)).
}
Our next example builds on this one.
}
%oli8: end{commentout}

% $\I(X;Y;Z)$ should not be negative 
% \cite{mackay2003information,cover}
For many, there is an
% Many people develop an
intuition that
$\I(X;Y;Z) < 0$
%oli10:
% requires
should require
a fundementally ``3-way'' interaction between the variables,
%oli10:
% and cannot arise through pairwise relationships alone.
%oli21: adding citation
and should not arise through pairwise interactions alone \cite{dit-stumble}.
% This has historically been a problem,
% But this intuition should be quashed
This has been a source of conflict 
%oli21: adding citations
\cite{williams2010nonnegative,mackay2003information,1219753,CoverThomas},
% \cite[Ex 8.8]{mackay2003information},
% This has created some tension,
%
because traditional ways of making 
%oli10:
% ``only pairwise relationships'' precise (e.g.,  maximum entropy subject to pairwise marginal constraints,
%oli21: line shave:  
% precise ``pairwise interactions alone'' 
precise ``pairwise interactions'' 
(e.g., maximum entropy subject to pairwise marginal constraints 
%oli10:
 % factoring over pairwise factors)
and pairwise factorization)
 do not ensure that $\I(X;Y;Z) \ge 0$. 
%oli10: oops the second half of this sentence got lost somewhere. Rewriting it completely, and slowing down. Also, explicitly bringing in SIM-compatibility.
% However, if  there is a causal model without any joint dependencies, because the 3-cycle is the most expressive structure,
But \scibility\ does. 
One can verify by enumeration that the 3-cycle is the most expressive causal structure with no joint dependencies, 
and we have already proven that \scibility\ with that \hgraph\ 
implies non-negative interaction information.
 % and we have already seen that \scibility\ with it implies non-negative interaction information.
% \commentout{
% to mean \scibility\ with the cycle, then, by \cref{theorem:sdef-le0}, this intuition is well-founded. 
% But historically, that intuition has been a sticking point for interpreting interaction information, because other ways of making ``2-way interactions'' precise, such as matching pairwise marginals, or factoring as a product of 2-variable factors, do not rule out negative interaction information $\I(X;Y;Z)$. 
% }
%oli10: this is a cute way to think about things, but not in the direct line of the story. Cutting.
% It seems the issue is neither with information theory or one's intuitions, but rather the notion of a mechanism.
%oli15: 
% Our next example builds on this one.
\Scibility\ has another even more noteworthy clarifying effect on information theory. 

%oli15: moved here.
% The notion that a distribution's information profile $\mat I_\mu$ captures all its structural information, dubbed "multivariate (Shannon) information theory," has fallen out of favor due to counter-examples of distributions with different intuitive structures sharing the same profile. However, when the intuitive notion of "structure" is replaced with \scibility, not only do such examples disappear, but they inadvertently suggest the opposite - that $\mat I_\mu$ does indeed fully characterize $\mu(\X)$'s structure under \scibility
There is a school of thought
%oli13: maybe not critical to name it, and saves lots of space
% , sometimes referred to as \emph{multivariate (Shannon) information theory},
that contends that \emph{all} structural information in $\mu(\X)$ is captured by its information profile $\mat I_\mu$.
%oli13: rewriting
% This position has become unpopular recently, in no small part because of standard counter-examples of distributions with intuitively very different structures that share an information profile.  However, when that notion of ``intuitively different structure'' is replaced with \scibility, the examples not only disappear, but betray their intended purpose by suggesting that the opposite is true. 
This position has fallen out of favor
%oli14: softening
in some communities
due to standard counterexamples: distributions that have intuitively different structures yet share an information profile
%oli21: adding citations
\citep{multivar-beyondshannon17}. 
%joe13*: As near as I can tell, you've shown that exactly one
%counterexample disappears.  Are there others?  If so, you eithier need
%to show that they disappear too, or tone this down significantly.
%You're basing a lot on a single example.  I find that completely
%inappropriate.  I keep trying to point that out, but you keep making
%only minor changes in that direction, which is part of why this is dragging
%out way too long
%oli14: I'm talking here about a cluster of closely related counterexamples, and the argument I present here defeats them all in the same way. I don't see why my presentation of the general argument under the heading "example 6" makes it sketchy to base a lot on it. That said, I'm happy to change plural to singular in a few places here so that it the number gramatically matches our presentation. 
%oli14: 
% However, when the notion of structure is made precise by \scibility, the counterexamples
%oli22: line shave
% However, when the notion of structure is made precise by \scibility,
However, with ``structure''  explicated by compatibility,
% However, if we view structure through the lens of \scibility,
% However, if we view structure through the lens of \scibility,
the prototypical counterexample of this kind
% not only disappear, but suggest that the opposite is true. 
% betray their original intension.
%oli14:
% not only cease to be counterexaples, but in fact suggest the opposite of their intended purpose. 
%joe17*: what was it a countereaxmple to?  This comes out of the blue
%and must be clarified
%oli22: I find this very clear from one sentence above, but perhaps we can avoid this entirely with a careful cut:
% not only ceases to be a counterexample, but also
suddenly
supports the very notion it was meant to challenge, suggesting in an unexpected way that the information profile may yet capture the essence of probabilistic structure.
% but also suggests the opposite of its intended purpose. 

\begin{wrapfigure}[5]{o}{1.8cm}
    \vspace{-2ex}
    \def\circsize{0.7}
    \def\radsize{0.42}
    \def\twosize{0.5}
    \scalebox{0.85}{%
    \begin{tikzpicture}[center base,scale=0.9]
        % \node at ({-\circsize-\radsize+0.1},{\circsize+\radsize-0.2}) {\underline{\textbf{(c)}}};
        \begin{scope}
            \path[fill=green!60!black,fill opacity=0.25]
                (210:\radsize) circle (\circsize) ++(210:\circsize)
            % node[label={[label distance=0pt,inner sep=0pt]left:\small$A$}]{}
                (210:\radsize) circle (\circsize) ++(210:\circsize)
            % node[label={[label distance=0pt,inner sep=0pt]left:\small$A$}]{}
                (-30:\radsize) circle (\circsize) ++(-30:\circsize);
             % node[label={[label distance=0pt,inner sep=0pt]right:\small$C$}]{};
        \end{scope}
        \begin{scope}[even odd rule]
             \clip (-30:\radsize) circle (\circsize)
                    (210:\radsize) circle (\circsize)
                    (90:\radsize) circle (\circsize);
            \fill[fill=white!90!black] 
                % (-2,-2) rectangle (2,2);
                    (-30:\radsize) circle (\circsize)
                    (210:\radsize) circle (\circsize)
                    (90:\radsize) circle (\circsize);
         \end{scope}
        \begin{scope}[gray]
            \draw[] (-30:\radsize) circle (\circsize);
            \draw[] (210:\radsize) circle (\circsize);
            \draw[] (90:\radsize) circle (\circsize);
        \end{scope}
        \node at (-90:\twosize) {\small +1};
        \node at (30:\twosize) {\small +1};
        \node at (150:\twosize) {\small +1};
    \end{tikzpicture}}
\end{wrapfigure}
%oli21: can't use example environment with wrapfig
% \begin{example} \label{example:ditrichotomy}
\refstepcounter{example} \label{example:ditrichotomy}
\textbf{Example {\theexample}.~}         
    % Now, consider the following distributions
    Let $A, B$, and $C$ be variables
    %oli15:
    % with $\V(A) = \V(B) = \V(C) = \{ (0,0), (0,1), (1,0), (1,1) \}$. 
    % each of which can take on a pair of bits. 
    with $\V(A), \V(B), \V(C) = 
    %oli21: 
    % \{ (0,0), (0,1), (1,0), (1,1) \}$. 
    \{ 0,1\}^2$. 
    % with $\V(A) = \V(B) = \V(C) = \{ 00, 01, 10, 11 \}$. 
    % Construct two joint distributions, $P$ and $Q$, 
    %     over the variables $A,B,C$ as follows.
    Using independent fair coin flips $X_1$, $X_2$, and $X_3$, define
    % Use three independent fair coin flips $X_1$, $X_2$, and $X_3$, 
        two joint distributions, $P$ and $Q$, 
        over $A,B,C$ as follows. 
%joe5*: In what sense does this table define the distributions?  The
%definition of the variable does not give the distribution.  I'm lost.
%oli6*: The definitions of the variables do give the distributions. Here's one way to think about it: I started with a distribution over (X1, X2, X3): three independent coin flips. Then, because I told you how (X1,X2,X3) determine (A,B,C), there is a unique way to extend the original distribution over (X1-3) to all six variables.  Tha distribution---or, if you prefer, its marginal on (A,B,C)---is the one I'm defining. 
%oli6*: Do you have an idea for how to clarify that compactly?
%oli15: Doing this inline now.
    % \begin{align*}
    %     &\underline{p(a,b,c)} && \underline{q(a,b,c)}\\
    %     a &:= (x_1, x_2); 
    %                         & a &:= (x_1, x_2); \\
    %     b &:= (x_1, x_3);
    %                         & b &:= (x_2, x_3); \\
    %     c &:= (x_1, x_2 \oplus x_3);
    %                         & c &:= (x_3, x_1).
    % \end{align*}
%oli15*: 
    Define $P(A,B,C)$ by letting $A := (X_1, X_2)$, $B := (X_2, X_3)$, and $C := (X_3, X_1)$.
    Define $Q$ by letting 
    $A := (X_1, X_2)$, $B := (X_1, X_3)$, and $C := (X_1, X_2 \oplus X_3)$.
    %oli2: continuing the example, which wansn't finished before
    %oli16:
    % $P$ and $Q$ appear to be very different, structurally. 
    Structurally, $P$ and $Q$ appear to be very different. 
    %oli15: reordering, 
    % According to $Q$, the first bit of all three variables is the same, yet, according to $P$, the first bits of $A$, $B$, and $C$ are mutually independent. 
    According to $P$, the first components of the three variables ($A,B,C$) are independent, yet they are identical according to $Q$. 
    Moreover, $P$ has only simple pairwise interactions between
        the variables, while $P$ has $\muxor$ 
        %oli20: streamlining word order
        % embedded (a clear 3-way interaction) within it. 
        (a clear 3-way interaction) embedded within it. 
    Yet $P$ and $Q$ have identical information profiles 
    %oli21: 
    % (\cref{fig:info-diag}(c));
    (see right):
    %oli15: reinstating for the time being; may have to again cut it later. 
    % \vfull{
    %oli21: 2x any -> each below
    in both cases, 
    each of $\{A,B,C\}$
    %oli21: shaving
    % can be
    is
    determined by the values of the other two,
    each pair share one bit of information given the third,
    %oli21: couldn't find another way to kill the orphan
    % and there is no interaction information. 
    and $\I(A;B;C) = 0$.
    % }
    
%oli6: saving space by moving this into the previous figure
\commentout{
\begin{center}
    % \begin{tikzpicture}[center base]
    %     \begin{scope}[every node/.style={dpad0}]
    %         \node (A1) at (210:1.4) {$A_1$};
    %         \node (B1) at ( 90:1.4) {$B_1$};
    %         \node (C1) at (-30:1.4) {$C_1$};
    %         \node (A2) at (210:0.5) {$A_2$};
    %         \node (B2) at ( 90:0.5) {$B_2$};
    %         \node (C2) at (-30:0.5) {$C_2$};
    %     \end{scope}
    % \end{tikzpicture}
    % ~~
    % \begin{tabular}{c}
    %     % $\H(A,B,C) = 3$ & $\H()$ & \\ 
    %     $\H(A | B,C) = \H(B|A,C) = \H(C | A,B) = 0$ \\
    %     $\I(A;B) = \I(A ; B | C ) = \I(B ; C | A) = \I(A ; C | B) = 1\,\text{bit}$ \\
    % \end{tabular}
    \def\circsize{0.7}
    \def\radsize{0.42}
    \def\twosize{0.5}
    \begin{tikzpicture}[center base]
        \path[fill=white!70!black] (210:\radsize) circle (\circsize) ++(210:\circsize) node[label={[label distance=0pt,inner sep=0pt]left:\small$A$}]{};
        \path[fill=white!70!black] (90:\radsize) circle (\circsize) ++(135:\circsize) node[label={left:\small$B$}]{};
        \path[fill=white!70!black] (-30:\radsize) circle (\circsize) ++(-30:\circsize) node[label={[label distance=0pt,inner sep=0pt]right:\small$C$}]{};
        \begin{scope}
            \draw[] (-30:\radsize) circle (\circsize);
            \draw[] (210:\radsize) circle (\circsize);
            \draw[] (90:\radsize) circle (\circsize);
        \end{scope}
        \node at (-90:\twosize) {\small +1};
        \node at (30:\twosize) {\small +1};
        \node at (150:\twosize) {\small +1};
    \end{tikzpicture}
\end{center}}

%joe17*: it seems that only now are you explaning what it's a
%counterexample to.  At a minimum, it seems that there should be some
%reordering here, as we ll as more explanation and intuition.
%There's too much that's in your head
%oli22: I hope my changes above helped here. 
This example has been used to argue that multivariate Shannon
%joe5
%    information is blind to important structural 
    information does not take into account important structural
    differences between distributions \citep{multivar-beyondshannon17}.
%oli6: 
    % Things get much more interesting when we apply our definition.
%joe10: I would expect some references here
%oli11: I don't know a reference; perhaps someone has said it before but I came up with it myself. I'll do some reading and try to track don some references in the next iteration or two. 
%oli11: commenting out the line below, and attaching the footnote elsehwere.
% While there are other responses to this criticism,%
%oli11
% we are in a position to give a novel and particularly persuasive one,
We are now in a position to give a novel and particularly persuasive response, by appealing to \scibility.%
%oli11: moved footnote + comments here
%joe10: this footnote is a distraction.  I strongly suggest that you
%cut it (or, at a minimum, drastically shorten it) and add a
%reference.  
%oli11: I think this is a very important and valuable point of clarification. I'll try to shorten it. Another option would be to put it in the appendix and expand it. It could certainly productively take up more space than we have for it in our primary story. 
%oli23: for space
\vfull{%
\footnote{%
%oli6: yeah, this sentence is probably worth cutting too
    % Indeed, while $P$ and $Q$ share an information profile over the variables
    % %joe5*: I have no idea of the sense in which it's ephemeral.  You're
    % %well into the weeds of this example, rather than telling a story
    % %about how exciting our semantics is.
    % $A,B,C$, this effect is ephemeral.
    % one need only split each variable (or even just $A$) into two distinct variables.
    %oli11: adding a transition because of deleted words above
    % Here is a weaker argument based on more primitive notions.
    %oli11: shortening
    % In order to capture the difference between $P$ and $Q$ using multivariate information, 
    % we can split the variables into their components.
    Note that $P$ and $Q$ no longer have the same profile if we split each variable into its two components.  
    % we need only split the variables into their components.
    %oli6: changing wording so this doesn't look like the main point.
    % However, as we have defined it,
    %oli11: more reducing
    % After all, as we have defined it, structural information does not include the set of possible values that a variable can take, on which our intuitions of ``first component'' are based, so it makes sense that splitting could be necessary.
    %oli15: reordering for clarity
    % Consequently, our postulate that $\V$ on is not structural already weakens the counterexample, since the notion of ``component'' is based on $\V$. 
    Since the notion of ``component'' is based on
    the assignment $\V$ of variables to possible values,
    our view that $\V$ is {not} structural information
    diffuses this counterexample by assumption---%
    but the present argument is much stronger.
}
}%
    Unsurprisingly, $P$ is \cible\ with the 3-cycle;
    % since it is constructed with``2-way interactions'': each pair of variables shares a bit. 
    it is clearly consists of ``2-way'' interactions, as each pair of variables shares a bit.
%oli11:
% But, surprisingly, the distribution $P$ \emph{also} is \cible\ with the 3-cycle. 
But, 
%oli14: everyone I've shown this to (including some theory profs) have been surprised by this fact; I think it's safe to call it counter-intuitive. 
% perhaps surprisingly,
counterintuitively, 
the distribution $Q$ is \emph{also} \cible\ with the 3-cycle!
%oli16: 
(The reader is encouraged to verify that 
    $U_1 = X_3 \oplus X_1$, $U_2 = X_2$, and $U_3 = X_3$
    serves as a witness.) 
To emphasize: this is despite the fact that $Q$ 
is just 
% is the tensor product of
    $\muxor$
    (which is certainly not compatible with the 3-cycle)
    %oli6:
    % (which has negative mutual information) $P$ itself 
    % (which has negative interaction information)
%oli15: I really mean "tensored with", but perhaps that's too technical
% conjoined with
% tensored with
%oli20:
% plus a seemingly irrelelvant bit $X_1$ written in the sky.
together with a seemingly irrelevant random bit $X_1$.
    %oli6:
    % $P$  \emph{is} \scible\ with the 3-cycle. 
%oli11: instead of repeating, changed the first part of the sentence
    % $P$ is \scible\ with the 3-cycle. 
    % By the results of \cref{sec:causal}, this means there is a causal model, made up of only 2-way interactions, giving rise to $Q$. 
By the results of \cref{sec:causal},
% By \cref{prop:sc-graph-arise} 
this means there is a causal model without joint dependence giving rise to $Q$---%
    %joe5*: I have no idea why this should be counterintuitive, and in any
%case, it depends on your intutuition.  In general, it's a bad idea to
%tell th reader they should find something counterintuitive.  At best
%you can say "this may seem counterintuitive".
%oli6: this sentence might be overkill anyway.
    % This is counterintuitive: $\muxor$ cannot arise through independent mechanisms in a cycle on its own---but it can if there is one extra bit written in the sky.
    % 
%oli6: moved below (with modification):
    % Suggestively, that one bit is precisely the amount of negative
    %oli6: cutting the rest
    % , and hence precisely what is 
    % necessary for us to even consider it possible to be \scible\ with the 
    % 3-cycle in light of \cref{theorem:sdef-le0}.
    % This also shows that the distribution $P$ does not so fundementally require a 3-way interaction between the three variables, even if it seems that way because of how it is constructed.
%joe5*: I'm totally lost.  In what way P fundamentally rqwuire
%anything.  More imoprtantly, this is far reomved  from anything even
%remotely related to what I see as the story of the paepr.
%oli6*: I disagree; in some sense, this is the punchline: it's the most interesting thing we prove here, and it's a direct application of SIM-compatibility, that clears up a real misconception that (judging by citations), hundreds of people have. I hope that I have now motivated it better, and described it more clearly.
%oli6: compressing; most of this has been said.
    % This shows that, despite appearances, $P$ does not fundementally require a 3-way interaction, unlike the distribution $\muxor$ on which $P$ is based.
    %oli15:
    % We conclude that (despite appearances)
    %oli20: adding for flow
    so,
    despite appearances,
    $Q$ does not 
    %oli20: shaving a line + perhaps better without 
    % fundamentally
    require a 3-way interaction.
    %oli6: expanding punchline, to come all the way back out
    %oli15:
    % so there is no reason $P$ and $Q$ should have different information profiles.
    % and so 
    Indeed, $P$ and $Q$ are \scible\ with precisely the same \hgraph s over $\{A,B,C\}$, 
    % and so there is no 
    suggesting that they don't have a structural difference after all. 
    % suggesting that their difference may not be structural after all.
%oli21: accomodating wrapfig: part 2
% \end{example}
\hfill$\triangle$

%oli20:
\commentout{%
The shared bit $X_1$ between the three variables, suggestively,
contains precisely the magnitude of negative interaction information in $\muxor$. One might therefore wonder if the converse of \cref{theorem:sdef-le0} holds. 
}%
In light of \cref{example:ditrichotomy}, one might reasonably conjecture that the converse of \cref{theorem:sdef-le0} holds.
% It does not (see \cref{appendix:converse-sdef-le0}).
%joe16
%Unfortunately, it does not (see \cref{appendix:converse-sdef-le0}),
%and so, the quantity $\IDef_{\!\Ar}(\mu)$ does not 
Unfortunately, it does not (see \cref{appendix:converse-sdef-le0});
the quantity $\IDef_{\!\Ar}(\mu)$ does not 
completely determine whether or not 
$\mu \models \Diamond \Ar$. We now pursue a new (entropy-based) scoring function that does.
%oli20: adding
This will allow us to generalize \cref{theorem:sdef-le0} to distributions that are only ``near-compatible'' with $\Ar$. 

%oli3:
% \subsection{Another Scoring Function: SInc}
%oli10:
% \subsection{Scoring Functions for \SCibility}
\subsection{A Scoring Function for \SCibility}
    \label{sec:scoring-funs}
%oli3:
% For each $a \in \Ar$, fix a positive real number $\lambda_a > 0$. Also, fix $\lambda_0 > 0$. With these, we can define a new scoring function paramaterized by
%oli3:  changing labels
% the weights $\boldsymbol\lambda = (\lambda_0, \lambda_1, \{\lambda_a\}_{a \in \Ar})$:
%oli20:
% We now derive a scoring function to directly measure how far a distribution $\mu$ is from being \scible\ with a directed hypergraph $\Ar$. 
Here is a function that measures how far a distribution $\mu$ is from being \scible\ with 
% a \hgraph\ $\Ar$. 
$\Ar$.
%oli14*: taking the weights and focus on weights out of the main text. It's worth mentioning how the weights line up, but it creates more friction than its worth conceptually, especially in stating the theorems later on. 
% Given $\Ar$ and $|\Ar|+2$ positive weights $\boldsymbol\lambda = (\lambda^{\text{(a)}}, \lambda^{\text{(b)}}, \{\lambda^{\text{(c)}}_a\}_{a \in \Ar})$, define the function
%
\begin{align*}
%joe10*: why SIMInc??? 
%oli14: we can choose something else, but I liked the idea of the name being directly tied to the name "SIM compatibility". The previous choice, SInc, we used in a previous paper for something else, so I'm trying to avoid confusion there---plus, 
    %oli15: removing weights on SIMInc
    % \SIMInc_{\Ar, \boldsymbol\lambda}(\mu) :=
    \SIMInc_{\Ar}(\mu) :=
    %oli14: for simpler version with no weights I'm, putting this back to being a constraint.
    % \inf_{\nu(\U,\X)}
    \inf_{\substack{
            \nu(\mathcal U\!,\, \X)\\
            \mathclap{\nu(\X) = \mu(\X)}}}
            \;
    %oli14: .. and deleting this term, which is now enforced by constraint
    % \lambda^{\text{(a)}} \kldiv{ \nu(\X) }{\mu(\X)} \\
    %oli14:
    % \lambda^{\text{(b)}} 
    % \Big(
    - \H_\nu (\mathcal U) + 
    \sum_{a \in \Ar} \H_\nu (U_a)
    % \Big)
    %oli15: in neurips format, no new line needed
    % \\[-1ex]
    +
        \sum_{a \in \Ar}
        %oli14:
        % \lambda_a^{\text{(c)}}
        \H_\nu(\Tgt a | \Src a , U_a)
    .  \numberthis\label{eq:siminc}
    \vspace{-1ex}
\end{align*}
% % \def\QIMI{\mathrm{QIM}\mathit{I}}
% \def\QIMI{\mathit{QIMI}}
% \begin{align*}
%     \QIMI_{\!\Ar}^W(\nu) :=   
%         \sum_{a \in \Ar} \H_\nu (U_a)
%          - \H_\nu (\mathcal U)
%         +
%             \sum_{a \in \Ar}
%             \H_\nu(\Tgt a | \Src a , U_a)
%     &&
%     \QIMI_{\!\!\Ar}(\mu) :=
%     \inf_{\substack{
%             \nu(\mathcal U\!,\, \X)\\
%             \mathclap{\nu(\X) = \mu(\X)}}}
%     \QIMI_{\Ar}^W(\nu)
%     .  \numberthis\label{eq:siminc2}
% \end{align*}

%oli3: revamping this for new labels, and compressing.
% This may look complicated, but the first group of terms is a generic (entropy-based) measure of how far $\nu$ is from satisfying (b'), while the group is a generic (entropy-based) measure of how far $\nu$ is from satisfying (d).
%oli14: now one of the conditions is in the constraint. Rewriting below.
\commentout{
This may look complicated, 
but each line is just a generic (entropy-based) measure of how far an extended joint distribution $\nu$ is from satisfying one of the conditions (a-c) of being a \scibility\ witness (\cref{defn:scompat}). 
Because each is non-negative and zero iff the appropriate condition is satisfied, 
% $\SIMInc_{\Ar,\boldsymbol\lambda}$
$\SIMInc_{\!\Ar}$
overall is non-negative, and zero iff all three conditions are satisfied.
}
%oli20:TODO
% This is a natural scoring function... 
%oli14:
%oli20: redoing this
% It may look complicated, but the terms on the first line are a measure discrepancy between $\nu$ and condition (b) for being a \scibility witness (\cref{defn:scompat}), those on the second line measure discrepancy with condition (c), and the constraint under the infemum ensures we only search over distributions satisfying (a). 
$\SIMInc$ is a direct translation of \cref{defn:scompat} (a-c); 
% the language of entropy. 
    % entropy.
it measures the (optimal) quality of an extended distribution $\nu$ as a witness.
The infimum restricts the search to $\nu$ satisfying (a), the first two terms measure $\nu$'s discrepancy of with (b), and the last term measures $\nu$'s discrepancy with (c). 
% So, by construction:
Therefore:
% So, unsurprisingly:

% The first is a measurement of dicrepancy between $\mu$ and the marginal $\nu(\X)$, and the third is the non-zero iff 
% but the first line is a generic (entropy-based) measure of how far $\nu$ is from satisfying property (a), the second is a generic (entropy-based) measure of discrepancy with property (b), and the third is a generic (entropy-based) measure of how far $\nu$ is from satisfying (c).
% The scalars $\lambda$ can be thought of as lagrange multipliers

\begin{linked}{prop}{sinc-nonneg-s2}
%oli14: simpler version for main text, without weights
\commentout{
    For all $\boldsymbol\lambda > 0$, we have
    $\SIMInc_{(\Ar,\boldsymbol\lambda)}(\mu) \ge 0$, and this holds
    with equality if and only if
    % $\mu$ is s2-compatible with $\Ar$.
    $\mu \models \Ar$.
}
    $\SIMInc_{\!\Ar}(\mu) \ge 0$, 
    with equality iff
    % $\mu$ is s2-compatible with $\Ar$.
    $\mu \models \Ar$.
\end{linked}

%oli11: mentioning drawbacks.
%oli14:
% However $\SIMInc$ is not exactly a linear function of its argument's information profile; because of the infemum, it encodes an optimization problem over such functions.
%oli20: cutting, as agreed
\commentout{
Although it is built from a sum of entropies, $\SIMInc_{\Ar}$ is not a linear function of its argument's information profile; rather, because of the infemum, it is an optimization problem over the information profiles of extended distributions.
This makes it difficult to compute, and obfuscates the connection to information theoretic primitives. 
}%
%oli20: replacement
Although they seem to be very different, $\SIMInc$ and $\IDef$ turn out to be closely related.  
%
%joe2: now sort of going back to your story, but the focus is not
%PDGs, but the scoring function, since that we want to focus on the
%new definition of structural incompatibility.
%oli2*: this new material, again, is problematic; cutting it. 
\commentout{
    We now want to compare the scoring function $\SIMInc$ to that introduced
    by \citet{pdg-aaai}.  We focus on qualitative PDGs (i.e., directed hypergraphs)
    %oli2: qualitative PDGs are not just \hgraph s. This is the first
    % time that we have the opportunity to define a qualitative PDG in 
    % one of our papers, and I feel very strongly that we must get
    % the definition right. That means including the weights \alpha 
    here.%
    \footnote{In \citet{pdg-aaai}, a qualitative PDG also includes a
    weight function $\alpha$ that associates with each \hyperarc\ $a$ a
    weight that intuitively represents the  modeler's degreee of
    confidence in the dependence represented by $a$.
    For simplicitly, we
    ignore the weights here (implicitly taking them all to be 1).
    %oli2: you say that it's for simplicity, but it's not; it's for 
    % expediency and to accomodate a story you're very fond of but is
    % slightly wrong. 
    }
}%
%
%oli10: let's be a bit more careful with notation + not sweep this under the rug
%oli14: in service of the story, we'll deal with the technical detail later, in the theorem statement, and be a bit looser here.
% Let $\SIMInc_{\Ar}$ be shorthand for $\SIMInc_{\Ar, \mat 1}$, in which all weights are equal to 1. 
%oli14:a better story
% Interestingly,
%oli20:
% Yet, modulo the infimum, there is a sense in which $\SIMInc_{\Ar}$ is a special case of 
In fact, modulo the infimum, $\SIMInc_{\Ar}$ is a special case of
%oli14: cutting
% the original scoring function
$\IDef$---not for the \hgraph\ $\Ar$, but rather for a transformed one $\Ar^\dagger$ that models the noise variables explcitly. 
% Moreover, this augmentation makes a lot of intuitive sense.
% Starting with a PDG $\dg M$ with \arc s $\Ar$,
%     replace each structural \arc 
%oli3:
% Starting with a directed hypergraph $\Ar$, replace each \hyperarc\ 
To construct $\Ar^{\dagger}$ from $\Ar$,
%oli6:
% start by replacing each \hyperarc\
add new nodes $\U = \{ U_a \}_{a \in \Ar}$, and
replace each \hyperarc\
% start by adding a new variable $U_a$ for each $a \in \Ar$ each \hyperarc\
\vspace{-1em}
\[
\begin{tikzpicture}[center base]
    \node[dpad0] (S) at (0,0) {$\Src a$};
    \node[dpad0] (T) at (1.3,0) {$\Tgt a$};
    %oli6: adding label
    \draw[arr2] (S) to node[above,pos=0.4]{$a$}
        % node[below,pos=0.4]{\color{gray}$\scriptstyle(\alpha_a)$}
        (T);
\end{tikzpicture}
% \qquad\text{with the \arc s} \qquad
% \qquad \begin{array}{c}\text{with the}\\\text{pair of \arc s}\end{array} \qquad
% \quad \begin{array}{c}\text{with the pair}\\\text{pair of \hyperarc s}\end{array} \quad
%oli6: delete extra "pair"
\quad 
%oli15:
% \begin{array}{c}\text{with the pair}\\\text{of \hyperarc s}\end{array}
\text{with the pair of \hyperarc s}
\quad
\begin{tikzpicture}
        % [baseline=0]
        [center base]
    \node[dpad0] (S) at (0,0) {$\Src a$};
    \node[dpad0] (T) at (1.4,0) {$\Tgt a$};
    \node[dpad0] (U) at (0.6,0.7) {$U_a$};
    %oli15: adding names for these arcs
    % \draw[arr,<-] (U) -- +(-1,0);
    \draw[arr,<-] (U) to node[above,pos=0.6]
        % {$a.0$}
        {$a_0$}
         +(-1.2,0);
    \mergearr[arr1] SUT
    %oli15:
    \node[below=2pt of center-SUT,xshift=-0.2em] 
        % {$a.1$};
        {$a_1$};
\end{tikzpicture}~.
\]
%oli6: I compressed and cleaned this part up a bit, but I still think this explanation is not worth the space it uses; instead, I bolstered the formalism and the English above.
\commentout{
    In words, construct $\Ar^{\dagger}$ by starting 
    %oli6: now that we're dealing with directed hypergraphs instead of PDGs, "purely structural" is just fluff
    % and two purely structural \hyperarc s
    and two \hyperarc s
    %oli6: compress
    % for each \arc\ $a \in \Ar$:
    % one that ouputs $U_a$ without taking inputs, and another
    one generating $U_a$ without input, and another
    %oli3:
    % that has source $\Src a \times U_a$ and target $\Tgt a$.
    from $\Src a \cup \{ U_a \}$ to $\Tgt a$.
}
%oli6: instead:
%\oli3: this "intuition" is weak; we've said much more than this already. Cutting.
% Intuitively, this modeling assumption corresponds to an assertion
%     there is a mechanism that determines each $\Tgt a$, given the value of $\Src a$ and $U_a$.
% Finally, add one more qualitative \hyperarc\ from $\mathcal U$ to $\X$, which
%spencer1: cut
% So far, the transforation has done almost nothing. 
%oli6: rewriting
% The magic is the final touch: add one last \hyperarc\ from $\mathcal U$ to $\X$, which corresponds to the belief that the variables $\X$ are determined by $\mathcal U$, up to noise.
% For the final touch, we add one additional
%oli20:
% What's important is the final touch: a final
%spencer1:
% What's important is the final touch: one additional
Finally, add one additional
\hyperarc\ $\mathcal U \to \X$
%oli20: sentence break
% \unskip, a second way in which $\U$ give rise to $\X$, creating
%spencer1:
% that, intuitively, creates functional dependencies in the spirit of \cref{theorem:func}.
\unskip. (Intuitively, this \hyperarc\ creates functional dependencies in the spirit of \cref{theorem:func}.)
% Call this PDG $\dg M^\dagger$.
% Call this directed hypergraph $\Ar^{\dagger}$. 
%oli6: this takes up one way too much space.
\commentout{
In total, it has \arc s
\[
    % \dg M^\bullet := \dg M ~~+~~
    \Ar^\dagger :=
        \Bigg \{~~
        \begin{tikzpicture}[center base]
            \node[dpad0] (S) at (0,0) {$\Src a$};
            \node[dpad0] (T) at (1.5,0) {$\Tgt a$};
            \node[dpad0] (U) at (0.8,1) {$U_a$};
            \draw[arr,<-] (U) -- +(-1,0);
            \mergearr SUT
    \end{tikzpicture}~~
    \Bigg \}_{a \in \Ar} ~~\cup~~ \Big\{ \ed {}{\mathcal U}{\X} \Big \}.
\]
}%
%oli3: not critical
\commentout{
    In a moment, we will see how $\SDef$ for $\Ar^\dagger$ corresponds to
    $\SIMInc$ for $\Ar$. But first, notice a potential issue with these assertions
    at face value: the final \arc\ seems to be double-counting
    causal information that was already given in the previous set of \arc s.
    How is it that the randomness variables $\mathcal U$ determine $\X$ up to noise?
    Via the mechanisms that have already been described!
    }%
% Let $\Ar^\bullet$ be the \arc s of $\dg M^\bullet$.
%
%oli14: toning down rhetoric
% With these definitions in place, we can state a powerful theorem that allows us to write this new scoring function $\SIMInc$ in terms of he original PDG scoring function $\SDef$.
%oli20: 
% With these definitions in place, we can write $\SIMInc$ in terms of $\IDef$.
With these definitions in place,
we can state a theorem that bounds $\SIMInc$ above and below with information deficiencies. 
%spencer1*: this theorem deserves more fanfare
% The lower bound generalizes \cref{theorem:sdef-le0} to cases where $\mu \not\models \Ar$,  while the tight upper bound shows that $\SIMInc_{\!\Ar}$ can be defined in terms of $\IDef_{\!\Ar^\dagger}$.
%oli20.1: "Its lower/upper bound" -> "The lower/upper bound"
The lower bound generalizes \cref{theorem:sdef-le0} by giving an upper limit on $\IDef_{\!\Ar}(\mu)$ even for distributions $\mu$ that are 
%oli20.1: stronger still: we give the bound for all distributions.
% only \emph{nearly} \scible\ with $\Ar$.
not \scible\ with $\Ar$.
The upper bound is tight in general, and shows that $\SIMInc_{\!\Ar}$ can be equivalently defined as a minimization over 
%oli20.1: \nu is unbound. 
    % $\IDef_{\!\Ar^\dagger}(\nu)$.
    $\IDef_{\!\Ar^\dagger}$.

\begin{linked}{theorem}{siminc-idef-bounds}
    %oli7: implementing your desire for parts (a) and (b)
    \begin{enumerate}[label={\normalfont(\alph*)},wide,topsep=0pt,itemsep=0pt,parsep=0pt] \item
    %oli3: rewriting the statement
    % Consider a joint distribution $\mu(\X)$, and
    % let $\nu(\X, \mathcal U)$ be a minimizer of the the infemum in \eqref{eq:siminc}.
    % Then  $\displaystyle
    % \SIMInc_{\Ar, \mat 1}(\mu) =
    % \SDef_{\Ar^\dagger, \mat 1}(\nu)
    % $.
    % If $(\X, \Ar)$ is a \hgraph\ whose nodes $\X$ are variables, $\U = \{ U_a \}_{a \in \Ar}$ is a collection of variables indexed by
    %oli20: overly verbose; cleaning this up
    \commentout{
    If $\Ar$ is a \hgraph\ whose nodes are the names of variables 
    % $\X \cong (\N,\V)$, $\U = \{ U_a \}_{a \in \Ar}$
    % $\X$, $\U = \{ U_a \}_{a \in \Ar}$
    $\X$, $\U$ is a collection of variables indexed by
    %oli7: saving line
    % the \arc s of $\Ar$,
    $\Ar$,
    $\mu$ is a distribution over $\X$, and $\nu(\X,\U)$ is an extension of $\mu$ to $\U$, then
    }
    %oli21:
    % If $(\Ar, \X)$ is a hypergraph,
    If $(\X,\Ar)$ is a hypergraph,
     $\mu(\X)$ is a distribution, and $\nu(\X,\U)$ is an extension of $\nu$ to additional variables $\U = \{U_a \}_{a \in \Ar}$ indexed by $\Ar$, then:
    \[
        \IDef_{\!\Ar}(\mu) \le
        \SIMInc_{\Ar}(\mu)
        \le \IDef_{\!\Ar^\dagger}(\nu).
    \]
    % \[
    %     \IDef_{\!\Ar}(\mu) \le
    %     \SIMInc_{\Ar}(\mu)
    %     =     \min_{\nu(\X\!,\,\U): 
    %                 {\nu(\X) = \mu(\X)}}
    %                 \;
    %     \IDef_{\!\Ar^\dagger}(\nu)
    % \]
    %oli7:
    \item 
    %-oli14: adding the missing technical detail cut from a few paragraphs above, but altered to fix a bug in the theorem.
    %-oli14: removed the new technical details.
    % when
    % $\boldsymbol\lambda = \boldsymbol\lambda_0 := (\infty, 1, \{1,\ldots,1\})$,
    % be a vector of unit weights, except for
    %  $\lambda^{\mathrm{(a)}} = \infty$
    %
    %oli20: minor tweaks to get it to fit on one line...
    % Futhermore, for all $\mu$ and $\Ar$ there is a choice of $\nu$ for which the second relationship holds with equality.
    For all $\mu$ and $\Ar$, there is a choice of $\nu$ that achieves the upper bound.
    That is, 
    %joe6*: Restating a comment I had before that seems to have
    %disappeared: I would add a part (b) to the theorem that shows equality,
    %using inf. 
    %oli7: Done. Sorry about the dropped comment; I read it but got distracted before moving on.
    % \[ 
    % %oli7: layout option 1
    %     \SIMInc_{\Ar}(\mu) = \inf_{\substack{\nu \in \Delta\V(\X\!,\,\hat \U_\Ar^\X) \\ \nu(\X) = \mu(\X)}} \SDef_{\Ar^\dagger}(\nu),
    % \]
    % \vspace{-1em}
    \[% ... and layout option 2
        % \SIMInc_{\Ar,\boldsymbol\lambda_0}(\mu)
        \SIMInc_{\Ar}(\mu)
        = \min \Big\{\ \IDef_{\!\Ar^\dagger}(\nu) : 
            \begin{array}{c}
            \nu \in  \Delta\V(\X,
            %oli10:
                % \hat \U_\Ar^\X
                \U
            ) \\ \nu(\X) = \mu(\X)
            \end{array}
         \Big\}
         %oli22
         .
    \]
%joe7*: What are the variables "along an arc".  Please explain before
%11 AM tomorrow. 
%joe7*: Moreover, if you cut Theorem 5 (as I think you
%should), the definition of response variable will disappear.  I'm
%soewhat surprised that uyou need response variables here.  I would
%have thought that the result would hold for a much larger choice of
%new variables.  And, of course, it holds if you consider all possible
%choices of variables, although that gives you a class.  (Actually,
%all you need is all choices where the domain is sufficiently large,
%and you can take the domain to consist of ordinals, so you're back
%down to a set.)
%oli8: Everything you say here is true. You're correct that I wrote
% it this way to avoid having to deal with the class.
    %
%oli10: 
\commentout{
    where $\hat \U_\Ar^\X$ is the set of response variables along the \arc s of $\Ar$ for variables $\X$,
    %     in which $\V(U_a) = \V( \Tgt a )^{\V(\Src a)}$,
        defined in \cref{sec:responsvars}.
}
%oli22*: for space reasons
\vfull{
%oli10:
where the minimization is over all possible ways of assigning values to 
%joe10
the variables in
%oli15:
% $\U$, and the
$\U$. The
minimum is achieved when $|\V(U_a)| \le |\V(\Tgt a)|^{|\V(\Src a)|}$.
}
%oli15:
% and so the entropy $\H(\U)$ needed to is at most
    \end{enumerate}
\end{linked}
%
% =======
% \end{prop}
% \begin{proof}
%     \begin{align*}
%         \SDef_{(\Ar^\bullet, \mat 1)}(\nu)
%             &=
%                 - \H_\nu(\X, \mathcal U)
%                 + \sum_{a \in \Ar} \H_{\nu}(U_a)
%                 + \sum_{a \in \Ar} \H_\nu(\Tgt a | \Src a, U_a)
%                 + \H_\nu(\X \mid \mathcal U) \\
%             &= \Big( - \H_\nu(\mathcal U) + \sum_{a \in \Ar} \H_{\nu}(U_a) \Big)
%             + \sum_{a \in \Ar} \H_\nu(\Tgt a | \Src a, U_a) \\
%             &= \SIMInc_{(\Ar,\mat 1)} (\mu). \qedhere
%     \end{align*}
% \end{proof}
% >>>>>>> Stashed changes
% 
% The correspondence looks a little stilted when written this way,
%     because it defines $\nu$, the argument to $\SDef$ in terms
%     of the definition of $\SIMInc$.
% But in the context of the existing PDG formalism, this infemum
%     is essentially already present, and so $\nu$ arises naturally.
%oli14: expanding on the rest of the inequalities.
%oli20: cutting the rest, as I've now said the important stuff above
\commentout{
The first result gives us bounds on $\SIMInc$ in terms of $\IDef$, making it easier to compute, and the second shows that, in the context of a minimization where it usually is \citep{pdg-aaai}, $\SIMInc$ is a special case of $\IDef$, for a graph that models the noise variables explicitly. 

The first inequality of \cref{theorem:siminc-idef-bounds}(a) is a slight strengthening of \cref{theorem:sdef-le0} that applies even when $\mu \not\models \Ar$. 
But \cref{theorem:sdef-le0} is not based on some choice of scoring function---it describes a direct relationship between the information profile and \scibility.
}

%oli2: Ok, the next result isn't worth the space it takes,  even if I had a finished proof
\commentout{
\begin{conj}
    % If $\mu$ is \scible\ with $\Ar$
    % and $\mu'$ is \scible\ with $\Ar'$,
    % and $\alpha \in [0,1]$, then $\mu
    Suppose $\mu \models \Ar$, $\mu' \models \Ar'$, and $\alpha \in [0,1]$.
    Then $\SDef_{\Ar{:}\alpha{:}\Ar'}((1-\alpha) \mu + (\alpha) \mu) \le 0$,
    where $\Ar{:}\alpha{:}\Ar'$ is the QDG that contains the \arc s $\Ar$
    with weight $\alpha$
\end{conj}
\begin{proof}
    $\SDef_{\Ar}(\mu) \le 0$, and $\SDef_{\Ar'}(\mu') \le 0$,
    so...
\end{proof}
}

%joe2*: This is clearly the theorem we want.  If it's true, prove it.
  %If not (or if you don't have a proof), then cut all this material.
%oli2*: your "clearly the theorem we want" doesn't typecheck, because
% \Ar' has additional variables, and so must accept an extended distribution
% over those additional variables.  The obvious modifications needed
% to make this typecheck result in Proposition {siminc-idef-bounds},
% above.  It probably needs to be stated more clearly, to prevent
% readers from being confused in the same way.  I haven't typeset the
% proof yet, but it is straightforward. 
% 



%oli6*: moving "Null Value Construction" section to the appendix as well. I'd be OK submitting without it; I think we have enough things to say as it is, and it's not in the critical path.  That said, it's the sort of technical experimental material that would not be out of place in the appendix.

%oli6*: banishing discussion of hypergraph union (which you asked for many months ago, and had some placeholder results) to the appendix. We don't have space for it. I know the appendix is not a dumping ground, and I am not planning on submitting it; I just want to keep track of it for now.

% \vspace{-1ex}
\section{Discussion}
%oli7: starting with something
%joe7
%We have shown how directed hypergraph s can be used as a (causal) modeling
%language for structural aspects of distributions---that they
%generalize BNs, other causal models, allow for independencies, and
%have deep connections to information theory.  
We have shown how directed hypergraph s can be used to represent 
structural aspects of distributions.  
%oli9: They didn't really ever generalize causal models (because they're purely qualitative), and now they don't capture those functional dependencies in all cases, either. What does "allows for independencies" mean? 
% Moreover, they can do so in a way that 
% generalizes BNs and causal models, allows for independencies, and
% has deep connections to information theory. 
Moreover, they can do so in a way that 
%spencer1:
% generalizes BN independencies, dependencies, and
generalizes conditional independencies and functional dependencies and
has deep connections to causality and information theory. 
%joe7
%Yet many open questions remain.
Many open questions remain.
\commentout{
    A simple sounding one is the following: are there any distributions that are compatible with the 3-cycle in one direction, but not in the opposite direction? 
    % We suspect of the symmetry, and because $\IDef$ is the same for both cycles%
    We suspect of the symmetry, and because $\IDef$ is the same for both cycles%
        ---but we have not been able to resolve the question one way or another (see \cref{sec:equivalence} for a partial answer).
    Another obvious question is the decision procedure for deciding whether or not $\mu \models \Ar$.
}%
%joe7*; You should probably say more here, and do so before 11 AM tomorrow.
%oli9:
% A major one is that of getting a good understanding of \scibility\ in cyclic models.  
A major one is that of
    more precisely
    understanding \scibility\ in cyclic models.  
We do not yet know, for example, whether the same set of distributions are \scible\ with the
%oli14:
%  3-cycle in both directions.
clockwise and counter-clockwise 3-cycles.
%joe7*:
%oli9:
% A second major question is finding a general inference procedure to
% determine if a distribution is \scible\ with a hypergraph.
%oli20:
% A related question is to find a general procedure to
% determine if a distribution is \scible\ with a hypergraph.
A related problem is to find an efficient procedure that can
determine whether a given distribution is \scible\ with a \hgraph.
%joe7*: new material
%oli9:  not relevavnt for my story; this is not a story about PDGs.
%oli11: I don't feel comfortable saying any of this.
\commentout{ 
Inference for PDGs was discussed in \citep{pdg-infer}, but the focus
was on the quantative aspects.
Getting analogous algorithms
that dealt with the qualitative aspects and \scibility\ would be of
great interest.  Finally, it would be useful to get more axiomatic
approaches for reasoning about \scibility.  
We believe that ideas of
monotonicity, together with axioms that relate independence and
depence, in the spirit of
(\ref{eq:conditional-self-independence-det}), might take us a long way.
}%
%oli20: shall we talk about GRPSEMs here also?
We hope to explore all these questions in future work.

%joe7*: cut; this story is weak.  It hurts the paper to include it.
%Leave it for future work.
\commentout{
    We discuss one approach in \cref{sec:null}
    that works well for small PDGs, 
    but we do not have a proof that it succeeds in all cases, 
    and it scales very poorly.
}

% \bibliography{qdg-refs,joe,z}
% \bibliography{qdg-refs,joe}
% \bibliography{joe}

\clearpage
\begin{subappendices}
% \appendix
% \onecolumn


\section{Proofs}
    \label{appendix:proofs}
    
%oli21: putting this here as a subsection; it's used in the proofs and doesnt' need its own section

%oli14*: new material lifted from type-1-edges.tex, which will come in handy in the main text.
We begin with a de-randomization construction, that will be useful for the proofs. 

\subsection{From CPDs to Distributions over Functions}
    \label{sec:cpd-derandomize}
Compare two objects:
\begin{itemize}[nosep]
    \item a cpd $p(Y|X)$, and
    \item a distribution $q(Y^X)$ over functions $g : \V X \to \V Y$. 
\end{itemize}

The latter is significantly larger --- if both $|\V X| = | \V Y | = N$, then
$q$ is a $N^N$ dimensional object, while $p$ is only dimension $N^2$. 
A choice of distribution $q(Y^X)$ corresponds to a unique choice cpd $p(Y|X)$, according to 
\[
    p(Y{=}y\mid X{=}x) :=
        q(Y^X(x) = y)
        % = \sum_{g : \V X \to \V Y} p(Y^X{=}g) \mathbbm1[g(x) = y]
        .
\]
\begin{claim}
\begin{enumerate}[nosep]
\item The definition above in fact yields a cpd, i.e., $\sum_y p(Y{=}y|X{=}x) = 1$
for all $x \in \V X$. 
\item 
This definition of $p(Y|X)$ is the conditional marginal of any
    joint distribution  $\mu(X,Y,Y^X)$ satisfying $\mu(Y^X) = q$ and
    $\mu(Y = Y^X(X)) = 1$. 
    % $\mu(Y = F(X)) = 1$. 
\end{enumerate}
\end{claim}
Both $p$ and $q$ give probabilistic information about $Y$ conditioned
on $X$. But $q(Y^X)$ contains strictly more information.
Not only does it specify the distribution over $Y$ given $X{=}x$,
but it also contains counter-factual information about the distribution of $Y$
    if $X$ were equal to $x'$, conditioned on the fact that, in reality, $X{=}x$.

Is there a natural construction that goes in the opposite direction, intuitively
making as many independence assumptions as possible? It turns out there is:
\[
    q(Y^X{=} g) = \prod_{x \in \V X} p(Y{=}g(x) \mid X{=}x).
\]
Think of $Y^X$ as a collection of variables $\{Y^x : x \in \V X\}$ describing the
value of the function for each input, so that $q$ is a joint distribution over them.
This construction simply asks that these variables be independent. 
Specifying a distribution with these independences amounts to a choice
    of ``marginal'' distribution $q(Y^x)$ for each $x \in V X$, and hence
    is essentially a funciton of type $\V\! X \to \Delta \V Y$, the same as $p$. 
In addition, if we apply the previous construction, we recover $p$, since:
\begin{align*}
    q(Y^X(x) = y)
        &= \sum_{g: \V\! X \to \V Y} \mathbbm1[g(x)=y]\prod_{x' \in \V\! X} p(Y{=}g(x') \mid X{=}x') \\
        &= \sum_{g: \V\! X \to \V Y} \mathbbm1[g(x)=y] p(Y{=}g(x)\mid X{=}x)\prod_{x' \ne x} p(Y{=}g(x') \mid X{=}x') \\
        &=  p(Y{=}y\mid X{=}x) \sum_{g: \V\! X \to \V Y} \mathbbm1[g(x)=y] \prod_{x' \ne x} p(Y{=}g(x') \mid X{=}x') \\
        &=  p(Y{=}y\mid X{=}x) \sum_{g: \V\! X \setminus \{x\} \to \V Y} \prod_{x' \in \V \! X \setminus \{x\} } p(Y{=}g(x') \mid X{=}x') \\
        &=  p(Y{=}y\mid X{=}x).
        % &= \sum_{g} \mathbbm1[g(x)=y]\prod_{x' \in \V X} p(Y{=}g(x') \mid X{=}x')
\end{align*}
The final equality holds because the remainder of the terms can be viewed as the probability of selecting any function from $X \setminus \{x\}$ to $Y$, under an analogous measure; thus, it equals 1. 
This will be a useful construction for us in general.
%oli21: no need for the notation, I think
\commentout{
given a cpd $p(Y|X)$, let's
write $\tilde p(Y^X)$ for this distribution.  
}%
% Call this the \emph{trivial mechanism}.


\subsection{Results on (In)dependence}

\begin{lemma} \label{lem:indep-fun}
    Suppose $X_1, \ldots, X_n$ are variables, $Y_1, \ldots, Y_n$ are sets,
        and for each $i \in \{1, \ldots n\}$, we have a function
    %oli5: ceding more parens
        % $f_i : \V X_i \to Y_i$.
        $f_i : \V (X_i) \to Y_i$.
    Then if $X_1, \ldots, X_n$ are mutually independent (according to a
        joint distribution $\mu$), then
    % so are $f(A)$ and $g(B)$.
    so are $f_1(X_1), \ldots, f_n(X_n)$.
\end{lemma}
\begin{proof}
    This is an intuitive fact, but we provide a
    proof for completeness.
    % Explicitly, mutual independence of $A$ and $B$ means that,
    Explicitly, mutual independence of $X_1, \ldots, X_n$ means that,
        for all joint settings $\mat x = (x_1, \ldots x_n)$,
    % we have $\mu(A{=}a,B{=}b) = \mu(A{=}a) \mu(B{=}b)$.
    we have $\mu(X_1{=}x_1,\ldots,X_n{=}x_n) = \prod_{i=1}^n \mu(X_i{=}x_i)$.
    % So, for any $x \in X$ and $y \in Y$, we have:
    So, for any joint setting $\mat y = (y_1, \ldots, y_n) \in Y_1 \times \cdots \times Y_n$, we have
    \begin{align*}
        \mu\Big(f_1(X_1){=}y_1, \ldots, f_n(X_n){=}y_n\Big)
            &= \mu(\{ \mat x : \mat f(\mat x) = \mat y\}) \\
            &= \sum_{\substack{(x_1,\ldots, x_n) \in \V(X_1,\ldots,X_n) \\ f_1(x_1) = y_1,\; \ldots,\; f_n(x_n) = y_n}}
                \mu(X_1{=}x_1, \,\ldots,\, X_n{=}x_n) \\
            % =
            % \sum_{\substack{(a, b) \in \V(A,B) \\ f(a) = x \\ g(b) = y}}
            %     \mu(A=a)\mu(B=b) \\
            &= \sum_{\substack{x_1 \in \V\! X_1 \\ f_1(x_1) = y_1}}
                    \cdots
                \sum_{\substack{x_n \in \V\! X_n \\ f_n(x_n) = y_n}}
                \mu(X_1{=}x_1, \,\ldots,\, X_n{=}x_n) \\
            &= \sum_{\substack{x_1 \in \V\! X_1 \\ f_1(x_1) = y_1}}
                    \cdots
                \sum_{\substack{x_n \in \V\! X_n \\ f_n(x_n) = y_n}}
                \prod_{i=1}^n \mu(X_i{=}x_i) \\
            &= \bigg(\sum_{\substack{x_1 \in \V\! X_1 \\ f_1(x_1) = y_1}}
                    \mu(X_1{=}x_1) \bigg) \cdots
                \bigg(\sum_{\substack{x_n \in \V\! X_n \\ f_n(x_n) = y_n}}
                    \mu(Y_1=y_1)\bigg) \\
            &= \prod_{i=1}^n \mu( f_i(X_i) = y_i ).
             \qedhere
    \end{align*}
    % \begin{align*}
    %     % \mu(A = a, B = b) = \mu(A= a)\mu(B=b) \\
    %     \mu(f(A) = x, g(B)=y) &= \mu(\{(a,b) : f(a) = x, g(b) = y\}) \\
    %         &= \sum_{\substack{(a, b) \in \V(A,B) \\ f(a) = x \\ g(b) = y}}
    %             \mu(A=a,B=b)
    %         =
    %         \sum_{\substack{(a, b) \in \V(A,B) \\ f(a) = x \\ g(b) = y}}
    %             \mu(A=a)\mu(B=b) \\
    %         &= \sum_{\substack{a \in \V A \\ f(a) = x}}
    %             \sum_{\substack{b \in \V B \\ g(b) = y}}
    %             \mu(A=a)\mu(B=b) \\
    %         &= \bigg(\sum_{\substack{a \in \V A \\ f(a) = x}}
    %                 \mu(A=a) \bigg)
    %             \bigg(\sum_{\substack{b \in \V B \\ g(b) = y}}
    %                 \mu(B=b)\bigg) \\
    %         &= \mu( \{a : f(a) = x\} ) \mu(\{ b : f(b) = y\}) \\
    %         &= \mu(f(A) = x) \mu(f(B) = y).\qedhere
    % \end{align*}
\end{proof}


\begin{lemma}[properties of determination]
        \label{lem:detprop}
    ~
    \begin{enumerate}
        \item 
        If $\nu \models A \tto B$ and $\nu \models A \tto C$, then $\nu \models A \tto (B,C)$. 

        \item 
        If $\nu \models A \tto B$ and $\nu \models B \tto C$, then $\nu \models A \tto C$. 
    \end{enumerate}
\end{lemma}
\begin{lproof}
$\nu \models X \tto Y$, means there exists a function $f : V(A) \to V(B)$ such that $\nu( f(Y) = X ) = 1$, i.e., the event $f(A) = B$  occurs with probability 1.
\begin{enumerate}
    \item Let $f : \V(A) \to \V(B)$ and $g : \V(A) \to \V(C)$ be such that $\nu(f(A) = B) = 1 = \nu(g(A) = C)$. Since both events happen with probability 1, so must the event $f(A) = B \cap g(A) = C$. Thus the event $(f(A), g(A)) = (B,C)$ occurs with probability 1.  Therefore, $\nu \models A \tto (B,C)$. 
    \item The same ideas, but faster: we have $f : \V(A) \to \V(B)$ as before, and $g : \V(B) \to \V(C)$, such that the events $f(A) = B$ and $g(B) = C$ occur with proability 1. By the same logic, it follows that their conjunction holds with probability 1, and hence $C = f(g(A))$ occurs with probability 1. So $\nu \models A \tto C$. 
\end{enumerate}
\end{lproof}

\recall{theorem:bns}
\begin{lproof}
    \label{proof:bns}
    % Suppose $\mu \models \Ar_G$.
    % Select a topological sort $\prec$ on the dag $G$. We proceed by induction. Suppose 
    % Let $\X$ be the variables of $\mu$ and the set of nodes in $G$. 
    % Consider a topological ordering $\X = (X_1, \ldots, X_n)$ , such that each variable comes after its parents in $G$.
    % $\Pa X_i := \Src X_i$
    Label the vertics of $G = (\N, E)$ by natural numbers so that they are a topological sort of $G$---that is, without loss of generality, suppose $\N = [n] := \{1, 2, \ldots, n\}$, and $i < j$ whenever  $i \to j \in E$. 
    % Since the index set of $\Ar$ is 
    % By \cref{defn:graph-to-hypergraph},
    By the definition of $\Ar_G$, 
    the arcs $\Ar_G = \{ \Src i \xrightarrow{i} i \}_{i=1}^n$ are also indexed by integers.
    Finally, write $\X = (X_1, \ldots, X_n)$ for the variables $\X$ corresponding to $\N$ over which $\mu$ is defined. 
    % Now use \cref{defn:graph-to-hypergraph}, to construct the \hgraph\ $\Ar$ whose arcs are labeled $\Ar = \{1,\ldots, n\}$.
    
    \textbf{($\implies$).}
    Suppose $\mu \models \Ar_G$. 
    This means there
    is an extension of $\bar\mu(\X, \U)$ of $\mu(\X)$ to additional
        independent variables $\U = (U_1, \ldots, U_n)$, such that $\bar\mu \models (\Src i, U_i) \tto i$ for all $i \in [n]$. 
    % We proceed by induction.
    % It suffices to show that a variable $X_i$ is independent of its non-descendents, given $\Src i$. 
    % It suffices to show that 
    % $\mu(\X) = \prod_{i=1}^n $ 
    
    First, we claim that if $\bar\mu$ is such a witness, then 
    % the setting $\mat x\in \V(\X)$ of all variables is a deterministic function of $\U$ in $\nu$.
    % $\bar\mu \models \U \tto \X$. 
    $\bar\mu \models (U_1, \ldots, U_k) \tto (X_1, \ldots, X_k)$
    for all $k \in [n]$, and so in particular, $\bar\mu \models \U \tto \X$. 
    This follows from \scibility's condition (c) and the fact that $G$ is acyclic, by induction.
    % Starting with $X_1$, which has no parents (so $S_1=\emptyset$),
    % (b) states that $\nu$ only has support on $(u, \omega)$ such that in particular,
    % $f_1(u) = X_1(\omega)$.
    In more detail:
    The base case of $k=0$ holds vacuously.
    % Suppose that in $\nu$, the first $k$ variables $X_1, \ldots, X_k$, for $k < n$, are determined by $\U$.
    Suppose that $\bar\mu \models (X_1, \ldots, X_k)$ for some $k < n$. 
    Now, conditon (c) of \cref{defn:scompat} says 
    % in particular that if $(\mat x, \mat u)$ is in the support of $\nu$, then $f_{k+1}(\Src {k+1}(\mat x), \mat u) = X_{k+1}(\mat x)$, so $X_{k+1}$ is a function of $\mat u$ and $\Src {k+1}$.
    $\bar\mu  \models (\Src {k+1}, U_{k+1}) \tto X_{k+1}$. 
    Because the varaibles are sorted in topological order, the parent variables $\Src {k+1}$ are a subset of $\{X_1, \ldots, X_n\}$, which are determined by $\U$ by the induction hypothesis;
    at the same time clearly $\bar\mu \models (U_1, \ldots, U_{k+1}) \tto U_{k+1}$  as well. 
    So, by two instances of \cref{lem:detprop}, 
        $\bar\mu \models (U_1, \ldots U_{k+1}) \tto X_{k+1}$.
    Combining with our inductive hypothesis, we find that
        $\bar\mu \models (U_1, \ldots U_{k+1}) \tto (X_1, \ldots, X_{k+1})$.
        % as promised.
    % So, by induction, the values of all variables $\X$ are determined by $U$ in $\nu$.
    So, by induction, 
    $\bar\mu \models (U_1, \ldots, U_k) \tto (X_1, \ldots, X_k)$
    for $k \in [n]$, and in particular, $\bar\mu \models \U \tto \X$.
    
    % By definition,
    % This means there is some function $f : \V(\U) \to \V(\X)$ that holds with probability 1. 

    With this in mind, we now return to proving that $\mu$ has the required independencies.
    % More precisely, we now show that 
    It suffices to show that $\mu(\X) = \prod_{i=1}^n \mu(X_i \mid \Src i)$.
    We do so by showing that, for all $k \in [n]$, 
    $\mu(X_1, \ldots, X_{k}) = \mu(X_1, \ldots, X_{k-1}) \mu(X_{k}\mid \Src {k})$.
    % The base case of $k=1$ is straightforward: since $\Src 1$ is an empty set of variables, $\mu(X_1) = 1 \cdot \mu(X_1 \mid \Src 1) = \mu(X_1)$ as desired.
    % Suppose that
    % $\mu(X_1, \ldots, X_{k}) = \mu(X_1, \ldots, X_{k-1}) \mu(X_{k}\mid \Src {k})$ for $k< n$. 
    % Let $f$
    By \scibility\ witness condition (c), we know that $\bar\mu\models (\Src k, U_k) \tto X_k$, and so there exists a function $f_k : \V(\Src k) \times \V(U_k) \to \V(X_k)$ for which the event $f_k(\Src k, U_k) = X_k$ occurs with probability 1. 
    Since $\bar\mu \models (U_1, \ldots, U_{k-1}) \tto (X_1, \ldots, X_{k-1})$, and $U_k$ is independent of $(U_1, \ldots, U_{k-1})$,
    it follows from \cref{lem:indep-fun} that 
    $\bar\mu \models (X_1, \ldots, X_{k-1}) \CI U_k$.
    Thus
    \begin{align*}
        \mu(X_1, \ldots, X_{k-1}, X_k)
             &= \sum_{u \in \V(U_k)}
                \mu(X_1, \ldots, X_{k-1}) \bar\mu(U_k = u) 
                    \cdot \mathbbm 1[X_k = f_k(\Src k, u)]
                    % \cdot \delta\!f_k(X_)
                    \\
            &= \mu(X_1, \ldots, X_{k-1}) \sum_{u \in \V(U_k)}\bar\mu(U_k = u) 
                \cdot \mathbbm 1[X_k = f_k(\Src k, u)]
    \end{align*}
    Observe that the quantity on the right, including the sum, is a function of $X_k$ and $S_k$, but no other variables; let  $\varphi(X_k, \Src k)$ denote this quantity. 
    Because $\mu$ is a probability distribution, know that
    % \begin{align*}
    %     1 &= \sum_{\mat z \in \V(X_1, \ldots, X_{k-1}), x \in \V(X_k)} \mu(\mat z, x) \\
    %     &=  \sum_{\mat z \in \V(X_1, \ldots, X_{k-1}), x \in \V(X_k)} \mu(\mat z) \phi()
    % \end{align*}
    % $\sum_{\mat x \in \V(X_1, \ldots, X_k)} \mu(\mat x) = 1$
    $\varphi(X_k, \Src k)$ must be the conditional probability of $X_k$ given $X_1,\ldots, X_{k-1}$, and it depends only on the variables $\Src k$. Thus
    $\mu(X_1, \ldots, X_{k}) = \mu(X_1, \ldots, X_{k-1}) \mu(X_{k}\mid \Src {k})$.
    
    Therefore $\nu(\X) = \mu(\X)$ factors as required by the BN $G$, meaning that $\mu$ has the independencies specified by $G$. (See Koller \& Friedman Thm 3.2, for instance.)
    % \newpage
    \bigskip
    
    \textbf{($\impliedby$).}
    Suppose $\mu$ satiesfies the independencies of $G$, meaning that each node is conditionally independent of its non-descendents given its parents.
    We now repeatedly apply the construction \cref{sec:cpd-derandomize} to construct a \scibility\ witness. 
    Specifically, for $k \in \{1, \ldots, n\}$, 
    let $U_k$ be a variable whose values $\V(U_k) := \V(X_k)^{\V(\Src k)}$ are functions from values of $X_k$'s parents, to values of $X_k$.
    Let $\mathcal U$ denote the joint variable $(U_1, \ldots, U_n)$,
    and observe that a setting $\mat g = (g_1, \ldots, g_n)$ of $\mathcal U$ uniquely picks out a value of $\X$, by evaluating each function in order. 
    Let's call this function $f : \V(\U) \to \V(\X)$. 
    % In more detail:
        % since $X_1$ has no parents, $g_1$ is effectively a value of $X_1$; and each $g_k$ depends on 
        
    To be more precise, we now construct $f(\mat g)$ inductively. 
    The first component we must produce is $X_1$, but since $X_1$ has no parents, $g_1$ effectively describes a single value of $X_1$, so we define the first component $f(\mat g)[X_1]$ to be that value.
    More generally, assuming that we have already defined the components $X_1, \ldots, X_{i-1}$, among which are the variables $\Src k$ on which $X_i$ depends, we can determine the value of $X_i$;
    formally, this means defining
    \[
        f(\mat g)[X_i] :=
            % g_i( (f(\mat g)[S] )_{S \in \Src i} ),
            g_i( f(\mat g)[\Src i] ),
    \]
    which, by our inductive assumption, is well-defined.
    Note that, for all $\mat g \in \V(\U)$ and $\mat x \in \V(\X)$, the function $f$ is characterized by the property 
    \begin{equation}
            \label{eq:bn-func-char}
        f(\mat g) = \mat x
        \quad\iff\quad
        \bigwedge_{i=1}^n g_i(\mat x[\Src i]) = \mat x[X_i]. 
    \end{equation}
    To quickly verify this: if $f(\mat g) = \mat x$, then in particular, for $i \in [n]$, then $\mat x[X_i] = f(\mat g)[X_i] = g_i( \mat x[\Src i])$ by the definition above.
    Conversely, if the right hand side of \eqref{eq:bn-func-char} holds, then we can prove $f(\mat g) = \mat x$ by induction over our construction of $f$: if $f(\mat g)[X_{j}] = \mat x[X_j]$ for all $j < i$, then $f(\mat g)[X_i] = g_i( f(\mat g) [\Src i]) = g_i(\mat x[\Src i]) = \mat x[X_i]$.
    % that $f(\mat g)[X_i] = g_i( (f))$
    % Call this property ($\star$). 
    
    Next, we define an unconditional probability over each $U_k$ according to
    \[
        \bar\mu_i(U_i = g) := \prod_{\mat s \in \V(\Src k)} \mu(X_i = g(s) \mid \Src i \,{=}\, \mat s),
    \]
    which, as verified in \cref{sec:cpd-derandomize}, is indeed a conditional probability, and has the property that
    $\bar\mu_i(U_i(\mat s) = x) = \mu(X_i \,{=}\, x \mid \Src i \,{=}\, \mat s)$ for all $x \in \V(X_i)$ and $\mat s \in \V(\Src i)$.
    %
    By taking an independent combination (tensor product) of each of these unconditional distributions, we obtain a joint distribution 
    $\bar\mu(\mathcal U) = \prod_{i=1}^n \bar\mu_i(U_i)$.
    % and extend this to a distribution $\bar\mu(\mathcal U, \X)$, where $\mathcal U$ is the joint variable $(U_1, \ldots, U_n)$. 
    Finally, we extend this distribution to a full joint distribution
        $\bar\mu(\U, \X)$ via the pushforward of $\bar \mu(\U)$ through the function $f$ defined by induction above.
    In this distribution, each $X_i$ is determined by $U_i$ and $\Src i$. 
    
    By construction, the variables $\U$ are mutually independent (for \cref{defn:scompat}(b)), and satisfy $(\Src k, U_k) \tto X_k$ for all $k \in [n]$ (\cref{defn:scompat}(c)). 
    It remains only to verify that the marginal of $\bar \mu$ on the variables $\X$ is the original distribution $\mu$ (\cref{defn:scompat}(a)). 
    Here is where we rely on the fact that $\mu$ satisfies the independencies of $G$, which means that we can factor $\mu(\X)$ as    $
        \mu(\X) = \prod_{i=1}^n \mu(X_i \mid \Src i).
    $
    % \end{align*}
    %
    \begin{align*}
        \bar\mu(\X{=}\mat x) 
        &= \sum_{\mat g \in \V(\U)} \bar\mu(\U{=}\mat g) 
            % \mathbbm1[\mat x = f(\mat g)] 
            \cdot\delta\!f(\mat x \mid \mat g)
        \\
        &= \!\! \sum_{{(g_1, \ldots, g_n) \in \V(\U)}}\!\!
            \mathbbm1
                \big[
                \mat x = f(\mat g)
                % ~\underbrace{\mat x = f(\mat g)}_{%
                %     = \bigwedge_{i=1}^n g_i( \mat x[\Src i] ) = \mat x[X_i]
                % }~
                \big] ~~
            \prod_{i=1}^n~ \bar\mu(U_i{=}g_i) \\
        &= \!\! \sum_{{(g_1, \ldots, g_n) \in \V(\U)}}\!\!
            \mathbbm1
                \Big[
                 \bigwedge_{i=1}^n g_i( \mat x[\Src i] ) = \mat x[X_i]
                \Big] ~~
            \prod_{i=1}^n~ \bar\mu(U_i{=}g_i) 
            & [\text{by \eqref{eq:bn-func-char}}]
            \\
        &= \prod_{i=1}^n~~
            \sum_{{g \in \V(U_i)}} ~
            \mathbbm 1\big[ g(\mat x[\Src i]) = \mat x[X_i] \big] 
            \cdot
            \bar\mu(U_i = g) \\
        &= \prod_{i=1}^n~~
            \bar\mu \Big(\Big\{
                    g \in \V(U_i) ~\Big|~
                    g( \mat s_i ) = x_i
                \Big\}\Big)
                \quad\text{ where }
                \begin{array}{l}
                    x_i := \mat x[X_i],\\
                    \mat s_i := \mat x[\Src i]
                \end{array} \\
        &= \prod_{i=1}^n \,
            \bar\mu \big( U_i(\mat s_i) \,{=}\, x_i \big) \\
        &= \prod_{i=1}^n
            \mu(X_i = x_i \mid \Src i = \mat s_i) \\
        &= \mu(\X = \mat x).
    \end{align*}
    Therefore, when $\mu$ satisfies the independencies of a BN $G$, it is \scible\ with $\Ar_G$.
\end{lproof}

%oli17:
Before we move on to proving the other results in the paper, we first illustrate how this relatively substantial first half of the proof of \cref{theorem:bns} can be dramatically simplified by relying on two information theoretic arguments.
 % Here is an alternate proof of \cref{theorem:bns}.

\begin{lproof}[Alternate, information-based proof]
    \textbf{($\implies$).}
    Let $G$ be a dag.     
    If $\mu \models \Ar_{G}$, then by \cref{theorem:sdef-le0}, 
    $\IDef_{\Ar_{G}}(\mu) \le 0$. In the appendix of \cite{pdg-aaai}, it is shown that $\IDef_{\Ar_G}(\mu) \ge 0$ with equality iff $\mu$ satisfies the BN's independencies. 
    Thus $\mu$ must satisfy the appropriate independencies.
    % 
    % \textbf{($\impliedby$).}
    % Conversely, if $\mu$ satisfies the appropriate independencies, 
\end{lproof}


\recall{theorem:func}~%
\begin{lproof}\label{proof:func}
\textbf{(a).} 
%oli16: updating proof for new statement. Have not marked changes carefully.
The forward direction is straightforward. 
Suppose that $\mu \models \Ar$ and $\mu \models X \tto Y$.
% The latter condition means that there is a function $f : \V(X) \to \V(Y)$ that occurs with probability 1, and 
The former condition gives us a witness
$\nu(\X,\U)$ in which $\U = \{ U_a \}_{a \in \Ar}$ are mutually independent variables indexed by $\Ar$, that determine their respective edges. 
``Extend'' $\nu$ in the unique way to $n$ additional constant variables $U_1, \ldots, U_n$, each of which can only take on one value. 
We claim that this ``extended'' distribution $\nu'$, which we conflate with $\nu$ because it is not meaningfully different, is a witness to $\mu \models  \Diamond \ArXY n$.
Since $\mu \models X \tto Y$ it must also be that $\nu \models X \tto Y$, and it follows that $\nu \models (X,U_i)\tto Y$ for all $i \in \{1,\ldots,n\}$, demonstrating that
% this takes care of conditions 3 and 4 (the new requirements imposed by \cref{defn:scompat}(c)).
the new requirements of $\nu'$ imposed by \cref{defn:scompat}(c) hold. 
(The remainder of the requirements for condition (c), namely that $\nu' \models (\Src a, U_a) \tto \Tgt a$ for $a \in \Ar$, still hold because $\nu'$ is an extension of $\nu$, which we know has this property.)
Finally, since $\U$ are mutually independent and each $U_i$ is a constant (and hence independent of everything), the variables $\U' := \U \sqcup \{ U_i \}_{i=1}^n$ are also mutually independent. 
Thus $\nu$ (or, more precisely, an isomorphic ``extension'' of it to additional trivial variables) is a witness of
 % $\mu \models \Ar' \sqcup \{ \ed1XY, \ed2XY\}$. 
 $\mu \models  \Diamond \ArXY n.$
 
The reverse direction is difficult to prove directly, yet it is a straightforward application of \cref{theorem:sdef-le0}.
Suppose that $\mu \models \ArXY n$ for all $n \ge 0$. 
% Observe that 
By \cref{theorem:sdef-le0}, we know that
\[
    0 \ge  \SDef_{\ArXY n}(\mu) = \SDef_{\Ar}(\mu) + n \H_\mu(Y|X).
\]
Because $\SDef_{Ar}(\mu)$ is bounded below (by $-\log |\V(\X)|$), it cannot be the case that $\H_\mu(Y|X) > 0$; otherwise, the inequality above would not hold for large $n$ (specifically, for $n > \log |\V(\X)| / \H_\mu(Y|X)$).  
By Gibbs inequailty, $\H_\mu(Y|X)$ is non-negative, and thus it must be the case that $\H_\mu(Y|X) = 0$. Thus $\mu \models X \tto Y$. 
It is also true that $\mu \models \Diamond \Ar$ by monotonicity (\cref{theorem:strong-mono}), which is itself a direct application of \cref{theorem:sdef-le0}

\medskip

\textbf{(b).}
Now $\Ar = \Ar_G$ for some graph $G$.
The forward direction of the equivalence is strictly weaker than the one we already proved in part (a); we have shown $\mu \models \Diamond \ArXY n$ for all $n \ge 0$, and needed only to show it for $n=1$. 
The reverse direction is what's interesting.
As before, we will take a significant shortcut by using \cref{theorem:sdef-le0}. 
Suppose $\mu \models \Diamond \ArXY 1$. 
In this case where $\Ar = \Ar_G$, it was shown by \citet{pdg-aaai} that  $\SDef_{\Ar}(\mu) \ge 0$.
It follows that
\[
    0 ~\overset{\text{(\cref{theorem:sdef-le0})}}\ge~  \SDef_{\ArXY n}(\mu) = \SDef_{\Ar}(\mu) + \H_\mu(Y|X) 
    ~
    % \overset{\citep{pdg-aaai}}
    \ge~ 0,
\]
and thus $\H_\mu(Y|X) = 0$, meaning that $\mu \models X \tto Y$ as promised.  As before, we also have $\mu \models \Diamond \Ar$ by monotonicity. 


\textbf{(c).}
As in part (b), the forward direction is a special case of the forward direction of part (a), and it remains only to prove the reverse direction.  
Equipped with the additional information that $\Ar \rightsquigarrow \{ \to \{ X \} \}$, 
suppose that $\mu \models \Diamond \ArXY 2$. 
By monotonicity, this means $\mu \models \Ar$ and also that $\mu \models\begin{tikzpicture}[center base]
    \node[dpad0] (X) {$X$};
    \node[dpad0,right of=X] (Y) {$Y$};
    \draw[arr1] (X) to[bend left=15] (Y);
    \draw[arr1] (X) to[bend right=15] (Y);
    \draw[arr1,<-] (X) to[] +(-0.8,0);
\end{tikzpicture}$. 
Let $\Ar'$ denote this \hgraph. 
Once again by appeal to \cref{theorem:sdef-le0}, 
we have that 
\[
    0 \ge \SDef_{\Ar'} = - \H_\mu(X,Y) + \H(X) + 2 \H_\mu(Y|X)
         = \H_\mu(Y|X) \ge 0. 
\]
It follows that $\H_\mu(Y|X) = 0$, and thus $\mu \models X\tto Y$. 
As mentioned above, we also know that $\mu \models \Ar$, 
and thus $\mu \models \Diamond \Ar ~\land~ X {\tto} Y$ as promised.
\end{lproof}


\newpage
\subsection{Causality Results of \cref{sec:causal}}
%oli15*: proof
\recall{prop:sc-graph-arise}
\begin{lproof}\label{proof:sc-graph-arise}
    % To simplify the presentation, let ($\star$) denote the
    % property that $\Pa_M(Y) \cap \X = \Pa_G(Y)$ for all $Y \in \X$
    %    
    \textbf{($\implies$).~} Suppose $\mu \models \Ar_G$. 
    Thus there exists some witness $\bar\mu(\X,\U)$ to this fact, satisfying conditions (a-c) of \cref{defn:scompat}. 
    Because $\Ar_G$ is partitional, the elements
    of $\PSEMs_{\Ar_G}(\bar\mu)$ are ordinary (i.e., not generalized) randomized PSEMs.
    We claim that every $\mathcal M = (M,P) \in \PSEMs_{\Ar_G}(\bar\mu)$ that is a randomized PSEM from which $\mu$ can arise,
    % and also has the property that $\Pa_M(Y) \cap \X = \Pa_G(Y)$ for all $Y \in \X$. 
    and also has the property that $\Pa_M(Y) \subseteq \Pa_G(Y) \cup \{U_Y\}$ for all $Y \in \X$. 
    \begin{itemize}[left=1em]
    \item 
    The \hyperarc s of $\Ar_G$ correspond to the vertices of $G$, which in turn correspond to the variables in $\X$; thus $\U = \{ U_X \}_{X \in \X}$. 
    By property (b) of \scibility\ witnesses (\cref{defn:scompat}), these variables $\{ U_X \}_{X \in \X}$ are mutually independent according to $\bar\mu$. 
    Furthermore, because $\mathcal M = (M, P) \in \PSEMs_{\Ar_{G}\!}(\bar\mu)$, we know that $\bar\mu(\U) = P$, 
    and thus the variables in $\U$ must be mutually independent according to $P$.
    % Finally, whenever $X \ne Y$, $\Pa_M(X) $
    By construction, in causal models $\mathcal M \in \PSEMs_{\Ar_G}(\bar\mu)$ the equation $f_Y$ can depend only on $\Src Y = \Pa_G(Y) \subseteq \X$ and $U_Y$. So, in particular, $f_Y$ does not depend on $U_X$ for $X \ne Y$.
    
    Altogether, we have shown that ${\cal M}$ contains exogenous variables $\{ U_X \}_{X \in \X}$ that are mutually independent according to $P$, and that $f_Y$ does not depend on $U_X$ when $X \ne Y$. 
    Thus, $\mathcal M$ is a randomized PSEM.

    \item 
    By condition (a) on \scibility\ witnesses (\cref{defn:scompat}), we know that $\bar\mu(\X) = \mu$. By \cref{prop:witness-model-properties}(a), we know that $\mu \in \SD{\cal M}$. 
    Together, the previous two sentences mean that $\mu$ can arise from $\mathcal M$.
    
    \item 
    % Finally, we show that $\Pa_M(Y) \cap \X = \Pa_G(Y)$ for all $Y \in \X$. 
    % Fix $Y \in \X$. 
    % % By part (c) of \scibility\ witness, we know that $\nu \models \Pa_G(Y) \cup \{U_Y\} \tto Y$.
    % % By construction, $M$'s equation $f_Y$ for $Y$ depends only on $\Src a$ and $U_a$, which in this case means $\Pa_G(Y)$ and $U_Y$. 
    % % Thus, $\Pa_M(G) \cap \subseteq $
    % As mentioned above, the equation $f_Y$ in $M$ can depend only on
    % $\Src Y = \Pa_G(Y)$ and on $U_Y$.  Thus $\Pa_M(Y) \cap \X \subseteq \Pa_G(Y)$. 
    Finally, as mentioned in the first bullet item, the equation $f_Y$ in $M$ can depend only on $\Src Y = \Pa_G(Y)$ and on $U_Y$. Thus
     $\Pa_M(Y) \subseteq \Pa_G(Y) \cup \{U_Y\}$ for all $Y \in \X$. 
    \end{itemize}
    %
    Under the assumption that $\mu \models \Ar_G$, we have now shown that there exists a randomized causal model $\cal M$ from which $\mu$ can arise, with the property that 
    $\Pa_{\cal M}(Y) \subseteq \Pa_G(Y) \cup \{ U_Y \}$ for all $Y \in \X$. 
    %
% this is a proof that you can produce a causal model with the property that $\Pa_M(Y) \cap \X \subseteq \Pa_G(Y)$ for all $Y \in \X$ --- but here we don't need something that strong. 
\commentout{
    We have now shown that $\mathcal M$ is a randomized causal model from which $\mu$ can arise, with the property that 
    $\Pa_M(Y) \cap \X \subseteq \Pa_G(Y)$ for all $Y \in \X$. 
    To get an analogous statement with equality, we now introduce one additional variable $U_\bullet$, with $\V(\U_\bullet) :=  \{\bullet\} \cup 
        \prod_{Y \in \X} \V(Y)^{\V(\Pa_G(Y))}
        % \V(\X)
        $.
    Take $P'(\U, U_\bullet) := P(\U) \delta(U_\bullet = \bullet)$ to be the unique distribution that extends $P$ with a dirac distribution stating that $U_\bullet$ always takes its special value $\bullet$. 
    Define new equations $\mathcal F' := \{ f'_Y \}_{Y \in \X}$
    according to 
    \[
    f'_Y(u_\bullet, \mat z) := 
    \begin{cases}
        & f_Y(\mat z) \text{ if } u_\bullet = \bullet \\
        & u_\bullet(Y)( \mat z[\Pa_G(Y)] ) \text{ if } u_\bullet \in \V(\X)
    \end{cases}.
    \]
    Let's break this down this notation. $u_\bullet$ is a value of $U_\bullet$, and hence a dependent function from variables $Y \in \X$ to functions of type $\V(\Pa_G(Y)) \to \V(Y)$. 
    Thus $u_\bullet(Y) : \V(\Pa_G(Y)) \to \V(Y)$.
    Next, $\mat z \in \V(\X \cup \U - Y)$, and so $\mat z[\Pa_M(Y)]$ consists of those components of $\mat z$ corresponding to the parents of $Y$ in the graph $G$. 
    So $u_\bullet(Y)( \mat z[\Pa_G(Y)] ) \in \V(Y)$ is a value of $Y$.
    Let $\mathcal M' = (M' = (\X, \U \cup \{ U_\bullet\}, \mathcal F'), P')$ denote this modified PSEM. 
    $M'$ still has the property that $\Pa_{M'}(Y) \cap \X \subseteq \Pa_{G}(Y)$, since $f'_Y$ depends on $U_\bullet$ and either the variables it previously depended on (when $u_\bullet = \bullet$) or on $\Pa_G(Y)$.
    But in $M'$, the endogenous variables on which $Y$ depends are precisely $\Pa_G(Y)$, not a subset.
    This is because, for all $X \in \Pa_G(Y)$, there is a tripple $(\mat u_\bullet, x, x')$ with $u_\bullet \in \V(U_\bullet)$ and $x,x' \in \V(X)$ such that $u_\bullet(Y)(\ldots, x, \ldots) \ne u_\bullet(Y)(\ldots, x', \ldots)$. 
    Therefore $\Pa_{M'}(Y) = \{ U_\bullet, U_Y \} \cup \Pa_G(Y)$,
        and hence $\Pa_{M'}(Y) \cap \X = \Pa_G(Y)$, as promised. 
    % This new equation $f'_Y$ still has the property $ $\Pa_G()$  
}
    
    \medskip
    
    \textbf{($\impliedby$).~} Conversely, suppose there is a randomized PSEM 
    % $\mathcal M = (M = (\X,\U, \mathcal F), P)$
    $\mathcal M = (M = (\mathcal Y,\U, \mathcal F), P)$
     with the property that
    % $\Pa_M(Y) \cap \X = \Pa_G(Y)$ for all $Y$,
    $\Pa_M(Y) \subseteq \Pa_G(Y) \cup \{ U_Y \}$ for all $Y$,
    from which $\mu$ can arise. 
    The last clause means there exists some $\nu \in \SD{\cal M}$ such that $\nu(\X) = \mu$.
    We claim that this $\nu$ is a witness to $\mu \models \Ar_G$. 
    We already know that condition (a) of being a \scibility\ witness is satisfied, since $\nu(\X) = \mu$. 
    Condition (b) holds because of the assumption that $\{ U_X \}_{X \in \X}$ are mutually independent in the distribution $P$ for a randomized PSEM (and the fact that $\nu(\U) = P$, since $\nu \in \SD{\mathcal M}$).
    Finally, we must show that (c) for each  $Y \in \X$, 
    $\nu \models \Pa_G(Y) \cup \{ U_Y \} \tto Y$. 
    Since $\nu \in \SD{\cal M}$, we know that $M$'s equation holds with probability 1 in $\nu$, and so it must be the case that $\nu \models \Pa_M(Y) \tto Y$. 
    Note that, in general, if $\mat A \subseteq \mat B$ and $\mat A \tto \mat C$, then $\mat B \tto \mat C$.
    By assumption, $\Pa_M(Y) \subseteq \Pa_G(Y) \cup \{U_Y\}$, and thus $\nu \models \Pa_G(Y) \cup \{U_Y\} \tto Y$.
    
    % For each $Y \in \X$, we know that $M$'s equation $f_Y$ 
    % % for the variable $Y$ depends (only) on $\Pa_G(Y)$; hence, because $f_Y$ 
    % holds with probability 1 in $\nu$ (since $\nu \in \SD{M}$), and so
    % % the variable $Y$ must be a function of $\Pa_G(Y)$
    % it must be the case that $\nu \models \Pa_M(Y) \tto Y$. 
    % 
    % By assumption, $\Pa_M(Y) \cap \X = \Pa_G(Y)$. 
    
    
    Thus $\nu$ satisfies all conditions (a-c) for a \scibility\ witness, and hence $\mu \models \Ar_G$. 
\end{lproof}

%oli16:
% \recall{prop:sc-causal-graph-arise}
% \begin{lproof}\label{proof:sc-causal-graph-arise}
%     Suppose 
% \end{lproof}


\recall{prop:gen-sim-compat-means-arise}
\begin{lproof}\label{proof:gen-sim-compat-means-arise}
    \textbf{($\implies$).~} Suppose $\mu \models \Ar$, meaning there exists a witness
    $\nu(\X,\U)$ with property \cref{defn:scompat}(c), meaning
    that, for all $a \in \Ar$, 
    there is a functional dependence $(\Src a, U_a) \tto \Tgt a$.
    Thus, there is some set of functions $\mathcal F$ with these types that holds with probability 1 according to $\nu$. 
    Meanwhile, by \cref{defn:scompat}(b), $\nu(\U)$ are mutually independent, so defining $P_a(U_a) := \nu(U_a)$, we have $\nu(\U) = \prod_{a \in \Ar}P_a(U_a)$. 
    Together, the previous two conditions (non-deterministically) define a 
    generalized randomized PSEM
    $\cal M$ of shape $\Ar$ for which $\nu \in \SD{\mathcal M}$. 
    Finally, by \cref{defn:scompat}(a), we know that $\mu$ can arise from $\mathcal M$.
    
    \textbf{($\impliedby$).~} Conversely, suppose there is a generalized randomized SEM $\mathcal M$ of shape $\Ar$ from which $\mu(\X)$ can arise. 
    Thus, there is some $\nu \in \SD{\mathcal M}$ whose marginal on $\X$ is $\mu$. 
    We claim that this $\nu$ is also a witness that $\mu \models \Ar$.
    The marginal constraint from \cref{defn:scompat}(a) is clearly satisfied. 
    Condition (b) is immediate as well, because $\nu(\U) = \prod_{a} P_a(U_a)$. 
    Finally, condition (c) is satisfied, because the equations of $\mathcal M$ hold with probability 1, ensuring the appropriate functional dependencies. 
\end{lproof}

%oli14*: starting proof
\recall{prop:witness-model-properties}
\begin{lproof}\label{proof:witness-model-properties}
    (a) is straightforward. 
    Suppose $\mathcal M \in \PSEMs(\nu)$. 
    By construction, the equations of $\mathcal M$ reflect functional dependencies in $\nu$, and hence hold with probability 1. 
    \unskip\footnote{When the probability of some combination of source variables is zero, there is typically more than one choice of functions that holds with probability 1; the choice of functions is essentially the choice of $\mathcal M \in \PSEMs(\nu)$.}
    Furthermore, the distribution $P(\U)$ in all $\mathcal M \in \PSEMs(\nu)$ is equal to $\nu(\U)$. 
    These two facts, demonstrate that $\nu$ satisfies the two constraints required for membership in $\SD{\mathcal M}$. 
    
    
    % We now turn to 
    (b). 
    We do the two directions separately. 
    First, suppose $\mathcal M \in \PSEMs(\nu)$.
    We have already shown (in part (a)) that $\nu \in \SD{\mathcal M}$. 
    The construction of $\PSEMs(\nu)$ depends on the hypergraph $\Ar$ (even if the dependence is not explicitly clear from our notation)
    in such a way that $f_X$ does not depend on any variables beyond $U_a$ and $\Src {a_{\!X}}$. Thus, $\Pa_{\mathcal M}(X) \subseteq \Src {a_{\!X}} \cup \{U_{a_{\!X}}\}$.
    
    Conversely, suppose $\mathcal M = (\X,\U, \mathcal F)$ is a PSEM satisfying $\nu \in \SD{\cal M}$ and $\Pa_{\cal M}(X) \subseteq \Src{a_{\!X}} \cup \{U_{a_{\!X}}\}$. 
    We would like to show that $\mathcal M \in \PSEMs(\nu)$. 
    Because $\nu \in \SD{\cal M}$, we know that the distribution $P(\U)$ over the exogenous variables in the PSEM $\cal M$ is equal to $\nu(\U)$, matching the first part of our construction.
    What remains is to show that the equations $\mathcal F$ are consistent with our transformation. 
    %oli15:
    % Choose any $X \in \X$,  $a_{\!X} \in \Ar$ such that $X \in \Tgt{ a_{\!X}}$, as well as values $\mat s \in \V(\Src {a_{\!X}})$ and $u \in \V(U_{a_{\!X}})$.
    Choose any $X \in \X$. Because $\Ar$ is \subpartl, there is a unique $a_{\!X} \in \Ar$ such that $X \in \Tgt{ a_{\!X}}$. Now choose any values $\mat s \in \V(\Src {a_{\!X}})$ and $u \in \V(U_{a_{\!X}})$.
    If $\nu(\mat s, u) > 0$, then we know there is a unique value of $x \in \V(X)$ such that $\nu(\mat s, u, x) > 0$. Since $\mathcal M$'s equation for $X$, $f_{X}$, depends only on $\mat s$ and $u$, and holds with probability 1, we know that $f_X(\mat s, u) = t$, as required. 
    On the other hand, if $\nu(\mat s, u) = 0$, then any choice of $f_X(\mat s, u)$ is consistent with our procedure. 
    Since this is true for all $X$, and all possible inputs to the equation $f_X$, we conclude that the equations $\mathcal F$ can arise from the procedure described in the main text,
    and therefore $\mathcal M \in \PSEMs(\nu)$.     
    % Because of the former condition, we know that the endogenous variables 
\end{lproof}

\recall{theorem:condition-intervention-alignment}
\begin{lproof}\label{proof:condition-intervention-alignment}
    \textbf{(part a).}
    \def\doXx{\mathrm{do}_{M}(\mat X{=}\mat x)}
    Let $(M, P) := \mathcal M$ be the SEM and probability over exogenous variable in the PSEM $\mathcal M$, and 
        $\mathcal F = \{ f_Y \}_{Y \in \X}$ be its set of equations. 
    Because we have assumed $\nu(\doXx) > 0$,
    the conditional distribution 
    \[ 
        \nu\mid \doXx = \nu(\U, \X) \cdot 
        \prod_{X \in \mat X}
        % \mathbbm 1[\hat U_X = \lambda \mat s. \mat x[X]]
        \mathbbm 1\big[\forall \mat s. f_X(U_X, \mat s) = \mat x[X]\big]
        ~\Big/~ 
        \nu(\doXx) 
    \]
    is defined. By assumption, $\mathcal M \in \PSEMs(\nu)$ and $\nu$ is a witness to $\mu \models \Ar$. Thus, by \cref{prop:witness-model-properties}, we know that $\nu \in \SD{\mathcal M}$. 
    So in particular, all equations of $\mathcal M$ hold for all joint settings $(\mat u, \omega) \in \V(\X \cup \U)$ in the support of $\nu$. But the support of the conditional distribution $\nu \mid \doXx$ is a subset of the support of $\nu$, so all equations of $\mathcal M$ also hold in the conditioned distribution.  
    Furthermore, 
        the event $\doXx$ is the event in which, for all $X \in \mat X$, the variable $U_X$ takes on a value such that $f_X(\ldots, U_X,\ldots) = \mat x[X]$.  
    Thus the equations corresponding to $\mat X = \mat x$ also hold with probability 1 in $\nu \mid \doXx$.

    This shows that all equations of $\mathcal M_{\mat X {\gets}\mat x}$ hold with probability 1 in $\nu \mid \doXx$. However, the marginal distribution $\nu(\U \mid \doXx)$ over $\U$ will typically not be the distribution $P(\U)$---indeed, we have altered collapsed distribution of the variables $\U_{\mat X} := \{ U_X : X \in \mat X\}$. 
    So, strictly speaking, $\nu \mid \doXx \notin \SD{\mathcal M_{\mat X {\gets}\mat x}}$.
    % Nevertheless, if we restrict our attention to the marginal on $\X$
    Our objective, therefore, is to show that there is a \emph{different}  distribution $\nu' \in \SD{\mathcal M_{\mat X {\gets} \mat x}}$ such that $\nu'(\X) = \nu(\X \mid \doXx)$. 
    Let $\mat Z := \X \setminus \mat X$, and $\U_{\mat Z} := \{ U_Z : Z \in \mat Z\}$. 
    We can define $\nu'$ according to
    \[
        \nu'(\X, \U_{\mat X}, \U_{\mat Z}) := 
            \nu(\X, \U_{\mat Z} \mid \doXx) P(\U_{\mat X}).
    \]
    This distribution satisfies 
    % all required properties:
    three critical properties:
    \begin{enumerate}
        \item Clearly $\nu'$ has the appropriate marginal $\nu'(\X) = \nu(\X \mid \doXx)$ on exogenous variables $\X$, by construction.
        \item At the same time, the marginal on exogenous variables is
        \begin{align*}
            \nu'(\U) &= \nu'(\U_{\mat X}, \U_{\mat Z})  \\
            &= \int_{\V(\X)} \nu(\omega, \U_{\mat Z}\mid \doXx) P(\U_{\mat X}) \,\mathrm d\omega\\
            &= P(\U_{\mat X}) \nu(\U_{\mat Z} \mid \doXx) 
                &\text{}\\
            &= P(\U_{\mat X}) P(\U_{\mat Z} \mid \doXx) 
                &\text{[ since $\doXx$ depends only on $\U$ ]}
                \\
            &= P(\U_{\mat X}) P(\U_{\mat Z}) 
                &\left[~\parbox{5cm}{ since $\doXx$ depends only on $\U_{\mat X}$, while $\U_{\mat X}$ and $\U_{\mat Z}$ are independent in $\nu$ (by the witness condition).}~\right]
                \\
            &= P(\U_{\mat X}, \U_{\mat Z})
                & [\text{  same reason as above }]
        \end{align*}
        \item Finally, $\nu'$ satisfies all equations of $\mathcal M_{\mat X{\gets} \mat x}$. It satisfies the equations for the variables $\mat X$ because $\mat X = \mat x$ holds with probability 1. 
        At the same time, the equations in $\mathcal M_{\mat X{\gets} \mat x}$ corresponding to the variables $\mat Z$ hold with probability 1, because the marginal $\nu'(\U_{\mat Z}, \X)$ is shared with the distribution $\nu \mid \doXx$---and that distribution satisfies these equations. (It suffices to show that they share this particular marginal because the equations for $\mat Z$ do not depend on $\U_{\mat X}$.)
    \end{enumerate}
    Together, items 2 and 3 show that $\nu' \in \SD{\mathcal M_{\mat X{\gets} \mat x}}$, and item 1 shows that $\nu(\X \mid \doXx)$ can arise from $\mathcal M_{\mat X{\gets} \mat x}$. 


    \bigskip

    \textbf{(part b).}
    We will again make use of the distribution $\nu'$ defined in part (a), and its three critical properties listed above. 
    Given a setting $\mat u \in \V(\U)$ of the exogenous variables, let
    \[
        \mathcal F_{\mat X \gets \mat x} (\mat u) 
        := \left\{ \omega \in \V(\X) ~\middle|~
        \begin{array}{rl}
            \forall X \in \mat X.~& \omega[X] = \mat x[X]\\
            \forall Y \in \X \setminus \mat X.~&\omega[Y] = f_X(\omega[\X \setminus Y], \mat u) 
        \end{array}
        % \forall X \in X.~
        % f'_{X}(\omega[\X\setminus X], \mat u)  = \omega[X] 
        \right\} 
    \]
    denote the set of joint settings of endogenous variables that are consistent with the equations of $\mathcal M_{\mat X \gets \mat x}$. 

    If $\mat u \in \V(\U)$ is such that 
    \begin{align*}
        (M, \mat u) \models [\mat X {\gets} \mat x] \varphi
        \quad&\iff\quad
        (M_{\mat X {\gets} \mat x}, \mat u) \models \varphi  \\
        &\iff \forall \omega \in \mathcal F_{\mat X {\gets} \mat x}(\mat u).~~\omega \in \varphi \\
        &\iff 
            % \forall \omega \in \V(\X).~
            % \
            \mathcal F_{\mat X {\gets} \mat x}(\mat u)
                ~\subseteq~ \varphi, 
    \end{align*}
    % for the equations $\mathcal F' = \{ f'_X  \}_{X \in \X}$ of $M_{\mat Y \gets \mat y}$. 
    then $\phi$ holds at all points that satisfy the equations of $M_{\mat X {\gets} \mat x}$. So, since $\nu'$ is supported only on such points (property 3), it must be that $\nu'(\varphi) = 1$. By property 1, 
    $\nu'(\varphi) =  \nu(\varphi \mid \doXx)$. 

    Furthermore, if $\nu'(\varphi) > 0$, then there must exist some $\omega \in \mathcal F_{\mat X {\gets} \mat x}(\mat u)$ satisfying $\varphi$, and thus 
    $(M, \mat u) \models \langle \mat X {\gets}\mat x\rangle\varphi$. Putting both of these observations together, and with a bit more care to the symbolic manipulation, we find that:
    \begin{align*}
        \Pr_{\mathcal M}([\mat X {\gets} \mat x]\varphi) &= P(\{ 
            \mat u \in \V(\U) : (M, \mat u) \models [\mat X {\gets}\mat x]\varphi
        \})
        \\
        &= \sum_{\mat u \in \V(\U)} P(\mat u) \mathbbm 1
            \big[ \mathcal F_{\mat X \gets \mat x}(\mat u) \subseteq \varphi \big] \\
        &\le \sum_{\mat u \in \V(\U)} P(\mat u) \nu'(\varphi \mid \mat u) 
        \qquad = \nu'(\varphi) = \nu(\varphi \mid \doXx) \\
        &\le \sum_{\mat u \in \V(\U)} P(\mat u) \mathbbm 1
        \big[ \mathcal F_{\mat X {\gets} \mat x}(\mat u) \cap \varphi \ne \emptyset \big] \\
        &= P(\{ 
            \mat u \in \V(\U) : (M, \mat u) \models \langle\mat X {\gets}\mat x\rangle\varphi
        \})
        \\&= \Pr_{\mathcal M}(\langle\mat X {\gets} \mat x\rangle\varphi),
        \qquad\text{    as desired. }
    \end{align*}

    Finally, if $\nu \models \U \tto \X$, then $\mathcal F_{\mat X {\gets} \mat x}(\mat u)$ is a singleton for all $\mat u$, and hence $\varphi$ holding for all $\omega \in \mathcal F_{\mat X {\gets} \mat x}$ and for some $\omega \in \mathcal F_{\mat X {\gets} \mat x}$ are equivalent. 
    So, in this case, 
    \[
        (M, \mat u) \models [\mat X{\gets}\mat x]\varphi
        \qquad \iff \qquad
        (M, \mat u) \models \langle\mat X{\gets}\mat x\rangle\varphi,
    \]
    and thus the probability of both formulas are the same---and it must also equal $\nu(\varphi \mid \doXx)$ which we have shown lies between them. 
\end{lproof}


%oli14: had previously added this partial proof, but it has issues; I've nowe decided to cut the material in the main text. Now, deleting this proof stub as well.
\commentout{
\recall{prop:all-models-formula-equiv}
\begin{lproof}\label{proof:all-models-formula-equiv}
    We prove the equivalence by induction on the structure of causal formulas $\varphi$. 
    \begin{itemize}
        \item \textbf{Case
        $\varphi = \varphi_1 \land \varphi_2$.} 
        Suppose inductively that $(\nu, \mat u) \models \varphi_1$ iff $(\mathcal M, \mat u) \models\varphi_1$ for all $\mathcal M \in \PSEMs(\nu)$, and the same for $\varphi_2$. 
        Then:
        \begin{align*}
            (\nu, \mat u) \models \varphi_1 \land \varphi_2
            \quad&\iff\quad
            (\nu, \mat u) \models \varphi_1  \quad\text{and}\quad
            (\nu, \mat u) \models \varphi_2 
            \\
            &\iff\quad
                \Big(\forall \mathcal M \in \PSEMs(\nu).~
                (\mathcal M, \mat u) \models \varphi_1 \Big)
                \text{ and }
                \Big(\forall \mathcal M \in \PSEMs(\nu).~
                (\mathcal M, \mat u) \models \varphi_2 \Big)
            \\
            &\iff\quad
            \forall \mathcal M \in \PSEMs(\nu).~
                \Big(
                (\mathcal M, \mat u) \models \varphi_1 \text{ and } 
                (\mathcal M, \mat u) \models \varphi_2 \Big)
            \\
            &\iff \quad
            \forall \mathcal M \in \PSEMs(\nu).~
            (\mathcal M, \mat u) \models \varphi_1 \land \varphi_2.
        \end{align*}
        \item \textbf{Case
        $\varphi = \lnot \varphi_1$.} 
        Suppose that the inductive hypthesis holds for $\varphi_1$. 
        \begin{align*}
            (\nu, \mat u) \models \lnot \varphi_1
                \quad& \iff\quad
                (\nu, \mat u) \not\models \varphi_1
                \\
                &\iff\quad
                \exists \mathcal M \in \PSEMs(\nu).~
                (\mathcal M, \mat u) \not\models \varphi_1.
                \\
                &\iff \quad 
                \exists \mathcal M \in \PSEMs(\nu).~
                (\mathcal M, \mat u) \not\models \varphi_1.
        \end{align*}
        {\color{red} oops, as expected, this doesn't work out.}
        
        \item \textbf{Case $\varphi = [Y {\gets}y]\varphi_1$}.
        %
        \begin{align*}
            (\nu, \mat u) \models [Y {\gets}y]\varphi_1
            \quad
            &\iff \quad
            \nu(\varphi_1 \mid \U{=}\mat u[\hat U_{\mat Y} \gets 
                \Lambda Y. \lambda\textunderscore.\mat y[Y]]) = 1
        \end{align*}
        This means $\varphi_1$ holds in all 
        
    \end{itemize}
    
    % First, suppose 
    $(\nu, \mat u) \models \varphi$. 
    By definition, this means   
\end{lproof}
}



%oli14*: starting this proof as well
% \recall{theorem:esem-properties}
% \begin{lproof}\label{proof:esem-properties}
%     \textbf{(part a).}
%     % By \cref{proof:witness-model-properties}, $\PSEMs(\nu)$ is the set of causal models 
%     % Suppose $\mu \emodels \Ar$.
%     % If the pair $(\omega, \mat u) \in \V(\X \cup \U)$ is consistent with the equations 
% 
%     % for all $X$, 
%     % $\nu(U_{a_{\!X}} = u, \Src{a_{\!X}} = \mat s) = 0$
%     % implies that 
% 
%     \textbf{(part b).}
% 
%     \textbf{(part c).}
% 
%     Alternate Statement. 
% 
%     Suppose $(\nu, \mathcal F)$ is a witness to $\mu \emodels \Ar$.
%     For all $\mathcal M \in \PSEMsA(\nu)$ and $\varphi \in \mathcal L(\U, \X)$, 
%     \[
%         \Pr_{\cal M}( [\mat X {\gets}\mat x]\varphi )
%             = \nu(\varphi \mid \mathrm{do}_{\cal M}(\mat X{=}\mat x) )
%         = \Pr_{\cal M}( \langle\mat X {\gets}\mat x\rangle\varphi ).
%     \]
% 
%     Now let's try to prove this. 
% 
% \end{lproof}


%oli21: no longer in paper
\vfull{
\recall{prop:causal-mrf}
\begin{lproof}
    \label{proof:causal-mrf}
    1.
    Suppose $\mu \models \Ar_G$, $\bar\mu(\U,\X)$ is a witness to this, and $\mathcal M \in \PSEMs_{\Ar_G \!}(\bar\mu)$. 
    
    \textbf{($\implies$).}
    Suppose $\mat X \CI_G \mat Y \mid \mat Z$, meaning every path from $\mat X$ to $\mat Y$ in $G$ goes through $\mat Z$.
    We now show that 
    
    
    \TODO
    
\end{lproof}
}


%oli21*: new proofs added
%oli22: removing from here
% \subsection{Monotoncity Results of \cref*{sec:monotone}}
% \recall{theorem:mrf-bn-monotone-impossible}



\subsection{Information Theoretic Results of \Cref{sec:info}}
%oli14*: writing up proof of the bound that I view as the centerpiece of section 5. 
To prove \cref{theorem:sdef-le0} and \cref{theorem:siminc-idef-bounds}(a), we will need the following Lemma. 

\begin{lemma}
        \label{lem:Y-filter}
    Consider a set of variables $\mat Y = \{Y_1, \ldots, Y_n\}$, and another (set of) variable(s) $X$.
    Every joint distribution $\mu(X, \mat Y)$ over the values of $X$ and $\mat Y$ satisfies
    \[
        \sum_{i=1}^n \I_\mu(X \,;\, Y_i) 
        ~\le~
        % \H_{\mu}(X) 
         \I_{\mu}(X \,;\, \mat Y) 
         + \sum_{i=1}^n \H_\mu(Y_i) - \H_\mu(\mat Y).
    \]
\end{lemma}
\begin{lproof}
    Since there is only one joint distribution in scope, we omit the subscript $\mu$, writing $\I(-)$ instead of $\I_\mu(-)$ and $\H(-)$ instead of $\H_\mu(-)$, in the body of this proof.
    The following fact will also be very useful:
    \begin{align}
        \I(A; B, C) &= \I(A; C) + \I(A; B \mid C)
            &\text{(the chain rule for mutual information).}
            \label{eq:chain-rule-mi}
    \end{align}
    
    % 
    % First, observe that, for all variables $A,B$,
    % \begin{align*}
    %     \I(X; A) + \I(X; B) 
    %         &= \I(X ; A,B) + \I(A ; B ; X)
    %     \\
    %     &\le \I(X ; A,B) + \I(A ; B)
    % \end{align*}
    % The inequality above follows from the fact that $\I(A;B;X) = \I(A;B) - \I(A;B|X)$, with $\I(A;B|X) \ge 0$.  
    % The first equality is an identity that is easily read off of an information diagram; for concreteness,  
    % it can also be derived by summing the following two better-known identities:
    % \begin{align}
    %     \I(Y_1 ; Y_2 ; X) &= 
    %         \I(Y_1 ; X) - \I(Y_1 ; X \mid Y_2)
    %         &\text{(one definition of $\I(Y_1; Y_2; Y_3)$)}
    %         \label{eq:interactinfo-defn}
    %     \\
    %     \I(X; Y_1, Y_2) &= \I(X;Y_2) + \I(X;Y_1 \mid Y_2)
    %         &\text{(the ``chain rule'' of mutual information).}
    %         \label{eq:chain-rule-mi}
    % \end{align}
    
    We prove this by induction on $n$. In the base case ($n=1$), 
    we must show that $\I(X;Y)  \le \I(X;Y) + \H(Y) - \H(Y)$, which is an obvious tautology. 
    % By repeatedly applying this fact
    % By induction, suppose that 
    Now, suppose inductively that 
    \[
        \sum_{i=1}^k \I(X\,;\,Y_i) 
        ~\le~
         \I(X \,;\, \mat Y_{1:k}) + \sum_{i=1}^k \H(Y_i) - \H(\mat Y_{1:k})
         \tag{IH$_k$}
    \]
    for some $k < n$, where $\mat Y_{1:k} = (Y_1, \ldots, Y_k)$. 
    We now prove that the analogue for $k+1$ also holds. 
    Some calculation reveals:
    % We know that 
    \begin{align*}
        % \I(X ; Y_{k+1}) &\le \I(X ; \mat Y_{1:k+1})
            % &\text{since } \I(X ; \mat Y_{1:k+1}) = 
            % &= \I()
        \I(X ; Y_{k+1}) &= \I(X ; \mat Y_{1:k+1}) - \I(X ; \mat Y_{1:k} \mid Y_{k+1}) 
            & \big[\text{ by MI chain rule \eqref{eq:chain-rule-mi} } \big]
        \\
            &\le \I(X ; \mat Y_{1:k+1})
            & \Big[\text{ since } \I(X ; \mat Y_{1:k} \mid Y_{k+1}) \ge 0\, \Big] \\
        &= \I(X;  Y_{k+1} \mid \mat Y_{1:k}) + \I(\mat Y_{1:k} ;  Y_{k+1})
            & \big[\text{ by MI chain rule \eqref{eq:chain-rule-mi} } \big] \\
        &= \left(
        \begin{array}{c}
        \I(X ; \mat Y_{1:k+1}) + \H(Y_{k+1}) - \H( \mat Y_{1:k+1}) \\
         - \I(X ; \mat Y_{1:k})  \qquad\quad\qquad + \H( \mat Y_{1:k} )           
     \end{array}\right)
        & \Big[
        \begin{array}{c}
        \text{ left: one more MI chain rule \eqref{eq:chain-rule-mi}; } \\
        \text{ right: defn of mutual information  }    
        \end{array}
        \Big].
    \end{align*}
    
    Observe: adding this inequality to our inductive hypothesis (IH$_k$) yields (IH$_{k+1}$)!
    So, by induction, the lemma holds for all $k$. \qedhere
    %
    \commentout{% see the better proof above.
    We start by proving this in the special case where $n=2$, and later use the result to prove the lemma for all $n$. 
    When $n=2$, we must prove that
    \[
    \I(Y_1;X) + \I(Y_2; X) \le \H(X) +  \H(Y_1) + \H(Y_2) - \H(Y_1,Y_2).
    \]
    % $\I(Y_1;X) + \I(Y_2; X) \le \H(X) + \I(Y_1;Y_2)$
    % (because $\I(Y_1;Y_2) = \H(Y_1) + \H(Y_2) - \H(Y_1,Y_2)$).
    % The last three terms are just the definition of $$.
    We calculate:
    \begin{align*}
        &\H(X) +  \H(Y_1) + \H(Y_2) - \H(Y_1,Y_2) \\
        &= \H(X) + \I(Y_1; Y_2) 
            &\text{by definition of $\I(Y_1;Y_2)$}
        \\
        &= \H(X) + \I(Y_1 ; Y_2 \mid X) + \I(Y_1 ; Y_2 ; X) 
            &\text{by definition of $\I(Y_1;Y_2;X)$}
        \\
        &\ge \H(X) + \I(Y_1 ; Y_2 ; X) 
            &\text{since $\I(Y_1;Y_2\mid X) \ge 0$}
        \\
        &\ge \I(X ; Y_1,Y_2) + \I(Y_1 ; Y_2 ; X)  
            &\text{as $\H(X) \ge \I(X;Z)$ for any $Z$}
        \\
        &= \I(X; Y_1) + \I(X; Y_2).
    \end{align*}
    The final step is the hardest to see symbolically, but it is an identity that is very easy to read off of an information diagram. 
    For those who prefer algebra, 
        it can be derived from the following two, better-known identities:
    \begin{align}
        \I(Y_1 ; Y_2 ; X) &= 
            \I(Y_1 ; X) - \I(Y_1 ; X \mid Y_2)
            &\text{(one definition of $\I(Y_1; Y_2; Y_3)$)}
            \label{eq:interactinfo-defn}
        \\
        \I(X; Y_1, Y_2) &= \I(X;Y_2) + \I(X;Y_1 \mid Y_2)
            &\text{(the ``chain rule'' of mutual information).}
            \label{eq:chain-rule-mi}
    \end{align}
    Specifically, the sum of \eqref{eq:interactinfo-defn} and \eqref{eq:chain-rule-mi} gives the final step of the previous manipulation. 
    This concludes the proof of the lemma, in the case of $n=2$. 
    }%
\end{lproof}

% \begin{lemma}
%         \label{lem:cmi-mi-weaken}
%     For any three variables $X,Y,Z$ and joint distribution $\mu(X,Y,Z)$ over them, we have that
%     \[
%         \I_{\mu}(X;Y \mid Z) \le \I_{\mu}(X;Y,Z).
%     \]
% \end{lemma}
% \begin{lproof}
%     % \begin{align*}
%     %     \I_\mu(X; Y \mid Z) 
%     %     &= \I_\mu(X; Y)
%     % \end{align*}
%     % It is not hard to see (e.g., with an information diagram) that 
%     $\I(X \,;\, Y,Z) = \I(X \,;\, Z) + \I(X \,;\,Y \mid Z)$. 
% \end{lproof}

\recall{theorem:sdef-le0}
\begin{lproof}
        \label{proof:sdef-le0}
    Suppose that $\mu \models \Ar$, meaning that there is a witness $\nu(\X, \U)$ that extends $\mu$, and has properties (a-c) of \cref{defn:scompat}. 
    For each \arc\ a, since
    $\nu \models (\Src a, U_a) \tto \Tgt a$, 
    we have
    $\H_\nu(\Tgt a \mid \Src a, U_a) = 0$, and so
    \[
    \H_\mu(\Tgt a \mid \Src a) = \H_\nu(\Tgt a \mid \Src a, U_a) + \I_\nu(\Tgt a ; U_a \mid \Src a) = \I_\nu(\Tgt a; U_a \mid \Src a).
     % \underset\le{}
        % \I_\nu(U_a; \Tgt a, \Src a)
    \]
    Thus, we compute    
    \begin{align*}
        \sum_{a \in \Ar} \H_\mu(\Tgt a \mid \Src a) 
        &= \sum_{a \in \Ar} \I_\nu(U_a ; \Tgt a \mid \Src a) 
        \\
        &= \sum_{a \in \Ar} \I_\nu(U_a ; \Tgt a, \Src a) - \I_\nu(U_a ; \Src a)
            & \text{ by MI chain rule \eqref{eq:chain-rule-mi}} \\
        &\le \sum_{a \in \Ar} \I_\nu(U_a ; \Tgt a, \Src a) 
            & \text{ since $\I_\nu(U_a\,;\,\Src a) \ge 0$} 
        \\
        &\le \sum_{a \in \Ar} \I_\nu(U_a ; \X) 
            &\text{ since $\X \tto (\Src a, \Tgt a)$ }
        \\
        &\le \I_\nu(\X ; \U) + \sum_{a \in \Ar} \H_\nu( U_a ) - \H_\nu(\U)
            & \text{ by \cref{lem:Y-filter}}
        \\
        &= \I_\nu(\X ; \U) & \begin{array}{r}\text{ since $\U$ are independent}\\\text{ (per condition (b) of \cref{defn:scompat})} \end{array} \\
        &\le \H_\nu(\X) = \H_\mu(\X).
            &\text{(per condition (a) of \cref{defn:scompat})}
    \end{align*}
    Thus, $\IDef_{\!\Ar}(\mu) \le 0$. \qedhere
\end{lproof}


\recall{prop:sinc-nonneg-s2}
\begin{lproof}\label{proof:sinc-nonneg-s2}
    % The fact that $\SIMInc_{(\Ar, \lambda)}(\mu) \ge 0$ is immediate
    The first term in the definition of $\SIMInc$ be written as
    \[
        \Big( - \H_\nu(\mathcal U) + \sum_{a \in \Ar} \H_\nu(U_a)  \Big)
            = \Ex_{\nu} \Big[ \log \frac{\nu(\mathcal U)}{\prod_{a} \nu(U_a)} \Big]
    \]
    and is therefore the relative entropy between $\nu(\mathcal U)$ and 
    the independent product distribution $\prod_{a \in \Ar}\nu(U_a)$. 
    %
    Thus, it is non-negative.
    %oli16: updating, for new presentation of SIMInc
    % The other terms of $\SIMInc_{(\Ar, \lambda)}(\mu)$, i.e., those multiplied by some $\lambda_a$, are all entropies, and hence non-negative as well.
    The remaining terms of $\SIMInc_{\Ar}(\mu)$, are all conditional entropies, and hence non-negative as well.
    % Thus $\SIMInc_{(\Ar,\lambda)}(\mu) \ge 0$. 
    Thus $\SIMInc_{\Ar}(\mu) \ge 0$. 
    
    Now, suppose $\mu$ is s2-comaptible with $\Ar$, i.e., there exists
    some $\nu(\mathcal U, \X)$ such that 
    (a) $\nu(\X) = \mu(\X)$,
    (b) $\H_\nu(\Tgt a | \Src a, U_a) = 0$,
    and (d) $\{U_a\}_{a \in \Ar}$ are mutually independent.
    Then clearly $\nu$ satisfies the condition under the infemum,
    every $\H_\nu(\Tgt a | \Src a, U_a)$ is zero.
    It is also immediate that the final term is zero as well, because it equals
    $\kldiv{\nu(\mathcal U)}{\prod_a \nu(U_a)}$, and 
    $\nu(\mathcal U) = \prod_a \nu(U_a)$, per the definition of mutual independence.
    Thus, $\nu$ witnesses that $\SIMInc_{(\Ar, \lambda)} = 0$. 
    
    % Conversely, if $\SIMInc_{(\Ar, \lambda)} > 0$, then
    % there is no distribution $\nu(\X, U)$ with marginal $\mu(\X)$
    % such that every $\H_\nu(\Tgt a | \Src a, U_a) = 0$
    % and $\kldiv{\nu(\mathcal U)}{\prod_a \nu(U_a)} = 0$.
    Conversely, suppose $\SIMInc_{(\Ar, \lambda)} = 0$.
    Because the feasible set is closed and bounded, as is the function,
    the infemum is achieved by some joint distribution $\nu(\X, \Ar)$ with marginal $\mu(\X)$.
    In this distribution $\nu$, we know that every $\H_\nu(\Tgt a | \Src a, U_a) = 0$
    and $\kldiv{\nu(\mathcal U)}{\prod_a \nu(U_a)} = 0$---
    because if any of these terms were positive, then the result
        would be positive as well.
    So $\nu$ satisfies (a) and (b) by definition. 
    And, because relative entropy is zero iff its arguments are identical
        we have $\nu(\mathcal U) = \prod_a \nu(U_a)$, so the $U_a$'s are mutually
        independent, and $\nu$ satisfies (d) as well.
\end{lproof}


\recall{theorem:siminc-idef-bounds}
\begin{lproof}
        \label{proof:siminc-idef-bounds}
    %oli14*: complete overhaul of proof, since statement has changed since the proof sketch was added
    Part (a).  The left hand side of the theorem 
    ($\IDef_{\Ar}(\nu)\le\SIMInc_{\Ar}(\mu)$)
    is a strengthening of the argument used to prove \cref{theorem:sdef-le0}. 
    Specifically,
    let $\nu^*$ be a minimizer of 
    % $Q(~\cdot~, \mu, \Ar)$.
        the optimization problem defining $\SIMInc$
    We calculate
    % , subject to the constraint $\nu^*(\)$
    \allowdisplaybreaks
    \begin{align*}
        &\SIMInc_{\Ar}(\mu) - \IDef_{\!\Ar}(\mu) 
        \\
        &= \left( \,
        \sum_{a \in \Ar} \H_{\nu^*}(\Tgt a \mid \Src a, U_a) - \H_{\nu^*}(\mathcal U) + \sum_{a \in \Ar} \H_{\nu^*}(U_a)
        \right) - \left(\,
        \sum_{a \in \Ar}\H_\mu(\Tgt a \mid \Src a) - \H_\mu(\X) \right)\\
        &=
        \sum_{a \in \Ar} \Big( \H_{\nu^*}(\Tgt a \mid \Src a, U_a) - \H_{\nu^*}(\Tgt a \mid \Src a) \Big)
            + \H_\mu(\X)
            - \H_{\nu^*}(\mathcal U) + \sum_{a \in \Ar} \H_{\nu^*}(U_a)
        \\
        &=
        - \sum_{a \in \Ar} \I_{\nu^*}( \Tgt a ; U_a \mid \Src a)
            \qquad + \H_\mu(\X)
            - \H_{\nu^*}(\mathcal U) + \sum_{a \in \Ar} \H_{\nu^*}(U_a).
    \end{align*}
    The argument given in the first five lines of the proof of \cref{theorem:sdef-le0}, gives us a particularly convenient bound for the first group of terms on the left:
    \[
        \sum_{a \in \Ar} \I_{\nu^*} (U_a ; \Tgt a\mid \Src a) 
        \le \I_{\nu^*}(\X; \U) + \sum_{a \in \Ar} \H_{\nu^*}(U_a) - \H_{\nu^*}(\U).
    \]
    Substituting this into our previous expression, we have:
    \begin{align*}
        &\SIMInc_{\Ar}(\mu) - \IDef_{\Ar}(\mu) 
        \\
        &\ge - \Big(\I_{\nu^*}(\X; \U) + \sum_{a \in \Ar} \H_{\nu^*}(U_a) - \H_{\nu^*}(\U) \Big)
            + \H_\mu(\X) - \H_{\nu^*}(\U) + \sum_{a \in \Ar} \H_{\nu^*}(U_a)
        \\
        &= \H_\mu(\X) - \I_{\nu^*}(\X ; \U)
        % \\
        % &= \H_{\nu^*}(\X) - \I_{\nu^*}(\X ; \U)
        \\
        &\ge 0. 
        %\qedhere
    \end{align*}
    The final inequality holds because of our assumption that the marginal $\nu^*(\X)$ equals $\mu(\X)$. 
    Thus, $\SIMInc_{\Ar}(\mu) \ge \IDef_{\Ar}(\mu)$, as proimised.

    We now turn to the right hand inequality, and part (b) of the theorem. 
    Recall that $\nu^*$ is defined to be a minimizer of the optimization problem defining $\SIMInc$. 
    For the right inequality $(\SIMInc_{\Ar}(\mu) \le \IDef_{\!\Ar^\dagger}(\nu))$ of part (a), observe that
    \begin{align*}
        \IDef_{\!\Ar^\dagger}(\nu)
            &=
                - \H_\nu(\X, \mathcal U)
                + \sum_{a \in \Ar} \H_{\nu}(U_a)
                + \sum_{a \in \Ar} \H_\nu(\Tgt a | \Src a, U_a)
                + \H_\nu(\X \mid \mathcal U) \\
            &= \Big( - \H_\nu(\mathcal U) + \sum_{a \in \Ar} \H_{\nu}(U_a) \Big)
            + \sum_{a \in \Ar} \H_\nu(\Tgt a | \Src a, U_a)
            % \\
            % &= \Big( - \H_\nu(\mathcal U) + \sum_{a \in \Ar} \H_{\nu}(U_a) \Big)
            % + \sum_{a \in \Ar} \H_\nu(\Tgt a | \Src a, U_a)
            %     + \underbrace{\kldiv{\nu(\X)}{\mu(\X)}}_{=0 \text{ for this choice of $\nu$ }}
            % &= \SIMInc_{(\Ar,\mat 1)} (\mu),
            \\&\ge \Big( - \H_{\nu^*}(\mathcal U) + \sum_{a \in \Ar} \H_{\nu^*}(U_a) \Big)
            + \sum_{a \in \Ar} \H_{\nu^*}(\Tgt a | \Src a, U_a)
            \\
            &= \SIMInc(\mu).
    \end{align*}
    % Let $Q(\nu,  \Ar)$ denote the final quantity on the right. 
    % Since $\SIMInc_{\Ar}(\mu)$ is defined to be $\inf_{\nu'} Q(\nu', \Ar)$, it must be at most $Q(\nu, \Ar)$ for this particular choice of $\nu$.  
    % Therefore
    % $ \IDef_{\!\Ar^\dagger}(\nu)  = Q(\nu,\mu,\Ar) \le \SIMInc_{\Ar}(\mu)$, 
    This proves the right hand side of the inequality of part (a). 
    Moreover, because the one inequality holds with equality when $\nu = \nu^*$ is a minimizer of this quantity (subject to having marginal $\mu(\X)$) we have shown part (b) as well.

    %oli23: cut
    \commentout{
    modulo the final claim about the size of the variables $\U$.
     To address that final missing piece, we claim that any minimizer $\nu(\U, \X)$ may be converted to another minimizing distribution $\hat \nu(\hat \U, \X)$ over the variables $\hat \U = \{ \hat U_a \}_{a \in \Ar}$, where $\V(U_a) = \{ \text{functions } \V(\Src a) \to \V(\Tgt a) \}$.
    % according to the transformation $\hat U_a = \Tgt a$. 
    To do this, we 
    
    \TODO
    }
    % Part (b). 
    % 
    % because $\nu(\X) = \mu(\X)$
\end{lproof}




%oli22*: moved full section to the appendix
% \subsection{Monotonicity of Structural Compatibility}
%oli3: rebranding section
% \section{Structural Equivalence and Monotonicity}
%oli6:
% \section{Reasoning about \SCibility}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%oli22: rebranding, for appendix
% \section{Monotonic Reasoning about \SCibility}
\section{Monotonicity and Undirected Graphical Models}
    \label{sec:monotone}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%joe17*: I haven't read any of the appendics.  I know that you really
%like this proof and find it hard to cut, but I would cut this subsection and the
%monotonicity section.  You're making the paper worse by keeping it!
%Please check with Spencer.
    \label{appendix:undirected PGMs}


%oli4: should I add eeven more motivation? I feel it distracts from the current story, but it still might help to motivate. Maybe the below should go elsewhere...
%joe5*: You should definitely add motivation.  But motivating this
%section by an appeal to cyclic models, which you haven't spent any
%time discussing in this paper, is not good at all.  The fact that
%they're complicated might say more about our approach than about
%cyclic models.  In any case, this definitely does not count as good
%motivation 
\commentout{
 The next two sections build up some theory that is necessary to understand the implications of our definition of \scibility\ for cyclic models---which turn out to be rather complicated. 
 }
%
\commentout{
    There are simple rules for manipulating
    %oli6: "quantiative" is not what I want to emphasize here
     % quantiative
    PDGs,
    %joe5: What is a proof language?  
    %oli6: maybe you'd be more comfortable calling it an axiom system? 
    % that allow them to be used as a proof language. 
    % that allow them to be used as an axiomatic proof system. 
    which can be used as an axiomatic proof system. 
}
%oli6*: wow, you were right! Now that equivalence material has been cut, the story becomes easier to state and sell. 
%oli6:* Why do I struggle so much to prune connections? Why is everything so strongly connected in my head? 
\commentout{
    %joe5: confidence is coming out of the blue here.
    %oli6: I was trying to be extra accurate, but I'll just suppress that thought.
    % Holding beliefs $p(X)$ and $p(Y|X)$ (with the same confidence),
    Holding beliefs $p(X)$ and $p(Y|X)$, 
    for example, is observationally equivalent to holding a single belief $p(X,Y)$ or beliefs $p(Y)$ and $p(Y|X)$.
}
%joe5: The reader will have no clue of what "nmonotocity of inconsistency" is.
%oli6: That's why I'm describing what it is.  And then I give a reference. You definitely aren't expected to know what "monotonicity of inconsistency" is here. To clarify, I am moving the reference to the beginning of the sentence, from the end.
% Monotonicity of inconsistency is a more powerful reasoning principle: many important statistical relationships (e.g., the data processing inequality and the evidence lower bound) can be proved directly, using only the fact that more beliefs cannot decrease inconsistency, and equivalences like the one above \citep{one-true-loss}.
%
%oli21*: cutting this down, for neurips submission
\vfull{%
Monotonicity of PDG inconsistency \citep{one-true-loss}
%joe16: all these inequalities need a reference
is a powerful reasoning principle. Many important inequalities (e.g., the data processing inequality, relationships between statistical distances, the evidence lower bound, \ldots) can be proved using only a simple inference rule: ``more beliefs can only increase inconsistency''.
In this section, we develop and apply an anlogous principle for \scibility.
But first, we start with something simple.
}
%oli21: instead:
%joe17
%Monotonicity of (quantitative) PDG inconsistency
The fact that (quantitative) PDG inconsistency  is monotonic
    % \citep{one-true-loss}
is a powerful reasoning principle that can be used to prove many important inequalities \citep{one-true-loss}. 
In this section, we develop a related principle for \scibility.
%joe4*: I have no idea what's going on in this section.  At one point I
%sent you an email explaining why I thought nonpartitional models
%weren't good models of causality.  I'm not convinced by the
%connection to models with constraints at all.   It might be better to
%restrict this section to partional models, where causality is well
%understood, and cut this subsection altogether.
%oli5: You are right not to be convinced by this subsection, which isn't very precise right now. Your suggestion to remove this section has been noted, and I'm not a priori opposed to it, but after reading your paper on causal models with constraints more carefully last week, I'm even more certain than I was before, that there's a deep connection here. (But I wasn't able to formulate "the one appropriate thing to prove", in part because your paper isn't clear about how the concept interacts with probabilities.) I think it's still worth saying a few words if we have space.
%
%oli6*: moved SIM equivalence to appendix for UAI paper
%
%
% Here is an alternate, equivalent characterization of this definition
% in terms of de-randomization.
%
% \begin{theorem}
%     $\mu(\X)$ is s-compatible with $\Ar$ if an only if
%     there exists a variable $U$, an indexed set of functions
%     $\mathcal F = \{ f_a : \V\Src a \times \V U \times \V \Tgt a\}_{a \in \Ar}$,
%     and an extended distribution $\nu(\X, U)$ such that
% \begin{enumerate}[label={\rm(\alph*')}]
%     \item $\mu(\X) = \nu(\X)$;
%     \item $\nu$ has support only on ``consistent pairs''
%         $(\omega, u) \in \V(\X \times U)$
%         such that
%         $f_a(\Src a (\omega), u) = \Tgt a(\omega)$
%         for every \arc\ $a \in \Ar$;
%
%     \item For every $a \in \Ar$, the pair $(f_a, \nu(U|\Src a))$
%         is a derandomization of $\mu(\Tgt a | \Src a)$.
%
%     \item
%         for every subset $A \subseteq \Ar$ of \arc s,
%         and every fixed choice of settings
%         $\mat{st} = \{ (s_a, t_a) \in \V(\Src a, \Tgt a) \}_{a \in A}$,
%         the set of events
%         \[
%             \Big\{
%                 \{ u \in \V U : f_a(s_a, u) = t_a \}
%                 ~\Big|~a \in \Ar \Big\}
%         \]
%         are mutually independent according to $\nu(U)$.
% \end{enumerate}
% \end{theorem}
%
%oli6: no longer need subsection here
% \subsection{Monotonicity}
%joe1*: I know that you won't like to hear this, but I found this
%section extremely weak.  At the end of the day, what have I learned?
%That \scibility\ is monotonic in one sense (adding
%certain edges) and anti-monotonic in another )Theorem 4) and that you
%can't have monotonicity and there's a strange, unmotivated sense in
%which you can't capture BNs and MRFs with a monotonic definition of
%|=.  While monotonicity seems like a nice property, it's not that
%compelling (particularly since you consider in some sense "positive"
%and "negative" version of it.  I see no reason why I should care
%about Theorem 5, which seems to be the punchline of this section.
%Unless you can come up with a much better story and motivation, I
%think we should cut this too.  To repeat, the paper is not a place to
%do a core dump of all the things you thought about.  I think this
%section weakens the paper.  I put in some comments in the rest of the
%section, but my strong preference would be  to cut it.
%
%oli1: After our discussion, we had a plan for how to proceed here.
% At a high level, though, let me respond to your concerns above
% in writing.
%
%    First, there's only one sense of monotonicity. The fact that it's
% covariant in outputs and contravariant in inputs is extremely
% good news given that functions and mappings work.  Inputs and 
% outputs play opposite roles. 
%    Second, this will be useful for a running example of a 3-cycle, 
% which is one of the things we're buliding towards, as described in the
% introduction. 
%    Finally, the last theorem (5/6) is more important than it looks
% at first. The "positive MRF capture" and "negative BN captue" are
% weaker than "capturing MRFs" and "capturing BNs", so these words
% actually strengthen the result, not weaken it. 
% The high-level of the theorem is: there's no "natural" (in particular,
% monotonic) definition of \scibility\ that captures both
% BNs and MRFs.
%
%
%oli7: breaks the flow; a good place to cut 
\vfull{
One classical representation of knowledge is a list of formulas
$[ \phi_1, \phi_2, \ldots, \phi_n]$
that one knows to be true.
This representation has a nice property:
learning an additional formula $\phi_{n+1}$
can only narrow the set of worlds one considers possible.
    % after including an additional formula $\phi_{n+1}$
    % % has the effect of strengthening knowledge:
    % one can still prove anything that one could prove before.  
    % (as do many standard representations of knowledge \citep{rak}).
%oli1: although I like this flourish, I believe I agreed
% to remove it in order to prevent the story from seeming to undercut
% itself if one doesn't read carefully.
%
% Qualitative Bayesian Networks (and MRFs) are monotonic,
% 	but in the opposite direction.
% 	One might be tempted to think that an edge $A{\to}B$
% 	in a BN represents knowledge (that $A$ influences $B$).
% 	But in fact it is the \emph{lack} of an edge that caries knowledge:
% 	$A{\not\to}B$ means that $A$ does not influence $B$ directly.
%joe1: I don't see why this is exactly right, but in any case, it's a
%distractin.  I cut it.
% 	Thus, a BN is more like a list of formulas that one considers possible,
% 	than a list of formulas one knows.
%oli1: to help you see why the above is true: an edge in a BN indicates the possibility of dependence, not the necessity of dependence; if all variables are independent, it is consistent with every BN structure. 
%   \Scibility, however, is monotonic in the direction
%   we expect of a representation of knowledge.
%
%oli1: replacing it with the following, condensed version:
The same is true of \scibility. 
%joe2*: this does not seem consistent with your English.  It does
%*not* say that anything you can prove from A' can also be proved from
%A.  It says that if all the formulas in A are true according a
%probability distribution \mu, then so are the formulas in a subset.
%This is quite a different statement.  
%oli2: Good point. The English I had and the proposition are (classically)
% equivalent, but not directly so; they're logical duals. I'll rewrite the
% English to try to make it more direct.  
% 
% In this analogy, \Ar' corresponds to [\phi_1, ... \phi_n], not 
% to the extension to \phi_{n+1}.  Also, one should not think of \Ar
% as a collection of formulas, because mechanisms interact. Just
% because \mu \models { a } for all a \in \Ar (which is always true),
% does not allow us to conclude that  \mu \models \Ar.
%
}%
%oli21: cutting proposition from condensed version
\vfull{%
\begin{prop}
        % [monotonicity]
        \label{prop:mono}
    % If $\Ar' \subseteq \Ar$, then
    %     $\bbra{\Ar} \subseteq \bbra{\Ar'}$.
    %oli2: changing the order so that \Ar
    % If $\Ar' \subseteq \Ar$ and $\mu \models \Ar$, then $\mu \models \Ar'$.
    % If $\Ar \subseteq \Ar'$ and $\mu \models \Ar'$, then $\mu \models \Ar$.
    If $\Ar \subseteq \Ar'$ and 
    %oli20: adding diamonds
    $\mu \models \Diamond\Ar'$, then $\mu \models \Diamond\Ar$.
\end{prop}
}%
%oli21: instead, a short inline discussion. 
Here is a direct but not very useful analague: if $\Ar \subseteq \Ar'$ and $\mu \models \Diamond\Ar'$, conclude $\mu \models \Diamond\Ar$. 
%
% This is an important requirement for any definition of mechanism independence.  If
% $\mu$ is consistent with a causal picture consisting of some set of independent mechanisms, then surely it must be consistent with a picture in which only a subset of those mechanisms are independent.  
%oli20: many cuts to shave a line
% This should be intuitive: if
%oli21: 
After all, if 
% This is intuitive: if
$\mu$ is consistent with a set of independent causal mechanisms, then surely 
%oli20: 
% it must be
it is
consistent with a causal picture
%oli20:
in which
% where
only a subset of those mechanisms 
%oli20:
% must be
are
present and independent.  
%oli21: bringing up from below. (material in brackets). 
{%
There is a sense in which 
%oli21: compress by using the acronyms
    % Bayesian Networks and Markov Random Fields are also monotonic,
    BNs and MRFs are also monotonic,
%oli21:
    % but in the direction opposite to \cref{prop:mono}:
    but in the opposite direction:
    adding edges to a graph results in a weaker
    %oli21:
    % statement about independence.
    independence statement.
%oli21: adding
    We will soon see why.
}%

% When we translate a DAG to a hypergraph,
% In compiling a DAG to a \hgraph\ (see \cref{sec:bns}),
%     adding more edges in the DAG corresponds to adding more tails to an \arc\ in the corresponding hypergraph.

%oli1:
% For directed edges, there is a finer notion of monotonicity at play.
%oli2:
% Since a hypergraph has directed \arc s,
%oli5: compressing with wrapfigure
\begin{wrapfigure}[5]{o}{1.6cm}
    % \fbox{
    \vspace{-3ex}
    % \hspace{-1.2em}
    \begin{tikzcd}[row sep=3ex,column sep=0.2em]
        A \ar[rr,"f"] &\ar[d,squiggly]& B\ar[d,hook,gray]  \\
        A'  \ar[u,hook,gray]\ar[rr, dashed]&{\vphantom{a}}& B'
    \end{tikzcd}
    % }
\end{wrapfigure}
%joe5: this is somewhat circilar
%oli6: not anymore, since I changed \hgraph back to "hypergraph"---but your version is still better, I think.
% Since our notion of a \hgraph\ has \emph{directed} \arc s,
Since we use \emph{directed} hypergraphs,
     there is actually a finer notion of monotonicity at play. 
%oli1: added
Inputs and ouputs play opposite roles, 
    and they are naturally monotonic in opposite directions. 
%oli3: out of order; this should come later.
% Consider a function $f : A \to B$.
%joe1*: I have no idea what subtype means, and I'm sure that a typical
%reader won't either.  We can't read your mind!
%oli1: it means a value of B can be implicitly converted to a value of B'. I'll change the language.
% If $B$ is a subtype of $B'$ (written $B \sqsubseteq B'$), then $f$ can be used as a function from $A$ to $B'$.
%oli3: "implicitly convert" -> "regard" for brevity
% If there is an obvious way to implicitly convert an element $B$ to an element of $B'$ (abbreviated $B \,{\color{gray}\hookrightarrow}\, B'$)
If there is an obvious way to regard an element of $B$ as an element of $B'$ (abbreviated $B \,{\color{gray}\hookrightarrow}\, B'$),
and $A' \,{\color{gray}\hookrightarrow}\, A$, 
then
%oli3: heres's where it should be
% $f$ can be used as a function from $A'$ to $B'$.
a function $f : A \to B$ can be regarded as one of type $A'\to B'$.
%joe1*: you've totally lost me here.  I think that this hurts far more
%than it helps.  I just cut it.
%oli1*: I think the fact that inputs work the opposite ways is actually
% the punchline. I've shortened it and moved it above.
% (Anti)symmetrically, if $A' \sqsubseteq A$, then $f$ can be used as a funciton from $A'$ to $B$
%oli1: shortening
% (This may be easier to see in the diagrammatic construction below, where ${\color{gray}\hookrightarrow}$ indicates the subtype inclusion.)
%oli5:
% This is depicted below.
%joe5: I don't find the figure particularly helpful.  I would cut it.  
%oli6: I like figures and commutative diagrams, and I find things like this very helpful. I know for a fact that there are a lot of people like me who pay more attention to diagrams and figures and equations, than they do to the text.   That said, I agree it was using a lot of space, so I found another fix for that. 
This is depicted to the right.
%oli5: moved the diagram above
% \[
% %oli1: don't need the first part of the equation; the commutative diagram is enough.
% % \begin{array}{l}
% %     A' \sqsubseteq A, \quad B \sqsubseteq B' \\
% %     \Rightarrow \quad (A \to B) \sqsubseteq (A' \to B').
% % \end{array}
% % \qquad
% \begin{tikzcd}[row sep=2ex,column sep=1em]
%     A \ar[rr,"f"description] &\ar[d,squiggly]& B\ar[d,hook,gray]  \\
%     A'  \ar[u,hook,gray]\ar[rr, dashed]&{\vphantom{a}}& B'
% \end{tikzcd}
% \]
%oli5: merging paragraphs
%
% These principiles of function variance also apply in our setting.
The same principile applies in our setting.
%oli1:
% have a natural analogue  in our setting, which we call structural weakening.
If $\mat X$ and $\mat Z$ are sets of variables and $\mat X \subseteq \mat Z$, then 
%oli3:
% there is an obvious way to implicitly convert a $\mat z \in \V(\mat Z)$ to 
%oli5:
% there is an obvious way to implicitly regard $\mat z \in \V(\mat Z)$ to 
% as an element of $\mat x = \mat z[\mat X] \in \V(\mat X)$: restrict $\mat z$ to the coordinates of $X$.
$\V(\mat Z) {\color{gray}\hookrightarrow} \V(\mat X)$, by restriction. 
%oli5: shorter
% It follows from the reasoning above that 
It follows, for example, that
    any mechanism by which $X$ determines $(Y,Y')$ can be viewed
        as a mechanism by which $(X,X')$ determines $Y$. 
%oli3: too verbose
% This allows us to conclude that some sets of
%    (independent) mechanisms imply the existence of others.
The general phenomenon is captured by the following
%oli5: save a line
    %\unskip definition.
    \unskip.
% Here, the relevant notion of subtype is not set inclusion, but
%     rather variable superset.
% Thus, the behavior of our definition follows the appropriate variance
% of function typing: a function $f : A \to B$ can be regarded as a function
%     from $A'$ to $B'$ if $A' \subseteq A$ and $B' \supseteq B$.

%joe5*: This is a definition of "weakening", not structural
%weakening.  Why do you keep insisting on adding the word "structural"
%everywhere.  It's certainly not an improvement
%oli6: ok, I'm willing to take "structural" out of the name
% \begin{defn}[structural weakening]
\begin{defn}
    % [weakening]
        \label{defn:weakening}
    % Consider two sets of \arc s, $\Ar = \{ \Src a \to \Tgt a\}_a$
    %     and $\Ar' = \{ \Src a{\!}' \to \Tgt a{\!}'\}_a$.
    % If $\Ar' = \{ \Src a{\!}' \to \Tgt a{\!}' \}_{a \in \Ar}$
    % Consider a collection of \hyperarc s
    %oli21: cleaning up this definition + cutting some lines. 
    \commentout{%
        Consider a directed hypergraph
        $\Ar = \{ \Src a \to \Tgt a \}_{a \in \Ar}$.
        If another graph $\Ar'$ can be obtained by adding sources to and removing targets from the \arc s of $\Ar$---that is, if $\Ar' = \{ \Src a{\!}' \to \Tgt a{\!}' \}_{a \in \Ar}$, with $\Tgt a{\!}' \subseteq \Tgt a$ and
         $\Src a{\!}' \supseteq \Src a$ for all $a \in \Ar$---then we
         say $\Ar'$ is a \emph{weakening} of $\Ar$ and
         write $\Ar \rightsquigarrow \Ar'$.
     }%
     % If $\Ar = \{ \Src a \to \Tgt a \}_{a \in \Ar}$,
     % $\Ar' = \{ \Src a{\!}' \to \Tgt a{\!}' \}_{a \in \Ar'}$,
     If $\Ar = \{ \ed aST  \}_{a}$, 
     $\Ar' = \{ \ed{a'}{S'}{T'} \}_{a'}$,
     % are hypergraphs,
     and there is an injective map $\iota : \Ar' \!\to\! \Ar$
     such that
     $\Tgt {a}{\!}' \subseteq \Tgt {\iota(a)}$ and
     $\Src {a}{\!}' \supseteq \Src {\iota(a)}$
     for all $a \in \Ar'$,
     % (that is, $\Ar$' is adding sources removing targets from the \arc s of $\Ar$), 
     then $\Ar'$ is a \emph{weakening} of $\Ar$
     % and we write $\Ar {\rightsquigarrow} \Ar'$.
     % ?and we write 
     (written $\Ar {\rightsquigarrow} \Ar'$).
\end{defn}

%oli21: this whole thing is unnecessary, and too expensive.
\vfull{
%joe16: no need
%Indeed, 
\scibility\ is monotonic with respect to weakening  ($\rightsquigarrow$).}
    
\begin{theorem}
    % [strong monotonicity]
        \label{theorem:strong-mono}
    % If $\Ar' = \{ \Src a{\!}' \to \Tgt a{\!}' \}_{a \in \Ar}$
    % is obtained by adding heads and removing tails
    % from each \arc\ of $\Ar$ (i.e., $\Tgt a{\!}' \supseteq \Tgt a$ and $\Src a{\!}' \subseteq \Tgt a$ for all $a \in \Ar$),
    % then
    % $\bbra{\Ar'} \subseteq \bbra{\Ar} $.
    If
    % $\mu$ is \scible\ with $\Ar$
    % $\mu \models \Ar$ and $\Ar \rightsquigarrow \Ar'$, then
    $\Ar \rightsquigarrow \Ar'$ 
    %oli9: strengthening. Not sure if English or notation is better here,
    % but I chose English because of the parens. If it is unclear to 
    % present the words in parens like this, then I think we should
    % go back to the symbolic notation. 
    and $\mu \models \Diamond \Ar$, then  $\mu \models \Diamond \Ar'$.
    %oli21: back to notation; adding diamonds. 
    \commentout{%
        and $\mu$ is 
        % (E)
        \scible\ with $\Ar$,
        then $\mu$ is also 
        % (E)
        \scible\ with $\Ar'$. 
        % $\mu$ is \scible\ with $\Ar'$
    }%
\end{theorem}

%oli21: not critical; streamlining. 
\vfull{%
\Cref{theorem:strong-mono} is strictly stronger than \cref{prop:mono}
because a \arc\ with no targets is vacuous, so
removing all targets of a \arc\ is equivalent to deleting it.
}%

%oli3: relocating this material here, from above the theorem. 
%oli21: moving this material above
% There is a sense in which 
%     BNs and MRFs are also monotonic,
%     but in the direction opposite to \cref{prop:mono}:
%     adding edges to a graph results in a weaker statement about independence.
% When we view a graph $G$ as specifying a directed hypergraph
%     $\Ar_G$ (as described in \cref{sec:bns}),
%     adding an edge to $G$ amounts to adding a tail to
%     a \hyperarc\ already present in $\Ar_G$.
% Thus \cref{defn:weakening} corresponds to 
% As mentioned before, this is structural weakening captures
% the way in which Bayesian Networks and Markov Networks
% are (anti)monotonic: if $G = (\X, E)$ and $G' = (\X, E')$
% are graphs with the same set of vertices and $E \subseteq E'$,
% then $\mathcal I(G) \supseteq \mathcal I(G')$,
% and so $\mu \models \mathcal I(G)$ implies $\mu \models \mathcal I(G')$.
%oli3:
% \cref{defn:weakening,defn:graph-to-hypergraph}
% captures the sense in which Bayesian Networks are (anti)monotonic.
%oli6:
% \Cref{defn:weakening} provides an explanation: the effect of adding an edge $X \to Y$ to a directed graph $G$ on $\Ar_G$, is to add $X$ to the sources the \arc\ that determines $Y$.
%oli21:
% \Cref{theorem:strong-mono} provides an explanation: adding $X \to Y$ to a graph $G$ means adding $X$ to the \emph{sources} the \arc\ whose target is $Y$, in the \hgraph\ $\Ar_G$.
\Cref{theorem:strong-mono} explains why BNs and MRFs are arguably \emph{anti}-monotonic: adding $X \to Y$ to a graph $G$ means adding $X$ to the \emph{sources} the \arc\ whose target is $Y$, 
%oli21: line shave
% in the \hgraph\ $\Ar_G$.
in $\Ar_G$.
%oli3: if I did my job before definition 5 properly, I will have already made this point.
% This can only weaken the corresponding statement of (in)dependence, because one can always throw out this new source information and use the old mechanism.
%oli6: merging paragraphs, combining subjects
%
%oli1: overly verbose.
%   Now that we know that
%     \scibility\ 
%     (\cref{defn:scompat})
%     is structurally monotonic, we have the tools
%     we need to at least partially understand more examples,
%     and cyclic graphs in particular.
% Structural montonicity is a helpful tool for understanding cyclic graphs.
%oli6:
% Structural montonicity is a helpful tool for understanding \scibility\ with respect to cyclic graphs.
%oli6.2: May as well make coreference easier, since it doesn't cost us a line
% It 
%oli21: 
% \Cref{theorem:strong-mono}

%oli22:
As mentioned in the main body of the paper, 
the far more important consequence of this result is that it
%oli16:
% is also helpful for understanding \scibility\ with cyclic \hgraph s.
helps us begin to understand what \scibility\ means for cyclic \hgraph s.
%oli22:
For the reader's convenience, we now restate the examples in the main text,
which are really about monotonicity..

\medskip
\textbf{\cref{example:xy-cycle}.}
% \begin{example}
        % \label{example:xy-cycle}
    Every $\mu(X,Y)$ is
    %oli21: removing a QIM
    % \scible\ with
    \cible\ with
    \begin{tikzpicture}[center base]
        \node[dpadinline] (X) {$X$};
        \node[dpadinline,right=1.2em of X] (Y) {$Y$};
        \draw[arr1] (X) to[bend left=13] (Y);
        \draw[arr1] (Y) to[bend left=13] (X);
    \end{tikzpicture}.
    This is because this cycle is weaker than
    % a set of \arc s
    a \hgraph\ 
    that can already represent any distribution, i.e.,
    %oli4: no more displaymode, to save a line
    % \[
    \begin{tikzpicture}[center base]
        \node[dpadinline] (X) {$X$};
        \node[dpadinline,right=1.0em of X] (Y) {$Y$};
        \draw[arr1] (X) to (Y);
        \draw[arr1,<-] (X) to +(-0.68,0);
    \end{tikzpicture}
    % ~~\rightsquigarrow~~
    $~\rightsquigarrow~$
    \begin{tikzpicture}[center base]
        \node[dpadinline] (X) {$X$};
        \node[dpadinline,right=1.2em of X] (Y) {$Y$};
        \draw[arr1] (X) to[bend left=13] (Y);
        \draw[arr1] (Y) to[bend left=13] (X);
    \end{tikzpicture}~. \qedhere
    % \]
    % $\to X \to Y$.
% \end{example}
\hfill$\triangle$.
\medskip

% \newlength{\cycleboxlen}
\settowidth{\cycleboxlen}{\usebox{\cyclebox}}
\begin{wrapfigure}[5]{o}{0.8\cycleboxlen}
    \vspace{-0.8em}
    % \usebox\cyclebox
    \begin{tikzpicture}[center base, scale=0.8]
        \node[dpad0] (X) at (0:.8) {$X$};
        \node[dpad0] (Y) at (120:.8) {$Y$};
        \node[dpad0] (Z) at (-120:.8) {$Z$};
        \draw[arr2] (X) to 
            % node[above right]{\small 1}
            (Y);
        \draw[arr2] (Y) to
            % node[left]{\small 2}
            (Z);
        \draw[arr2] (Z) to 
            % node[below]{\small 3}
            (X);
    \end{tikzpicture}
\end{wrapfigure}
% \begin{example}
% \refstepcounter{example}
% \label{example:xyz-cycle-1}
\textbf{\cref{example:xyz-cycle-1}.~} 
    % \newlength{\cycleboxlen}
    % \settowidth{\cycleboxlen}{\usebox{\cyclebox}}
    % \begin{wrapfigure}{o}{\cycleboxlen}
    %     \usebox\cyclebox
    % \end{wrapfigure}
    % Consider the set of \hyperarc s
    % to the right.
    What
    %oli21:
    %  distributions
    $\mu(X,Y,Z)$
    %oli5:
    % $\mu(A,B,C)$
    %oli21: removing a QIM 
    % are \scible\ 
    are \cible\ 
    with 
    %oli5: compressing, by putting to the right
    % \[
    % \Ar_{\text{3-cycle}} := 
    %     % \Ar_{3\bigcirc} :=
    %     \usebox\cyclebox\qquad ?
    % \]
    %oli21: 
    % the hypergraph $\Ar_{\text{3-cycle}}$, on the right? 
    % By monotonicity, we know that 
    the 3-cycle shown, on the right?
    By monotonicity,
    % \[
    %
    % \]
    %oli21: fixing several issues, and compressing
    % the chain $\to X \to Y \to Z$, and all permutations of it,  
    % are \scible\ with this graph. Thus,
    among them must be all distributions consistent with a linear chain ${\to}X{\to}Y{\to}Z$. Thus,  
    %oli21:
    % any distribution $\mu(X,Y,Z)$
    %joe17
        %    any distribution
        %spencer3: I think ``any distribution'' is what we want to say here. Reverting.
    % a distribution
        any distribution
    in which two variables are conditionally independent given the third
    is compatible with 
    the 3-cycle.
    %oli1: replacing long attempt to finish example at a high level, with
    % this sentence:
    Are there any distributions that are \emph{not} compatible with 
    %oli20:
    % this graph? It is not obvious.
    this hypergraph? It is not obvious.
    We return to this
   %oli21: line shave
    %  question
      in \cref{sec:pdgs}. 
    %
    %
    % By randomly selecting distributions $\Pr(U_1),
    % \Pr(U_2)$, and $\Pr(U_3)$ (see \cref{sec:null}), one finds that the set of distributions that are consistent with this 3-cycle has larger dimension than the set of distributions that factorize according to $\Pr(X,Y,Z) \propto \phi_1(X,Y) \phi_2(Y,Z) \phi_3(Z,X)$.
    % Thus, our definition does not coincide with the corresponding factor graph.
    %
    %oli1: removing the final paragraph, which is an unnecessarily verbose
    % forward pointer. 
    %
    % But what about negative examples? Are there any distributions that
    % are \emph{not} compatible with the cycle?
    % It seems intuitively clear that the XOR distribution $\muxor$ (in which $X,Y$ are independent random coin flips, and $Z = X \oplus Y$) cannot arise from mechanisms of this shape.
    % But it is not so obvious how to prove this.
    % We will return to this example once we have a few more tools available.
% \end{example}
\hfill$\triangle$

\begin{wrapfigure}[5]{i}
        % {1.7cm}
        {1.5cm}
    \vspace{-1em}
    \centering
    \begin{tikzpicture}[center base]
        \node[dpad0] (A) at (-0.6,0) {$A$};
        \node[dpad0] (B) at (0,1) {$B$};
        \node[dpad0] (C) at (0.6,0) {$C$};

        \draw[arr2,<-] (A) -- (B);
        \draw[arr2,<-] (C) -- (B);
        \mergearr[arr2] ACB
    \end{tikzpicture}
\end{wrapfigure}
% \section{BNs, MRFs, and Structural Compatibility}
% Initially, our
% We were quite surprised to discover that this definition,
% which is fundamentally a much better match to directed models,
% goes a long ways for undirected models as well.
%joe1*: In what sense can we "handle" cycles.
%oli1: I just mean that our definition applies for cylcic graphs.  I'll try to change the wording to clarify.
% Now that we can handle cycles, it is natural to wonder whether our definition can capture undirected models.
% Again by monotonicity, it is easy to see that the complete graph 
%oli21: compressing
% Because our definition of \scibility\ applies to cyclic models, it is natural to wonder whether 
Because \scibility\ applies to cyclic structures,  one might wonder if
    it also captures the independencies of undirected models 
    %oli6: not helpful here, and breaks the flow
    % (i.e., Markov Random Fields).
    \unskip.
%joe5: You have to explain what cyclic models have to do with MRFs
%oli6: one moment...
%oli1
% In many contexts, it is standard to regard 
%oli2:
% It is common to identify an undirected edge $A {-} B$
Undirected edges $A {-} B$ are commonly identified
%oli6: answering your question above:
% with a pair of directed edges $\{ A{\to}B, B{\to}A\}$,
with a (cylic) pair of directed edges $\{ A{\to}B, B{\to}A\}$,
%joe5: ?? Where did you do this in Equation (2):
%oli6: Equation (2) is the conversion of graphs to directed hypergraphs; looking at it closely should reveal why it's implicitly identifying an unorded edge with a pair of directed edges in each direction.
as we have implicitly done in
%oli21: 
% \cref{defn:graph-to-hypergraph}.
defining $\Ar_G$. 
%joe5
%In this way, undirected graphs, too, naturally generate to directed hypergraph s.
In this way, undirected graphs, too, naturally correspond to directed hypergraph s.
For example, 
%oli21: shortening
% the undirected graph
$G = A{-}B{-}C$
%joe5
%results in the directed hypergraph\
%oli21: 
% corresponds to the directed hypergraph\
corresponds to the \hgraph\ 
%oli6: compressing to save space
\commentout{
\[
    \Ar_{G} = \Bigg\{
            \begin{array}{r@{}l} \{B\}&{\to}\{A\},\\ \{A,C\}&{\to}\{B\},\\ \{B\}&{\to}\{C\}
            \end{array}\Bigg\}
    =     \begin{tikzpicture}[center base]
        \node[dpad1] (A) at (-0.6,0) {$A$};
        \node[dpad1] (B) at (0,1) {$B$};
        \node[dpad1] (C) at (0.6,0) {$C$};

        \draw[arr2,<-] (A) -- (B);
        \draw[arr2,<-] (C) -- (B);
        \mergearr[arr2] ACB
    \end{tikzpicture}
    ~.
\]
}
%oli6:
$\Ar_G$ shown on the left.
% 
%joe5: I don't know what "match" means.  This needs to be explained
%better.  If you want to say something about the set of distributions
%compatible with the resulting hypergraph, say that. 
%oli6: ok
    % Applying \cref{defn:scompat} to the resulting \hgraph\ $\Ar_G$, however,
    % does not match any of the three standard Markov properties
    %     which define semantics for MRFS \citep{lauritzen}.
%oli21: cutting QIM
% \Scibility\
Compatibility
with $\Ar_G$, however, does not coincide with any of the standard Markov properties
%oli6: can be simplified
 % for the MRF corresponding to $G$ \citep{lauritzen-mrf-indeps}.
corresponding to $G$ \citep{koller2009probabilistic}.
%oli15: adding, for clarity
%oli21: not helpful; removing. 
% as we will soon see.
%
%oli1:
% There is a sense in which this is unavoidable.  
%oli21:
% This may seem to be a flaw in our particular definition of \scibility\ (\cref{defn:scompat}), but it is unavoidable.  
This may appear to be a flaw in \cref{defn:scompat} (\scibility), but it is unavoidable.  
%oli3:
% While BNs and MRFs are both monotonic, 
%     it is impossible to monotonically capture
%     both Bayesian Networks (directed graphical models) and Markov Random Fields (undirected graphical models) with a single monotonic definition of \scibility.
%oli21: making use of abbreviations we've had all along to save some space
% While both Bayesian Networks (directed graphical models)
%     and Markov Random Fields (undirected graphical models)
%     are both monotonic on their own,
%     it is impossible to capure both BNs and MRFs monotoncially together.
While both BNs and MRFs are monotonic, it is impossible to capture both classes with a monotonic definition.
% The proof is a straightforward application of monotonicity.
% and one instance of a graphical proof technique.

% \begin{linked}{theorem}{mrf-bn-monotone-impossible}
\begin{theorem}\label{theorem:mrf-bn-monotone-impossible}
    % No monotonic definition $\bbr{-}$ of structural
    % compatibility that captures BN structures can also capture
     % with dag $D$ as $\bbr{\Ar_D}$,
     % also compatibility with an undirected graph $G$ as $\bbr{\Ar_G}$.
     %oli1: added \prime to \models in this definition throughout
    % No definition $\models$ of \scibility\ with a set of \hyperarc s can satisfy all three of the following:
    %oli2:
    % It is possible to give a definition  $\models'$ of structural compatibility 
    %oli9:
    % \def\newmodels{\models^{\mathllap{\cdot~~~}}}
    % \def\newmodels{\models^{\mathllap{\bullet~}}}
%joe5*: You need to discuss and motivate what you mean by a
%"compatibility" relation before you state the theorem.  This is
%coming out of the blue.
%oli6: I don't mean anything technical. I will remove the quotes and the word, for the theorem statement.
    % It is possible to define a ``compatibility'' relation  $\models'$ 
    It is possible to define a  relation  $\dotmodels$ 
    %oli5: saving a line, part 1:
        % between probability distributions $\mu$ and directed hypergraph s $\Ar$
        between distributions $\mu$ and directed hypergraph s $\Ar$
        % between a distribution and directed hypergraph
    satisfying 
    any two, but not all three, of the
    %oli5: ... and part 2.
    % following properties.
    following.
    \begin{description}[itemsep=0pt,parsep=0.3ex,topsep=0pt]
        \item [\rm(monotonicity)]
            % If $\Ar \Rightarrow \Ar'$ then $\bbr{\Ar} \supseteq \bbr{\Ar'}$.
            % If $\mu$ is \scible\ with $\Ar$
            If $\mu \dotmodels \Ar$
            and $\Ar \rightsquigarrow \Ar'$,
            then
             % $\mu$ is \scible\ with $\Ar'$.
            $\mu \dotmodels \Ar'$.
        \item [\rm(positive BN capture)]
            % For directed graphs $G$,
            % $\mu \in \bbr{\Ar_G}$ if and only if $\mu \models \mathcal I(G)$.
            %oli1:
            % If $\mu \models \mathcal I(G)$ for a directed graph $G$, then
            If $\mu$ satisfies the independencies
            %oli3:
            % of a BN $G$,
            %oli15: saving a line; also I don't like "implied"
            % implied by a directed acyclic graph $G$,
            $\mathcal I(G)$ of a dag $G$,
            then
                % $\mu$ is \scible\ with $\Ar_G$.
            $\mu \dotmodels \Ar_G$.
        \item [\rm(negative MRF capture)]
            % For undirected graphs $H$,
            % if $\mu \in \bbr{\Ar_G}$ then $\mu \models \mathcal I(G)$.
            % If $\mu$ is \scible\ with $\Ar_G$ for an undirected
            If $\mu \dotmodels \Ar_G$ for an undirected directed graph $G$,
            %oli2: oops, the undefined notation didn't get fixed
            % then $\mu \models \mathcal I(G)$.
            %oli3: missing citation
            % then $\mu$ has a Markov property with respect to $G$.
            then $\mu$ has one of the Markov properties 
                % \citep{lauritzen-mrf-indeps} 
                with respect to $G$.
    \end{description}
\end{theorem}


\label{proof:mrf-bn-monotone-impossible}
    %oli1: added to make it explicit
%joe5: is this what you meant
% a notion of
%oli6: yes, but I now removed it; see below.
%oli5: removed because I've now said it better above
% In particular, there's no way to define of \scibility\ 
%     that fully captures BNs and MRFs in a monotonic way. 
The proof is
a direct and easy-to-visualize application
% an easy-to-visualize application
of monotonicity (\cref{theorem:strong-mono}).
% {\renewcommand*{\proofname}{Proof Sketch}
% \begin{proof}
%oli21: expanding
Assume montonicity and positive BN capture. 
Let $\muxor(A,B,C)$ be the joint distribution in which 
%oli3: two variables
$A$ and $C$ are independent fair coins, and
$B = A \oplus C$ is their parity.
% Finally, let $G$ be the undirected graph $A{-}B{-}C$. 
%joe16
% Then, we have:
We then have:
%oli20: saving a line:
% \[
\\$
\muxor
\models
\begin{tikzpicture}[center base,scale=0.8]
    \node[dpadinline] (A) at (-0.6,0) {$A$};
    \node[dpadinline] (B) at (0,1) {$B$};
    \node[dpadinline] (C) at (0.6,0) {$C$};

    \draw[arr2,<-] (A) -- +(0,1);
    \draw[arr2,<-] (C) -- +(0,1);
    \mergearr[arr2] ACB
\end{tikzpicture}
~~\rightsquigarrow~~
\begin{tikzpicture}[center base,scale=0.8]
    \node[dpadinline] (A) at (-0.6,0) {$A$};
    \node[dpadinline] (B) at (0,1) {$B$};
    \node[dpadinline] (C) at (0.6,0) {$C$};

    \draw[arr2,<-] (A) -- (B);
    \draw[arr2,<-] (C) -- (B);
    \mergearr[arr2] ACB
\end{tikzpicture}
~=~ \Ar_{A{-}B{-}C}.
%oli20:
% \]
$
~~
But
$\muxor \not\models A \CI C \mid B$.
% \qedhere
% and so
% $\muxor$ does not satisfy any of the three
% markov properties with respect to the graph $A{-}B{-}C$.
% \end{proof}}
\hfill\qedsymbol
% \end{lproof}


%oli21*: moved the proof that was previously here, to the appendix. 
%
%joe3*: Perhaps you found it surprising, but I suspect that most
%readers won't appreciate the points you made.  I have no clue what
%the connection is between the first paragfraph and the result, let
%alone why it should make me feel the result is surprising.  The same
%goes for the second paragraph (although at least I understand better
%what it's saying).  If all readers were clones of you, this might
%interest them.  I strongly suspect that for most readers, these two
%paragraphs detract from the paper.
%oli3: ok, I'll try to distill these paragraphs to something that others can find more useful. 
% We found this result surprising, for two reasons. First, for an undirected graph, $\Ar_G$ is a consistent dependency network stucture \citep{heckerman2000dependency} that corresponds exactly to that Markov Network (for positive distributions), intuitively by placing independent mechanisms along each \arc. This is because dependency networks with a fixed sampling order only capture those BNs consistent with that order, and if the order changes depending on the graph, then the definition is not monotonic. 
%joe5
%We found this result surprising for a few reasons.
%First, if $G$ is an undirected graph then $\Ar_G$ is the structure of
%a consistent dependency network \citep{heckerman2000dependency}, and
%
%joe5*: this requires more explanation.  The "represents precisely" 
%presumably means "according to the semantics of DNs".  If that's
%what it means, you have to say so; if that's not what it means, then
%I'm confused and yet more explanation is needed.
%oli6: that's right; I've modified the below as you requested.
%joe16*: I feel very strongly that this should be cut.  It will
%confuse lots of readers.  You're basically saying to the (very small)
%set of readers who may be confused "here's why you shouldn't be
%confused", at the price of confusing many other readers, who didn't
%caref about this at all.  But not that, at the end of this section,
%you basically have one result, that is no where close to the results
%of other sections in terms of conceptual imporance.   I would cut this
%section altogether.  It makes the paper worse.
%oli21*: I see where you're coming from. I agree that this next part is nowhere near valuable enough to keep in the main paper at this point. I still think that the section itself is valuable though. It's not as valuable, but it's also shorter.  It will ultimately take up less than one page. 
%joe17*: I'm not talking about "valuable".  I'm saying that adding this
%to the paper makes it seem worse (i.e., less good from the point of
%view of the reader/reviewer).   We can debate its value (I don't
%think it's that high), but that's %not what I'm talking about.  I a
%giving you my judgment on what will make the paper more likely to be
%accepted at NeurIPS.  As is often the case, you think you know
%better, and I don't know how to convince you.  Please talk to Spencer.
%oli21*: adding replacement and link to appendix, where the technical discussion to un-confuse the very small subset of graphical-model  readers, will be at home. 
%joe17*: I feel strongly that this should be cut.  I don't even know
%what "qualitative smeantics of any graphical model" even means.  Do
%all graphical models have qualitative semantics?  On top of that, we
%should talk about "lesser-known models".  After all the work you've
%gone through to cut material, this is an easy cut that should
%definitely be made.  (You should also cut Appendix B.  Less, in this
%case, is much more.  The appendix is not a dumping ground.) 
%We emphasize that \cref{theorem:mrf-bn-monotone-impossible} has
%implications for the qualitative semantics of \emph{any} graphical
%model (even rejecting \scibility).  
%It may be especially surprising to experts in lesser-known models
%such as dependency networks \cite{heckerman2000dependency}. 
%For a full discussion, see \cref{appendix:undirected PGMs}. 
% To readers familiar with \emph{dependency networks (DNs)} ... 
%spencer3: I agree; this at least needs more explanation, and probably
% should be cut, at least for NeurIPS.
We emphasize that \cref{theorem:mrf-bn-monotone-impossible} has implications for the qualitative semantics of \emph{any} graphical model (even if one were to reject the definition \scibility). 
%oli22: bridging; this is now appendix B.
% It may be especially surprising to experts in lesser-known models.
% such as dependency networks \cite{heckerman2000dependency}.
% For a full discussion, see \cref{appendix:undirected PGMs}.
% It may be especially surprising to experts 
%
We now look into the implications for some lesser-known graphical models, which may appear not to comply with \cref{theorem:mrf-bn-monotone-impossible}.
% The result may be especially 
% surprising to experts in lesser-known graphical models, which may appear not to comply with \cref{theorem:mrf-bn-monotone-impossible} .

%joe18*: I still feel quite strongly that we should cut this, and not
%talk about "lesser-known" models.   I haven't got the time to deal
%with it now though.
%oli23: we took this out for the NeurIPS deadline, but I still want material along these lines here for the full paper. 
\paragraph{Dependecny Networks}
To readers familiar with \emph{dependency networks (DNs)} \citep{heckerman2000dependency},
\cref{theorem:mrf-bn-monotone-impossible} may raise some conceptual issues.
%oli16:
% If $G$ is an undirected graph, then $\Ar_G$ is the structure of
% a consistent \emph{dependency network (DN)} \citep{heckerman2000dependency}, 
When $G$ is an undirected graph, $\Ar_G$ is the structure of a consistent DN.
% We found this result surprising for a few reasons. First, if $G$ is an undirected graph then $\Ar_G$ is the structure of a consistent dependency network \citep{heckerman2000dependency}, and
The semantics of such a DN,
    % precisely the corresponding MRF (for positive distributions).
which intuitively describe an independent mechanism on each \arc,
%oli21: getting rid of footnote; putting the content below.
% \unskip \footnote{\textbf{Reconciliation with \cref{theorem:mrf-bn-monotone-impossible}.} 
%     %oli6: this last comment is even more in the weeds. I am  pre-empting a possible worry that only those very familiar with DNs would have; it doesn't need to go into the paper. 
%     \commentout{ Alternatively, if we choose the order based on the graph so as to capture BNs, then the notion ceases to be monotonic. }
%     }
coincide with the MRFs for $G$ (at least for positive distributions). 
%joe5*: I have no idea what the next sentence is saying.  The
%footnote is even worse.  It will be comprehensible onlly to people
%who are deeply familiar with DNs.  At a aminimum, you have to
%explain why a reconciliation is needed and say more aboug DNs.  
%oli6: ok; trying something short.
% Moreover, it does so with an independent mechanisms along each \arc.%
% Moreover, it does so with an orderd sampling procedure, which is another way of making precise the intuition of ``an independent mechanisms along each \arc''.%
% Moreover, the DN semantics essentially place an independent mechanism on each \arc.%
%oli6:
% Furthermore, quantitatively, PDGs capture both BNs and MRFs
%oli21: putting footnote material here, then paragraph break
In more detail, DN semantics are given by the fixed point of a markov chain that repeatedly generates independent samples along the \arc s of $\Ar_G$ for some (typically cyclic) directed graph $G$. The precise definition requires an order in which to do sampling. Although this choice doesn't matter for the ``consistent DNs'' that represent MRFs, it does in general. With a fixed sampling order, the DN is monotonic and captures MRFs, but can represent only BNs for which that order is a topological sort.

% %oli16: not building, but parallel
% % Furthermore,
% Meanwhile,
% PDGs capture BNs and MRFs quantiatively
% \citep[Theorems 4.4 \& 4.5]{pdg-aaai} with a monotonic scoring function,
% %joe5*: I"m lost.  Why is this relevant and why should it make the result surprising
% %oli6: because this was called the "PDG Markov Property", and is an independence property that we prove holds of all optimal distributions in a PDG (for 0 < \gamma < \min \beta/\alpha).
% and the independence property exploited by the PDG inference procedure \citep[Theorem 6]{pdg-infer} is the same as that of 
% %oli6:
% % a factor graph
% a MRF with the same structure. 
% % The reason for this discrepancy is simpler: 
% %oli3:
% % Here, there is a much simpler explanation: the semantics we have detailed here is not identical to the the original PDG semantics.%
% %joe5* This is the wrong story.  You have to say something about the
% %how the quantitive sejantics that we gave focuses on distributions
% %that agree the the cpts.  It says nothing about independence, which
% %is what compatibility talks about.  So being able to capture MRFs
% %with the quantitative semantics doesn't say anything about being able
% %to capture it with the qualitative semantics.  
% %
% %oli6: It may be the wrong story, but what you say is not entirely true.
% % First, PDGs can say something about independence (with the qualitative part).  Second, if you can 
% % capture exactly the set of MRFs quantiatively, with models that have 
% % the same structures as those MRFs, there is a sense in which you have
% % captured precisely those independencies.  Intuitively, the Markov Property from our inference paper should corespond to what is possible to articulate with any PDG. PDGs can also capture qualitative BNs directly. These facts together appear to violate the impossibility theorem. 
% %
% %joe5*: Please replace all the
% %material below (incluing the footnote) by this simple observation.
% % For these concerns, there is a very simple explanation:
% %oli6: ok; that's a good suggestion.
% % But \scibility\ does not coincide exactly with the original PDG semantics.%
% Yet capturing a MRF quantitatively is not the same as capturing its structure,
% % and a PDG structure
% %oli20:
% and 
% a PDG's underlying \hgraph\ $\Ar$
% may well imply more independencies than the inference procedure exploits. 
% %oli3: rewriting footnote to streamline and clarify
%     % \footnote{ In fact, the original PDG semantics can capture arbitrary (in)dependencies including those of a MRF. Essentially, this circumvents our impossibility theorem \cref{theorem:mrf-bn-monotone-impossible} because \arc s can be annotated with with negative structural weights, which are monotonic in the in the opposite direction. }
% %
% %oli6: this footnote is relevant to the current discussion, but it helps to clarify things only a very small number of people who happen to care enough about PDGs to slow down here. It's certainly not worth the space, given our constraints. 
% %oli15: probably disrupts the flow of the story even for full paper
% % \vfull{
% \commentout{
%     \footnote{%
%     In fact, the original PDG semantics can represnt arbitrary independencies including both MRFs and BNs. Although PDG semantics are typically monotonic, \cref{theorem:mrf-bn-monotone-impossible} is not violated because capturing a MRF qualitatively may require annotating some \arc s with \emph{negative} structural weights, which make those \arc s \emph{anti}-monotonic.
%     }
% }
% %oli3:
% % Nevertheless, as we will see in the next section, the two semantics are very closely related. 
% %oli6: now out of place
% % Nevertheless, the two concepts are very closely related,
%     % as we will see in the next section. 
% %oli6: we still need something... what about...

% % While monotonicity allows us to prove \scibility\ by weakening, 
% % it does not useful in showing a distribution is 
% %joe3*: I still feel strongly that the material on equivalence and
% %monotonicity is eak.  Perhaps you think it's very useful, but you
% %certainly haven't explained that to the reader.  I could imagine a
% %paper where you show the reader why these notions are really
% %interesting, but that would be very different from this paper.  It
% %odes *not* make the paper better to add this material, which seems
% %random and doesn't fit into the story at all.  I would cugt it.
% %While we're at it, I think you should spend more time fitting
% %causality into the story.  (Besides dependence and independence,
% %using type-1 semantics we can capture causality.)  
% %
%oli3*: I get what you're saying, but I think it feels random because
% I haven't yet made the connections sufficiently obvious. Let me quickly
% give you a brief list of ways I think it's intrinsically valuable,
% and how it ties into other material.
%  1.  
%
%


%oli2: this is interesting, but we might not have time to deal with
% it properly, and it's no longer an effective transition. Cutting.
%oli16: this point is actually quite interesting; reinstating
%oli20*: interesting as it is, we have no space to make this point, and it needs expansion. Defering to the full paper. 
\vfull{
\Cref{theorem:mrf-bn-monotone-impossible} shows that
    \scibility\ does not capture MRFs (at least, in the obvious way) at a purely observational level. 
Nevertheless,
    there is still a sense in which \scibility\ 
    captures MRFs \emph{causally}---that is, 
    if we \emph{intervene} instead of conditioning.

\begin{linked}{prop}{causal-mrf}
    % Then $\mu \models \Ar_G$ if and only if,
    % for every witness $\bar\mu$
    % \begin{enumerate}
    % \item 
    % If $\mu \models \Ar_G$
    %  then for every witness $\bar\mu$ and causal model $\mathcal M \in \PSEMs_{\Ar_G}(\bar\mu)$,
    % \[
    % \mat X \CI_G \mat Y \mid \mat Z
    % \quad\iff\quad
    % \forall \mat z \in \V(\mat Z).~~
    % \bar\mu \models \mat X \CI \mat Y \mid \mathrm{do}_{\mathcal M}(\mat Z = \mat z).
    % \]
    % \item Conversely, if $\mu \not\models \Ar_G$, then 
    % there does not exist a causal model $\mathcal M$ giving rise to $\mu$ that has this property.
    % \end{enumerate}
    Let $G$ be an undirected graph whose vertices correspond to variables $\X$. 
    \begin{enumerate}
        \item Let $\mu(\X)$ be a positive distribution (i.e., $\forall \mat x \in \V(\X).~\mu(\X{=}\mat x) > 0$).
        % Suppose $\mat X, \mat Y, \mat Z \subseteq \X$ are such that  $\mat X \CI_G \mat Y \mid \mat Z$.
        If $\mu \models \Ar_G$, then 
        for every witness $\bar\mu$ and causal model $\mathcal M \in \PSEMs_{\Ar_G}(\bar\mu)$,
        whenever $\mat X, \mat Y, \mat Z \subseteq \X$ are such that  $\mat X \CI_G \mat Y \mid \mat Z$, 
        it is the case that
        $\bar\mu \models \mat X \CI \mat Y \mid \mathrm{do}_{\mathcal M}(\mat Z = \mat z)$.
        
        \item
        % Conversely, let $G$ be an undirected graph, and $\mu(\X)$ a distribution over values of its vertices. 
        Convesely, there exists some distribution $\mu(\X)$
        If $\mu \not \models \Ar_G$, then 
        % If $\bar\mu \models \mat X \CI \mat Y \mid \mathrm{do}_{\mathcal M}(\mat Z = \mat z)$
        
    \end{enumerate}
\end{linked}
}


\section{Information Theory, PDGs, and \SCibility}
\subsection{More Detailed Primer on Information Theory}
    \label{appendix:info-theory-primer}
%oli16: expanded this primer a bit
%oli22: adding a link back
We now expand on the fundemental information quantities introduced at the beginning of \cref{sec:info}.
Let $\mu$ be a probability distribution, and be $X,Y,Z$
%oli6:
% (sets of) random variables (taking on finitely many values).
    be (sets of) discrete random variables.
%oli16:
The \emph{entropy} of $X$ is the uncertainty in $X$, when it is distributed according to $\mu$, as measured by the number of bits of information needed (in expectation) needed to determine it, if the distribution $\mu$ is known.  It is given by 
\[
    \H_\mu(X) := \sum_{x \in \V(X)} \mu(X{=}x) \log \frac{1}{\mu(X{=}x)} \qquad= -\Ex_{\mu}[\log \mu(X)],
\]
and a few very important properties; chief among them, $\H_\mu(X)$ is non-negative, and equal to zero iff $X$ is a constant according to $\mu$. 
The ``joint entropy'' $\H(X,Y)$ is just the entropy of the combined variable $(X,Y)$ whose values are pairs $(x,y)$ for $x \in \V(X),y \in \V(Y)$; this is the same as the entropy of the variable $X \cup Y$ when $X$ and $Y$ are themselves sets of variables. 

The \emph{conditional entropy} of $Y$ given $X$
measures the uncertainty present in $Y$ if one knows the value of $X$
(think: the information in $Y$ but not $X$),
and is equivalently defined as any of the following three quantities:
\[
\H_\mu( Y | X) :=
        \quad
    \Ex_{\mu} [~\log \nicefrac1{\mu(Y | X)}~]
        \quad
    =\H_\mu(X,Y) - \H_\mu(X)
        \quad
    =\Ex_{x \sim \mu(X)} [~\H_{\mu \mid X{=}x}(Y)~]    
.
\]
The \emph{mutual information} $\I(X;Y)$,
and its conditional variant $\I(X;Y|Z)$, 
are given, respectively, by
\[
    \I_\mu(X;Y) :=
        \Ex_{\mu} \Big[ \log \frac{\mu(X,Y)}{\mu(X) \mu(Y)}\Big],
    \quad\text{and}\quad
    \I(X;Y|Z):= 
        \Ex_{\mu} \Big[ \log \frac{\mu(X,Y,Z)\mu(Z)}{\mu(X,Z) \mu(Y,Z)}\Big].
\]
The former is non-negative and equal to zero iff $\mu \models X \CI Y$, and the latter is non-negative and equal to zero iff $\mu \models X \CI Y \mid Z$. 
All of these quantities are purely ``structural'' or ``qualitative'' in the sense that they are invariant to relabelings of values, and 

Just as conditional entropy can be written as a linear combination of unconditional entropies, so too can conditional mutual information be written as a linear combination of unconditional mutual informations: $\I(X;Y|Z) = \I(X;(Y,Z)) - \I(X;Z)$.  
Thus conditional quantities are easily derived from the unconditional ones. But at the same time, the unconditional versions are clearly special cases of the conditional ones; for example, $\H_\mu(X)$ is clearly the special case of $\H(X|Z)$ when $Z$ is a constant (e.g., $Z = \emptyset$). 
Furthermore, entropy and mutual information are also interdefinable and generated by linear combinations of one another. 
It is easy to verify that 
$\I_\mu(X;Y) 
    % = \H_\mu(X,Y) - \H(Y|X) - \H(X|Y)
    = \H_\mu(X) + \H_\mu(Y) - \H(X,Y)
$
and 
$\I_\mu(X;Y|Z)
    = \H_\mu(X|Z) + \H_\mu(Y|Z) - \H(X,Y|Z)
$,
and thus mutual information is derived from entropy. 
Yet on the other hand, $\I_\mu(Y;Y) = \H_\mu(Y)$ and $\I_\mu(Y;Y|X) = \H_\mu(Y|X)$---thus entropy is a special case of mutual information. 
    
%joe17*: cut this subsection!  It feels like you've just dumped a lot
%of restuls.  The appendix is not a dumping ground!  This is not how
%to write a paper!
%oli22*: I see why you say this; rewriting this subsetion to clarify its current role.  (It's in another document, so I'm working on this concurrently.)
\subsection{Structural Deficiency: More Motivation, and Examples}
% \input{original-scoring-function-intuition.tex}
\input{fragments/idef-intuition}

%oli22: cutting for NeurIPS
\vfull{%
\subsection{Weights for SIM-Inc}
Given $\Ar$ and $|\Ar|+2$ positive weights $\boldsymbol\lambda = (\lambda^{\text{(a)}}, \lambda^{\text{(b)}}, \{\lambda^{\text{(c)}}_a\}_{a \in \Ar})$, define the function


\begin{align*}
%joe10*: why SIMInc??? 
    \SIMInc_{\Ar, \boldsymbol\lambda}(\mu) :=
    % \inf_{\substack{
    %         \nu(\mathcal U\!,\, \X)\\
    %         \nu(\X) = \mu(\X)}}\;\;
    & \inf_{\nu(\U,\X)}
    \left\{
    \begin{array}{rl}
     % \lambda_{!}
    &\lambda^{\text{(a)}}
     \kldiv[\big]{ \nu(\X) }{\mu(\X)}\\[1ex]
    + & \displaystyle
        \lambda^{\text{(b)}} \Big( - \H_\nu (\mathcal U) + 
    \sum_{a \in \Ar} \H_\nu (U_a)
         \Big) \\
    + & \displaystyle
        \sum_{\smash{a \in \Ar}}
        \lambda_a^{\text{(c)}} \H_\nu(\Tgt a | \Src a , U_a)
    \end{array}\right.
    .  \numberthis\label{eq:siminc-weighted}
\end{align*}

When $\boldsymbol\lambda = (\infty, 1, \mat 1)$, we get the analogous 
quantity defined in \eqref{eq:siminc} in the main text. 


%oli15: 
Here are some analogous results for this generalized version with weights. For a weighted hypergraph $(\Ar, \balpha)$, here is a strengthening of \cref{theorem:siminc-idef-bounds}, and the appropriate translateion of the hypergraph.
%
Given $(\Ar, \balpha)$ translate it to a new derandomized hypergraph 
$(\Ar, \balpha)^\dagger$ by replacing each weighted hyperarc 
\[
\begin{tikzpicture}[center base]
    \node[dpad0] (S) at (0,0) {$\Src a$};
    \node[dpad0] (T) at (1.6,0) {$\Tgt a$};
    %oli6: adding label
    \draw[arr2] (S) to node[above,pos=0.4]{$a$}
        node[below,pos=0.4,inner sep=1pt]{\color{gray}$\scriptstyle(\alpha_a)$}
        (T);
\end{tikzpicture}
\quad 
\text{with the pair of weighted \hyperarc s}
\quad
\begin{tikzpicture}
        % [baseline=0]
        [center base]
    \node[dpad0] (S) at (0,0) {$\Src a$};
    \node[dpad0] (T) at (1.4,0) {$\Tgt a$};
    \node[dpad0] (U) at (0.6,0.7) {$U_a$};
    %oli15: adding names for these arcs
    % \draw[arr,<-] (U) -- +(-1,0);
    \draw[arr,<-] (U) to node[above,pos=0.6]
        % {$a.0$}
        {$a_0$}
        node[below,pos=0.6,inner sep=1pt]{\color{gray}$\scriptstyle(\alpha_a)$}
         +(-1.2,0);
    \mergearr[arr1] SUT
    %oli15:
    \node[below=2pt of center-SUT,xshift=-0.2em] (a1)
        % {$a.1$};
        {$a_1$};
    \node[below=0pt of a1, inner sep=1pt] {\color{gray}$\scriptstyle(\alpha_a)$};
\end{tikzpicture}~.
\]

% \begin{conj}
%     \[
%         \SIMInc_{\Ar,\boldsymbol\lambda} (\mu) 
%     \]
% \end{conj}

\TODO
}%

\subsection{Counter-Examples to the Converse of Theorem \ref{theorem:sdef-le0}}
    \label{appendix:converse-sdef-le0}

In light of 
%oli3
% its usefulness
\cref{example:ditrichotomy}
and its connections to 
%oli3:
% the original PDG definition,
$\SDef$
through \cref{theorem:sdef-le0},
one might hope this criterion is not just a bound, but
    a precise characterization of the distributions that are \scible\ 
    with the 3-cycle. 
Unfortunately, it does not, and the converse of \cref{theorem:sdef-le0} is false.

\begin{example}
    Suppose $\mu(X,Y,Z) = \mathrm{Unif}(X,Z) \delta \mathrm{id} (Y|X)$ and $\Ar = \{\to X, \to Y\}$,
    % Consider $\Ar = \{\to X, \to Y\}$, ,
    where all variables are binary.
    Then $\SDef_{\!\Ar}(\mu) \,{=}\, 0$, but $X$ and $Y$ are not independent.
    \qedhere
\end{example}
%oli3: shortening; previous "story" is not so interesting
% This problem is easily dealt with by requiring that $\SDef_{\!\Ar}(\mu) \le 0$
% not just for $\Ar$ and $\mu$, but also for all restrictions of them.
% Yet here is another kind of example that is not so easily dealt with
Here is another counter-example, of a very different kind.
\begin{example}
    Suppose  $A, B, C$ are binary variables.
    It can be shown by enumeration (see appendix) that
    no distribution supported on seven of the eight
    possible joint settings of of $\V(A,B,C)$ can be
    %oli5: here and below: \Ar_\circ  -->  \Ar_{3\circ}
    \scible\ with the 3-cycle $\Ar_{3\circ}$. Yet it is easy
    to find examples of such distributions $\mu$ that have positive
    interaction information $\I(A;B;C)$,
    and thus $\SDef_{\mu}(\Ar_{3\circ}) \le 0$ for such distributions.
\end{example}


%joe17*: Cut this section.  (See my previous coment on the appendix
%not being a dumping ground.)  
%oli22: This material directly supports claims made in the main paper.  When I've described Theorem 2 to people, I've essentially always recieved questions about this. 
\section{\SCibility\ Constructions and Counterexamples}
    \label{sec:func-counterexamples}

We now give a counterexample to a 
% earlier version
simpler previously conjectured strengthening of \cref{theorem:func},
in which part (a) is an if-and-only-if. 
This may be surprising.
In the unconditional case, it is true that, two arcs $\{ \ed1{}X, \ed2{}X \}$ precisely encode that $X$ is a constant, as illustrated by \cref{example:two-edge-det}.
The following, slightly more general result, 
    is an immediate correlary of \cref{theorem:func}(c).

\begin{prop}
    $\mu \models \Ar \sqcup \{ \ed1{}X, \ed2{}X \}$ if and only if $\mu \models \Ar$ and
    %  $\H_\mu(X) = 0$.
    $\mu \models \emptyset \tto X$. 
\end{prop}

One can be forgiven for imagining that the conditional case would be analogous---that \scibility\ with a \hgraph\ that has two parallel arcs from $X$ to $Y$ would imply that $Y$ is a function of $X$. But this is not the case.
Furthermore, our counterexample also shows that 
%
neither of the two properties we consider in the main text
    (requiring that $\Ar$ is \partl, or that the \scibility\ with $\mu$ is even) 
    are enough to ensure this. 
That is, there are \partl\ graphs $\Ar$ such that $\mu \emodels \Ar$ but $\mu \not\models \Ar \sqcup \{ \ed1XY, \ed2XY \}$.
 % for which \cref{theorem:func} (b) does not hold. 

\begin{example}
        \label{ex:counterexample-func-simple}
    We will construct a witness of SIM-compatibility
    for the \hgraph\ 
    \[
        \Ar := 
        \begin{tikzpicture}[center base]
            \node[dpad0] (X) {$X$};
            \node[dpad0] (Y) at (2,0) {$Y$};
            \draw[arr1] (X) to[bend left=60,looseness=1.2] node[below]{$\vphantom{|}\smash{\scriptstyle\vdots}$} 
                node[above]{\small 1} (Y);
            \draw[arr1] (X) to[bend left=20,looseness=1.2] node[below]{\small $n$} (Y);
            \draw[arr1] (Y) to [bend left=40] node[below]{\small 0}(X);
        \end{tikzpicture},
    \]  
    in which $Y$ is \emph{not} a function of $X$, 
    which for $n=3$ will disprove the analogue of \cref{theorem:func} for the \partl\ context $\Ar'$ equal to the 2-cycle. 
    
    Let  $\U = (U_0, U_1, \ldots, U_n)$ be a vector of $n$ mutually independent random coins, and $A$ is one more independent random coin. 
    For notational convenience, define the random vector $\mat U := (U_0, \ldots, U_n)$ consisting of all variables $U_i$ except for $U_0$. 
    Then, define variables $X$ and $Y$ according to:
    \begin{align*}
        X &:= 
            ( A \oplus U_1 , \ldots,  A \oplus U_n ,  ~~ U_0 \oplus U_1, U_0 \oplus U_2, \ldots,  U_0 \oplus U_n ) 
            \\
            &= (A \oplus \mat U,~~ U_0 \oplus \mat U)
            \\
        Y &:= (A, U_0 \oplus \mat U ) = (A, ~U_0 \oplus U_1, U_0 \oplus U_2, \ldots,  U_0 \oplus U_n ) 
            % ,  ~~U_1 \oplus U_0, \ldots, U_n \oplus U_0) \\
            ,
    \end{align*}    
    where and the operation $Z \oplus \mat V$ is element-wise xor (or addition in $\mathbb F_2^n$), after implicitly converting the scalar $Z$ to a vector by taking $n$ copies of it. Call the resulting distribution 
    $\nu(X, Y, \U)$. 
    
    It we now show that $\nu$ witnesses that its marginal on $X,Y$ is 
        \scible\ with $\Ar$, which is straightforward.
    \begin{enumerate}[label=(\alph*), start=2]
        \item $\U$ are mutually independent by assumption;
        \item[(c.0)] $Y = (A, \mat B)$ and $U_0$ determine $X$ according to:
        \begin{align*}
            g(A, \mat B, U_0) &= (A \oplus U_0 \oplus \mat B,\; \mat B) \\
                &= (A \oplus U_0 \oplus U_0 \oplus \mat U,\;  U_0 \oplus \mat U) & \text{since } \mat B =U_0 \oplus \mat U \\
                &= (A \oplus \mat U, U_0 \oplus \mat U) = X
        \end{align*}
        \item[(c.1--$n$)] for $i \in \{1, \ldots, n\}$, $U_i$ and $X = (\mat V, \mat B)$ together determine $Y$ according to
        \begin{align*}
            f_i(\mat V, \mat B, U_i) := (V_i \oplus U_i,\; \mat B) 
                = (A \oplus U_i \oplus U_i,\; U_0 \oplus \mat U) = Y.
        \end{align*}
    \end{enumerate}
    %oli15: verify not esim-compatibile either
    In addition, this distribution $\nu(\U, X,Y)$ satisfies condition
    \begin{enumerate}[label=(\alph*), start=4]
    \item $\nu(X,Y \mid \U) = \frac{1}{2} \mathbbm1[ g(Y, U_0) = X] \prod_{i = 1}^n \mathbbm1[ f_i(X, U_i) = Y]$,
    since, for all joint settings of $\U$, there are two possible values of $(X,Y)$, corresponding to the two values of $A$, and both happen with probability $\frac12$. 
    \end{enumerate}
    
    Thus, we have constructed a distribution that witnessing the fact that $\mu(X,Y) \emodels \Ar$.  
    
    Yet, observe that $X$ alone does not determine $Y$ in this distribution, because $X$ alone is not enough to determine $A$ (without also knowing some $U_i$). 
    %
        
    For those who are interested, 
    observe that the bound of \cref{theorem:sdef-le0} tells that
    we must satisfy 
    \begin{align*}
    0 \ge
        \IDef_{\!\Ar}(\mu)
        &= - \H_\mu(X,Y) + n \H_\mu(Y \mid  X) + \H_\mu(X \mid Y) \\
        &= - \I_\mu(X;Y) + (n-1) \H_\mu(Y \mid X)  
    \end{align*}
    Indeed, this distribution has information profile 
    \[
        \H(X \mid Y) = 1\,\text{bit},\qquad
        \I(X ; Y) = n\,\text{bits},\qquad
        \H(Y \mid X) = 1\,\text{bit},
        % \qedhere
    \]
    and so $\IDef_{\!\Ar}(\mu) = -1\,\text{bit}$. 
    Intuitively, this one missing bit corresponds to the value of $A$ that is not determined by the structure of $\Ar$.
\end{example}


%oli16: moved this section here. SHould eventually be a subsection of something. 
% \subsection{PSEMs as \Scibility\ Witnesses}
% \subsection{From PSEMs to \Scibility\ Witnesses}
\section{From Causal Models to Witnesses}
    \label{appendix:sem2witness}
    
%oli16:
We now return to the ``easy'' direction of the correspondence between \scibility\ witnesses and causal models, mentioned at the beginning of \cref{sec:witness-to-causal-model}.
Given a (generalized) randomized PSEM $\cal M$, we now show that distributions $\nu \in \SD{\cal M}$, are \scibility\ witness showing that the marginals of $\nu$ are \scible\ with the hypergraph $\Ar_{\cal M}$. 
More formally:

%oli22: killing this material; was a dumping ground
% \commentout{
% Recall that a SEM $M$ with exogenous variables $\U$ and endogenous variables $\enV$ determines a \hgraph\ according to
% $\Ar_{M} = (\U \cup \enV, \{ \Pa_G(X) \to \{X\} \}_{X \in \enV})$,
% }
% %oli16: very buggy; basically wrong!
% % why? 
% \commentout{
% \begin{linked}{prop}{sc-causal-graph-arise}
%     For all PSEMs $\mathcal M = (\U, \enV, \mathcal F, P)$,
%     a distribution $\mu(\X)$ over $\X \subseteq \U \cup \enV$, can arise from $\cal M$
%     if and only if $\mu \models \Diamond \Ar_{\cal M}$.
% \end{linked}}
%
% One direction of the relationship between is obvious: 
% % a solution to a randomized causal model
% given a causal model $\cal M$, an extended distribution $\nu \in \SD{\cal M}$ witnesses \scibility\ of its marginals with the dependency structure of $\cal M$.
%oli22*: wrong place
\commentout{
\begin{prop}{causal-model-structure-capture}
    %joe4: added next line
    % If $M$ is a SEM, then
    If $M$ is a SEM, then
    $\mu(\enV,\U) \models \Ar_{M}$ 
    %oli5: compressing
        % if and only if 
        iff
%joe4*: this seems false.  Take M = M', Surely Pa_M = Pa_{M'}.  
%oli5*: I'm not sure I understand your complaint. You don't get to choose M'; this says that if \mu(\enV,\U) is SIM-compatible with a hypergraph \Ar_M, then there exists a way of filling in the equations and probabilities to recover $\mu$.  Note that when we form $\Ar_M$, we have forgotten about the equations. So the fact that some arbitrary distribution \mu is compatible with the structure, does not mean that distribution models the same equations. But it does model the same *structural dependencies* 
%joe4*: On top
%of that, why should the fact that we have the right functional
%dependencies ensure that all the required independencies hold.  
%oli5: there are no independencies in this construction (except insofar as they are being used to model functional dependencies). A SEM just has functional dependencies, no (traditional) independencies. 
%joe4: I'm lost.  I see no reason for this to be true.
%oli5: I'm adding the proof in the appendix. 
        there exists $M'$ such that $\Pa_M = \Pa_{M'}$ and
        $\mu \in \SD{M'}$.
\end{linked}
}

%oli14: this is obvious, but seems worth stating anyway.
\begin{prop}
    %oli15: now part of the definition; cutting
    % Suppose $\mathcal M = (M, P)$ is a randomized PSEM whose noise variables $\{ U_X \}_{X \in \X}$ are mutually independent according to $P$. Then
    If $\mathcal M = (M
        {=} (\U,\enV,\mathcal F)
        , P)$ is a randomized PSEM, then
    every $\nu \in \SD{\mathcal M}$ 
    witnesses
    the \scibility\ of its marginal on its exogenous variables, with the dependency structure of $\mathcal M$.
    That is,
    %oli15: redoing this inline with previous notation
    % \[
    % % \forall \nu \in\SD{\cal M}.\quad
    % % \nu(\X)  
    % % \nu(\enV)
    % \nu(\mathcal Y) 
    % \models 
    %     \Big
    %     \{ 
    %         \Pa\nolimits_{M}(X) \to X 
    %     \Big
    %     %oli15: 
    %     % \}_{X \in \X}. 
    %     \}_{X \in \enV}. 
    % \qedhere
    % \]
    for all $\nu \in \SD{\cal M}$ 
    and $\mathcal Y \subseteq \U \cup \V$, 
    $\nu(\mathcal Y) \models \Diamond \Ar_{\cal M}$. 
\end{prop}

The proof is straightforward: by definition, if $\nu \in \SD{\cal M}$, 
then it must satisfy the equations, and so automatically fulfills condition (c). 
Condition (a) is also satisfied trivially, by assumption: the distribution we're considering is defined to be a marginal of $\nu$.
Finally, (b) is also satisfied by construction: we assumed that $\U_{\Ar} = \{ U_a \}_{a \in \Ar}$ are independent.

% \section{Illustration of Monotonicity \& Usefulness}
% 
% \begin{enumerate}
%     \item Proof that Markov Chains are reversible. 
%     \item Proof that 3-cycle is same in both directions, in the presence of one extra arc on any varaible 
%     \item 
% \end{enumerate}
%joe7*: Again, cut this unless you can connect it well with the
%paper.  The appendix is not a dumping ground!  Please do this by 11
%AM tomorrow.  I would *strongly* suggest cutting this.
%
%oli16: cuting this section. This was material you previously wanted to focus on, but right now it no longer fits in. It deserves a lot more space somewhere completely different. 
\commentout{
\section{Combining Qualitative Dependency Graphs}
    \label{sec:union}
    
% In addition to having the wrong variance, when
% Once we state the proprety this way, another unsavory aspect of it
% becomes clear:
In a BN or an MRF, the choice of whether
or not to place an edge between $A$ and $B$
does not just depend on the relationship between $A$ and $B$---it also
    depends on what other variables are present in the graph.
For example, suppose $X$ is a variable that mediates
    the interaction between $A$ and $B$.

In a ``coarse'' model that does not contain $X$, it would be appropriate
to include an edge $A{\to}B$, but in a ``fine'' model that does, that edge
would be in appropriate, and instead one should include edges $A{\to}X{\to}B$.
% This decision is more transparent for a modeler
Although the situation is similar for our case, the reasons
for this modeling choice are more transparent: if the model already contains $A{\to}X{\to}B$,
one should not also include the \arc\ $A{\to}B$ because it is not independent of the two mechanisms already present.    

[...] It follows that notion of \scibility\ cannot be
defined inductively over unions of independent mechanisms. 
}

%joe7*: Yet again, cut this unless you can connect it well with the
%paper.  The appendix is not a dumping ground!  And yet again, I would
%strongly suggest cutting it.

%oli21: If we somehow get extra time, this would be valuable to pull together, but it looks like it'll be cut for the neurips submission.
\vfull{%

\section{An Algorithm for Finding Witnesses: The Null Value Construction}
    \label{sec:null}

We have now built up a body of examples, but it is still not clear
    how to compute \scibility.
In other words, it is still not clear how to solve the decision problem: given $\mu$ and $\Ar$, determine whether or not $\mu \models \Ar$.
In this section, we discuss one approach to this problem. 

% For cyclic graphs, it can be difficult to construct witnesses
% of compatibility. 
If you start with a distribution $\nu(\X)$,
it's not at all obvious how to extend it with a conditional distribution
$\nu(\U|\X)$ such that the variables $\U$ are \emph{unconditionally}
dependent, given that they cannot be independent of $\X$.
% The only way we see
It seems that the only way to ensure this unconditional independence
is to start with a distribution $\nu(\U) = \prod_{a} \nu(U_a)$
and then figure out how to extend it to the variables $\X$.


% Let $\Vnull X := \V\!X \cup \{ \bullet \}$.
% \begin{align*}
%     \SDef_{}
% \end{align*}

To begin, for each $a \in \Ar$, take $U_a$ to be a response variable,
taking values $\V(U_a) = (\V \Tgt a) ^{(\V \Src a)}$, just 
as in \cref{sec:causal}.
But now we run into a problem: without carefully selecting the
supports of the distributions over $\U$, it is entirely possible
that there will be some joint setting $\mat u \in \V \U$
% with $\nu(\mat u) > 0$ and 
occurs with positive problability, but represents a collection
    of functions that has no fixed point.  
For example, take the graph 


\begin{example}[5, continued]
    By randomly selecting distributions $\Pr(U_1),
    \Pr(U_2)$, and $\Pr(U_3)$ (see \cref{sec:null}), one finds that the set of distributions that are consistent with this 3-cycle has larger dimension than the set of distributions that factorize according to $\Pr(X,Y,Z) \propto \phi_1(X,Y) \phi_2(Y,Z) \phi_3(Z,X)$.%
        \footnote{see appendix for details.}
    Thus, our definition does not coincide with the corresponding factor graph.
\end{example}


\begin{conj}
    If $\mu_0 \models \Ar$, and $\mu'$ lies on the path $\mu(t)$ of
    gradient flow minimizing $\SDef_{\Ar}(\mu')$, starting at $\mu(0) = \mu_0$, then $\mu' \models \Ar$.
\end{conj}

The following has emperical support.
% using the null value construction desecribed in \cref{sec:null}.

\begin{conj}
    % If $\mu \models \Ar$
    The distribution (s?)
    $\hat \mu := \arg\min_{\mu : \mu \models \Ar} \kldiv{\mu}{\hat\mu}$
    have the same information profile as $\mu$. 
\end{conj}



%oli16: moved to appendix, for now. Barring a major breakthrough, this is less important than the other material, 
%oli10*: added
%joe10*: You need a *much* better story here.  Why should I care?
%oli11: true; this story only got one pass. More pressingly, part (b) of Prop 12 wasn't quite right, and what story I tell depends somewhat on how I can bring that idea back out in a more correct way. I have some ideas but need to write them up. 
%oli11: For now, perhaps its best to ignore this section
\section{Even Structural Compatibility}
%oli16: a brief story tying things together here

%oli9*: new subsection
\subsection{Even \SCibility}

If $\mathcal M$ is a cyclic or \subpartl\ PSEM, then $\SD{\mathcal M}$ may 
contain many distributions. Still, so long as it is non-empty, there is
still a unique distribution that, arguably, best describes the distribution of the PSEM (in the absence of interventions)---namely, the one that, for any given value $\mat u \in \V(\U)$, treats all ``fixed-points''
% \ifvfull
%     $\mat x \in \mathcal F(\mat u)$
% \else
%     $\{\mat x \in \V(\X) : \forall a \in \Ar. f_a(\Src a(\mat x), u_a) = \Src a(\mat x) \}$
% \fi
$\mat x \in \mathcal F(\mat u)$ of the equations $\mathcal F$ symmetrically.
\vfull{
\[
\Big(\text{ Recall that }\qquad
\mathcal F(\mat u) := \{\mat x \in \V(\X) : \forall a \in \Ar. f_a(\Src a(\mat x), u_a) = \Src a(\mat x) \}.
 \Big)
\]}
For example, if $\mathcal M$ has 
no exogenous variables ($\U = \emptyset$),
endogenous variables $\X = [X_1, \ldots, X_n]$ that are all binary, 
and equations 
% $f_{X_i}(\X\setminus X_i) = X_{(i+1)\pmod n}$,
$f_{X_i}(\X\setminus X_i) = X_{(i+1)\% n}$,
then $\SD{\mathcal M}$ is a 1-dimensional specturm of distributions
    supported on the two points $\{ (0, \ldots, 0), (1, \ldots, 1) \}$. 
The distribution that gives the two an equal weight of $\frac12$ is somehow special, in that it is the unique one that does not break the symmetry by preferring either $(0, \ldots, 0)$ or $(1, \ldots, 1)$. 
This intuition is made precise, and generalized to \scibility\ witnesses (rather than PSEMs), by the following definition.

\begin{defn}
        \label{defn:esim-compat}
    We say a witness $\nu(\U,\X)$ to $\mu \models \Ar$ is \emph{even}, iff
    it satisfies properties (a,b) of \cref{defn:scompat}, and also
    the following strengthening of property (c):
    \begin{enumerate}[start=4,label={(\alph*)}, nosep]
    \item $\displaystyle
        \nu(\X \mid \U) \propto \mathbbm{1}\Big[ \bigwedge_{a \in \Ar}
            f_a(\Src a, U_a) = \Tgt a \Big],
    $
    % \qedhere
    \end{enumerate}
    for some set
    $
    \mathcal F = 
    \{ f_a : \V(\Src a, U_a) \to \V(\Tgt a) \}_{a \in \Ar}
    $
     of equations.
     In this case, we say $\mu$ is \emph{evenly} \scible\ (\escible) with $\Ar$,
     write $\mu \emodels \Ar$, and 
    %oli14*:
    %  call $\nu$ a witness of E\scibility.
    call the pair $(\nu, \mathcal F)$ a witness of E\scibility.
\end{defn}

% This narrower conception of compatibility 
E\scibility\ 
clearly implies \scibility, 
and thus is a stricter notion. 
Furthermore, E\scibility\ witnesses have an even sharper relationship to causal models:
%oli14:
% and has some particularly nice properties.
A witness $(\bar\mu(\U, \X), \mathcal F)$ to E\scibility, 
can be equivalently viewed as a PSEM $\mathcal M = (\U,\X, \mathcal F, \nu(\U))$, 
% and a choice $\nu \in \SD{\mathcal M}$ of fixed point solutions. 
because the rest of the distribution $\nu(\X \mid \U)$ is determined by $\mathcal F$ and property (d). 

\begin{prop}
    \begin{itemize}
        \item There is a 1-1 correspondence between E\scibility\ witnesses and GRPSEMs $\cal M$ in which $\SD{\cal M} \ne \emptyset$. 
    \end{itemize}
\end{prop}
\begin{proof}
    1. Given a E\scibility\ witness $(\nu(\X, \U_\Ar), \mathcal F)$, 
    
    by \cref{prop:gen-sim-compat-means-arise}
    $\PSEMsA(\nu)$
\end{proof}

%
%oli14*: cutting; the statement had some bugs, and I don't think fixing it will add anything beyond the remarks I've now added above and below. 
\commentout{
\begin{linked}{theorem}{esem-properties}
    \label{theorem:esem-properties}
    % If $\mu \emodels \Ar$, then there is exactly one witness of E\scibility.
    % If $\mu \emodels \Ar$, then
    % \unskip:
    \begin{enumerate}[wide,label={(\alph*)}]
    \item
    % For all $\mu$ and $\Ar$, there is at most one witness of $\mu \emodels \Ar$. 
    % there is exactly one witness $\nu$ of E\scibility, and
    %oli14: This isn't exactly true...
    % If $(\nu, \mathcal F)$ witnesses that $\mu \emodels \Ar$, then 
    % $\PSEMs(\nu)$ is a singleton. 
    

    \item
    Conversely, if $\mathcal M = (\X,\U,\mathcal F, P)$ is a (generalized) PSEM, 
    then there is a unique distribution $\nu(\U,\X) \in \SD{\mathcal M}$
    with marginal $\nu(\X) = P$,
    and that also satisfies property (d) of \cref{defn:esim-compat} for the equations $\mathcal F$.
    \item
    If $\nu$ is a witness to $\mu \emodels \Ar$, then 
    $\nu$ and its unique corresponding causal model $\mathcal M$
    ascribe the same probabilities to causal formulas. That is,
    $
        \forall \varphi \in \mathcal L(\X,\U). ~
        % \qquad 
        \Pr_{\nu}(\varphi) = P(\varphi).
    % \]
    $
    \end{enumerate}
\end{linked}
}%
% Thus, there is a direct 1-1 correspondence between causal models, 
%oli14: first part isn't great; toning down. 
% Thus, causal models with arbitrary sets of equations and E\scibility\ witnesses are in direct 1-1 correspondence,
% and PSEMs correspond directly with E\scibility\ witnesses with \hgraph s $\Ar = \Ar_G$ for some graph $G$.
Thus, PSEMs are in direct 1-1 correspondence with E\scibility\ witnesses when \hgraph s $\Ar = \Ar_G$ for some graph $G$.

%oli16: added
We now verify that various results from the main text extend to E\Scibility. 
\begin{itemize}
    \item[{[\cref{theorem:bns}]}]
    When $G$ is acyclic
    (and, more generally, when $\mathcal M \models \U \tto \X$), 
    the extra condition (d) holds trivially, and so
    E\scibility\ coincides with \scibility, and the analogue of \cref{theorem:bns} in which $\models$ is replaced with $\emodels$ also holds. 
    
    \item [{$\vdots$}]
        
    \item[{[\cref{theorem:strong-mono}]}]
    As we show in the \hyperref[proof:strong-mono]{the proof of \cref*{theorem:strong-mono}}, E\Scibility\ is also monotonic with respect to weakening ($\rightsquigarrow$). 
\end{itemize}


The original scoring function $\IDef$ is related to even \scibility. 

\begin{conj}
    There is a constant $\kappa = \kappa(\Ar, \V)$ depending on the \hgraph\ $\Ar$ and the possible values $\V$ that the variables can take, such that
    %
    \[
        \IDef_{\Ar}(\mu) \le \kappa
        \quad\iff\quad
        \mu \emodels \Ar  
    \] 
\end{conj}
\begin{proof}
    \textbf{($\impliedby$).}
    Suppose $\mu \emodels \Ar$. Then there is some witness $\bar\mu$ extending $\mu$ to independent variables $\U_\Ar = \{ U_a \}_{a \in \Ar}$. 

    \textbf{($\implies$).}
    Suppose that $\IDef_{\Ar}(\mu) \le \kappa$. 
\end{proof}

% Finally, it is worth noting that E\scibility\ coincides exactly with \scibility\ when $\Ar = \Ar_G$ for a directed acyclic graph $G$, and so \cref{theorem:bns} also applies when $\models$ is replaced with $\emodels$. 

\subsection{ESIM Compatibility Scoring Rules}
%oli11: added, but this is no longer the biggest problem with this section
We have now seen that, $\IDef_{\Ar^\dagger}$ measures distance from 
    being a witness to \scibility\ (\cref{theorem:siminc-idef-bounds}(b)).
Modulo a constant offset or limit, $\IDef_{\Ar}$, i.e., the original scoring function applied to the original \hgraph\ 

Let's now repeat the same approach as the previous section, by explicitly constructing a scoring function for E\scibility.
Extend our previous weight vector by one entry, so that $\boldsymbol\lambda = (\lambda^{\text{(a)}},\lambda^{\text{(b)}},\{\lambda^{\text{(c)}}_a\}_{a \in \Ar}, \lambda^{\text{(d)}})$
% , and let $\boldsymbol\lambda^{\text{(a--c)}}$ refer to all but the last component.
\unskip.
%
\begin{align*}
    \mathrm{E}\mskip-1mu\SIMInc_{\Ar, \boldsymbol\lambda}(\mu) 
        &:= 
        % \SIMInc_{\Ar,\boldsymbol\lambda^{\text{(a--c)}}}(\mu)
        \inf_{\nu(\X\!,\,\U)} 
            % \boldsymbol\lambda^{\text{(a--c)}} \cdot 
            \blacksquare
        \\ + \lambda^{\text{(d)}}
        \inf_{\mathcal F}& \Ex_{\mat u \sim \nu(\U)}
            \Big[ 
            \kldiv[\big]{ \nu(\X \mid \U{=}\mat u) }{ \mathrm{Unif}[{\mathcal F(\mat u)}] }
            \Big],
\end{align*}
where $\blacksquare$ consists of everything but the infemum from \cref{eq:siminc},
%oli11: expanding to fully explain notation 
$\mathcal F$ ranges over sets of equations along $\Ar$ (as in \cref{defn:SEM,defn:esim-compat}), and $\mathrm{Unif}[{\mathcal F(\mat u)}]$ is the uniform distribution over joint settings of $\X$ that are consistent with the equations after fixing context $\U = \mat u$. 
Recall that the key step of constructing $\Ar^\dagger$ was to add the hyperarc $\U \to \X$. But for even compatbility, we want to effectively do the opposite---that is, subject to satisfying the other constraints, we want to incentivize, rather than penalize the conditional entropy $\H(\X|\,\U)$. This is made precise by the following proposition.


\begin{prop}
    % $\SDef_{\mu}$
    \begin{enumerate}[label={\normalfont(\alph*)},wide]
    \item For all $\boldsymbol{\lambda} > \mat 0$, $\mathrm{E}\mskip-1mu\SIMInc_{\Ar, \boldsymbol\lambda}(\mu) \ge 0$ with equality iff $\mu \emodels \Ar$.
    %oli11*: this isn't quite right. 
    \commentout{
    \item 
    % $\lim_{k \to \infty} \mathrm{E}\mskip-1mu\SIMInc_{\Ar, (\boldsymbol\lambda,k)}(\mu) \ge 0$
    $\mathrm{E}\mskip-1mu\SIMInc_{\Ar}(\mu) = \IDef_{\!\Ar}(\mu) + \kappa_{\Ar}(\mu)$,
    where $\kappa_{\Ar}(\mu)$ 
    %oli11: moving this below
    % is a (possibly infinite) piecewise constant function of $\mu$ with finitely many pieces.
    }
    \end{enumerate}
\end{prop}

In other words, $\IDef$ itself already measures distance from \emph{even \scibility}, once we find the appropriate constant to make it non-negative. 
Although has an enormous benefit of not requiring an infemum. 

\TODO[ TODO:  There's an issue here I still need to finish working out. ]
\[
    \IDef_{\!\Ar}(\mu)
    \le \SIMInc_{\Ar}(\mu)
    \le \IDef_{\!\Ar^\dagger}(\nu^*)
    \le \IDef_{\Ar}(\mu) + \kappa()
    .
\]

$\kappa_\Ar$ is a (possibly infinite) piecewise constant function of $\mu$ with finitely many pieces,
%oli11:
and finite when $\mu \models \Ar$. 

%oli15: even if this works out, there's not enough space. 
%oli14*: I'm not done here, and unsure exactly how this will work out. I get the sense that this construction might connect nicely to some things we said before and will say in the future (note the extra arcc \U ->> \X needed to make IDef turn into SIMInc, for instance). 
\subsection{Complete Derandomization for Cyclic Models}
    \label{ssec:full-derandomize}
We have seen that a number of properties of causal models are simpler when $\mathcal M \models \U \tto \X$. 
In some sense, the job of a causal model is model $\X$ by adding variables $\U$ that account for any uncertainty. 
When $\mathcal M \not\models \U \tto \X$, this job is in some sense incomplete; cycles can create a new source of uncertainty. 
In this subsection, we explore the effects of adding one more variable $U_0$ to account for the remaining uncertainty, by explaining it as randomenss.
Technically speaking, this means looking into one way of converting a 
    randomized PSEM $\cal M$ to one in which $\U \tto \X$.


Our construction is parameterized by a PSEM $\cal M$ and a choice of $\nu \in \SD{\mathcal M}$. In brief, we use a natural construction explained in the next subsection (\ref{sec:cpd-derandomize}) to obtain a ``maximally independent'' derandomization of $\nu(\X \mid \U)$.
The result is a new generalized randomized PSEM, which we call $\derind{\mathcal M}{\nu}$,
    differing from $\cal M$ in that it 
    has one extra \arc\ $\U \to \X$, and, correspondingly, an extra variable $U_0$, and an extra equation $f_0 : \U \to \X$.
This new causal model has two important properties: 
\begin{enumerate}
    \item 
    Only $\nu$ can arise from $\derind{\cal M}\nu$
    \hfill (i.e., $\SD{\derind{\cal M}\nu} = \{ \nu \}$), 
    and 
    \item 
    the exogenous variables determine the values endogenous ones
    \hfill(i.e.,
    $\derind{\cal M}\nu \models (\U,U_0) \tto \X$
    ). 
\end{enumerate}


\subsection*{Constructing the Causal Model $\derind{\cal M}{\nu}$}
We now apply the general technique above to obtain the causal model 
$\derind{\cal M}{\nu}$ discussed in \cref{ssec:full-derandomize}. 
%
% The idea is to add a new variable $U_0$ that accounts for any uncertainty in $\X$ given $\U$. 
Concretely, let 
$\V(U_0) := \prod_{\mat u \in \V(\U)} \mathcal F(\mat u)$
consist of all functions from $\V(\U)$ to $\V(\X)$ consistent with the equations $\mathcal F$,
and add an equation
$\X = U_0(\mat u) = f_{0}(U_0, \mat u)$
along the \arc\ $\U \to \X$.

Given $\nu \in \SD{\mathcal M}$,
define
$P(U_0 {=} f) := \prod_{\mat u \in \V(\U)} \nu(f(\mat u) \mid \mat u)$.
As shown more generally in \cref{sec:},

\begin{enumerate}
\item
 this indeed a probability distribution, and
 \item 
 the joint distribution $P(\U, U_0) = P(\U)P(U_0)$ 
 extends uniquely along the function defined by $U_0$, to a distribution $P(\U, U_0, \X)$, and the marginal of that distribution on ($\U, \X$) equals $\nu$.
\end{enumerate}
\commentout{% here's the latter proof; decided to just put the more general result in the appendix, because it's notationally simpler. 
Moreover, this distribution has marginal $P(\U, \X)$, because
\begin{align*}
    &\sum_{f \in \V(U_0)} P(\U, \X, U_0{=}f)
    \\&= 
    \sum_{f \in \V(U_0)} P(\U, f) \mathbbm1[\X = f(\U)]
    \\
    &= P(\U) \sum_{f \in \V(U_0)} \mathbbm1[\X = f(\U)] \prod_{\mat u \in \V(\U)} \nu(f(\mat u) \mid \mat u)
    \\
    &=  P(\U) \sum_{f \in \V(U_0)} \mathbbm1[\X = f(\U)] \nu(f(\U) \mid \U) \prod_{\mat u \ne \U} \nu(f(\mat u) \mid \mat u)
    \\
    &=  P(\U) \sum_{f \in \V(U_0)} \mathbbm1[\X = f(\U)] \nu(\X \mid \U) \prod_{\mat u \ne \U} \nu(f(\mat u) \mid \mat u)
    \\
    &=  P(\U) \nu(\X \mid \U) \sum_{f \in \V(U_0)} \mathbbm1[\X = f(\U)] \prod_{\mat u \ne \U} \nu(f(\mat u) \mid \mat u)
    \\
    &=  P(\U) \nu(\X \mid \U) \sum_{f \in \prod_{\mat u'}} \mathbbm1[\X = f(\U)] \prod_{\mat u \ne \U} \nu(f(\mat u) \mid \mat u)
    \\
    &= P(\U) \nu(\X \mid \U)
\end{align*}}%

We must also be careful about how to respect interventions. In the most general form, an intervention of the form $f_a \gets g$, for some function $g : \V(\Src a) \to \V(\Tgt a)$, not only modifies the equation $f_a$ by setting it equal to $g$, but also modifies the equation $f_0$ according to:
\[
    f_0 ( \mat u ) := f_0(\mat u)[ ]
\]

\TODO

For instance, if $\mathcal M$ is a PSEM, then to perform an intervention $\mat X {\gets} \mat x$ on the causal model $\derind {\cal M}\nu$,
we mean not only to replace the equation fo $f_X$, but also modify 
\[
    f_0^{\text{new}}(\mat u) := 
    f_0^{\text{old}}(\mat u)[ \mat X \mapsto \mat x]
\]
% At the level of such causal model, we define the intervened model


\TODO

\begin{prop}
    \begin{enumerate}
    \item $\Pr_{\derind{\mathcal M}{\nu}}(\varphi)  = 
     \Pr_{\cal M}(\varphi)$
        
    \end{enumerate}
\end{prop}
%
}%oli21: end \vfull.

\end{subappendices}
